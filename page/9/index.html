<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="bluemouse&#39;s blog">
<meta property="og:url" content="https://blueeemouse.github.io/page/9/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/page/9/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-Lifelong-ReID/Unsupervised%20Lifelong%20Person%20Re-identification%20%20via%20Contrastive%20Rehearsal/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-Lifelong-ReID/Unsupervised%20Lifelong%20Person%20Re-identification%20%20via%20Contrastive%20Rehearsal/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-17 22:22:45 / Modified: 23:22:13" itemprop="dateCreated datePublished" datetime="2025-02-17T22:22:45+08:00">2025-02-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="arxiv-22"><a href="#arxiv-22" class="headerlink" title="arxiv 22"></a>arxiv 22</h1><h1 id="这篇论文提出的设定倒是在现实生活里有意义，不过它的动机好像也是提的比较模糊，都是针对几个设定加起来的时候产生的问题"><a href="#这篇论文提出的设定倒是在现实生活里有意义，不过它的动机好像也是提的比较模糊，都是针对几个设定加起来的时候产生的问题" class="headerlink" title="这篇论文提出的设定倒是在现实生活里有意义，不过它的动机好像也是提的比较模糊，都是针对几个设定加起来的时候产生的问题"></a>这篇论文提出的设定倒是在现实生活里有意义，不过它的动机好像也是提的比较模糊，都是针对几个设定加起来的时候产生的问题</h1><h1 id="它讲的故事：However-a-real-world-video-monitoring-system-can-record-new-data-every-day-and-from-new-locations-when-new-cameras-are-added-into-an-existing-system-；When-new-data-is-recorded-every-day-people-have-to-annotate-new-data-manually-before-deployment-which-is-cumbersome-and-time-consuming-（这里在说明，无监督方法研究的必要性）Towards-a-generalizable-ReID-model-lifelong-person-ReID-has-been-recently-proposed（这里在说明，lifelong-learning研究的必要性）"><a href="#它讲的故事：However-a-real-world-video-monitoring-system-can-record-new-data-every-day-and-from-new-locations-when-new-cameras-are-added-into-an-existing-system-；When-new-data-is-recorded-every-day-people-have-to-annotate-new-data-manually-before-deployment-which-is-cumbersome-and-time-consuming-（这里在说明，无监督方法研究的必要性）Towards-a-generalizable-ReID-model-lifelong-person-ReID-has-been-recently-proposed（这里在说明，lifelong-learning研究的必要性）" class="headerlink" title="它讲的故事：However, a real-world video monitoring system can record new data every day and from new locations, when new cameras are added into an existing system.；When new data is recorded every day, people have to annotate new data manually before deployment, which is cumbersome and time-consuming.（这里在说明，无监督方法研究的必要性）Towards a generalizable ReID model, lifelong person ReID has been recently proposed（这里在说明，lifelong learning研究的必要性）"></a>它讲的故事：However, a real-world video monitoring system can record new data every day and from new locations, when new cameras are added into an existing system.；When new data is recorded every day, people have to annotate new data manually before deployment, which is cumbersome and time-consuming.（这里在说明，无监督方法研究的必要性）<br>Towards a generalizable ReID model, lifelong person ReID has been recently proposed（这里在说明，lifelong learning研究的必要性）</h1><h1 id="它的方法，设计的倒也算简单，就是分别针对灾难性遗忘、学习新数据的问题"><a href="#它的方法，设计的倒也算简单，就是分别针对灾难性遗忘、学习新数据的问题" class="headerlink" title="它的方法，设计的倒也算简单，就是分别针对灾难性遗忘、学习新数据的问题"></a>它的方法，设计的倒也算简单，就是分别针对灾难性遗忘、学习新数据的问题</h1><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="1-Current-domain-contrastive-baseline"><a href="#1-Current-domain-contrastive-baseline" class="headerlink" title="1.Current domain contrastive baseline"></a>1.Current domain contrastive baseline</h2><h3 id="它主要是进行经典的拉近样本和簇中心的距离（如今已经是一个非常常见的操作了），还有一个-L-cam-，就是对同一个簇里，属于同一个相机的数据，也求一个prototype（就是这些数据的均值），并要拉近样本与这个prototype的距离（还是对比损失的形式）"><a href="#它主要是进行经典的拉近样本和簇中心的距离（如今已经是一个非常常见的操作了），还有一个-L-cam-，就是对同一个簇里，属于同一个相机的数据，也求一个prototype（就是这些数据的均值），并要拉近样本与这个prototype的距离（还是对比损失的形式）" class="headerlink" title="它主要是进行经典的拉近样本和簇中心的距离（如今已经是一个非常常见的操作了），还有一个$L_{cam}$，就是对同一个簇里，属于同一个相机的数据，也求一个prototype（就是这些数据的均值），并要拉近样本与这个prototype的距离（还是对比损失的形式）"></a>它主要是进行经典的拉近样本和簇中心的距离（如今已经是一个非常常见的操作了），还有一个$L_{cam}$，就是对同一个簇里，属于同一个相机的数据，也求一个prototype（就是这些数据的均值），并要拉近样本与这个prototype的距离（还是对比损失的形式）</h3><h4 id="不过，论文里还提到，-L-cam-过于依赖相机标签，局限在了ReID任务里了；如果想更加泛化，其实也有其它的损失可以用，比如在一个batch里，与batch里的hardest-positives进行对比"><a href="#不过，论文里还提到，-L-cam-过于依赖相机标签，局限在了ReID任务里了；如果想更加泛化，其实也有其它的损失可以用，比如在一个batch里，与batch里的hardest-positives进行对比" class="headerlink" title="不过，论文里还提到，$L_{cam}$过于依赖相机标签，局限在了ReID任务里了；如果想更加泛化，其实也有其它的损失可以用，比如在一个batch里，与batch里的hardest positives进行对比"></a>不过，论文里还提到，$L_{cam}$过于依赖相机标签，局限在了ReID任务里了；如果想更加泛化，其实也有其它的损失可以用，比如在一个batch里，与batch里的hardest positives进行对比</h4><h4 id="（另，注意，这个方法会有online-encoder和momentum-encoder；在这一步里，聚类用的特征是momentum-encoder得到的，而不是online-encoder）"><a href="#（另，注意，这个方法会有online-encoder和momentum-encoder；在这一步里，聚类用的特征是momentum-encoder得到的，而不是online-encoder）" class="headerlink" title="（另，注意，这个方法会有online encoder和momentum encoder；在这一步里，聚类用的特征是momentum encoder得到的，而不是online encoder）"></a>（另，注意，这个方法会有online encoder和momentum encoder；在这一步里，聚类用的特征是momentum encoder得到的，而不是online encoder）</h4><h2 id="2-Old-domain-contrastive-rehearsal"><a href="#2-Old-domain-contrastive-rehearsal" class="headerlink" title="2.Old domain contrastive rehearsal"></a>2.Old domain contrastive rehearsal</h2><h3 id="这个方法里会存储一些旧数据集的prototype（也就是我们得到的簇中心的特征），以及各个簇的若干个可靠样本（就是比较接近簇中心的）。这一步的核心思想是，给定一个已知伪标签的旧数据样本，我们希望当前的online-encoder对其编码后，得到的特征应该还是尽可能接近原来它所属的那个簇（由此我们认为online-encoder对旧数据集知识的保存做的比较好）"><a href="#这个方法里会存储一些旧数据集的prototype（也就是我们得到的簇中心的特征），以及各个簇的若干个可靠样本（就是比较接近簇中心的）。这一步的核心思想是，给定一个已知伪标签的旧数据样本，我们希望当前的online-encoder对其编码后，得到的特征应该还是尽可能接近原来它所属的那个簇（由此我们认为online-encoder对旧数据集知识的保存做的比较好）" class="headerlink" title="这个方法里会存储一些旧数据集的prototype（也就是我们得到的簇中心的特征），以及各个簇的若干个可靠样本（就是比较接近簇中心的）。这一步的核心思想是，给定一个已知伪标签的旧数据样本，我们希望当前的online encoder对其编码后，得到的特征应该还是尽可能接近原来它所属的那个簇（由此我们认为online encoder对旧数据集知识的保存做的比较好）"></a>这个方法里会存储一些旧数据集的prototype（也就是我们得到的簇中心的特征），以及各个簇的若干个可靠样本（就是比较接近簇中心的）。这一步的核心思想是，给定一个已知伪标签的旧数据样本，我们希望当前的online encoder对其编码后，得到的特征应该还是尽可能接近原来它所属的那个簇（由此我们认为online encoder对旧数据集知识的保存做的比较好）</h3><h2 id="3-Image-to-Image-Similarity-Constraint"><a href="#3-Image-to-Image-Similarity-Constraint" class="headerlink" title="3.Image-to-Image Similarity Constraint"></a>3.Image-to-Image Similarity Constraint</h2><h3 id="这一步的作用是抗遗忘，然后它的核心思想是，给定一个batch，我们用encoder得到特征后，可以得到一个样本之间的相似性矩阵。如果新模型得到的相似性矩阵与旧模型得到的相似性矩阵很接近（对于同一个batch而言），那么我们就认为这个新模型保留旧知识时，保留得比较好。论文里，新模型就对应online-encoder，旧模型就对应momentum-encoder（它保留了比较多的旧知识）。衡量两个相似性矩阵的距离时，用的是KL散度"><a href="#这一步的作用是抗遗忘，然后它的核心思想是，给定一个batch，我们用encoder得到特征后，可以得到一个样本之间的相似性矩阵。如果新模型得到的相似性矩阵与旧模型得到的相似性矩阵很接近（对于同一个batch而言），那么我们就认为这个新模型保留旧知识时，保留得比较好。论文里，新模型就对应online-encoder，旧模型就对应momentum-encoder（它保留了比较多的旧知识）。衡量两个相似性矩阵的距离时，用的是KL散度" class="headerlink" title="这一步的作用是抗遗忘，然后它的核心思想是，给定一个batch，我们用encoder得到特征后，可以得到一个样本之间的相似性矩阵。如果新模型得到的相似性矩阵与旧模型得到的相似性矩阵很接近（对于同一个batch而言），那么我们就认为这个新模型保留旧知识时，保留得比较好。论文里，新模型就对应online encoder，旧模型就对应momentum encoder（它保留了比较多的旧知识）。衡量两个相似性矩阵的距离时，用的是KL散度"></a>这一步的作用是抗遗忘，然后它的核心思想是，给定一个batch，我们用encoder得到特征后，可以得到一个样本之间的相似性矩阵。如果新模型得到的相似性矩阵与旧模型得到的相似性矩阵很接近（对于同一个batch而言），那么我们就认为这个新模型保留旧知识时，保留得比较好。论文里，新模型就对应online encoder，旧模型就对应momentum encoder（它保留了比较多的旧知识）。衡量两个相似性矩阵的距离时，用的是KL散度</h3><h3 id="但此处有个小改进，就是，它的目标矩阵确实是用旧模型得到的，但它的新的相似性矩阵，计算的时候，求行特征用的是新模型，求列特征用的是旧模型。论文里说这样的效果比单纯用新模型会好。确实有一些道理吧，因为求列特征用的是旧模型，那么，要迫使两个相似性矩阵尽可能接近，就要求新模型的输出要尽可能接近旧模型的输出（无形中其实就进行了蒸馏了吧）"><a href="#但此处有个小改进，就是，它的目标矩阵确实是用旧模型得到的，但它的新的相似性矩阵，计算的时候，求行特征用的是新模型，求列特征用的是旧模型。论文里说这样的效果比单纯用新模型会好。确实有一些道理吧，因为求列特征用的是旧模型，那么，要迫使两个相似性矩阵尽可能接近，就要求新模型的输出要尽可能接近旧模型的输出（无形中其实就进行了蒸馏了吧）" class="headerlink" title="但此处有个小改进，就是，它的目标矩阵确实是用旧模型得到的，但它的新的相似性矩阵，计算的时候，求行特征用的是新模型，求列特征用的是旧模型。论文里说这样的效果比单纯用新模型会好。确实有一些道理吧，因为求列特征用的是旧模型，那么，要迫使两个相似性矩阵尽可能接近，就要求新模型的输出要尽可能接近旧模型的输出（无形中其实就进行了蒸馏了吧）"></a>但此处有个小改进，就是，它的目标矩阵确实是用旧模型得到的，但它的新的相似性矩阵，计算的时候，求行特征用的是新模型，求列特征用的是旧模型。论文里说这样的效果比单纯用新模型会好。确实有一些道理吧，因为求列特征用的是旧模型，那么，要迫使两个相似性矩阵尽可能接近，就要求新模型的输出要尽可能接近旧模型的输出（无形中其实就进行了蒸馏了吧）</h3><h1 id="trick总结"><a href="#trick总结" class="headerlink" title="trick总结"></a>trick总结</h1><h2 id="1-在进行对比的时候，既可以和所有的prototype来对比，也可以挑出若干个hard-sample，保证效果的同时减少一些计算量"><a href="#1-在进行对比的时候，既可以和所有的prototype来对比，也可以挑出若干个hard-sample，保证效果的同时减少一些计算量" class="headerlink" title="1.在进行对比的时候，既可以和所有的prototype来对比，也可以挑出若干个hard sample，保证效果的同时减少一些计算量"></a>1.在进行对比的时候，既可以和所有的prototype来对比，也可以挑出若干个hard sample，保证效果的同时减少一些计算量</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/Robust%20Pseudo-label%20Learning%20with%20Neighbor%20Relation%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/Robust%20Pseudo-label%20Learning%20with%20Neighbor%20Relation%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-16 11:51:35 / Modified: 21:47:48" itemprop="dateCreated datePublished" datetime="2025-02-16T11:51:35+08:00">2025-02-16</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="acm-mm24"><a href="#acm-mm24" class="headerlink" title="acm mm24"></a>acm mm24</h1><h1 id="它也是要解决伪标签带噪的问题。提出一个Noisy-Pseudo-label-Calibration-module来校正伪标签；提出一个Nieghbor-Relation-Learning-module来获取模态内更加一致的特征；提出一个Optimal-Transport-Prototype-Matching-module来进行跨模态的匹配，建立跨模态对应关系；提出一个Memory-Hybrid-Leraning-module来更好提取出modality-invariant和modality-specific-feature"><a href="#它也是要解决伪标签带噪的问题。提出一个Noisy-Pseudo-label-Calibration-module来校正伪标签；提出一个Nieghbor-Relation-Learning-module来获取模态内更加一致的特征；提出一个Optimal-Transport-Prototype-Matching-module来进行跨模态的匹配，建立跨模态对应关系；提出一个Memory-Hybrid-Leraning-module来更好提取出modality-invariant和modality-specific-feature" class="headerlink" title="它也是要解决伪标签带噪的问题。提出一个Noisy Pseudo-label Calibration module来校正伪标签；提出一个Nieghbor Relation Learning module来获取模态内更加一致的特征；提出一个Optimal Transport Prototype Matching module来进行跨模态的匹配，建立跨模态对应关系；提出一个Memory Hybrid Leraning module来更好提取出modality-invariant和modality-specific feature"></a>它也是要解决伪标签带噪的问题。提出一个Noisy Pseudo-label Calibration module来<em><strong>校正伪标签</strong></em>；提出一个Nieghbor Relation Learning module来获取模态内更加一致的特征；提出一个Optimal Transport Prototype Matching module来进行跨模态的匹配，建立跨模态对应关系；提出一个Memory Hybrid Leraning module来更好提取出modality-invariant和modality-specific feature</h1><h1 id="insight"><a href="#insight" class="headerlink" title="insight"></a>insight</h1><h1 id="动机：无监督vi-reid中，伪标签带噪声是很常见的问题。而论文提出，一般的方法聚焦于减少带噪标签的影响，但忽略了去校正带噪标签（其实这么说好像也不是很严谨，那些标签平滑的操作，我觉得某种程度上也是在校正标签的）"><a href="#动机：无监督vi-reid中，伪标签带噪声是很常见的问题。而论文提出，一般的方法聚焦于减少带噪标签的影响，但忽略了去校正带噪标签（其实这么说好像也不是很严谨，那些标签平滑的操作，我觉得某种程度上也是在校正标签的）" class="headerlink" title="动机：无监督vi-reid中，伪标签带噪声是很常见的问题。而论文提出，一般的方法聚焦于减少带噪标签的影响，但忽略了去校正带噪标签（其实这么说好像也不是很严谨，那些标签平滑的操作，我觉得某种程度上也是在校正标签的）"></a>动机：无监督vi-reid中，伪标签带噪声是很常见的问题。而论文提出，一般的方法聚焦于减少带噪标签的影响，但忽略了去校正带噪标签（其实这么说好像也不是很严谨，那些标签平滑的操作，我觉得某种程度上也是在校正标签的）</h1><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="1-Noisy-Pseudo-label-Calibration（NPC），这个模块就是很显然的，回应了动机里它想要校正标签的想法。它评价两个样本相似度的方法是用邻居的相似度（或者说重合度），而不是用样本特征本身（这种间接衡量相似性的方法，和-GUR论文-Towards-Grand-Unified-Representation-Learning-for-Unsupervised-Visible-Infrared-Person-Re-Identification-里的操作，思想上是比较相近的，只不过那里用的是概率分布要尽可能相似）而它校正标签的方法，则是，先提取出每个簇内的“可靠”的样本（“可靠”的判断方法中，核心思想是，一个簇内，如果有很多样本和当前样本非常相似，那么当前样本应该就是比较”可靠“的），然后簇中心就初始化为这些“可靠”样本的均值；之后，每个样本再和这些新的簇中心进行比较，分配到最相似的那个簇中心那里"><a href="#1-Noisy-Pseudo-label-Calibration（NPC），这个模块就是很显然的，回应了动机里它想要校正标签的想法。它评价两个样本相似度的方法是用邻居的相似度（或者说重合度），而不是用样本特征本身（这种间接衡量相似性的方法，和-GUR论文-Towards-Grand-Unified-Representation-Learning-for-Unsupervised-Visible-Infrared-Person-Re-Identification-里的操作，思想上是比较相近的，只不过那里用的是概率分布要尽可能相似）而它校正标签的方法，则是，先提取出每个簇内的“可靠”的样本（“可靠”的判断方法中，核心思想是，一个簇内，如果有很多样本和当前样本非常相似，那么当前样本应该就是比较”可靠“的），然后簇中心就初始化为这些“可靠”样本的均值；之后，每个样本再和这些新的簇中心进行比较，分配到最相似的那个簇中心那里" class="headerlink" title="1.Noisy Pseudo-label Calibration（NPC），这个模块就是很显然的，回应了动机里它想要校正标签的想法。它评价两个样本相似度的方法是用邻居的相似度（或者说重合度），而不是用样本特征本身（这种间接衡量相似性的方法，和(GUR论文[[Towards Grand Unified Representation Learning for Unsupervised Visible-Infrared Person Re-Identification]])里的操作，思想上是比较相近的，只不过那里用的是概率分布要尽可能相似）而它校正标签的方法，则是，先提取出每个簇内的“可靠”的样本（“可靠”的判断方法中，核心思想是，一个簇内，如果有很多样本和当前样本非常相似，那么当前样本应该就是比较”可靠“的），然后簇中心就初始化为这些“可靠”样本的均值；之后，每个样本再和这些新的簇中心进行比较，分配到最相似的那个簇中心那里"></a>1.Noisy Pseudo-label Calibration（NPC），这个模块就是很显然的，回应了动机里它想要校正标签的想法。它评价两个样本相似度的方法是用邻居的相似度（或者说重合度），而不是用样本特征本身（这种间接衡量相似性的方法，和(GUR论文[[Towards Grand Unified Representation Learning for Unsupervised Visible-Infrared Person Re-Identification]])里的操作，思想上是比较相近的，只不过那里用的是概率分布要尽可能相似）<br>而它校正标签的方法，则是，先提取出每个簇内的“可靠”的样本（“可靠”的判断方法中，核心思想是，一个簇内，如果有很多样本和当前样本非常相似，那么当前样本应该就是比较”可靠“的），然后簇中心就初始化为这些“可靠”样本的均值；之后，每个样本再和这些新的簇中心进行比较，分配到最相似的那个簇中心那里</h2><h2 id="2-Neighbor-Relation-Learning（NRL），原文里说是为了reduce-high-intra-class-variations，我们可以认为，是为了提取更好的特征，以便更好地进行模态内的聚类。所以它应该主要聚焦在挖掘模态内的信息。但它的损失，没太搞懂。。"><a href="#2-Neighbor-Relation-Learning（NRL），原文里说是为了reduce-high-intra-class-variations，我们可以认为，是为了提取更好的特征，以便更好地进行模态内的聚类。所以它应该主要聚焦在挖掘模态内的信息。但它的损失，没太搞懂。。" class="headerlink" title="2.Neighbor Relation Learning（NRL），原文里说是为了reduce high intra-class variations，我们可以认为，是为了提取更好的特征，以便更好地进行模态内的聚类。所以它应该主要聚焦在挖掘模态内的信息。但它的损失，没太搞懂。。"></a>2.Neighbor Relation Learning（NRL），原文里说是为了reduce high intra-class variations，我们可以认为，是为了提取更好的特征，以便更好地进行模态内的聚类。所以它应该主要聚焦在挖掘模态内的信息。但它的损失，没太搞懂。。</h2><h2 id="Optimal-Transport-Prototype-Matching（OTPA），聚焦于挖掘跨模态对应的信息"><a href="#Optimal-Transport-Prototype-Matching（OTPA），聚焦于挖掘跨模态对应的信息" class="headerlink" title="Optimal Transport Prototype Matching（OTPA），聚焦于挖掘跨模态对应的信息"></a>Optimal Transport Prototype Matching（OTPA），聚焦于挖掘跨模态对应的信息</h2><h2 id="Memory-Hybrid-Learning，聚焦于学习modality-invariant-feature。具体来说，它在经典的模态内的ClusterNCE-loss的基础上，还构建了一个modality-hybrid-memory（这个memory的特征是由红外cluster和对应的可见光cluster加权求和得到），然后交替地拉近红外实例、可见光实例与这个混合memory中的对应cluster之间的距离"><a href="#Memory-Hybrid-Learning，聚焦于学习modality-invariant-feature。具体来说，它在经典的模态内的ClusterNCE-loss的基础上，还构建了一个modality-hybrid-memory（这个memory的特征是由红外cluster和对应的可见光cluster加权求和得到），然后交替地拉近红外实例、可见光实例与这个混合memory中的对应cluster之间的距离" class="headerlink" title="Memory Hybrid Learning，聚焦于学习modality-invariant feature。具体来说，它在经典的模态内的ClusterNCE loss的基础上，还构建了一个modality-hybrid memory（这个memory的特征是由红外cluster和对应的可见光cluster加权求和得到），然后交替地拉近红外实例、可见光实例与这个混合memory中的对应cluster之间的距离"></a>Memory Hybrid Learning，聚焦于学习modality-invariant feature。具体来说，它在经典的模态内的ClusterNCE loss的基础上，还构建了一个modality-hybrid memory（这个memory的特征是由红外cluster和对应的可见光cluster加权求和得到），然后交替地拉近红外实例、可见光实例与这个混合memory中的对应cluster之间的距离</h2><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-Lifelong-VI-ReID/%E9%9A%BE%E7%82%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-Lifelong-VI-ReID/%E9%9A%BE%E7%82%B9/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-16 10:28:47" itemprop="dateCreated datePublished" datetime="2025-02-16T10:28:47+08:00">2025-02-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-18 15:34:16" itemprop="dateModified" datetime="2025-02-18T15:34:16+08:00">2025-02-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>首先，无监督条件下的跨模态对应依然是难点。而加入了lifelong条件之后，显然会有lifelong的常见问题，就是灾难性遗忘。并且，<br>在新数据集上，因为分布不同，所以之前学到的跨模态对应知识，以及单模态内的知识可能都不适用，而这会导致在学习新数据集的时候，初期的特征不够好，进而会导致聚类得到的伪标签也不够好，后续对应也很难对好，故训练完得到的模型性能可能也不够好&#x2F;在新数据集上，反而是有一些先验知识会比较好，有利于度过初期训练？（如果是会产生负面影响，那么标签校正应该是非常有必要的了）</p>
<p>解决方法上，提出一种比较有效的标签置信度求解方法，来帮助过度前期训练，或许是一个解决方法（比如，对每个实例，都找出和它距离前k近的实例，把这些邻居的标签加权求和，看看求和得到的标签和它的簇标签之间的距离远近？）<br>或许修正标签是更加直接的方法（修正，而不是直接把带噪声的标签给去除掉）</p>
<p>尝试：一个momentum encoder，一个online encoder，然后在新数据集上，用online encoder先学一下并聚类（可以适当加载一些momentum encoder，以及采取warmup），之后再慢慢地过渡到正常训练</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-16 00:07:08 / Modified: 15:04:15" itemprop="dateCreated datePublished" datetime="2025-02-16T00:07:08+08:00">2025-02-16</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="无监督vi-reid，大多数的流程都是，先通过某种聚类方法（通常是DBSCAN），得到一批伪标签，然后开始构建跨模态的对应（一般是对应簇，而不是一个一个实例去对应。因为对应完簇，我们就认为这两个簇里的实例都是一个ID的）。所以其实可以说，一个常见的范式是，在各个模态内进行聚类，然后进行跨模态的簇的对应（这么说来，ECCV24那篇，有没有可能提出了一个新的范式，就是在聚类的时候就已经考虑上跨模态的对应了？）"><a href="#无监督vi-reid，大多数的流程都是，先通过某种聚类方法（通常是DBSCAN），得到一批伪标签，然后开始构建跨模态的对应（一般是对应簇，而不是一个一个实例去对应。因为对应完簇，我们就认为这两个簇里的实例都是一个ID的）。所以其实可以说，一个常见的范式是，在各个模态内进行聚类，然后进行跨模态的簇的对应（这么说来，ECCV24那篇，有没有可能提出了一个新的范式，就是在聚类的时候就已经考虑上跨模态的对应了？）" class="headerlink" title="无监督vi-reid，大多数的流程都是，先通过某种聚类方法（通常是DBSCAN），得到一批伪标签，然后开始构建跨模态的对应（一般是对应簇，而不是一个一个实例去对应。因为对应完簇，我们就认为这两个簇里的实例都是一个ID的）。所以其实可以说，一个常见的范式是，在各个模态内进行聚类，然后进行跨模态的簇的对应（这么说来，ECCV24那篇，有没有可能提出了一个新的范式，就是在聚类的时候就已经考虑上跨模态的对应了？）"></a>无监督vi-reid，大多数的流程都是，先通过某种聚类方法（通常是DBSCAN），得到一批伪标签，然后开始构建跨模态的对应（一般是对应簇，而不是一个一个实例去对应。因为对应完簇，我们就认为这两个簇里的实例都是一个ID的）。所以其实可以说，一个常见的范式是，在各个模态内进行聚类，然后进行跨模态的簇的对应（这么说来，ECCV24那篇，有没有可能提出了一个新的范式，就是在聚类的时候就已经考虑上跨模态的对应了？）</h1><h1 id="通常的改进会聚焦在，"><a href="#通常的改进会聚焦在，" class="headerlink" title="通常的改进会聚焦在，"></a>通常的改进会聚焦在，</h1><h2 id="如何更好的构建伪标签的跨模态对应（PGM）（OTPM）"><a href="#如何更好的构建伪标签的跨模态对应（PGM）（OTPM）" class="headerlink" title="如何更好的构建伪标签的跨模态对应（PGM）（OTPM）"></a>如何更好的构建伪标签的跨模态对应（PGM）（OTPM）</h2><h3 id="（毕竟没标签，很难对应上；最理想的情况当然是我们的聚类结果恰好就等于真实标签，此时无监督vi-reid就会变成vi-reid）（实际还有个很重要的问题，就是，我们的对应很可能是有错的，怎么在有错误对应（即所谓噪声）的情况下不断改进对应结果也是很重要的。所以才会有很多progressive的方法）；"><a href="#（毕竟没标签，很难对应上；最理想的情况当然是我们的聚类结果恰好就等于真实标签，此时无监督vi-reid就会变成vi-reid）（实际还有个很重要的问题，就是，我们的对应很可能是有错的，怎么在有错误对应（即所谓噪声）的情况下不断改进对应结果也是很重要的。所以才会有很多progressive的方法）；" class="headerlink" title="（毕竟没标签，很难对应上；最理想的情况当然是我们的聚类结果恰好就等于真实标签，此时无监督vi-reid就会变成vi-reid）（实际还有个很重要的问题，就是，我们的对应很可能是有错的，怎么在有错误对应（即所谓噪声）的情况下不断改进对应结果也是很重要的。所以才会有很多progressive的方法）；"></a>（毕竟没标签，很难对应上；最理想的情况当然是我们的聚类结果恰好就等于真实标签，此时无监督vi-reid就会变成vi-reid）（实际还有个很重要的问题，就是，我们的对应很可能是有错的，怎么在有错误对应（即所谓噪声）的情况下不断改进对应结果也是很重要的。所以才会有很多progressive的方法）；</h3><h2 id="还有一部分可能会聚焦在伪标签的可靠性上？（Multi-memory，PGM）"><a href="#还有一部分可能会聚焦在伪标签的可靠性上？（Multi-memory，PGM）" class="headerlink" title="还有一部分可能会聚焦在伪标签的可靠性上？（Multi-memory，PGM）"></a>还有一部分可能会聚焦在伪标签的可靠性上？（Multi-memory，PGM）</h2><h2 id="提取更加鲁棒的特征，以改善聚类效果（Shallow-Deep，GUR）"><a href="#提取更加鲁棒的特征，以改善聚类效果（Shallow-Deep，GUR）" class="headerlink" title="提取更加鲁棒的特征，以改善聚类效果（Shallow-Deep，GUR）"></a>提取更加鲁棒的特征，以改善聚类效果（Shallow-Deep，GUR）</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94%20unsupervised%20vi-reid/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94%20unsupervised%20vi-reid/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-14 09:55:45" itemprop="dateCreated datePublished" datetime="2025-02-14T09:55:45+08:00">2025-02-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-19 17:08:31" itemprop="dateModified" datetime="2025-02-19T17:08:31+08:00">2025-02-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="cvpr24"><a href="#cvpr24" class="headerlink" title="cvpr24"></a>cvpr24</h1><h2 id="Shallow-Deep-Collaborative-Learning-for-Unsupervised-Visible-Infrared-Person-Re-Identification"><a href="#Shallow-Deep-Collaborative-Learning-for-Unsupervised-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Shallow-Deep Collaborative Learning for Unsupervised Visible-Infrared Person Re-Identification"></a>Shallow-Deep Collaborative Learning for Unsupervised Visible-Infrared Person Re-Identification</h2><h1 id="nips24有一篇，iclr和icml24没有"><a href="#nips24有一篇，iclr和icml24没有" class="headerlink" title="nips24有一篇，iclr和icml24没有"></a>nips24有一篇，iclr和icml24没有</h1><h1 id="eccv24"><a href="#eccv24" class="headerlink" title="eccv24"></a>eccv24</h1><h2 id="Multi-Memory-Matching-for-Unsupervised-Visible-Infrared-Person-Re-Identification"><a href="#Multi-Memory-Matching-for-Unsupervised-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Multi-Memory Matching for Unsupervised Visible-Infrared Person Re-Identification"></a><a target="_blank" rel="noopener" href="https://eccv.ecva.net/virtual/2024/poster/1841">Multi-Memory Matching for Unsupervised Visible-Infrared Person Re-Identification</a></h2><h1 id="aaai24-ijcai24无"><a href="#aaai24-ijcai24无" class="headerlink" title="aaai24&amp;ijcai24无"></a>aaai24&amp;ijcai24无</h1><h1 id="cvpr23"><a href="#cvpr23" class="headerlink" title="cvpr23"></a>cvpr23</h1><h2 id="Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning"><a href="#Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning" class="headerlink" title="Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning"></a><a target="_blank" rel="noopener" href="https://github.com/zesenwu23/USL-VI-ReID">Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning</a></h2><h1 id="iccv23"><a href="#iccv23" class="headerlink" title="iccv23"></a>iccv23</h1><h2 id="Towards-Grand-Unified-Representation-Learning-for-Unsupervised-Visible-Infrared-Person-Re-Identification（无监督）"><a href="#Towards-Grand-Unified-Representation-Learning-for-Unsupervised-Visible-Infrared-Person-Re-Identification（无监督）" class="headerlink" title="Towards Grand Unified Representation Learning for Unsupervised Visible-Infrared Person Re-Identification（无监督）"></a><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Towards_Grand_Unified_Representation_Learning_for_Unsupervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.html">Towards Grand Unified Representation Learning for Unsupervised Visible-Infrared Person Re-Identification</a>（无监督）</h2><h1 id="acm-mm24"><a href="#acm-mm24" class="headerlink" title="acm mm24"></a>acm mm24</h1><h2 id="Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification（无监督的）"><a href="#Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification（无监督的）" class="headerlink" title="Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification（无监督的）"></a><strong>Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification</strong>（无监督的）</h2><h2 id="Enhancing-Unsupervised-Visible-Infrared-Person-Re-Identification-with-Bidirectional-Consistency-Gradual-Matching（无监督的）"><a href="#Enhancing-Unsupervised-Visible-Infrared-Person-Re-Identification-with-Bidirectional-Consistency-Gradual-Matching（无监督的）" class="headerlink" title="Enhancing Unsupervised Visible-Infrared Person Re-Identification with Bidirectional-Consistency Gradual Matching（无监督的）"></a><strong>Enhancing Unsupervised Visible-Infrared Person Re-Identification with Bidirectional-Consistency Gradual Matching</strong>（无监督的）</h2><h1 id="acm-mm23"><a href="#acm-mm23" class="headerlink" title="acm mm23"></a>acm mm23</h1><h2 id="Efficient-Bilateral-Cross-Modality-Cluster-Matching-for-Unsupervised-Visible-Infrared-Person-ReID（无监督）"><a href="#Efficient-Bilateral-Cross-Modality-Cluster-Matching-for-Unsupervised-Visible-Infrared-Person-ReID（无监督）" class="headerlink" title="Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised Visible-Infrared Person ReID（无监督）"></a>Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised Visible-Infrared Person ReID（无监督）</h2><h2 id="Unveiling-the-Power-of-CLIP-in-Unsupervised-Visible-Infrared-Person-Re-Identification（无监督）"><a href="#Unveiling-the-Power-of-CLIP-in-Unsupervised-Visible-Infrared-Person-Re-Identification（无监督）" class="headerlink" title="Unveiling the Power of CLIP in Unsupervised Visible-Infrared Person Re-Identification（无监督）"></a>Unveiling the Power of CLIP in Unsupervised Visible-Infrared Person Re-Identification（无监督）</h2><h2 id="Unsupervised-Visible-Infrared-Person-ReID-by-Collaborative-Learning-with-Neighbor-Guided-Label-Refinement（无监督）"><a href="#Unsupervised-Visible-Infrared-Person-ReID-by-Collaborative-Learning-with-Neighbor-Guided-Label-Refinement（无监督）" class="headerlink" title="Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement（无监督）"></a>Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement（无监督）</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/09/python%E7%9B%B8%E5%85%B3bugs%E6%88%96%E7%9F%A5%E8%AF%86/pytorch/torch.nn%E4%B8%8Etorch.nn.functional%E7%9A%84%E5%85%B3%E7%B3%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/09/python%E7%9B%B8%E5%85%B3bugs%E6%88%96%E7%9F%A5%E8%AF%86/pytorch/torch.nn%E4%B8%8Etorch.nn.functional%E7%9A%84%E5%85%B3%E7%B3%BB/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-09 01:27:48 / Modified: 01:36:25" itemprop="dateCreated datePublished" datetime="2025-02-09T01:27:48+08:00">2025-02-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>有些东西是既可以用torch.nn.functional来调用，也可以用torch.nn来调用，不禁让人疑惑：这不是多此一举吗？其实它们都是有必要的<br>结论上来说，写神经网络的时候，一般都是统一用torch.nn来调用（即使像relu这种没有参数的激活函数，用torch.nn.functional调用，虽然效果差不太多，但是为了统一，用torch.nn调用也许是更好的），因为此时调用的是类，它的<br>参数定义（用torch.nn调用的话，它们都是继承了torch.nn.Module类，所以一旦初始化，就已经有参数了；而用torch.nn.functional调用，则需要自己定义参数，并且在调用的时候手动传入，相比之下复杂许多）<br>参数管理（比如，用类的时候就可以结合优化器，自动更新参数；而如果是用torch.nn.functional来调用，不仅参数需要我们自己传入，还需要我们自己指定是否要求梯度，并且即使我们指定了要求梯度，优化器也不能实现自动更新参数，必须得我们手动更细，极其麻烦）<br>设备控制（就是控制在cpu还是gpu&#x2F;哪块gpu）<br>模型参数保存（torch.nn类保存参数可以用torch.save(model.state_dict(), ‘model.pth’)这种语句很快捷地保存，而torch.nn.functional的话，就需要手动保存和加载参数）<br>等方面都要方便很多</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%B8%80%E8%AE%B2%20%20%E5%BF%AB%E6%8E%92&%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F&%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%B8%80%E8%AE%B2%20%20%E5%BF%AB%E6%8E%92&%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F&%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" class="post-title-link" itemprop="url">chap1.第一讲 快排&归并排序&二分查找</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-04 16:00:00" itemprop="dateCreated datePublished" datetime="2025-02-04T16:00:00+08:00">2025-02-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-13 21:15:25" itemprop="dateModified" datetime="2025-04-13T21:15:25+08:00">2025-04-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algo/" itemprop="url" rel="index"><span itemprop="name">algo</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-快速排序"><a href="#1-快速排序" class="headerlink" title="1.快速排序"></a>1.快速排序</h1><h2 id="1-1-核心思想：分治思想"><a href="#1-1-核心思想：分治思想" class="headerlink" title="1.1.核心思想：分治思想"></a>1.1.核心思想：分治思想</h2><h2 id="1-2-步骤："><a href="#1-2-步骤：" class="headerlink" title="1.2.步骤："></a>1.2.步骤：</h2><h3 id="1-2-1-对于一个给定的数组，选定一个中间值x（理论上来说，选哪个都行，但后面的代码需要注意相应的搭配，否则会陷入死循环。后面具体讲），常见的就是选数组的左边-右边-中间值-随机选"><a href="#1-2-1-对于一个给定的数组，选定一个中间值x（理论上来说，选哪个都行，但后面的代码需要注意相应的搭配，否则会陷入死循环。后面具体讲），常见的就是选数组的左边-右边-中间值-随机选" class="headerlink" title="1.2.1.对于一个给定的数组，选定一个中间值x（理论上来说，选哪个都行，但后面的代码需要注意相应的搭配，否则会陷入死循环。后面具体讲），常见的就是选数组的左边&#x2F;右边&#x2F;中间值&#x2F;随机选"></a>1.2.1.对于一个给定的数组，选定一个中间值x（理论上来说，选哪个都行，但后面的代码需要注意相应的搭配，否则会陷入死循环。后面具体讲），常见的就是选数组的左边&#x2F;右边&#x2F;中间值&#x2F;随机选</h3>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%B8%80%E8%AE%B2%20%20%E5%BF%AB%E6%8E%92&%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F&%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%BA%8C%E8%AE%B2%20%E9%AB%98%E7%B2%BE%E5%BA%A6&%E5%89%8D%E7%BC%80%E5%92%8C&%E5%B7%AE%E5%88%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%BA%8C%E8%AE%B2%20%E9%AB%98%E7%B2%BE%E5%BA%A6&%E5%89%8D%E7%BC%80%E5%92%8C&%E5%B7%AE%E5%88%86/" class="post-title-link" itemprop="url">chap1.第二讲 高精度&前缀和&差分</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-04 16:00:00" itemprop="dateCreated datePublished" datetime="2025-02-04T16:00:00+08:00">2025-02-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-16 17:04:46" itemprop="dateModified" datetime="2025-02-16T17:04:46+08:00">2025-02-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algo/" itemprop="url" rel="index"><span itemprop="name">algo</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-高精度"><a href="#1-高精度" class="headerlink" title="1.高精度"></a>1.高精度</h1><h2 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1.简介"></a>1.1.简介</h2><h3 id="所谓高精度，在这里指的是大数相关的运算。且不是每一种语言都需要自己实现这种算法。像java和python就不需要（大体上是因为它们已经自带这个功能了）。而算法常用的c-c-则没有这种功能，因此需要自己实现而这门课讲的高精度，主要局限在两个大数相加-相减，以及一个大数乘以一个小数（这里的大数，一般指位数很多的正整数，-len-A-leq-1e6-；小数一般指位数不那么多的正整数，-len-B-leq-9-）"><a href="#所谓高精度，在这里指的是大数相关的运算。且不是每一种语言都需要自己实现这种算法。像java和python就不需要（大体上是因为它们已经自带这个功能了）。而算法常用的c-c-则没有这种功能，因此需要自己实现而这门课讲的高精度，主要局限在两个大数相加-相减，以及一个大数乘以一个小数（这里的大数，一般指位数很多的正整数，-len-A-leq-1e6-；小数一般指位数不那么多的正整数，-len-B-leq-9-）" class="headerlink" title="所谓高精度，在这里指的是大数相关的运算。且不是每一种语言都需要自己实现这种算法。像java和python就不需要（大体上是因为它们已经自带这个功能了）。而算法常用的c&#x2F;c++则没有这种功能，因此需要自己实现而这门课讲的高精度，主要局限在两个大数相加&#x2F;相减，以及一个大数乘以一个小数（这里的大数，一般指位数很多的正整数，$len(A)\leq 1e6$；小数一般指位数不那么多的正整数，$len(B)\leq 9$）"></a>所谓高精度，在这里指的是大数相关的运算。且不是每一种语言都需要自己实现这种算法。像java和python就不需要（大体上是因为它们已经自带这个功能了）。而算法常用的c&#x2F;c++则没有这种功能，因此需要自己实现<br>而这门课讲的高精度，主要局限在两个大数相加&#x2F;相减，以及一个大数乘以一个小数（这里的大数，一般指位数很多的正整数，$len(A)\leq 1e6$；小数一般指位数不那么多的正整数，$len(B)\leq 9$）</h3>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%BA%8C%E8%AE%B2%20%E9%AB%98%E7%B2%BE%E5%BA%A6&%E5%89%8D%E7%BC%80%E5%92%8C&%E5%B7%AE%E5%88%86/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/03/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/03/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-03 17:28:21" itemprop="dateCreated datePublished" datetime="2025-02-03T17:28:21+08:00">2025-02-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-30 00:25:39" itemprop="dateCreated datePublished" datetime="2025-01-30T00:25:39+08:00">2025-01-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-10 02:31:27" itemprop="dateModified" datetime="2025-02-10T02:31:27+08:00">2025-02-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="From-Cross-Modal-to-Mixed-Modal-Visible-Infrared-Re-Identification"><a href="#From-Cross-Modal-to-Mixed-Modal-Visible-Infrared-Re-Identification" class="headerlink" title="From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification"></a>From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification</h1><h2 id="arxiv-2025-1-23"><a href="#arxiv-2025-1-23" class="headerlink" title="arxiv 2025.1.23"></a>arxiv 2025.1.23</h2><h2 id="这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery-images中有两种模态的图像，而不止一种模态。"><a href="#这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery-images中有两种模态的图像，而不止一种模态。" class="headerlink" title="这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery images中有两种模态的图像，而不止一种模态。"></a>这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery images中有两种模态的图像，而不止一种模态。</h2><h2 id="动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared-infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片-晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain-gap显然会比不同模态的domain-gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id-1的一个红外图像，一个可见光图像，和一个id-2的可见光图像。现在有一个id-2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id-1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。这篇论文就是想解决这个问题"><a href="#动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared-infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片-晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain-gap显然会比不同模态的domain-gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id-1的一个红外图像，一个可见光图像，和一个id-2的可见光图像。现在有一个id-2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id-1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。这篇论文就是想解决这个问题" class="headerlink" title="动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared&#x2F;infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片&#x2F;晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain gap显然会比不同模态的domain gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id 1的一个红外图像，一个可见光图像，和一个id 2的可见光图像。现在有一个id 2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id 1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。这篇论文就是想解决这个问题"></a>动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared&#x2F;infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片&#x2F;晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain gap显然会比不同模态的domain gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id 1的一个红外图像，一个可见光图像，和一个id 2的可见光图像。现在有一个id 2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id 1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。<br>这篇论文就是想解决这个问题</h2><h2 id="方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared-feature，也要用到modality-specific-feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared-feature，一个代表specific-feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared-feature和specific-feature呢？）"><a href="#方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared-feature，也要用到modality-specific-feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared-feature，一个代表specific-feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared-feature和specific-feature呢？）" class="headerlink" title="方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared feature，也要用到modality-specific feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared feature，一个代表specific feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared feature和specific feature呢？）"></a>方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared feature，也要用到modality-specific feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared feature，一个代表specific feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared feature和specific feature呢？）</h2><h4 id="（话说回来，在经典的vi-reid中，真的有必要用到modality-specific-feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific-feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）"><a href="#（话说回来，在经典的vi-reid中，真的有必要用到modality-specific-feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific-feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）" class="headerlink" title="（话说回来，在经典的vi-reid中，真的有必要用到modality-specific feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）"></a>（话说回来，在经典的vi-reid中，真的有必要用到modality-specific feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）</h4><h2 id="分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality-scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific-feature（或者说原文的modality-erased-related-feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：-max-Z-m-e-Z-m-r-MI-Z-m-e-Z-m-r-Y-s-t-MI-Z-m-e-Z-m-r-0-MI-Z-m-e-M-0-and-MI-Z-m-r-Y-M-0-上面的-Z-m-e-代表模态m下的modality-erased-feature，也就是与模态无关的特征；-Z-m-r-代表模态m下的modality-related-feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased-feature与modality-related-feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为-Z-m-e-代表modality-erased-feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望-Z-m-r-不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，-Z-m-r-就是只能用来推断模态信息，不能用来推断ID信息。那它和-Z-m-e-组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased-feature和modality-related-feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased-feature（但这样不是很麻烦吗？得再看看代码了可能）总而言之，得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题。得到结果如下：且针对四个部分，都进行了各自的优化"><a href="#分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality-scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific-feature（或者说原文的modality-erased-related-feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：-max-Z-m-e-Z-m-r-MI-Z-m-e-Z-m-r-Y-s-t-MI-Z-m-e-Z-m-r-0-MI-Z-m-e-M-0-and-MI-Z-m-r-Y-M-0-上面的-Z-m-e-代表模态m下的modality-erased-feature，也就是与模态无关的特征；-Z-m-r-代表模态m下的modality-related-feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased-feature与modality-related-feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为-Z-m-e-代表modality-erased-feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望-Z-m-r-不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，-Z-m-r-就是只能用来推断模态信息，不能用来推断ID信息。那它和-Z-m-e-组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased-feature和modality-related-feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased-feature（但这样不是很麻烦吗？得再看看代码了可能）总而言之，得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题。得到结果如下：且针对四个部分，都进行了各自的优化" class="headerlink" title="分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific feature（或者说原文的modality-erased&#x2F;-related feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：$$\max_{Z_{m}^{e},Z_{m}^{r}}{MI(Z_{m}^{e},Z_{m}^{r};Y)}\ \ s.t.MI(Z_{m}^{e},Z_{m}^{r})&#x3D;0,MI(Z_{m}^{e};M)&#x3D;0,and\ MI(Z_{m}^{r};Y|M)&#x3D;0$$上面的$Z_{m}^{e}$代表模态m下的modality-erased feature，也就是与模态无关的特征；$Z_{m}^{r}$代表模态m下的modality-related feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased feature与modality-related feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为$Z_{m}^{e}$代表modality-erased feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望$Z_{m}^{r}$不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，$Z_{m}^{r}$就是只能用来推断模态信息，不能用来推断ID信息。那它和$Z_{m}^{e}$组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased feature和modality-related feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased feature（但这样不是很麻烦吗？得再看看代码了可能）总而言之，得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题。得到结果如下：且针对四个部分，都进行了各自的优化"></a>分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific feature（或者说原文的modality-erased&#x2F;-related feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：$$\max_{Z_{m}^{e},Z_{m}^{r}}{MI(Z_{m}^{e},Z_{m}^{r};Y)}\ \ s.t.MI(Z_{m}^{e},Z_{m}^{r})&#x3D;0,MI(Z_{m}^{e};M)&#x3D;0,and\ MI(Z_{m}^{r};Y|M)&#x3D;0$$<br>上面的$Z_{m}^{e}$代表模态m下的modality-erased feature，也就是与模态无关的特征；$Z_{m}^{r}$代表模态m下的modality-related feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased feature与modality-related feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为$Z_{m}^{e}$代表modality-erased feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望$Z_{m}^{r}$不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，$Z_{m}^{r}$就是只能用来推断模态信息，不能用来推断ID信息。那它和$Z_{m}^{e}$组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased feature和modality-related feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased feature（但这样不是很麻烦吗？得再看看代码了可能）<br>总而言之，<strong>得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题</strong>。得到结果如下：<img src="/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1/%E6%B3%9B%E8%AF%BB/pic/cross-modal.png" alt="|500"><br>且针对四个部分，都进行了各自的优化</h2><h2 id="评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析"><a href="#评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析" class="headerlink" title="评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析"></a>评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析</h2><h2 id="trick总结："><a href="#trick总结：" class="headerlink" title="trick总结："></a>trick总结：</h2><h3 id="1-想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替"><a href="#1-想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替" class="headerlink" title="1.想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替"></a>1.想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替</h3><h3 id="2-如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化-MI-Z-m-e-Y-，即要最大化modality-erased-feature与标签变量之间的互信息，其实目的就是让modality-erased-feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）"><a href="#2-如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化-MI-Z-m-e-Y-，即要最大化modality-erased-feature与标签变量之间的互信息，其实目的就是让modality-erased-feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）" class="headerlink" title="2.如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化$MI(Z_{m}^{e};Y)$，即要最大化modality-erased feature与标签变量之间的互信息，其实目的就是让modality-erased feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）"></a>2.如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化$MI(Z_{m}^{e};Y)$，即要最大化modality-erased feature与标签变量之间的互信息，其实目的就是让modality-erased feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）</h3><h3 id="3-要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层"><a href="#3-要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层" class="headerlink" title="3.要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层"></a>3.要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层</h3><h3 id="4-要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：-L-max-x-y-alpha-0-max-y-x-alpha-0-，-alpha-是正常数，也是超参数。那么，x和y必须相差绝对值不大于-alpha-，这部分损失才为0，否则就产生损失了"><a href="#4-要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：-L-max-x-y-alpha-0-max-y-x-alpha-0-，-alpha-是正常数，也是超参数。那么，x和y必须相差绝对值不大于-alpha-，这部分损失才为0，否则就产生损失了" class="headerlink" title="4.要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：$L&#x3D;max(x-y+\alpha,0)+max(y-x+\alpha,0)$，$\alpha$是正常数，也是超参数。那么，x和y必须相差绝对值不大于$\alpha$，这部分损失才为0，否则就产生损失了"></a>4.要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：$L&#x3D;max(x-y+\alpha,0)+max(y-x+\alpha,0)$，$\alpha$是正常数，也是超参数。那么，x和y必须相差绝对值不大于$\alpha$，这部分损失才为0，否则就产生损失了</h3><h3 id="5-这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific-feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID-2；如果它是infrared的图像，则ID-ID-2-1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific-feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific-feature"><a href="#5-这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific-feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID-2；如果它是infrared的图像，则ID-ID-2-1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific-feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific-feature" class="headerlink" title="5.这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID * 2；如果它是infrared的图像，则ID&#x3D;ID * 2 + 1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific feature"></a>5.这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID * 2；如果它是infrared的图像，则ID&#x3D;ID * 2 + 1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific feature</h3><h1 id="Spectral-Enhancement-and-Pseudo-Anchor-Guidance-for-Infrared-Visible-Person-Re-Identification"><a href="#Spectral-Enhancement-and-Pseudo-Anchor-Guidance-for-Infrared-Visible-Person-Re-Identification" class="headerlink" title="Spectral Enhancement and Pseudo-Anchor Guidance  for Infrared-Visible Person Re-Identification"></a>Spectral Enhancement and Pseudo-Anchor Guidance  for Infrared-Visible Person Re-Identification</h1><h2 id="arxiv-2025-1-2"><a href="#arxiv-2025-1-2" class="headerlink" title="arxiv, 2025.1.2"></a>arxiv, 2025.1.2</h2><h2 id="这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral-gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）"><a href="#这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral-gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）" class="headerlink" title="这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）"></a>这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）</h2><h2 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h2><h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><h2 id="分析：第一步是生成一个Semantically-Enhanced-Grey-Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically-Enhanced-Grey-Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇，-就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific-feature第三步是PABA-loss，即Pseudo-Anchor-guided-Bidirectional-Aggregation-Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征-F-shared-seg-和-F-shared-ir-，先进行一个分块，分成N块，之后对每个块都施加PABA-loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet-loss，这里增加了跨模态的考虑；而相比于cross-center-loss，这里的改进在于分了块，且是对每个块都施加PABA-loss，故更加细粒度（这里其实要结合公式来看才清楚）除了上面提到的PABA-loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared-feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）"><a href="#分析：第一步是生成一个Semantically-Enhanced-Grey-Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically-Enhanced-Grey-Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇，-就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific-feature第三步是PABA-loss，即Pseudo-Anchor-guided-Bidirectional-Aggregation-Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征-F-shared-seg-和-F-shared-ir-，先进行一个分块，分成N块，之后对每个块都施加PABA-loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet-loss，这里增加了跨模态的考虑；而相比于cross-center-loss，这里的改进在于分了块，且是对每个块都施加PABA-loss，故更加细粒度（这里其实要结合公式来看才清楚）除了上面提到的PABA-loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared-feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）" class="headerlink" title="分析：第一步是生成一个Semantically Enhanced Grey Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically Enhanced Grey Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇， 就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific feature第三步是PABA loss，即Pseudo Anchor-guided Bidirectional Aggregation Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征$F_{shared}^{seg}$和$F_{shared}^{ir}$，先进行一个分块，分成N块，之后对每个块都施加PABA loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet loss，这里增加了跨模态的考虑；而相比于cross-center loss，这里的改进在于分了块，且是对每个块都施加PABA loss，故更加细粒度（这里其实要结合公式来看才清楚）除了上面提到的PABA loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）"></a>分析：第一步是生成一个Semantically Enhanced Grey Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically Enhanced Grey Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了<br><br>第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇， 就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific feature<br><br>第三步是PABA loss，即Pseudo Anchor-guided Bidirectional Aggregation Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征$F_{shared}^{seg}$和$F_{shared}^{ir}$，先进行一个分块，分成N块，之后对每个块都施加PABA loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet loss，这里增加了跨模态的考虑；而相比于cross-center loss，这里的改进在于分了块，且是对每个块都施加PABA loss，故更加细粒度（这里其实要结合公式来看才清楚）<br>除了上面提到的PABA loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）</h2><h1 id="Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification"><a href="#Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification"></a>Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification</h1><h2 id="arxiv，2024-12-11"><a href="#arxiv，2024-12-11" class="headerlink" title="arxiv，2024.12.11"></a>arxiv，2024.12.11</h2><h2 id="这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由"><a href="#这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由" class="headerlink" title="这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由"></a>这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由</h2><h2 id="insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）"><a href="#insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）" class="headerlink" title="insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）"></a>insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）</h2><h2 id="动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人-llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）"><a href="#动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人-llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）" class="headerlink" title="动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人&#x2F;llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）"></a>动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人&#x2F;llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）</h2><h2 id="方法：-1"><a href="#方法：-1" class="headerlink" title="方法："></a>方法：</h2><h3 id="提出一个EEES框架，包含三个部分，各司其职第一个是ESE模块（Explicit-Semantics-Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息"><a href="#提出一个EEES框架，包含三个部分，各司其职第一个是ESE模块（Explicit-Semantics-Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息" class="headerlink" title="提出一个EEES框架，包含三个部分，各司其职第一个是ESE模块（Explicit Semantics Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息"></a>提出一个EEES框架，包含三个部分，各司其职<br>第一个是ESE模块（Explicit Semantics Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息</h3><h3 id="第二个是CVSC（Cross-View-Semantics-Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共-M-1-个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是训练时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下："><a href="#第二个是CVSC（Cross-View-Semantics-Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共-M-1-个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是训练时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下：" class="headerlink" title="第二个是CVSC（Cross-View Semantics Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共$M+1$个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是训练时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下："></a>第二个是CVSC（Cross-View Semantics Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共$M+1$个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息<br>然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是<strong>训练</strong>时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下：<img src="/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1/%E6%B3%9B%E8%AF%BB/pic/distillation_loss.png" alt="|500"></h3><h3 id="第三个是CMSP模块（Cross-Modality-Semantics-Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp-loss，公式如下：-L-cmsp-frac-1-N-sum-i-1-N-d-i-vv-d-i-vr-2-frac-1-N-sum-i-1-N-d-i-rr-d-i-rv-2-其中，-d-i-vv-lVert-f-i-v-t-i-v-rVert-2-，-d-i-vr-lVert-f-i-v-t-i-r-rVert-2-，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧"><a href="#第三个是CMSP模块（Cross-Modality-Semantics-Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp-loss，公式如下：-L-cmsp-frac-1-N-sum-i-1-N-d-i-vv-d-i-vr-2-frac-1-N-sum-i-1-N-d-i-rr-d-i-rv-2-其中，-d-i-vv-lVert-f-i-v-t-i-v-rVert-2-，-d-i-vr-lVert-f-i-v-t-i-r-rVert-2-，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧" class="headerlink" title="第三个是CMSP模块（Cross-Modality Semantics Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp loss，公式如下：$$L_{cmsp}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{vv}-d_{i}^{vr})}^{2}+\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{rr}-d_{i}^{rv})}^{2}$$其中，$d_{i}^{vv}&#x3D;\lVert f_{i}^{v}-t_{i}^{v}\rVert_{2}$，$d_{i}^{vr}&#x3D;\lVert f_{i}^{v}-t_{i}^{r}\rVert_{2}$，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧"></a>第三个是CMSP模块（Cross-Modality Semantics Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp loss，公式如下：$$L_{cmsp}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{vv}-d_{i}^{vr})}^{2}+\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{rr}-d_{i}^{rv})}^{2}$$<br>其中，$d_{i}^{vv}&#x3D;\lVert f_{i}^{v}-t_{i}^{v}\rVert_{2}$，$d_{i}^{vr}&#x3D;\lVert f_{i}^{v}-t_{i}^{r}\rVert_{2}$，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧</h3><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。"><a href="#第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。" class="headerlink" title="第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。"></a>第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。</h3><h3 id="第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光-红外，图像-文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）"><a href="#第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光-红外，图像-文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）" class="headerlink" title="第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光&#x2F;红外，图像&#x2F;文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）"></a>第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光&#x2F;红外，图像&#x2F;文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）</h3><h3 id="第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp-loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的-一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字-图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。"><a href="#第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp-loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的-一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字-图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。" class="headerlink" title="第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的&#x2F;一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字&#x2F;图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。"></a>第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的&#x2F;一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字&#x2F;图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。</h3><h1 id="Prototype-Driven-Multi-Feature-Generation-for-Visible-Infrared-Person-Re-identification"><a href="#Prototype-Driven-Multi-Feature-Generation-for-Visible-Infrared-Person-Re-identification" class="headerlink" title="Prototype-Driven Multi-Feature Generation for  Visible-Infrared Person Re-identification"></a>Prototype-Driven Multi-Feature Generation for  Visible-Infrared Person Re-identification</h1><h2 id="ICASSP，2024-9-9"><a href="#ICASSP，2024-9-9" class="headerlink" title="ICASSP，2024.9.9"></a>ICASSP，2024.9.9</h2><h2 id="动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了"><a href="#动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了" class="headerlink" title="动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了"></a>动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了</h2><h2 id="insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性"><a href="#insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性" class="headerlink" title="insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性"></a>insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性</h2><h2 id="动机：-1"><a href="#动机：-1" class="headerlink" title="动机："></a>动机：</h2><h2 id="方法：-2"><a href="#方法：-2" class="headerlink" title="方法："></a>方法：</h2><h3 id="提出的PDM框架，主要是两个组成部分，分别是Multi-Feature-Generation-Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype-Learning-Moduel（PLM）"><a href="#提出的PDM框架，主要是两个组成部分，分别是Multi-Feature-Generation-Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype-Learning-Moduel（PLM）" class="headerlink" title="提出的PDM框架，主要是两个组成部分，分别是Multi-Feature Generation Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype Learning Moduel（PLM）"></a>提出的PDM框架，主要是两个组成部分，分别是Multi-Feature Generation Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype Learning Moduel（PLM）</h3><h3 id="MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation-convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided-Pair-Mining-Loss"><a href="#MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation-convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided-Pair-Mining-Loss" class="headerlink" title="MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided Pair Mining Loss"></a>MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided Pair Mining Loss</h3><h3 id="PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）"><a href="#PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）" class="headerlink" title="PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）"></a>PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）</h3><h3 id="此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine-Heterogeneity-Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center-Seperation-Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）"><a href="#此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine-Heterogeneity-Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center-Seperation-Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）" class="headerlink" title="此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine Heterogeneity Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center Seperation Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）"></a>此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine Heterogeneity Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center Seperation Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）</h3><h1 id="Parameter-Hierarchical-Optimization-for-Visible-Infrared-Person-Re-Identification"><a href="#Parameter-Hierarchical-Optimization-for-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Parameter Hierarchical Optimization for  Visible-Infrared Person Re-Identification"></a>Parameter Hierarchical Optimization for  Visible-Infrared Person Re-Identification</h1><h2 id="arxiv，2024-4-11"><a href="#arxiv，2024-4-11" class="headerlink" title="arxiv，2024.4.11"></a>arxiv，2024.4.11</h2><h2 id="好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外-可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没"><a href="#好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外-可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没" class="headerlink" title="好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外&#x2F;可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没"></a>好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外&#x2F;可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没</h2><h1 id="Bidirectional-Multi-Step-Domain-Generalization-for-Visible-Infrared-Person-Re-Identification"><a href="#Bidirectional-Multi-Step-Domain-Generalization-for-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification"></a>Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification</h1><h2 id="arxiv，2024-3-16"><a href="#arxiv，2024-3-16" class="headerlink" title="arxiv，2024.3.16"></a>arxiv，2024.3.16</h2><h2 id="这篇论文想解决-改善的点在于，大部分方法都是把vis和ir-images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional-Multi-Step-Domain-Generalization）主要由两部分组成，一是part-prototype-alignment-learning（负责提取出局部的身体部位信息）；二是bidirectional-multi-step-learning（通过多步学习，从红外和可见光分别出发，以减小modality-gap）"><a href="#这篇论文想解决-改善的点在于，大部分方法都是把vis和ir-images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional-Multi-Step-Domain-Generalization）主要由两部分组成，一是part-prototype-alignment-learning（负责提取出局部的身体部位信息）；二是bidirectional-multi-step-learning（通过多步学习，从红外和可见光分别出发，以减小modality-gap）" class="headerlink" title="这篇论文想解决&#x2F;改善的点在于，大部分方法都是把vis和ir images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional Multi-Step Domain Generalization）主要由两部分组成，一是part prototype alignment learning（负责提取出局部的身体部位信息）；二是bidirectional multi-step learning（通过多步学习，从红外和可见光分别出发，以减小modality gap）"></a>这篇论文想解决&#x2F;改善的点在于，大部分方法都是把vis和ir images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional Multi-Step Domain Generalization）主要由两部分组成，一是part prototype alignment learning（负责提取出局部的身体部位信息）；二是bidirectional multi-step learning（通过多步学习，从红外和可见光分别出发，以减小modality gap）</h2><h2 id="insight"><a href="#insight" class="headerlink" title="insight"></a>insight</h2><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="part-prototype-alignment-learning主要由三个部分组成："><a href="#part-prototype-alignment-learning主要由三个部分组成：" class="headerlink" title="part prototype alignment learning主要由三个部分组成："></a>part prototype alignment learning主要由三个部分组成：</h3><h4 id="1-prototype-discovery（这个部分是用来获取身体部位细节信息的，并且要在feature-map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是"><a href="#1-prototype-discovery（这个部分是用来获取身体部位细节信息的，并且要在feature-map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是" class="headerlink" title="1.prototype discovery（这个部分是用来获取身体部位细节信息的，并且要在feature map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是"></a>1.prototype discovery（这个部分是用来获取身体部位细节信息的，并且要在feature map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是</h4><h1 id="CLIP-Driven-Semantic-Discovery-Network-for-Visible-Infrared-Person-Re-Identification"><a href="#CLIP-Driven-Semantic-Discovery-Network-for-Visible-Infrared-Person-Re-Identification" class="headerlink" title="CLIP-Driven Semantic Discovery Network for  Visible-Infrared Person Re-Identification"></a>CLIP-Driven Semantic Discovery Network for  Visible-Infrared Person Re-Identification</h1><h2 id="TMM，2024-1-11"><a href="#TMM，2024-1-11" class="headerlink" title="TMM，2024.1.11"></a>TMM，2024.1.11</h2><h2 id="这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点-出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification）的出发点有点类似"><a href="#这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点-出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification）的出发点有点类似" class="headerlink" title="这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点&#x2F;出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification）的出发点有点类似"></a>这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点&#x2F;出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification）的出发点有点类似</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">134</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
