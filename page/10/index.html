<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="bluemouse&#39;s blog">
<meta property="og:url" content="https://blueeemouse.github.io/page/10/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-Lifelong-ReID/Unsupervised%20Lifelong%20Person%20Re-identification%20%20via%20Contrastive%20Rehearsal/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-Lifelong-ReID/Unsupervised%20Lifelong%20Person%20Re-identification%20%20via%20Contrastive%20Rehearsal/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-17 22:22:45 / Modified: 23:22:13" itemprop="dateCreated datePublished" datetime="2025-02-17T22:22:45+08:00">2025-02-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="arxiv-22">arxiv 22</h1>
<h1 id="这篇论文提出的设定倒是在现实生活里有意义不过它的动机好像也是提的比较模糊都是针对几个设定加起来的时候产生的问题">这篇论文提出的设定倒是在现实生活里有意义，不过它的动机好像也是提的比较模糊，都是针对几个设定加起来的时候产生的问题</h1>
<h1 id="它讲的故事however-a-real-world-video-monitoring-system-can-record-new-data-every-day-and-from-new-locations-when-new-cameras-are-added-into-an-existing-system.when-new-data-is-recorded-every-day-people-have-to-annotate-new-data-manually-before-deployment-which-is-cumbersome-and-time-consuming.这里在说明无监督方法研究的必要性-towards-a-generalizable-reid-model-lifelong-person-reid-has-been-recently-proposed这里在说明lifelong-learning研究的必要性">它讲的故事：However,
a real-world video monitoring system can record new data every day and
from new locations, when new cameras are added into an existing
system.；When new data is recorded every day, people have to annotate
new data manually before deployment, which is cumbersome and
time-consuming.（这里在说明，无监督方法研究的必要性）<br>Towards a
generalizable ReID model, lifelong person ReID has been recently
proposed（这里在说明，lifelong learning研究的必要性）</h1>
<h1 id="它的方法设计的倒也算简单就是分别针对灾难性遗忘学习新数据的问题">它的方法，设计的倒也算简单，就是分别针对灾难性遗忘、学习新数据的问题</h1>
<h1 id="方法">方法</h1>
<h2 id="current-domain-contrastive-baseline">1.Current domain
contrastive baseline</h2>
<h3 id="它主要是进行经典的拉近样本和簇中心的距离如今已经是一个非常常见的操作了还有一个l_cam就是对同一个簇里属于同一个相机的数据也求一个prototype就是这些数据的均值并要拉近样本与这个prototype的距离还是对比损失的形式">它主要是进行经典的拉近样本和簇中心的距离（如今已经是一个非常常见的操作了），还有一个<span class="math inline">\(L_{cam}\)</span>，就是对同一个簇里，属于同一个相机的数据，也求一个prototype（就是这些数据的均值），并要拉近样本与这个prototype的距离（还是对比损失的形式）</h3>
<h4 id="不过论文里还提到l_cam过于依赖相机标签局限在了reid任务里了如果想更加泛化其实也有其它的损失可以用比如在一个batch里与batch里的hardest-positives进行对比">不过，论文里还提到，<span class="math inline">\(L_{cam}\)</span>过于依赖相机标签，局限在了ReID任务里了；如果想更加泛化，其实也有其它的损失可以用，比如在一个batch里，与batch里的hardest
positives进行对比</h4>
<h4 id="另注意这个方法会有online-encoder和momentum-encoder在这一步里聚类用的特征是momentum-encoder得到的而不是online-encoder">（另，注意，这个方法会有online
encoder和momentum encoder；在这一步里，聚类用的特征是momentum
encoder得到的，而不是online encoder）</h4>
<h2 id="old-domain-contrastive-rehearsal">2.Old domain contrastive
rehearsal</h2>
<h3 id="这个方法里会存储一些旧数据集的prototype也就是我们得到的簇中心的特征以及各个簇的若干个可靠样本就是比较接近簇中心的这一步的核心思想是给定一个已知伪标签的旧数据样本我们希望当前的online-encoder对其编码后得到的特征应该还是尽可能接近原来它所属的那个簇由此我们认为online-encoder对旧数据集知识的保存做的比较好">这个方法里会存储一些旧数据集的prototype（也就是我们得到的簇中心的特征），以及各个簇的若干个可靠样本（就是比较接近簇中心的）。这一步的核心思想是，给定一个已知伪标签的旧数据样本，我们希望当前的online
encoder对其编码后，得到的特征应该还是尽可能接近原来它所属的那个簇（由此我们认为online
encoder对旧数据集知识的保存做的比较好）</h3>
<h2 id="image-to-image-similarity-constraint">3.Image-to-Image
Similarity Constraint</h2>
<h3 id="这一步的作用是抗遗忘然后它的核心思想是给定一个batch我们用encoder得到特征后可以得到一个样本之间的相似性矩阵如果新模型得到的相似性矩阵与旧模型得到的相似性矩阵很接近对于同一个batch而言那么我们就认为这个新模型保留旧知识时保留得比较好论文里新模型就对应online-encoder旧模型就对应momentum-encoder它保留了比较多的旧知识衡量两个相似性矩阵的距离时用的是kl散度">这一步的作用是抗遗忘，然后它的核心思想是，给定一个batch，我们用encoder得到特征后，可以得到一个样本之间的相似性矩阵。如果新模型得到的相似性矩阵与旧模型得到的相似性矩阵很接近（对于同一个batch而言），那么我们就认为这个新模型保留旧知识时，保留得比较好。论文里，新模型就对应online
encoder，旧模型就对应momentum
encoder（它保留了比较多的旧知识）。衡量两个相似性矩阵的距离时，用的是KL散度</h3>
<h3 id="但此处有个小改进就是它的目标矩阵确实是用旧模型得到的但它的新的相似性矩阵计算的时候求行特征用的是新模型求列特征用的是旧模型论文里说这样的效果比单纯用新模型会好确实有一些道理吧因为求列特征用的是旧模型那么要迫使两个相似性矩阵尽可能接近就要求新模型的输出要尽可能接近旧模型的输出无形中其实就进行了蒸馏了吧">但此处有个小改进，就是，它的目标矩阵确实是用旧模型得到的，但它的新的相似性矩阵，计算的时候，求行特征用的是新模型，求列特征用的是旧模型。论文里说这样的效果比单纯用新模型会好。确实有一些道理吧，因为求列特征用的是旧模型，那么，要迫使两个相似性矩阵尽可能接近，就要求新模型的输出要尽可能接近旧模型的输出（无形中其实就进行了蒸馏了吧）</h3>
<h1 id="trick总结">trick总结</h1>
<h2 id="在进行对比的时候既可以和所有的prototype来对比也可以挑出若干个hard-sample保证效果的同时减少一些计算量">1.在进行对比的时候，既可以和所有的prototype来对比，也可以挑出若干个hard
sample，保证效果的同时减少一些计算量</h2>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/Robust%20Pseudo-label%20Learning%20with%20Neighbor%20Relation%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/Robust%20Pseudo-label%20Learning%20with%20Neighbor%20Relation%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-16 11:51:35 / Modified: 21:47:48" itemprop="dateCreated datePublished" datetime="2025-02-16T11:51:35+08:00">2025-02-16</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="acm-mm24">acm mm24</h1>
<h1 id="它也是要解决伪标签带噪的问题提出一个noisy-pseudo-label-calibration-module来校正伪标签提出一个nieghbor-relation-learning-module来获取模态内更加一致的特征提出一个optimal-transport-prototype-matching-module来进行跨模态的匹配建立跨模态对应关系提出一个memory-hybrid-leraning-module来更好提取出modality-invariant和modality-specific-feature">它也是要解决伪标签带噪的问题。提出一个Noisy
Pseudo-label Calibration
module来<strong><em>校正伪标签</em></strong>；提出一个Nieghbor Relation
Learning module来获取模态内更加一致的特征；提出一个Optimal Transport
Prototype Matching
module来进行跨模态的匹配，建立跨模态对应关系；提出一个Memory Hybrid
Leraning module来更好提取出modality-invariant和modality-specific
feature</h1>
<h1 id="insight">insight</h1>
<h1 id="动机无监督vi-reid中伪标签带噪声是很常见的问题而论文提出一般的方法聚焦于减少带噪标签的影响但忽略了去校正带噪标签其实这么说好像也不是很严谨那些标签平滑的操作我觉得某种程度上也是在校正标签的">动机：无监督vi-reid中，伪标签带噪声是很常见的问题。而论文提出，一般的方法聚焦于减少带噪标签的影响，但忽略了去校正带噪标签（其实这么说好像也不是很严谨，那些标签平滑的操作，我觉得某种程度上也是在校正标签的）</h1>
<h1 id="方法">方法</h1>
<h2 id="noisy-pseudo-label-calibrationnpc这个模块就是很显然的回应了动机里它想要校正标签的想法它评价两个样本相似度的方法是用邻居的相似度或者说重合度而不是用样本特征本身这种间接衡量相似性的方法和gur论文towards-grand-unified-representation-learning-for-unsupervised-visible-infrared-person-re-identification里的操作思想上是比较相近的只不过那里用的是概率分布要尽可能相似-而它校正标签的方法则是先提取出每个簇内的可靠的样本可靠的判断方法中核心思想是一个簇内如果有很多样本和当前样本非常相似那么当前样本应该就是比较可靠的然后簇中心就初始化为这些可靠样本的均值之后每个样本再和这些新的簇中心进行比较分配到最相似的那个簇中心那里">1.Noisy
Pseudo-label
Calibration（NPC），这个模块就是很显然的，回应了动机里它想要校正标签的想法。它评价两个样本相似度的方法是用邻居的相似度（或者说重合度），而不是用样本特征本身（这种间接衡量相似性的方法，和(GUR论文[[Towards
Grand Unified Representation Learning for Unsupervised Visible-Infrared
Person
Re-Identification]])里的操作，思想上是比较相近的，只不过那里用的是概率分布要尽可能相似）<br>而它校正标签的方法，则是，先提取出每个簇内的“可靠”的样本（“可靠”的判断方法中，核心思想是，一个簇内，如果有很多样本和当前样本非常相似，那么当前样本应该就是比较”可靠“的），然后簇中心就初始化为这些“可靠”样本的均值；之后，每个样本再和这些新的簇中心进行比较，分配到最相似的那个簇中心那里</h2>
<h2 id="neighbor-relation-learningnrl原文里说是为了reduce-high-intra-class-variations我们可以认为是为了提取更好的特征以便更好地进行模态内的聚类所以它应该主要聚焦在挖掘模态内的信息但它的损失没太搞懂">2.Neighbor
Relation Learning（NRL），原文里说是为了reduce high intra-class
variations，我们可以认为，是为了提取更好的特征，以便更好地进行模态内的聚类。所以它应该主要聚焦在挖掘模态内的信息。但它的损失，没太搞懂。。</h2>
<h2 id="optimal-transport-prototype-matchingotpa聚焦于挖掘跨模态对应的信息">Optimal
Transport Prototype Matching（OTPA），聚焦于挖掘跨模态对应的信息</h2>
<h2 id="memory-hybrid-learning聚焦于学习modality-invariant-feature具体来说它在经典的模态内的clusternce-loss的基础上还构建了一个modality-hybrid-memory这个memory的特征是由红外cluster和对应的可见光cluster加权求和得到然后交替地拉近红外实例可见光实例与这个混合memory中的对应cluster之间的距离">Memory
Hybrid Learning，聚焦于学习modality-invariant
feature。具体来说，它在经典的模态内的ClusterNCE
loss的基础上，还构建了一个modality-hybrid
memory（这个memory的特征是由红外cluster和对应的可见光cluster加权求和得到），然后交替地拉近红外实例、可见光实例与这个混合memory中的对应cluster之间的距离</h2>
<h1 id="分析">分析</h1>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-Lifelong-VI-ReID/%E9%9A%BE%E7%82%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-Lifelong-VI-ReID/%E9%9A%BE%E7%82%B9/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-16 10:28:47" itemprop="dateCreated datePublished" datetime="2025-02-16T10:28:47+08:00">2025-02-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-18 15:34:16" itemprop="dateModified" datetime="2025-02-18T15:34:16+08:00">2025-02-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>首先，无监督条件下的跨模态对应依然是难点。而加入了lifelong条件之后，显然会有lifelong的常见问题，就是灾难性遗忘。并且，
在新数据集上，因为分布不同，所以之前学到的跨模态对应知识，以及单模态内的知识可能都不适用，而这会导致在学习新数据集的时候，初期的特征不够好，进而会导致聚类得到的伪标签也不够好，后续对应也很难对好，故训练完得到的模型性能可能也不够好/在新数据集上，反而是有一些先验知识会比较好，有利于度过初期训练？（如果是会产生负面影响，那么标签校正应该是非常有必要的了）</p>
<p>解决方法上，提出一种比较有效的标签置信度求解方法，来帮助过度前期训练，或许是一个解决方法（比如，对每个实例，都找出和它距离前k近的实例，把这些邻居的标签加权求和，看看求和得到的标签和它的簇标签之间的距离远近？）
或许修正标签是更加直接的方法（修正，而不是直接把带噪声的标签给去除掉）</p>
<p>尝试：一个momentum encoder，一个online
encoder，然后在新数据集上，用online
encoder先学一下并聚类（可以适当加载一些momentum
encoder，以及采取warmup），之后再慢慢地过渡到正常训练</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-16 00:07:08 / Modified: 15:04:15" itemprop="dateCreated datePublished" datetime="2025-02-16T00:07:08+08:00">2025-02-16</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="无监督vi-reid大多数的流程都是先通过某种聚类方法通常是dbscan得到一批伪标签然后开始构建跨模态的对应一般是对应簇而不是一个一个实例去对应因为对应完簇我们就认为这两个簇里的实例都是一个id的所以其实可以说一个常见的范式是在各个模态内进行聚类然后进行跨模态的簇的对应这么说来eccv24那篇有没有可能提出了一个新的范式就是在聚类的时候就已经考虑上跨模态的对应了">无监督vi-reid，大多数的流程都是，先通过某种聚类方法（通常是DBSCAN），得到一批伪标签，然后开始构建跨模态的对应（一般是对应簇，而不是一个一个实例去对应。因为对应完簇，我们就认为这两个簇里的实例都是一个ID的）。所以其实可以说，一个常见的范式是，在各个模态内进行聚类，然后进行跨模态的簇的对应（这么说来，ECCV24那篇，有没有可能提出了一个新的范式，就是在聚类的时候就已经考虑上跨模态的对应了？）</h1>
<h1 id="通常的改进会聚焦在">通常的改进会聚焦在，</h1>
<h2 id="如何更好的构建伪标签的跨模态对应pgmotpm">如何更好的构建伪标签的跨模态对应（PGM）（OTPM）</h2>
<h3 id="毕竟没标签很难对应上最理想的情况当然是我们的聚类结果恰好就等于真实标签此时无监督vi-reid就会变成vi-reid实际还有个很重要的问题就是我们的对应很可能是有错的怎么在有错误对应即所谓噪声的情况下不断改进对应结果也是很重要的所以才会有很多progressive的方法">（毕竟没标签，很难对应上；最理想的情况当然是我们的聚类结果恰好就等于真实标签，此时无监督vi-reid就会变成vi-reid）（实际还有个很重要的问题，就是，我们的对应很可能是有错的，怎么在有错误对应（即所谓噪声）的情况下不断改进对应结果也是很重要的。所以才会有很多progressive的方法）；</h3>
<h2 id="还有一部分可能会聚焦在伪标签的可靠性上multi-memorypgm">还有一部分可能会聚焦在伪标签的可靠性上？（Multi-memory，PGM）</h2>
<h2 id="提取更加鲁棒的特征以改善聚类效果shallow-deepgur">提取更加鲁棒的特征，以改善聚类效果（Shallow-Deep，GUR）</h2>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94%20unsupervised%20vi-reid/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/Unsupervised-VI-ReID/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94%20unsupervised%20vi-reid/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-14 09:55:45" itemprop="dateCreated datePublished" datetime="2025-02-14T09:55:45+08:00">2025-02-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-19 17:08:31" itemprop="dateModified" datetime="2025-02-19T17:08:31+08:00">2025-02-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="cvpr24">cvpr24</h1>
<h2 id="shallow-deep-collaborative-learning-for-unsupervised-visible-infrared-person-re-identification">Shallow-Deep
Collaborative Learning for Unsupervised Visible-Infrared Person
Re-Identification</h2>
<h1 id="nips24有一篇iclr和icml24没有">nips24有一篇，iclr和icml24没有</h1>
<h1 id="eccv24">eccv24</h1>
<h2 id="multi-memory-matching-for-unsupervised-visible-infrared-person-re-identification"><a target="_blank" rel="noopener" href="https://eccv.ecva.net/virtual/2024/poster/1841">Multi-Memory
Matching for Unsupervised Visible-Infrared Person
Re-Identification</a></h2>
<h1 id="aaai24ijcai24无">aaai24&amp;ijcai24无</h1>
<h1 id="cvpr23">cvpr23</h1>
<h2 id="unsupervised-visible-infrared-person-re-identification-via-progressive-graph-matching-and-alternate-learning"><a target="_blank" rel="noopener" href="https://github.com/zesenwu23/USL-VI-ReID">Unsupervised
Visible-Infrared Person Re-Identification via Progressive Graph Matching
and Alternate Learning</a></h2>
<h1 id="iccv23">iccv23</h1>
<h2 id="towards-grand-unified-representation-learning-for-unsupervised-visible-infrared-person-re-identification无监督"><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Towards_Grand_Unified_Representation_Learning_for_Unsupervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.html">Towards
Grand Unified Representation Learning for Unsupervised Visible-Infrared
Person Re-Identification</a>（无监督）</h2>
<h1 id="acm-mm24">acm mm24</h1>
<h2 id="robust-pseudo-label-learning-with-neighbor-relation-for-unsupervised-visible-infrared-person-re-identification无监督的"><strong>Robust
Pseudo-label Learning with Neighbor Relation for Unsupervised
Visible-Infrared Person Re-Identification</strong>（无监督的）</h2>
<h2 id="enhancing-unsupervised-visible-infrared-person-re-identification-with-bidirectional-consistency-gradual-matching无监督的"><strong>Enhancing
Unsupervised Visible-Infrared Person Re-Identification with
Bidirectional-Consistency Gradual Matching</strong>（无监督的）</h2>
<h1 id="acm-mm23">acm mm23</h1>
<h2 id="efficient-bilateral-cross-modality-cluster-matching-for-unsupervised-visible-infrared-person-reid无监督">Efficient
Bilateral Cross-Modality Cluster Matching for Unsupervised
Visible-Infrared Person ReID（无监督）</h2>
<h2 id="unveiling-the-power-of-clip-in-unsupervised-visible-infrared-person-re-identification无监督">Unveiling
the Power of CLIP in Unsupervised Visible-Infrared Person
Re-Identification（无监督）</h2>
<h2 id="unsupervised-visible-infrared-person-reid-by-collaborative-learning-with-neighbor-guided-label-refinement无监督">Unsupervised
Visible-Infrared Person ReID by Collaborative Learning with
Neighbor-Guided Label Refinement（无监督）</h2>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/09/python%E7%9B%B8%E5%85%B3bugs%E6%88%96%E7%9F%A5%E8%AF%86/pytorch/torch.nn%E4%B8%8Etorch.nn.functional%E7%9A%84%E5%85%B3%E7%B3%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/09/python%E7%9B%B8%E5%85%B3bugs%E6%88%96%E7%9F%A5%E8%AF%86/pytorch/torch.nn%E4%B8%8Etorch.nn.functional%E7%9A%84%E5%85%B3%E7%B3%BB/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-09 01:27:48 / Modified: 01:36:25" itemprop="dateCreated datePublished" datetime="2025-02-09T01:27:48+08:00">2025-02-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>有些东西是既可以用torch.nn.functional来调用，也可以用torch.nn来调用，不禁让人疑惑：这不是多此一举吗？其实它们都是有必要的
结论上来说，写神经网络的时候，一般都是统一用torch.nn来调用（即使像relu这种没有参数的激活函数，用torch.nn.functional调用，虽然效果差不太多，但是为了统一，用torch.nn调用也许是更好的），因为此时调用的是类，它的
参数定义（用torch.nn调用的话，它们都是继承了torch.nn.Module类，所以一旦初始化，就已经有参数了；而用torch.nn.functional调用，则需要自己定义参数，并且在调用的时候手动传入，相比之下复杂许多）
参数管理（比如，用类的时候就可以结合优化器，自动更新参数；而如果是用torch.nn.functional来调用，不仅参数需要我们自己传入，还需要我们自己指定是否要求梯度，并且即使我们指定了要求梯度，优化器也不能实现自动更新参数，必须得我们手动更细，极其麻烦）
设备控制（就是控制在cpu还是gpu/哪块gpu）
模型参数保存（torch.nn类保存参数可以用torch.save(model.state_dict(),
'model.pth')这种语句很快捷地保存，而torch.nn.functional的话，就需要手动保存和加载参数）
等方面都要方便很多</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%B8%80%E8%AE%B2%20%20%E5%BF%AB%E6%8E%92&%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F&%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%B8%80%E8%AE%B2%20%20%E5%BF%AB%E6%8E%92&%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F&%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" class="post-title-link" itemprop="url">chap1.第一讲 快排&归并排序&二分查找</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-04 16:00:00" itemprop="dateCreated datePublished" datetime="2025-02-04T16:00:00+08:00">2025-02-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-13 21:15:25" itemprop="dateModified" datetime="2025-04-13T21:15:25+08:00">2025-04-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algo/" itemprop="url" rel="index"><span itemprop="name">algo</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="快速排序">1.快速排序</h1>
<h2 id="核心思想分治思想">1.1.核心思想：分治思想</h2>
<h2 id="步骤">1.2.步骤：</h2>
<h3 id="对于一个给定的数组选定一个中间值x理论上来说选哪个都行但后面的代码需要注意相应的搭配否则会陷入死循环后面具体讲常见的就是选数组的左边右边中间值随机选">1.2.1.对于一个给定的数组，选定一个中间值x（理论上来说，选哪个都行，但后面的代码需要注意相应的搭配，否则会陷入死循环。后面具体讲），常见的就是选数组的左边/右边/中间值/随机选</h3>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%B8%80%E8%AE%B2%20%20%E5%BF%AB%E6%8E%92&%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F&%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%BA%8C%E8%AE%B2%20%E9%AB%98%E7%B2%BE%E5%BA%A6&%E5%89%8D%E7%BC%80%E5%92%8C&%E5%B7%AE%E5%88%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%BA%8C%E8%AE%B2%20%E9%AB%98%E7%B2%BE%E5%BA%A6&%E5%89%8D%E7%BC%80%E5%92%8C&%E5%B7%AE%E5%88%86/" class="post-title-link" itemprop="url">chap1.第二讲 高精度&前缀和&差分</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-04 16:00:00" itemprop="dateCreated datePublished" datetime="2025-02-04T16:00:00+08:00">2025-02-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-16 17:04:46" itemprop="dateModified" datetime="2025-02-16T17:04:46+08:00">2025-02-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algo/" itemprop="url" rel="index"><span itemprop="name">algo</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="高精度">1.高精度</h1>
<h2 id="简介">1.1.简介</h2>
<h3 id="所谓高精度在这里指的是大数相关的运算且不是每一种语言都需要自己实现这种算法像java和python就不需要大体上是因为它们已经自带这个功能了而算法常用的cc则没有这种功能因此需要自己实现-而这门课讲的高精度主要局限在两个大数相加相减以及一个大数乘以一个小数这里的大数一般指位数很多的正整数lenaleq-1e6小数一般指位数不那么多的正整数lenbleq-9">所谓高精度，在这里指的是大数相关的运算。且不是每一种语言都需要自己实现这种算法。像java和python就不需要（大体上是因为它们已经自带这个功能了）。而算法常用的c/c++则没有这种功能，因此需要自己实现<br>而这门课讲的高精度，主要局限在两个大数相加/相减，以及一个大数乘以一个小数（这里的大数，一般指位数很多的正整数，<span class="math inline">\(len(A)\leq
1e6\)</span>；小数一般指位数不那么多的正整数，<span class="math inline">\(len(B)\leq 9\)</span>）</h3>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/04/algo/%E5%9F%BA%E7%A1%80%E8%AF%BE/chap1.%E7%AC%AC%E4%BA%8C%E8%AE%B2%20%E9%AB%98%E7%B2%BE%E5%BA%A6&%E5%89%8D%E7%BC%80%E5%92%8C&%E5%B7%AE%E5%88%86/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/02/03/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/03/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-03 17:28:21" itemprop="dateCreated datePublished" datetime="2025-02-03T17:28:21+08:00">2025-02-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-30 00:25:39" itemprop="dateCreated datePublished" datetime="2025-01-30T00:25:39+08:00">2025-01-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-10 02:31:27" itemprop="dateModified" datetime="2025-02-10T02:31:27+08:00">2025-02-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="from-cross-modal-to-mixed-modal-visible-infrared-re-identification">From
Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification</h1>
<h2 id="arxiv-2025.1.23">arxiv 2025.1.23</h2>
<h2 id="这篇论文主要是提出一个vi-reid中的新的场景吧就是gallery-images中有两种模态的图像而不止一种模态">这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery
images中有两种模态的图像，而不止一种模态。</h2>
<h2 id="动机经典的vi-reid实验的时候都是query和gallery模态不同那么其实就是一种visible-infraredinfrared-visible的匹配对应到实际情况可能就是白天照片-晚上照片晚上照片-白天照片但更实际的情况应该是匹配的时候可能既有可见光照片也有红外照片所以gallery中有两种模态的图像也就是文中提到的mixed-modal的setting这会带来一个问题就是gallery中同一模态的图像的domain-gap显然会比不同模态的domain-gap要小然而其中同一模态的图像却有可能是属于不同id的这可能导致匹配错误举个具体例子gallery中有id-1的一个红外图像一个可见光图像和一个id-2的可见光图像现在有一个id-2的红外query如果特征学的不够好进行匹配的时候很可能会把query匹配到id-1的那个红外图像上了毕竟都是红外图像本身的gap就比较小而红外和可见光之间的gap可就大了-这篇论文就是想解决这个问题">动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared/infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片/晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain
gap显然会比不同模态的domain
gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id
1的一个红外图像，一个可见光图像，和一个id 2的可见光图像。现在有一个id
2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id
1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。<br>这篇论文就是想解决这个问题</h2>
<h2 id="方法它的大致思想是既然gallery中有两种模态的图像那么匹配的时候就既要用到modality-shared-feature也要用到modality-specific-feature文章提出一种方法把特征分解到两个正交的子空间中一个代表shared-feature一个代表specific-feature好经典的做法和讲故事的套路有没有什么手段能验证可视化这俩确实正交确实代表了shared-feature和specific-feature呢">方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared
feature，也要用到modality-specific
feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared
feature，一个代表specific
feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared
feature和specific feature呢？）</h2>
<h4 id="话说回来在经典的vi-reid中真的有必要用到modality-specific-feature吗在经典设定下不是query和gallery一定不同模态吗那modality-specific-feature只有一方有匹配的时候用不上啊事实上人来进行红外和可见光的匹配的时候也是通过神态和脸来判断的吧服装那些基本用不上啊换装设定是不是也是这个思路">（话说回来，在经典的vi-reid中，真的有必要用到modality-specific
feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific
feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）</h4>
<h2 id="分析这篇文章用到一些互信息的公式来进行推导和阐述因为要用学到的模型生成的embedding来判断id所以要最大化embedding和id之间的标签而考虑到现在的mixed-modality-scenario需要把embedding拆分成两块就是上面提到的modality-shared和-specific-feature或者说原文的modality-erased-related-feature下面就是怎么把这两部分学到的问题论文把这部分转化成一个待优化的式子max_z_mez_mrmiz_mez_mry-s.t.miz_mez_mr0miz_mem0and-miz_mrym0-上面的z_me代表模态m下的modality-erased-feature也就是与模态无关的特征z_mr代表模态m下的modality-related-feature也就是与模态相关的特征模态m可能是visible也可能是infrared三个约束也是保证学到的特征有效的关键第一个约束是想让modality-erased-feature与modality-related-feature相互独立这个是合理的毕竟理论上这两部分的交集是空集并集就是所有的特征信息了第二个约束是因为z_me代表modality-erased-feature所以用这个特征应该推断不出模态信息所以它应该与模态独立也合理第三个约束没搞明白它想干什么按原论文它是希望z_mr不包含id相关的信息但为什么要这样如果真的满足了这个约束z_mr就是只能用来推断模态信息不能用来推断id信息那它和z_me组合起来用又是为什么还是说它是彻底的把模态和id语义信息给分开了分到两个特征里所以推断的时候是怎么推断的是说如果是同一模态的图像来匹配则用的特征是上面提到的modality-erased-feature和modality-related-feature拼接起来的一个特征如果是跨模态的匹配则只用modality-erased-feature但这样不是很麻烦吗得再看看代码了可能-总而言之得到了上面的待优化式子下面就是把它转化成一个近似的式子用损失去刻画从而变成可以用深度学习解决的问题得到结果如下500-且针对四个部分都进行了各自的优化">分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality
scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific
feature（或者说原文的modality-erased/-related
feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：<span class="math display">\[\max_{Z_{m}^{e},Z_{m}^{r}}\{MI(Z_{m}^{e},Z_{m}^{r};Y)\}\
\ s.t.MI(Z_{m}^{e},Z_{m}^{r})=0,MI(Z_{m}^{e};M)=0,and\
MI(Z_{m}^{r};Y|M)=0\]</span><br>上面的<span class="math inline">\(Z_{m}^{e}\)</span>代表模态m下的modality-erased
feature，也就是与模态无关的特征；<span class="math inline">\(Z_{m}^{r}\)</span>代表模态m下的modality-related
feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased
feature与modality-related
feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为<span class="math inline">\(Z_{m}^{e}\)</span>代表modality-erased
feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望<span class="math inline">\(Z_{m}^{r}\)</span>不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，<span class="math inline">\(Z_{m}^{r}\)</span>就是只能用来推断模态信息，不能用来推断ID信息。那它和<span class="math inline">\(Z_{m}^{e}\)</span>组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased
feature和modality-related
feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased
feature（但这样不是很麻烦吗？得再看看代码了可能）<br>总而言之，<strong>得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题</strong>。得到结果如下：<img src="/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1/泛读/pic/cross-modal.png" alt="|500"><br>且针对四个部分，都进行了各自的优化</h2>
<h2 id="评价虽然这篇论文乍一看很烂特别是摘要那里还把数据集的名字打错了更是降低印象分可是看完以后一来它提出的新场景也不算空中楼阁确实是更有实际意义的二来它提出方法以后对每一部分的刻画和阐述都还算合理有一点娓娓道来的感觉都是就事论事的分析">评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析</h2>
<h2 id="trick总结">trick总结：</h2>
<h3 id="想让两个向量尽可能独立可以用互信息为0来进行理论上的刻画虽说其实可能是有失偏颇的然后用正交损失来近似代替">1.想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替</h3>
<h3 id="如果要最大化某个互信息那么可以从语义上去理解给出相应的损失比如论文里提到的要最大化miz_mey即要最大化modality-erased-feature与标签变量之间的互信息其实目的就是让modality-erased-feature能尽可能揭示出id信息所以用一个交叉熵的分类损失就是理所应当的了可以认为是对常见的交叉熵分类损失给出一个理论上的解释">2.如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化<span class="math inline">\(MI(Z_{m}^{e};Y)\)</span>，即要最大化modality-erased
feature与标签变量之间的互信息，其实目的就是让modality-erased
feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）</h3>
<h3 id="要让模型混淆某几个部分这可能是出于模型设计的考虑可以考虑进行分类但加上梯度反转层">3.要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层</h3>
<h3 id="要约束某两个量xy尽可能接近可以考虑给出这个形式的损失lmaxx-yalpha0maxy-xalpha0alpha是正常数也是超参数那么x和y必须相差绝对值不大于alpha这部分损失才为0否则就产生损失了">4.要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：<span class="math inline">\(L=max(x-y+\alpha,0)+max(y-x+\alpha,0)\)</span>，<span class="math inline">\(\alpha\)</span>是正常数，也是超参数。那么，x和y必须相差绝对值不大于<span class="math inline">\(\alpha\)</span>，这部分损失才为0，否则就产生损失了</h3>
<h3 id="这个trick目前看下来好像还是比较局限的就是针对跨模态行人重识别的时候才会用到就是如果要学习modality-specific-feature且每个id都有两种模态的数据则可以考虑把每一个id给doubled比如如果它是visible的图像则id-2如果它是infrared的图像则idid-2-1基于这一套新的标签进行分类并用交叉熵分类损失来优化这里之所以能起到效果是因为我们把同一id的不同模态的数据给视为不同标签相当于进行更细粒度的分类模型如果要正确分类就必须学会捕获同一id下visible和infrared图像的各自的有判别力的特征这样模型就能学会提取出modality-specific-feature当然这个trick也并不唯一肯定是还有别的方法的就比如再用个损失约束同一id的不同模态的数据要尽可能远离有点对比学习的意思通过让不同类别的数据尽可能远离让模型潜移默化地学会了语义信息应该也能学会提取modality-specific-feature">5.这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific
feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID
* 2；如果它是infrared的图像，则ID=ID * 2 +
1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific
feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific
feature</h3>
<h1 id="spectral-enhancement-and-pseudo-anchor-guidance-for-infrared-visible-person-re-identification">Spectral
Enhancement and Pseudo-Anchor Guidance for Infrared-Visible Person
Re-Identification</h1>
<h2 id="arxiv-2025.1.2">arxiv, 2025.1.2</h2>
<h2 id="这篇文章主要是从谱域的角度来缓解两种模态的巨大差异但它只是直接指出这两种模态的数据有巨大的spectral-gap却并没有分析为什么有这种gap或者说我们怎么确认这种gap确实存在而不只是说说而已有种拿到谱域方法就来试一下的感觉">这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral
gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）</h2>
<h2 id="动机">动机：</h2>
<h2 id="方法">方法：</h2>
<h2 id="分析第一步是生成一个semantically-enhanced-grey-images姑且可以认为是一种拉近两种模态的数据的方法只不过是从可见光图像出发往红外上靠拢也合理毕竟可见光图像信息更多红外图像信息更少从信息更多的往信息更少的变换是比较容易的它主要做的其实是在很常见的灰度映射之余加上了高频信息具体来说它会先对可见光图像进行一个傅里叶变换提取它的频域信息然后用其中的频率成分进行傅里叶逆变换这里是为了提取轮廓信息然后将逆变换的结果与灰度映射的结果相加得到semantically-enhanced-grey-images下简称seg图像btw这个方法倒是可以借鉴一下感觉还是行得通的起码可以试试用到我们的方法里就是加上傅里叶变换之后可能计算量大了-第二步是对红外图像和seg图像进行进一步的特征提取也是为了进一步缩小两种模态的数据之间的差异因为最后还是要用特征进行相似度计算的这里其实平平无奇-就是用resnet-50的前三块共享权重来提取共同的特征之后又用resnet-50的最后一个块不同权重来分别提取modality-specific-feature-第三步是paba-loss即pseudo-anchor-guided-bidirectional-aggregation-loss它的目的是对于前面的共同特征希望它们能更加兼容也就是尽可能的变换到一个空间里但也不能就失去判断力了而所谓anchor-guided其实就是说用向anchor靠拢来实现两种数据的靠拢举个具体例子我有同一id的两种模态的若干数据我希望把它们变换到一个统一的空间里则可以考虑求一下可见光的一个anchor比如所有可见光数据的均值然后拉近所有红外数据与这个anchor的距离从而实现红外数据与可见光数据的靠拢说回这里它是对上面提到的共同特征f_sharedseg和f_sharedir先进行一个分块分成n块之后对每个块都施加paba-loss拉近同一id的不同模态的数据的距离同时拉远不同id的不同模态的数据的距离相比于常见的triplet-loss这里增加了跨模态的考虑而相比于cross-center-loss这里的改进在于分了块且是对每个块都施加paba-loss故更加细粒度这里其实要结合公式来看才清楚-除了上面提到的paba-loss经典的交叉熵分类损失当然也是不能少的针对specific和shared-feature都有这个分类损失三部分损失加权求和即得到总的损失权重是超参">分析：第一步是生成一个Semantically
Enhanced Grey
Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically
Enhanced Grey
Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了<br><br>第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇，
就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific
feature<br><br>第三步是PABA loss，即Pseudo Anchor-guided Bidirectional
Aggregation
Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征<span class="math inline">\(F_{shared}^{seg}\)</span>和<span class="math inline">\(F_{shared}^{ir}\)</span>，先进行一个分块，分成N块，之后对每个块都施加PABA
loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet
loss，这里增加了跨模态的考虑；而相比于cross-center
loss，这里的改进在于分了块，且是对每个块都施加PABA
loss，故更加细粒度（这里其实要结合公式来看才清楚）<br>除了上面提到的PABA
loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared
feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）</h2>
<h1 id="embedding-and-enriching-explicit-semantics-for-visible-infrared-person-re-identification">Embedding
and Enriching Explicit Semantics for Visible-Infrared Person
Re-Identification</h1>
<h2 id="arxiv2024.12.11">arxiv，2024.12.11</h2>
<h2 id="这篇文章的方法看起来有点复杂然后动机上看感觉也有些牵强更多是为了用上现在很火的大模型所以硬凑了个理由">这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由</h2>
<h2 id="insight利用了一下text用文本信息来直接补充一部分图像的语义信息可能会比单纯用图像encoder来提取语义信息要好一点提出若干损失来利用多视角信息但是否合理有待考察至少看它的消融实验结果证明了这部分是有效的">insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）</h2>
<h2 id="动机文章指出现在的vi-reid方法有两大类一类是基于生成式模型的方法效果不够好个人不太了解一类是基于判别式模型的方法也就是用模型把图像进行embed用embedding来进行匹配通常需要各种魔改网络架构这种方法的效果会更好也更主流而文章指出这种方法仅利用图像对语义信息的提取不够因此希望加入文本描述来帮助更好的提取文本信息就是这一点让我感觉略有点牵强它所谓的文本描述也是人llm基于图像进行的描述语义信息本身就是来自图像的至于说提取的不够可以认为是网络架构不够好数据不够多等问题导致的加入文本描述更多可能是希望能让模型能比较容易地学到语义信息不过考虑到现在reid的骨干基本都是一个resnet50模型容量应该是有上限的那么用文字embedding来辅助一下也有道理">动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人/llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）</h2>
<h2 id="方法-1">方法：</h2>
<h3 id="提出一个eees框架包含三个部分各司其职-第一个是ese模块explicit-semantics-embedding它的目的是利用文本让图像embedding学到更多语义信息主要做的是引入vllm为图像生成文本描述然后用对比学习拉近图像embedding和对应的文本描述的embedding让图像embedding学到文本的语义信息">提出一个EEES框架，包含三个部分，各司其职<br>第一个是ESE模块（Explicit
Semantics
Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息</h3>
<h3 id="第二个是cvsccross-view-semantics-compensation它的目的是利用多视角的图像让模型能提取出更加丰富的语义信息举个例子对于同一个人一个摄像头视角拍到的照片所蕴含的信息终归是有限的可能这个视角下的照片能拍到身材轮廓脸却被遮住了这时如果能用到其它视角下这个人的照片可能就能获得脸部信息了综合起来就能提取出这个人的更多特征所以训练的时候对于一个batch里的每个图像这套方法都会去利用同id在其它视角下的图像而具体的利用方法就是在训练时对一个batch中的每个图像都去随机取m个同id的不同摄像头下的图像然后求个平均值共m1个图像我们就认为这个均值图像里蕴含了多视角的信息对于文本也是进行类似的操作从而得到蕴含了多视角信息的文本embedding当然都必须是同模态下的就是说可见光图像要去找同id的不同摄像头下的其它可见光图像其余类似之后会用一个对齐损失去双向约束多视角信息下的图像embedding和文字embedding毕竟即使是综合了多视角信息对于同一个人的图像embedding和文字embedding依然应该尽可能相近确保图像学到更多合理的语义信息-然而实际测试的时候我们都是拿到一个图像就要去进行匹配了也就是说没有多视角的信息可以利用上面也提到了是训练时可以这样利用多视角信息那为了弥补这一缺点论文提出进行一个蒸馏就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding文字embedding类似两种模态都进行这个操作所以这个蒸馏损失就是四项了公式如下500">第二个是CVSC（Cross-View
Semantics
Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共<span class="math inline">\(M+1\)</span>个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息<br>然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是<strong>训练</strong>时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下：<img src="/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1/泛读/pic/distillation_loss.png" alt="|500"></h3>
<h3 id="第三个是cmsp模块cross-modality-semantics-purification它的目的是避免学到的语义发生冲突具体来说同一个id会有灰度图和可见光图像也会有对应的灰度图的文字描述和可见光图像的文字描述而这两种文字描述可能发生冲突比如llm针对可见光图像可能可以准确地描述出人穿的衣服的颜色也许是蓝色而拿到灰度图像全部都是会的它可能就会输出人穿的衣服的颜色是灰的这无疑会造成矛盾所以我们希望对两种模态的文字描述可以不同互补当然是好现象但不能矛盾因此论文提出一个cmsp-loss公式如下l_cmspfrac1nsum_i1nd_ivv-d_ivr2frac1nsum_i1nd_irr-d_irv2-其中d_ivvlvert-f_iv-t_ivrvert_2d_ivrlvert-f_iv-t_irrvert_2其余定义类似可以看到我们的目的是让这个损失约束同一id下的红外图像文字描述要尽可能接近它的可见光图像的文字描述至于为什么模型不会塌缩直接让红外图像文字描述与可见光图像文字描述完全一样可能是因为前面的对比损失分类损失约束了这些文字embedding让它们必须有合理的语义信息吧">第三个是CMSP模块（Cross-Modality
Semantics
Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp
loss，公式如下：<span class="math display">\[L_{cmsp}=\frac{1}{N}\sum_{i=1}^{N}{(d_{i}^{vv}-d_{i}^{vr})}^{2}+\frac{1}{N}\sum_{i=1}^{N}{(d_{i}^{rr}-d_{i}^{rv})}^{2}\]</span><br>其中，<span class="math inline">\(d_{i}^{vv}=\lVert
f_{i}^{v}-t_{i}^{v}\rVert_{2}\)</span>，<span class="math inline">\(d_{i}^{vr}=\lVert
f_{i}^{v}-t_{i}^{r}\rVert_{2}\)</span>，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧</h3>
<h2 id="分析">分析</h2>
<h3 id="第一部分是稀松平常的但好像也就第一部分看起来靠点谱">第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。</h3>
<h3 id="第二部分想法是挺好的但一来训练的时候凭什么说求个均值图像就蕴含了多视角的信息或者说这种方法是不是太简单粗暴了还有测试的时候那个蒸馏就是说希望单视角下的embedding能尽可能接近多视角下的embedding可见光红外图像文字也就是说我们竟然是希望模型拿到一个单视角的图像就能脑补出多视角下的图像信息这合理吗只能说有一些合理之处毕竟人拿到一张图像确实能靠空间想象力脑补出一些东西但也有些不合理之处比如上面的脸部遮挡例子这个是无论如何也不能脑补出来的逼迫模型去学这个也是不合理的难说最后学出来的是什么可能过拟合了">第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光/红外，图像/文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）</h3>
<h3 id="第三部分同样出发点倒也没错就是这个损失看起来有些奇怪以cmsp-loss的第一部分为例其实我们可以说这个损失是在以可见光图像的embedding为锚点让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近我猜想作者的目的是让两种文字embedding尽可能接近这样就相当于说让文字embedding尽可能提取出共性的语义信息而不会去提取出矛盾的一方独有的信息如果真是这样它就应该直接约束两个文字embedding了没必要掺和上图像embedding但几何上看要让损失最小它们也可以在一个以可见光图像为圆心的某个圆上只不过是高维的此时它们未必就距离很近这样得到的两种embedding有什么含义呢不能认为说在同一个圆周上语义信息就不矛盾了吧有点怪或者我们认为文字embedding不动优化的是图像embedding这样最理想的情况是图像embedding落在了两种文字embedding的中垂线上了这样就能认为图像embedding没有学到什么矛盾的语义信息吗还是没道理啊又或者上面的分析都是假定了一种embedding是不动的文字图像实际是一起优化的那它想优化什么呢还是不太明白">第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp
loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的/一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字/图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。</h3>
<h1 id="prototype-driven-multi-feature-generation-for-visible-infrared-person-re-identification">Prototype-Driven
Multi-Feature Generation for Visible-Infrared Person
Re-identification</h1>
<h2 id="icassp2024.9.9">ICASSP，2024.9.9</h2>
<h2 id="动机其实感觉不太明确似乎是魔改完有效果了就开始讲故事了">动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了</h2>
<h2 id="insight提出一个mfgm来获取多样特征但其实不是很新奇了跟deen差不多的思路就是多几个分支加上一些损失约束让这些生成的特征不太一样用一个prototype来提取特征感觉本质只是11的卷积跨模态用同一套模板然后又用一个损失来约束模板提取不同的特征和上面约束生成特征的差不多这里只是换了个cos的套子拉近同id特征拉远不同id特征来确保特征的语义信息的正确性">insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性</h2>
<h2 id="动机-1">动机：</h2>
<h2 id="方法-2">方法：</h2>
<h3 id="提出的pdm框架主要是两个组成部分分别是multi-feature-generation-modulemfgm用于生成更多多样的特征参考deen和prototype-learning-moduelplm">提出的PDM框架，主要是两个组成部分，分别是Multi-Feature
Generation Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype
Learning Moduel（PLM）</h3>
<h3 id="mfgm感觉真的和deen很像都是用几个分支生成多几个特征然后分支上都有dilation-convolution之后又用一些损失来把某些特征拉远以实现多样化特征这里的损失就是文中的center-guided-pair-mining-loss">MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation
convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided
Pair Mining Loss</h3>
<h3 id="plm的话可能是我论文看少了相对没那么常见但它这里提出prototype也不是一般的为了保留知识而是为了提取知识它的大概思想和cnn里的卷积核差不多吧因为对卷积核的一种理解就是它学到了一些特定的模式然后用卷积操作来进行模式匹配这里提取知识也是类似的用一组可学习的prototype类比卷积核来和图像进行哈达玛积再求均值类比卷积操作并且是两种模态的数据用同一组prototype所以以此来促进跨模态的共性知识提取但怎么保证prototype提取的就一定是局部信息呢仅仅是因为prototype的形状是cchannel维的向量吗这样的话倒是能稍微解释一下为什么不直接用一个cnn了或者说这其实就是一种特殊的cnn只不过是11卷积了这么说来其实还是讲故事啊">PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）</h3>
<h3 id="此外为了保证它的不同prototype提取出的特征也尽可能多样提出了cosine-heterogeneity-loss其实很老套了跟欧氏距离差不多只不过这里换成了用cos还有一个dual-center-seperation-loss这个损失似乎并没有说明它解决了什么问题感觉很可能只是为了提点而已本身它的思想也很常见了就是换个法子让同id的特征近一点不同id的特征远一点">此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine
Heterogeneity
Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center
Seperation
Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）</h3>
<h1 id="parameter-hierarchical-optimization-for-visible-infrared-person-re-identification">Parameter
Hierarchical Optimization for Visible-Infrared Person
Re-Identification</h1>
<h2 id="arxiv2024.4.11">arxiv，2024.4.11</h2>
<h2 id="好像确实有点新奇把参数划分为两种类型一种是正常在深度学习中用梯度反传来优化一种是基于一些规则来优化应该是比较传统的那种以此来减少需要梯度反传来优化的参数这个是可以减少一些计算量但其它好处呢基于规则来优化效果有保证吗原理是什么还提出一个sas可以把红外可见光两种模态的图像相互转化这个倒是可以看看最后还有一个一致性学习估摸着是提出某个损失又是拉近同id的不同模态的特征之间的距离回头看看猜对没">好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外/可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没</h2>
<h1 id="bidirectional-multi-step-domain-generalization-for-visible-infrared-person-re-identification">Bidirectional
Multi-Step Domain Generalization for Visible-Infrared Person
Re-Identification</h1>
<h2 id="arxiv2024.3.16">arxiv，2024.3.16</h2>
<h2 id="这篇论文想解决改善的点在于大部分方法都是把vis和ir-images投影到一个公共空间然后用这个空间里的embedding进行相似度匹配这种方法叫所谓单中间域生成而论文提到这个方法提取的信息不够好因为它可能提取出了一些公共的背景信息而这部分信息是没用的所以从结果上来说就是单单投影到同一空间效果不够好至于说提取出背景信息不好判断是讲故事还是说真的主要没有实验来辅佐验证论文提出的方法bmdgbidirectional-multi-step-domain-generalization主要由两部分组成一是part-prototype-alignment-learning负责提取出局部的身体部位信息二是bidirectional-multi-step-learning通过多步学习从红外和可见光分别出发以减小modality-gap">这篇论文想解决/改善的点在于，大部分方法都是把vis和ir
images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional
Multi-Step Domain Generalization）主要由两部分组成，一是part prototype
alignment learning（负责提取出局部的身体部位信息）；二是bidirectional
multi-step
learning（通过多步学习，从红外和可见光分别出发，以减小modality
gap）</h2>
<h2 id="insight">insight</h2>
<h2 id="动机-2">动机</h2>
<h2 id="方法-3">方法</h2>
<h3 id="part-prototype-alignment-learning主要由三个部分组成">part
prototype alignment learning主要由三个部分组成：</h3>
<h4 id="prototype-discovery这个部分是用来获取身体部位细节信息的并且要在feature-map上挖掘出具有判别性的位置具体来说它的一个prototype的形状是">1.prototype
discovery（这个部分是用来获取身体部位细节信息的，并且要在feature
map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是</h4>
<h1 id="clip-driven-semantic-discovery-network-for-visible-infrared-person-re-identification">CLIP-Driven
Semantic Discovery Network for Visible-Infrared Person
Re-Identification</h1>
<h2 id="tmm2024.1.11">TMM，2024.1.11</h2>
<h2 id="这篇文章又是引入clip好多文章引入了clip看来单纯做vi-reid还在魔改网络的话可能上限不太高了而且太卷了又不创新所以试着引入clip提高能力也显得创新一点文章的大体的亮点出发点在于利用clip的获取图像语义信息的能力提取一些高层语义信息用来辅助检索图像这个和前面eees那篇embedding-and-enriching-explicit-semantics-for-visible-infrared-person-re-identification的出发点有点类似">这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点/出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding
and Enriching Explicit Semantics for Visible-Infrared Person
Re-Identification）的出发点有点类似</h2>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">152</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">75</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
