<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models（多模态的，不过是audio相关的） REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark（多模态的，但是benchmark） VideoRAG: Retrie">
<meta property="og:type" content="article">
<meta property="og:title" content="bluemouse&#39;s blog">
<meta property="og:url" content="https://blueeemouse.github.io/2025/09/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4%E5%A4%9A%E6%A8%A1%E6%80%81RAG%20paper%E8%B0%83%E7%A0%94/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models（多模态的，不过是audio相关的） REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark（多模态的，但是benchmark） VideoRAG: Retrie">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-09-11T13:20:10.563Z">
<meta property="article:modified_time" content="2025-09-11T13:35:05.524Z">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/2025/09/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4%E5%A4%9A%E6%A8%A1%E6%80%81RAG%20paper%E8%B0%83%E7%A0%94/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title> | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4%E5%A4%9A%E6%A8%A1%E6%80%81RAG%20paper%E8%B0%83%E7%A0%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-09-11 21:20:10 / Modified: 21:35:05" itemprop="dateCreated datePublished" datetime="2025-09-11T21:20:10+08:00">2025-09-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="wavrag-audio-integrated-retrieval-augmented-generation-for-spoken-dialogue-models多模态的不过是audio相关的"><strong>WavRAG:
Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue
Models</strong>（多模态的，不过是audio相关的）</h3>
<h3 id="real-mm-rag-a-real-world-multi-modal-retrieval-benchmark多模态的但是benchmark"><strong>REAL-MM-RAG:
A Real-World Multi-Modal Retrieval
Benchmark</strong>（多模态的，但是benchmark）</h3>
<h3 id="videorag-retrieval-augmented-generation-over-video-corpus多模态的video相关"><strong>VideoRAG:
Retrieval-Augmented Generation over Video
Corpus</strong>（多模态的）（video相关）</h3>
<h3 id="mes-rag-bringing-multi-modal-entity-storage-and-secure-enhancements-to-rag多模态的"><strong>MES-RAG:
Bringing Multi-modal, Entity-Storage, and Secure Enhancements to
RAG</strong>（多模态的）</h3>
<h3 id="unirag-universal-retrieval-augmentation-for-large-vision-language-models多模态的关于vlm"><strong>UniRAG:
Universal Retrieval Augmentation for Large Vision Language
Models</strong>（多模态的，关于VLM）</h3>
<h3 id="rule-reliable-multimodal-rag-for-factuality-in-medical-vision-language-models多模态的但是医学相关的"><strong>RULE:
Reliable Multimodal RAG for Factuality in Medical Vision Language
Models</strong>（多模态的，但是医学相关的）</h3>
<h2 id="poisonedeye-knowledge-poisoning-attack-on-retrieval-augmented-generation-based-large-vision-language-models多模态的关于vlm"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46373">PoisonedEye: Knowledge
Poisoning Attack on Retrieval-Augmented Generation based Large
Vision-Language Models</a>（多模态的）（关于VLM）</h2>
<h2 id="she-streaming-media-hashing-retrieval疑似多模态的"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">SHE: Streaming-media
Hashing Retrieval</a>（疑似多模态的）</h2>
<h2 id="qure-query-relevant-retrieval-through-hard-negative-sampling-in-composed-image-retrieval多模态的composed-image-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43476">QuRe: Query-Relevant
Retrieval through Hard Negative Sampling in Composed Image
Retrieval</a>（多模态的，composed Image retrieval）</h2>
<h2 id="learning-attribute-aware-hash-codes-for-fine-grained-image-retrieval-via-query-optimization多模态的又是image-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43868">Learning
Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query
Optimization</a>（多模态的）（又是Image retrieval）</h2>
<h2 id="visual-abstraction-a-plug-and-play-approach-for-text-visual-retrieval多模态的text-visual-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">Visual Abstraction: A
Plug-and-Play Approach for Text-Visual
Retrieval</a>（多模态的）（text-visual retrieval）</h2>
<h2 id="docks-rag-optimizing-document-level-relation-extraction-through-llm-enhanced-hybrid-prompt-tuning多模态的文档类"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45220">DocKS-RAG: Optimizing
Document-Level Relation Extraction through LLM-Enhanced Hybrid Prompt
Tuning</a>（多模态的，文档类）</h2>
<h2 id="retrieval-augmented-perception-high-resolution-image-perception-meets-visual-rag多模态的王中王神中神high-resolution-image-perception"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44979">Retrieval-Augmented
Perception: High-resolution Image Perception Meets Visual
RAG</a>（多模态的，王中王，神中神）（High-resolution Image
Perception）</h2>
<h2 id="realrag-retrieval-augmented-realistic-image-generation-via-self-reflective-contrastive-learning也算多模态的吧结合rag与diffusion"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44615">RealRAG:
Retrieval-augmented Realistic Image Generation via Self-reflective
Contrastive Learning</a>（也算多模态的吧，结合RAG与diffusion）</h2>
<h2 id="colpali-efficient-document-retrieval-with-vision-language-models优先多模态的关于vlm的">-
ColPali: Efficient Document Retrieval with Vision Language
Models（优先）（多模态的）（关于VLM的）</h2>
<h2 id="visrag-vision-based-retrieval-augmented-generation-on-multi-modality-documents优先多模态的visual-rag">-
VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality
Documents（优先）（多模态的）（visual RAG）</h2>
<h2 id="ra-tta-retrieval-augmented-test-time-adaptation-for-vision-language-models多模态的关于vlm的tta">RA-TTA:
Retrieval-Augmented Test-Time Adaptation for Vision-Language
Models（多模态的）（关于VLM的TTA）</h2>
<h2 id="streaming-video-question-answering-with-in-context-video-kv-cache-retrieval多模态的video-qa">-
Streaming Video Question-Answering with In-context Video KV-Cache
Retrieval（多模态的）（video qa）</h2>
<h2 id="mai-a-multi-turn-aggregation-iteration-model-for-composed-image-retrieval多模态的composed-image-retrieval">MAI:
A Multi-turn Aggregation-Iteration Model for Composed Image
Retrieval（多模态的）（composed Image retrieval）</h2>
<h2 id="bridging-information-asymmetry-in-text-video-retrieval-a-data-centric-approach多模态的text-video-retrieval">Bridging
Information Asymmetry in Text-video Retrieval: A Data-centric
Approach（多模态的）（text-video retrieval）</h2>
<h2 id="benchmarking-multimodal-retrieval-augmented-generation-with-dynamic-vqa-dataset-and-self-adaptive-planning-agent算多模态的涉及vqa了但是是benchmark类的论文">Benchmarking
Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and
Self-adaptive Planning
Agent（算多模态的？涉及VQA了。但是是benchmark类的论文）</h2>
<h2 id="retrieval-augmented-diffusion-model-for-structure-informed-antibody-design-and-optimization算多模态的涉及diffusionwc是ai4sci的吧都有抗体了算了">Retrieval
Augmented Diffusion Model for Structure-informed Antibody Design and
Optimization（算多模态的？涉及diffusion）（wc，是ai4sci的吧，都有抗体了，算了）</h2>
<h2 id="mm-embed-universal-multimodal-retrieval-with-multimodal-llms多模态的">MM-EMBED:
Universal Multimodal Retrieval with Multimodal LLMs（多模态的）</h2>
<h2 id="tiger-unifying-text-to-image-generation-and-retrieval-with-large-multimodal-models多模态的好像是在做文生图和检索的统一工作">TIGeR:
Unifying Text-to-Image Generation and Retrieval with Large Multimodal
Models（多模态的）（好像是在做文生图和检索的统一工作）</h2>
<h2 id="mrag-bench-vision-centric-evaluation-for-retrieval-augmented-multimodal-models多模态的只不过是benchmark类的">MRAG-Bench:
Vision-Centric Evaluation for Retrieval-Augmented Multimodal
Models（多模态的，只不过是benchmark类的）</h2>
<h2 id="learning-fine-grained-representations-through-textual-token-disentanglement-in-composed-video-retrieval多模态的composed-video-retrieval">Learning
Fine-Grained Representations through Textual Token Disentanglement in
Composed Video Retrieval（多模态的）（composed video retrieval）</h2>
<h2 id="tempme-video-temporal-token-merging-for-efficient-text-video-retrieval多模态的text-video-retrieval">TempMe:
Video Temporal Token Merging for Efficient Text-Video
Retrieval（多模态的）（text-video retrieval）</h2>
<h2 id="mllm-as-retriever-interactively-learning-multimodal-retrieval-for-embodied-agents多模态的不过似乎是应用到具身方面">MLLM
as Retriever: Interactively Learning Multimodal Retrieval for Embodied
Agents（多模态的，不过似乎是应用到具身方面？）</h2>
<h2 id="generalized-video-moment-retrieval多模态的video相关的似乎具体是moment相关">Generalized
Video Moment
Retrieval（多模态的）（video相关的，似乎具体是moment相关？）</h2>
<h2 id="exploiting-distribution-constraints-for-scalable-and-efficient-image-retrieval多模态的image-retrieval">Exploiting
Distribution Constraints for Scalable and Efficient Image
Retrieval（多模态的）（Image Retrieval）</h2>
<h2 id="rapid-retrieval-augmented-training-of-differentially-private-diffusion-models算多模态的涉及diffusion-models">RAPID:
Retrieval Augmented Training of Differentially Private Diffusion
Models（算多模态的？涉及diffusion models）</h2>
<h2 id="g-retriever-retrieval-augmented-generation-for-textual-graph-understanding-and-question-answering"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95524">G-Retriever:
Retrieval-Augmented Generation for Textual Graph Understanding and
Question Answering</a></h2>
<h2 id="an-end-to-end-graph-attention-network-hashing-for-cross-modal-retrieval"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95267">An End-To-End Graph
Attention Network Hashing for Cross-Modal Retrieval</a></h2>
<h2 id="wheres-waldo-diffusion-features-for-personalized-segmentation-and-retrieval"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95609">Where's Waldo:
Diffusion Features For Personalized Segmentation and Retrieval</a></h2>
<h2 id="aligning-vision-models-with-human-aesthetics-in-retrieval-benchmarks-and-algorithmsbenchmark类啊"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/93151">Aligning Vision Models
with Human Aesthetics in Retrieval: Benchmarks and
Algorithms</a>（benchmark类啊）</h2>
<h2 id="diffusion-inspired-truncated-sampler-for-text-video-retrieval"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95072">Diffusion-Inspired
Truncated Sampler for Text-Video Retrieval</a></h2>
<h2 id="inquire-a-natural-world-text-to-image-retrieval-benchmark又是benchmark类"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97543">INQUIRE: A Natural
World Text-to-Image Retrieval Benchmark</a>（又是benchmark类）</h2>
<h2 id="bivlc-extending-vision-language-compositionality-evaluation-with-text-to-image-retrieval"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97657">BiVLC: Extending
Vision-Language Compositionality Evaluation with Text-to-Image
Retrieval</a></h2>
<h2 id="verified-a-video-corpus-moment-retrieval-benchmark-for-fine-grained-video-understanding依然benchmark"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97632">VERIFIED: A Video
Corpus Moment Retrieval Benchmark for Fine-Grained Video
Understanding</a>（依然benchmark）</h2>
<h2 id="wikido-a-new-benchmark-evaluating-cross-modal-retrieval-for-vision-language-modelsbenchmark-again"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97785">WikiDO: A New Benchmark
Evaluating Cross-Modal Retrieval for Vision-Language
Models</a>（benchmark again）</h2>
<h2 id="retrieval-fine-tuning-for-in-context-tabular-models似乎是研究表格型数据的算吗"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/96776">Retrieval &amp;
Fine-Tuning for In-Context Tabular
Models</a>（似乎是研究表格型数据的？算吗？）</h2>
<h2 id="uda-a-benchmark-suite-for-retrieval-augmented-generation-in-real-world-document-analysis研究document-analysis的"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97735">UDA: A Benchmark Suite
for Retrieval Augmented Generation in Real-World Document
Analysis</a>（研究document analysis的）</h2>
<h2 id="semi-open-3d-object-retrieval-via-hierarchical-equilibrium-on-hypergraph有点杂啊检索3d物体"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/96281">Semi-Open 3D Object
Retrieval via Hierarchical Equilibrium on
Hypergraph</a>（有点杂啊，检索3D物体）</h2>
<h2 id="assembly-fuzzy-representation-on-hypergraph-for-open-set-3d-object-retrieval我去还有一篇吗研究3d-retrieval的"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/93088">Assembly Fuzzy
Representation on Hypergraph for Open-Set 3D Object
Retrieval</a>（我去，还有一篇吗，研究3D retrieval的）</h2>
<h2 id="drvideo-document-retrieval-based-long-video-understandingvideo-understanding"><strong>DrVideo:
Document Retrieval Based Long Video Understanding</strong>（video
understanding）</h2>
<h2 id="salova-segment-augmented-long-video-assistant-for-targeted-retrieval-and-routing-in-long-form-video-analysisvideo相关应该是长视频理解"><strong><a target="_blank" rel="noopener" href="https://ivy-lvlm.github.io/SALOVA/">SALOVA: Segment-Augmented Long
Video Assistant for Targeted Retrieval and Routing in Long-Form Video
Analysis</a></strong>（video相关，应该是长视频理解）</h2>
<h2 id="vlog-video-language-models-by-generative-retrieval-of-narration-vocabularyvideo相关"><strong>VLog:
Video-Language Models by Generative Retrieval of Narration
Vocabulary</strong>（video相关）</h2>
<h2 id="collm-a-large-language-model-for-composed-image-retrievalcomposed-image-retrieval"><a target="_blank" rel="noopener" href="https://collm-cvpr25.github.io/">CoLLM: A Large Language Model for
Composed Image Retrieval</a>（Composed Image retrieval）</h2>
<h2 id="search-and-detect-training-free-long-tail-object-detection-via-web-image-retrieval有点杂物体检测用到了image-retrieval技术"><strong>Search
and Detect: Training-Free Long Tail Object Detection via Web-Image
Retrieval</strong>（有点杂，物体检测，用到了Image Retrieval技术）</h2>
<h2 id="learning-audio-guided-video-representation-with-gated-attention-for-video-text-retrievalaudio相关"><strong>Learning
Audio-guided Video Representation with Gated Attention for Video-Text
Retrieval</strong>（audio相关）</h2>
<h2 id="lamra-large-multimodal-model-as-your-advanced-retrieval-assistantvlm相关"><a target="_blank" rel="noopener" href="https://code-kunkun.github.io/LamRA/">LamRA: Large Multimodal
Model as Your Advanced Retrieval Assistant</a>（VLM相关）</h2>
<h2 id="recurrence-enhanced-vision-and-language-transformers-for-robust-multimodal-document-retrievalvlm相关以及文档相关"><a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT">Recurrence-Enhanced
Vision-and-Language Transformers for Robust Multimodal Document
Retrieval</a>（VLM相关，以及文档相关）</h2>
<h2 id="missing-target-relevant-information-prediction-with-world-model-for-accurate-zero-shot-composed-image-retrievalcomposed-image-retrieval"><strong>Missing
Target-Relevant Information Prediction with World Model for Accurate
Zero-Shot Composed Image Retrieval</strong>（composed Image
retrieval）</h2>
<h2 id="ilias-instance-level-image-retrieval-at-scaleimage-retrieval"><a target="_blank" rel="noopener" href="https://vrg.fel.cvut.cz/ilias/">ILIAS: Instance-Level Image
retrieval At Scale</a>（Image retrieval）</h2>
<h2 id="clip-is-almost-all-you-need-towards-parameter-efficient-scene-text-retrieval-without-ocrscene-text-retrieval"><strong>CLIP
is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval
without OCR</strong>（scene-text retrieval）</h2>
<h2 id="rethinking-noisy-video-text-retrieval-via-relation-aware-alignmentvideo相关video-text-retrieval只不过多了一个noisy设定这个感觉真好灌水吧"><strong>Rethinking
Noisy Video-Text Retrieval via Relation-aware
Alignment</strong>（video相关，video-text
retrieval，只不过多了一个noisy设定。这个感觉真好灌水吧……）</h2>
<h2 id="bridging-modalities-improving-universal-multimodal-retrieval-by-multimodal-large-language-modelsvlm相关"><a target="_blank" rel="noopener" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct">Bridging
Modalities: Improving Universal Multimodal Retrieval by Multimodal Large
Language Models</a>（VLM相关）</h2>
<h2 id="reason-before-retrieve-one-stage-reflective-chain-of-thoughts-for-training-free-zero-shot-composed-image-retrievalcomposed-image-retrieval"><a target="_blank" rel="noopener" href="https://github.com/Pter61/osrcir">Reason-before-Retrieve:
One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot
Composed Image Retrieval</a>（Composed Image Retrieval）</h2>
<h2 id="vdocrag-retrieval-augmented-generation-over-visually-rich-documents这篇明确出现rag了啊document相关"><a target="_blank" rel="noopener" href="https://vdocrag.github.io/">VDocRAG: Retrieval-Augmented
Generation over Visually-Rich
Documents</a>（这篇明确出现RAG了啊）（document相关）</h2>
<h2 id="rap-retrieval-augmented-personalization-for-multimodal-large-language-models我去这么巧和icml-oral那篇感觉做的题材有点像啊vlm相关"><a target="_blank" rel="noopener" href="https://hoar012.github.io/RAP-Project/">RAP: Retrieval-Augmented
Personalization for Multimodal Large Language
Models</a>（我去，这么巧，和ICML
oral那篇感觉做的题材有点像啊）（VLM相关）</h2>
<h2 id="video-colbert-contextualized-late-interaction-for-text-to-video-retrievalvideo相关video-text-retrieval"><strong>Video-ColBERT:
Contextualized Late Interaction for Text-to-Video
Retrieval</strong>（video相关，video-text retrieval）</h2>
<h2 id="imagine-and-seek-improving-composed-image-retrieval-with-an-imagined-proxycomposed-image-retrieval"><strong>Imagine
and Seek: Improving Composed Image Retrieval with an Imagined
Proxy</strong>（Composed Image Retrieval）</h2>
<h2 id="the-devil-is-in-the-prompts-retrieval-augmented-prompt-optimization-for-text-to-video-generation不会是prompt-engineering吧video相关text-video-retrieval"><strong>The
Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for
Text-to-Video Generation</strong>（不会是prompt
engineering吧……）（video相关，text-video retrieval）</h2>
<h2 id="narrating-the-video-boosting-text-video-retrieval-via-comprehensive-utilization-of-frame-level-captionsvideo相关text-video-retrieval"><a target="_blank" rel="noopener" href="https://multimodal-understanding-group.github.io/NarVid/">Narrating
the Video: Boosting Text-Video Retrieval via Comprehensive Utilization
of Frame-Level Captions</a>（video相关，text-video retrieval）</h2>
<h2 id="learning-with-noisy-triplet-correspondence-for-composed-image-retrievalcomposed-image-retrieval"><a target="_blank" rel="noopener" href="https://github.com/CharlesNeilWilliams/TME">Learning with Noisy
Triplet Correspondence for Composed Image Retrieval</a>（Composed Image
Retrieval）</h2>
<h2 id="ccin-compositional-conflict-identification-and-neutralization-for-composed-image-retrievalcomposed-image-retrieval"><strong>CCIN:
Compositional Conflict Identification and Neutralization for Composed
Image Retrieval</strong>（Composed Image Retrieval）</h2>
<h2 id="generative-zero-shot-composed-image-retrievalcomposed-image-retrieval"><strong>Generative
Zero-Shot Composed Image Retrieval</strong>（Composed Image
Retrieval）</h2>
<h2 id="context-cir-learning-from-concepts-in-text-for-composed-image-retrieval好多这种composed-image-retrieval啊这个是啥啊"><strong>ConText-CIR:
Learning from Concepts in Text for Composed Image
Retrieval</strong>（好多这种composed image
retrieval啊，这个是啥啊）</h2>
<h2 id="discovla-discrepancy-reduction-in-vision-language-and-alignment-for-parameter-efficient-video-text-retrievaltext-video-retrieval"><a target="_blank" rel="noopener" href="https://github.com/LunarShen/DsicoVLA">DiscoVLA: Discrepancy
Reduction in Vision, Language, and Alignment for Parameter-Efficient
Video-Text Retrieval</a>（text-video retrieval）</h2>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/09/08/%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3/%E6%9C%AC%E5%9C%B0latex/%E5%85%B3%E4%BA%8E%E7%BC%96%E8%AF%91%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" rel="prev" title="关于编译的一些常见问题">
      <i class="fa fa-chevron-left"></i> 关于编译的一些常见问题
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/rl-based/VRAG-RL%EF%BC%9AEmpower%20Vision-Perception-Based%20RAG%20for%20Visually%20Rich%20Information%20Understanding%20via%20Iterative%20Reasoning%20with%20Reinforcement%20Learning/" rel="next" title="VRAG-RL：Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning">
      VRAG-RL：Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#wavrag-audio-integrated-retrieval-augmented-generation-for-spoken-dialogue-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E4%B8%8D%E8%BF%87%E6%98%AFaudio%E7%9B%B8%E5%85%B3%E7%9A%84"><span class="nav-number">1.</span> <span class="nav-text">WavRAG:
Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue
Models（多模态的，不过是audio相关的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#real-mm-rag-a-real-world-multi-modal-retrieval-benchmark%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E4%BD%86%E6%98%AFbenchmark"><span class="nav-number">2.</span> <span class="nav-text">REAL-MM-RAG:
A Real-World Multi-Modal Retrieval
Benchmark（多模态的，但是benchmark）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#videorag-retrieval-augmented-generation-over-video-corpus%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84video%E7%9B%B8%E5%85%B3"><span class="nav-number">3.</span> <span class="nav-text">VideoRAG:
Retrieval-Augmented Generation over Video
Corpus（多模态的）（video相关）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mes-rag-bringing-multi-modal-entity-storage-and-secure-enhancements-to-rag%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number">4.</span> <span class="nav-text">MES-RAG:
Bringing Multi-modal, Entity-Storage, and Secure Enhancements to
RAG（多模态的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unirag-universal-retrieval-augmentation-for-large-vision-language-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E5%85%B3%E4%BA%8Evlm"><span class="nav-number">5.</span> <span class="nav-text">UniRAG:
Universal Retrieval Augmentation for Large Vision Language
Models（多模态的，关于VLM）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rule-reliable-multimodal-rag-for-factuality-in-medical-vision-language-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E4%BD%86%E6%98%AF%E5%8C%BB%E5%AD%A6%E7%9B%B8%E5%85%B3%E7%9A%84"><span class="nav-number">6.</span> <span class="nav-text">RULE:
Reliable Multimodal RAG for Factuality in Medical Vision Language
Models（多模态的，但是医学相关的）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#poisonedeye-knowledge-poisoning-attack-on-retrieval-augmented-generation-based-large-vision-language-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E5%85%B3%E4%BA%8Evlm"><span class="nav-number"></span> <span class="nav-text">PoisonedEye: Knowledge
Poisoning Attack on Retrieval-Augmented Generation based Large
Vision-Language Models（多模态的）（关于VLM）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#she-streaming-media-hashing-retrieval%E7%96%91%E4%BC%BC%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number"></span> <span class="nav-text">SHE: Streaming-media
Hashing Retrieval（疑似多模态的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qure-query-relevant-retrieval-through-hard-negative-sampling-in-composed-image-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84composed-image-retrieval"><span class="nav-number"></span> <span class="nav-text">QuRe: Query-Relevant
Retrieval through Hard Negative Sampling in Composed Image
Retrieval（多模态的，composed Image retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-attribute-aware-hash-codes-for-fine-grained-image-retrieval-via-query-optimization%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E5%8F%88%E6%98%AFimage-retrieval"><span class="nav-number"></span> <span class="nav-text">Learning
Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query
Optimization（多模态的）（又是Image retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#visual-abstraction-a-plug-and-play-approach-for-text-visual-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84text-visual-retrieval"><span class="nav-number"></span> <span class="nav-text">Visual Abstraction: A
Plug-and-Play Approach for Text-Visual
Retrieval（多模态的）（text-visual retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#docks-rag-optimizing-document-level-relation-extraction-through-llm-enhanced-hybrid-prompt-tuning%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E6%96%87%E6%A1%A3%E7%B1%BB"><span class="nav-number"></span> <span class="nav-text">DocKS-RAG: Optimizing
Document-Level Relation Extraction through LLM-Enhanced Hybrid Prompt
Tuning（多模态的，文档类）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieval-augmented-perception-high-resolution-image-perception-meets-visual-rag%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E7%8E%8B%E4%B8%AD%E7%8E%8B%E7%A5%9E%E4%B8%AD%E7%A5%9Ehigh-resolution-image-perception"><span class="nav-number"></span> <span class="nav-text">Retrieval-Augmented
Perception: High-resolution Image Perception Meets Visual
RAG（多模态的，王中王，神中神）（High-resolution Image
Perception）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#realrag-retrieval-augmented-realistic-image-generation-via-self-reflective-contrastive-learning%E4%B9%9F%E7%AE%97%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E5%90%A7%E7%BB%93%E5%90%88rag%E4%B8%8Ediffusion"><span class="nav-number"></span> <span class="nav-text">RealRAG:
Retrieval-augmented Realistic Image Generation via Self-reflective
Contrastive Learning（也算多模态的吧，结合RAG与diffusion）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#colpali-efficient-document-retrieval-with-vision-language-models%E4%BC%98%E5%85%88%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E5%85%B3%E4%BA%8Evlm%E7%9A%84"><span class="nav-number"></span> <span class="nav-text">-
ColPali: Efficient Document Retrieval with Vision Language
Models（优先）（多模态的）（关于VLM的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#visrag-vision-based-retrieval-augmented-generation-on-multi-modality-documents%E4%BC%98%E5%85%88%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84visual-rag"><span class="nav-number"></span> <span class="nav-text">-
VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality
Documents（优先）（多模态的）（visual RAG）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ra-tta-retrieval-augmented-test-time-adaptation-for-vision-language-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E5%85%B3%E4%BA%8Evlm%E7%9A%84tta"><span class="nav-number"></span> <span class="nav-text">RA-TTA:
Retrieval-Augmented Test-Time Adaptation for Vision-Language
Models（多模态的）（关于VLM的TTA）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#streaming-video-question-answering-with-in-context-video-kv-cache-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84video-qa"><span class="nav-number"></span> <span class="nav-text">-
Streaming Video Question-Answering with In-context Video KV-Cache
Retrieval（多模态的）（video qa）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mai-a-multi-turn-aggregation-iteration-model-for-composed-image-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84composed-image-retrieval"><span class="nav-number"></span> <span class="nav-text">MAI:
A Multi-turn Aggregation-Iteration Model for Composed Image
Retrieval（多模态的）（composed Image retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bridging-information-asymmetry-in-text-video-retrieval-a-data-centric-approach%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84text-video-retrieval"><span class="nav-number"></span> <span class="nav-text">Bridging
Information Asymmetry in Text-video Retrieval: A Data-centric
Approach（多模态的）（text-video retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#benchmarking-multimodal-retrieval-augmented-generation-with-dynamic-vqa-dataset-and-self-adaptive-planning-agent%E7%AE%97%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E6%B6%89%E5%8F%8Avqa%E4%BA%86%E4%BD%86%E6%98%AF%E6%98%AFbenchmark%E7%B1%BB%E7%9A%84%E8%AE%BA%E6%96%87"><span class="nav-number"></span> <span class="nav-text">Benchmarking
Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and
Self-adaptive Planning
Agent（算多模态的？涉及VQA了。但是是benchmark类的论文）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieval-augmented-diffusion-model-for-structure-informed-antibody-design-and-optimization%E7%AE%97%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E6%B6%89%E5%8F%8Adiffusionwc%E6%98%AFai4sci%E7%9A%84%E5%90%A7%E9%83%BD%E6%9C%89%E6%8A%97%E4%BD%93%E4%BA%86%E7%AE%97%E4%BA%86"><span class="nav-number"></span> <span class="nav-text">Retrieval
Augmented Diffusion Model for Structure-informed Antibody Design and
Optimization（算多模态的？涉及diffusion）（wc，是ai4sci的吧，都有抗体了，算了）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mm-embed-universal-multimodal-retrieval-with-multimodal-llms%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number"></span> <span class="nav-text">MM-EMBED:
Universal Multimodal Retrieval with Multimodal LLMs（多模态的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tiger-unifying-text-to-image-generation-and-retrieval-with-large-multimodal-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E5%A5%BD%E5%83%8F%E6%98%AF%E5%9C%A8%E5%81%9A%E6%96%87%E7%94%9F%E5%9B%BE%E5%92%8C%E6%A3%80%E7%B4%A2%E7%9A%84%E7%BB%9F%E4%B8%80%E5%B7%A5%E4%BD%9C"><span class="nav-number"></span> <span class="nav-text">TIGeR:
Unifying Text-to-Image Generation and Retrieval with Large Multimodal
Models（多模态的）（好像是在做文生图和检索的统一工作）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mrag-bench-vision-centric-evaluation-for-retrieval-augmented-multimodal-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E5%8F%AA%E4%B8%8D%E8%BF%87%E6%98%AFbenchmark%E7%B1%BB%E7%9A%84"><span class="nav-number"></span> <span class="nav-text">MRAG-Bench:
Vision-Centric Evaluation for Retrieval-Augmented Multimodal
Models（多模态的，只不过是benchmark类的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-fine-grained-representations-through-textual-token-disentanglement-in-composed-video-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84composed-video-retrieval"><span class="nav-number"></span> <span class="nav-text">Learning
Fine-Grained Representations through Textual Token Disentanglement in
Composed Video Retrieval（多模态的）（composed video retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tempme-video-temporal-token-merging-for-efficient-text-video-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84text-video-retrieval"><span class="nav-number"></span> <span class="nav-text">TempMe:
Video Temporal Token Merging for Efficient Text-Video
Retrieval（多模态的）（text-video retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mllm-as-retriever-interactively-learning-multimodal-retrieval-for-embodied-agents%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E4%B8%8D%E8%BF%87%E4%BC%BC%E4%B9%8E%E6%98%AF%E5%BA%94%E7%94%A8%E5%88%B0%E5%85%B7%E8%BA%AB%E6%96%B9%E9%9D%A2"><span class="nav-number"></span> <span class="nav-text">MLLM
as Retriever: Interactively Learning Multimodal Retrieval for Embodied
Agents（多模态的，不过似乎是应用到具身方面？）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generalized-video-moment-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84video%E7%9B%B8%E5%85%B3%E7%9A%84%E4%BC%BC%E4%B9%8E%E5%85%B7%E4%BD%93%E6%98%AFmoment%E7%9B%B8%E5%85%B3"><span class="nav-number"></span> <span class="nav-text">Generalized
Video Moment
Retrieval（多模态的）（video相关的，似乎具体是moment相关？）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#exploiting-distribution-constraints-for-scalable-and-efficient-image-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84image-retrieval"><span class="nav-number"></span> <span class="nav-text">Exploiting
Distribution Constraints for Scalable and Efficient Image
Retrieval（多模态的）（Image Retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rapid-retrieval-augmented-training-of-differentially-private-diffusion-models%E7%AE%97%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E6%B6%89%E5%8F%8Adiffusion-models"><span class="nav-number"></span> <span class="nav-text">RAPID:
Retrieval Augmented Training of Differentially Private Diffusion
Models（算多模态的？涉及diffusion models）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#g-retriever-retrieval-augmented-generation-for-textual-graph-understanding-and-question-answering"><span class="nav-number"></span> <span class="nav-text">G-Retriever:
Retrieval-Augmented Generation for Textual Graph Understanding and
Question Answering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#an-end-to-end-graph-attention-network-hashing-for-cross-modal-retrieval"><span class="nav-number"></span> <span class="nav-text">An End-To-End Graph
Attention Network Hashing for Cross-Modal Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wheres-waldo-diffusion-features-for-personalized-segmentation-and-retrieval"><span class="nav-number"></span> <span class="nav-text">Where&#39;s Waldo:
Diffusion Features For Personalized Segmentation and Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aligning-vision-models-with-human-aesthetics-in-retrieval-benchmarks-and-algorithmsbenchmark%E7%B1%BB%E5%95%8A"><span class="nav-number"></span> <span class="nav-text">Aligning Vision Models
with Human Aesthetics in Retrieval: Benchmarks and
Algorithms（benchmark类啊）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#diffusion-inspired-truncated-sampler-for-text-video-retrieval"><span class="nav-number"></span> <span class="nav-text">Diffusion-Inspired
Truncated Sampler for Text-Video Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inquire-a-natural-world-text-to-image-retrieval-benchmark%E5%8F%88%E6%98%AFbenchmark%E7%B1%BB"><span class="nav-number"></span> <span class="nav-text">INQUIRE: A Natural
World Text-to-Image Retrieval Benchmark（又是benchmark类）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bivlc-extending-vision-language-compositionality-evaluation-with-text-to-image-retrieval"><span class="nav-number"></span> <span class="nav-text">BiVLC: Extending
Vision-Language Compositionality Evaluation with Text-to-Image
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#verified-a-video-corpus-moment-retrieval-benchmark-for-fine-grained-video-understanding%E4%BE%9D%E7%84%B6benchmark"><span class="nav-number"></span> <span class="nav-text">VERIFIED: A Video
Corpus Moment Retrieval Benchmark for Fine-Grained Video
Understanding（依然benchmark）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wikido-a-new-benchmark-evaluating-cross-modal-retrieval-for-vision-language-modelsbenchmark-again"><span class="nav-number"></span> <span class="nav-text">WikiDO: A New Benchmark
Evaluating Cross-Modal Retrieval for Vision-Language
Models（benchmark again）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieval-fine-tuning-for-in-context-tabular-models%E4%BC%BC%E4%B9%8E%E6%98%AF%E7%A0%94%E7%A9%B6%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%E7%9A%84%E7%AE%97%E5%90%97"><span class="nav-number"></span> <span class="nav-text">Retrieval &amp;
Fine-Tuning for In-Context Tabular
Models（似乎是研究表格型数据的？算吗？）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#uda-a-benchmark-suite-for-retrieval-augmented-generation-in-real-world-document-analysis%E7%A0%94%E7%A9%B6document-analysis%E7%9A%84"><span class="nav-number"></span> <span class="nav-text">UDA: A Benchmark Suite
for Retrieval Augmented Generation in Real-World Document
Analysis（研究document analysis的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#semi-open-3d-object-retrieval-via-hierarchical-equilibrium-on-hypergraph%E6%9C%89%E7%82%B9%E6%9D%82%E5%95%8A%E6%A3%80%E7%B4%A23d%E7%89%A9%E4%BD%93"><span class="nav-number"></span> <span class="nav-text">Semi-Open 3D Object
Retrieval via Hierarchical Equilibrium on
Hypergraph（有点杂啊，检索3D物体）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#assembly-fuzzy-representation-on-hypergraph-for-open-set-3d-object-retrieval%E6%88%91%E5%8E%BB%E8%BF%98%E6%9C%89%E4%B8%80%E7%AF%87%E5%90%97%E7%A0%94%E7%A9%B63d-retrieval%E7%9A%84"><span class="nav-number"></span> <span class="nav-text">Assembly Fuzzy
Representation on Hypergraph for Open-Set 3D Object
Retrieval（我去，还有一篇吗，研究3D retrieval的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#drvideo-document-retrieval-based-long-video-understandingvideo-understanding"><span class="nav-number"></span> <span class="nav-text">DrVideo:
Document Retrieval Based Long Video Understanding（video
understanding）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#salova-segment-augmented-long-video-assistant-for-targeted-retrieval-and-routing-in-long-form-video-analysisvideo%E7%9B%B8%E5%85%B3%E5%BA%94%E8%AF%A5%E6%98%AF%E9%95%BF%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3"><span class="nav-number"></span> <span class="nav-text">SALOVA: Segment-Augmented Long
Video Assistant for Targeted Retrieval and Routing in Long-Form Video
Analysis（video相关，应该是长视频理解）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vlog-video-language-models-by-generative-retrieval-of-narration-vocabularyvideo%E7%9B%B8%E5%85%B3"><span class="nav-number"></span> <span class="nav-text">VLog:
Video-Language Models by Generative Retrieval of Narration
Vocabulary（video相关）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#collm-a-large-language-model-for-composed-image-retrievalcomposed-image-retrieval"><span class="nav-number"></span> <span class="nav-text">CoLLM: A Large Language Model for
Composed Image Retrieval（Composed Image retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#search-and-detect-training-free-long-tail-object-detection-via-web-image-retrieval%E6%9C%89%E7%82%B9%E6%9D%82%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%94%A8%E5%88%B0%E4%BA%86image-retrieval%E6%8A%80%E6%9C%AF"><span class="nav-number"></span> <span class="nav-text">Search
and Detect: Training-Free Long Tail Object Detection via Web-Image
Retrieval（有点杂，物体检测，用到了Image Retrieval技术）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-audio-guided-video-representation-with-gated-attention-for-video-text-retrievalaudio%E7%9B%B8%E5%85%B3"><span class="nav-number"></span> <span class="nav-text">Learning
Audio-guided Video Representation with Gated Attention for Video-Text
Retrieval（audio相关）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lamra-large-multimodal-model-as-your-advanced-retrieval-assistantvlm%E7%9B%B8%E5%85%B3"><span class="nav-number"></span> <span class="nav-text">LamRA: Large Multimodal
Model as Your Advanced Retrieval Assistant（VLM相关）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#recurrence-enhanced-vision-and-language-transformers-for-robust-multimodal-document-retrievalvlm%E7%9B%B8%E5%85%B3%E4%BB%A5%E5%8F%8A%E6%96%87%E6%A1%A3%E7%9B%B8%E5%85%B3"><span class="nav-number"></span> <span class="nav-text">Recurrence-Enhanced
Vision-and-Language Transformers for Robust Multimodal Document
Retrieval（VLM相关，以及文档相关）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#missing-target-relevant-information-prediction-with-world-model-for-accurate-zero-shot-composed-image-retrievalcomposed-image-retrieval"><span class="nav-number"></span> <span class="nav-text">Missing
Target-Relevant Information Prediction with World Model for Accurate
Zero-Shot Composed Image Retrieval（composed Image
retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ilias-instance-level-image-retrieval-at-scaleimage-retrieval"><span class="nav-number"></span> <span class="nav-text">ILIAS: Instance-Level Image
retrieval At Scale（Image retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#clip-is-almost-all-you-need-towards-parameter-efficient-scene-text-retrieval-without-ocrscene-text-retrieval"><span class="nav-number"></span> <span class="nav-text">CLIP
is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval
without OCR（scene-text retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rethinking-noisy-video-text-retrieval-via-relation-aware-alignmentvideo%E7%9B%B8%E5%85%B3video-text-retrieval%E5%8F%AA%E4%B8%8D%E8%BF%87%E5%A4%9A%E4%BA%86%E4%B8%80%E4%B8%AAnoisy%E8%AE%BE%E5%AE%9A%E8%BF%99%E4%B8%AA%E6%84%9F%E8%A7%89%E7%9C%9F%E5%A5%BD%E7%81%8C%E6%B0%B4%E5%90%A7"><span class="nav-number"></span> <span class="nav-text">Rethinking
Noisy Video-Text Retrieval via Relation-aware
Alignment（video相关，video-text
retrieval，只不过多了一个noisy设定。这个感觉真好灌水吧……）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bridging-modalities-improving-universal-multimodal-retrieval-by-multimodal-large-language-modelsvlm%E7%9B%B8%E5%85%B3"><span class="nav-number"></span> <span class="nav-text">Bridging
Modalities: Improving Universal Multimodal Retrieval by Multimodal Large
Language Models（VLM相关）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reason-before-retrieve-one-stage-reflective-chain-of-thoughts-for-training-free-zero-shot-composed-image-retrievalcomposed-image-retrieval"><span class="nav-number"></span> <span class="nav-text">Reason-before-Retrieve:
One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot
Composed Image Retrieval（Composed Image Retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vdocrag-retrieval-augmented-generation-over-visually-rich-documents%E8%BF%99%E7%AF%87%E6%98%8E%E7%A1%AE%E5%87%BA%E7%8E%B0rag%E4%BA%86%E5%95%8Adocument%E7%9B%B8%E5%85%B3"><span class="nav-number"></span> <span class="nav-text">VDocRAG: Retrieval-Augmented
Generation over Visually-Rich
Documents（这篇明确出现RAG了啊）（document相关）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rap-retrieval-augmented-personalization-for-multimodal-large-language-models%E6%88%91%E5%8E%BB%E8%BF%99%E4%B9%88%E5%B7%A7%E5%92%8Cicml-oral%E9%82%A3%E7%AF%87%E6%84%9F%E8%A7%89%E5%81%9A%E7%9A%84%E9%A2%98%E6%9D%90%E6%9C%89%E7%82%B9%E5%83%8F%E5%95%8Avlm%E7%9B%B8%E5%85%B3"><span class="nav-number"></span> <span class="nav-text">RAP: Retrieval-Augmented
Personalization for Multimodal Large Language
Models（我去，这么巧，和ICML
oral那篇感觉做的题材有点像啊）（VLM相关）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#video-colbert-contextualized-late-interaction-for-text-to-video-retrievalvideo%E7%9B%B8%E5%85%B3video-text-retrieval"><span class="nav-number"></span> <span class="nav-text">Video-ColBERT:
Contextualized Late Interaction for Text-to-Video
Retrieval（video相关，video-text retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#imagine-and-seek-improving-composed-image-retrieval-with-an-imagined-proxycomposed-image-retrieval"><span class="nav-number"></span> <span class="nav-text">Imagine
and Seek: Improving Composed Image Retrieval with an Imagined
Proxy（Composed Image Retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-devil-is-in-the-prompts-retrieval-augmented-prompt-optimization-for-text-to-video-generation%E4%B8%8D%E4%BC%9A%E6%98%AFprompt-engineering%E5%90%A7video%E7%9B%B8%E5%85%B3text-video-retrieval"><span class="nav-number"></span> <span class="nav-text">The
Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for
Text-to-Video Generation（不会是prompt
engineering吧……）（video相关，text-video retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#narrating-the-video-boosting-text-video-retrieval-via-comprehensive-utilization-of-frame-level-captionsvideo%E7%9B%B8%E5%85%B3text-video-retrieval"><span class="nav-number"></span> <span class="nav-text">Narrating
the Video: Boosting Text-Video Retrieval via Comprehensive Utilization
of Frame-Level Captions（video相关，text-video retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-with-noisy-triplet-correspondence-for-composed-image-retrievalcomposed-image-retrieval"><span class="nav-number"></span> <span class="nav-text">Learning with Noisy
Triplet Correspondence for Composed Image Retrieval（Composed Image
Retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ccin-compositional-conflict-identification-and-neutralization-for-composed-image-retrievalcomposed-image-retrieval"><span class="nav-number"></span> <span class="nav-text">CCIN:
Compositional Conflict Identification and Neutralization for Composed
Image Retrieval（Composed Image Retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generative-zero-shot-composed-image-retrievalcomposed-image-retrieval"><span class="nav-number"></span> <span class="nav-text">Generative
Zero-Shot Composed Image Retrieval（Composed Image
Retrieval）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#context-cir-learning-from-concepts-in-text-for-composed-image-retrieval%E5%A5%BD%E5%A4%9A%E8%BF%99%E7%A7%8Dcomposed-image-retrieval%E5%95%8A%E8%BF%99%E4%B8%AA%E6%98%AF%E5%95%A5%E5%95%8A"><span class="nav-number"></span> <span class="nav-text">ConText-CIR:
Learning from Concepts in Text for Composed Image
Retrieval（好多这种composed image
retrieval啊，这个是啥啊）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#discovla-discrepancy-reduction-in-vision-language-and-alignment-for-parameter-efficient-video-text-retrievaltext-video-retrieval"><span class="nav-number"></span> <span class="nav-text">DiscoVLA: Discrepancy
Reduction in Vision, Language, and Alignment for Parameter-Efficient
Video-Text Retrieval（text-video retrieval）</span></a></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">217</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">67</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">103</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
