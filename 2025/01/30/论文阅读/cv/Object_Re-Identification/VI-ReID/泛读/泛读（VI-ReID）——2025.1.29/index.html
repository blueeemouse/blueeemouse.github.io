<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identificationarxiv 2025.1.23这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery images中有两种模态的图像，而不止一种模态。动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-in">
<meta property="og:type" content="article">
<meta property="og:title" content="bluemouse&#39;s blog">
<meta property="og:url" content="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identificationarxiv 2025.1.23这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery images中有两种模态的图像，而不止一种模态。动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-in">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1/%E6%B3%9B%E8%AF%BB/pic/cross-modal.png">
<meta property="og:image" content="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1/%E6%B3%9B%E8%AF%BB/pic/distillation_loss.png">
<meta property="article:published_time" content="2025-01-29T16:25:39.695Z">
<meta property="article:modified_time" content="2025-02-09T18:31:27.826Z">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1/%E6%B3%9B%E8%AF%BB/pic/cross-modal.png">

<link rel="canonical" href="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title> | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-30 00:25:39" itemprop="dateCreated datePublished" datetime="2025-01-30T00:25:39+08:00">2025-01-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-10 02:31:27" itemprop="dateModified" datetime="2025-02-10T02:31:27+08:00">2025-02-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="From-Cross-Modal-to-Mixed-Modal-Visible-Infrared-Re-Identification"><a href="#From-Cross-Modal-to-Mixed-Modal-Visible-Infrared-Re-Identification" class="headerlink" title="From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification"></a>From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification</h1><h2 id="arxiv-2025-1-23"><a href="#arxiv-2025-1-23" class="headerlink" title="arxiv 2025.1.23"></a>arxiv 2025.1.23</h2><h2 id="这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery-images中有两种模态的图像，而不止一种模态。"><a href="#这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery-images中有两种模态的图像，而不止一种模态。" class="headerlink" title="这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery images中有两种模态的图像，而不止一种模态。"></a>这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery images中有两种模态的图像，而不止一种模态。</h2><h2 id="动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared-infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片-晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain-gap显然会比不同模态的domain-gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id-1的一个红外图像，一个可见光图像，和一个id-2的可见光图像。现在有一个id-2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id-1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。这篇论文就是想解决这个问题"><a href="#动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared-infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片-晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain-gap显然会比不同模态的domain-gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id-1的一个红外图像，一个可见光图像，和一个id-2的可见光图像。现在有一个id-2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id-1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。这篇论文就是想解决这个问题" class="headerlink" title="动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared&#x2F;infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片&#x2F;晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain gap显然会比不同模态的domain gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id 1的一个红外图像，一个可见光图像，和一个id 2的可见光图像。现在有一个id 2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id 1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。这篇论文就是想解决这个问题"></a>动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared&#x2F;infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片&#x2F;晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain gap显然会比不同模态的domain gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id 1的一个红外图像，一个可见光图像，和一个id 2的可见光图像。现在有一个id 2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id 1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。<br>这篇论文就是想解决这个问题</h2><h2 id="方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared-feature，也要用到modality-specific-feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared-feature，一个代表specific-feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared-feature和specific-feature呢？）"><a href="#方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared-feature，也要用到modality-specific-feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared-feature，一个代表specific-feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared-feature和specific-feature呢？）" class="headerlink" title="方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared feature，也要用到modality-specific feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared feature，一个代表specific feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared feature和specific feature呢？）"></a>方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared feature，也要用到modality-specific feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared feature，一个代表specific feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared feature和specific feature呢？）</h2><h4 id="（话说回来，在经典的vi-reid中，真的有必要用到modality-specific-feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific-feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）"><a href="#（话说回来，在经典的vi-reid中，真的有必要用到modality-specific-feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific-feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）" class="headerlink" title="（话说回来，在经典的vi-reid中，真的有必要用到modality-specific feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）"></a>（话说回来，在经典的vi-reid中，真的有必要用到modality-specific feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）</h4><h2 id="分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality-scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific-feature（或者说原文的modality-erased-related-feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：-max-Z-m-e-Z-m-r-MI-Z-m-e-Z-m-r-Y-s-t-MI-Z-m-e-Z-m-r-0-MI-Z-m-e-M-0-and-MI-Z-m-r-Y-M-0-上面的-Z-m-e-代表模态m下的modality-erased-feature，也就是与模态无关的特征；-Z-m-r-代表模态m下的modality-related-feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased-feature与modality-related-feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为-Z-m-e-代表modality-erased-feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望-Z-m-r-不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，-Z-m-r-就是只能用来推断模态信息，不能用来推断ID信息。那它和-Z-m-e-组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased-feature和modality-related-feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased-feature（但这样不是很麻烦吗？得再看看代码了可能）总而言之，得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题。得到结果如下：且针对四个部分，都进行了各自的优化"><a href="#分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality-scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific-feature（或者说原文的modality-erased-related-feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：-max-Z-m-e-Z-m-r-MI-Z-m-e-Z-m-r-Y-s-t-MI-Z-m-e-Z-m-r-0-MI-Z-m-e-M-0-and-MI-Z-m-r-Y-M-0-上面的-Z-m-e-代表模态m下的modality-erased-feature，也就是与模态无关的特征；-Z-m-r-代表模态m下的modality-related-feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased-feature与modality-related-feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为-Z-m-e-代表modality-erased-feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望-Z-m-r-不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，-Z-m-r-就是只能用来推断模态信息，不能用来推断ID信息。那它和-Z-m-e-组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased-feature和modality-related-feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased-feature（但这样不是很麻烦吗？得再看看代码了可能）总而言之，得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题。得到结果如下：且针对四个部分，都进行了各自的优化" class="headerlink" title="分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific feature（或者说原文的modality-erased&#x2F;-related feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：$$\max_{Z_{m}^{e},Z_{m}^{r}}{MI(Z_{m}^{e},Z_{m}^{r};Y)}\ \ s.t.MI(Z_{m}^{e},Z_{m}^{r})&#x3D;0,MI(Z_{m}^{e};M)&#x3D;0,and\ MI(Z_{m}^{r};Y|M)&#x3D;0$$上面的$Z_{m}^{e}$代表模态m下的modality-erased feature，也就是与模态无关的特征；$Z_{m}^{r}$代表模态m下的modality-related feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased feature与modality-related feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为$Z_{m}^{e}$代表modality-erased feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望$Z_{m}^{r}$不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，$Z_{m}^{r}$就是只能用来推断模态信息，不能用来推断ID信息。那它和$Z_{m}^{e}$组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased feature和modality-related feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased feature（但这样不是很麻烦吗？得再看看代码了可能）总而言之，得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题。得到结果如下：且针对四个部分，都进行了各自的优化"></a>分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific feature（或者说原文的modality-erased&#x2F;-related feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：$$\max_{Z_{m}^{e},Z_{m}^{r}}{MI(Z_{m}^{e},Z_{m}^{r};Y)}\ \ s.t.MI(Z_{m}^{e},Z_{m}^{r})&#x3D;0,MI(Z_{m}^{e};M)&#x3D;0,and\ MI(Z_{m}^{r};Y|M)&#x3D;0$$<br>上面的$Z_{m}^{e}$代表模态m下的modality-erased feature，也就是与模态无关的特征；$Z_{m}^{r}$代表模态m下的modality-related feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased feature与modality-related feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为$Z_{m}^{e}$代表modality-erased feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望$Z_{m}^{r}$不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，$Z_{m}^{r}$就是只能用来推断模态信息，不能用来推断ID信息。那它和$Z_{m}^{e}$组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased feature和modality-related feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased feature（但这样不是很麻烦吗？得再看看代码了可能）<br>总而言之，<strong>得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题</strong>。得到结果如下：<img src="/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1/%E6%B3%9B%E8%AF%BB/pic/cross-modal.png" alt="|500"><br>且针对四个部分，都进行了各自的优化</h2><h2 id="评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析"><a href="#评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析" class="headerlink" title="评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析"></a>评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析</h2><h2 id="trick总结："><a href="#trick总结：" class="headerlink" title="trick总结："></a>trick总结：</h2><h3 id="1-想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替"><a href="#1-想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替" class="headerlink" title="1.想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替"></a>1.想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替</h3><h3 id="2-如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化-MI-Z-m-e-Y-，即要最大化modality-erased-feature与标签变量之间的互信息，其实目的就是让modality-erased-feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）"><a href="#2-如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化-MI-Z-m-e-Y-，即要最大化modality-erased-feature与标签变量之间的互信息，其实目的就是让modality-erased-feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）" class="headerlink" title="2.如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化$MI(Z_{m}^{e};Y)$，即要最大化modality-erased feature与标签变量之间的互信息，其实目的就是让modality-erased feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）"></a>2.如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化$MI(Z_{m}^{e};Y)$，即要最大化modality-erased feature与标签变量之间的互信息，其实目的就是让modality-erased feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）</h3><h3 id="3-要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层"><a href="#3-要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层" class="headerlink" title="3.要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层"></a>3.要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层</h3><h3 id="4-要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：-L-max-x-y-alpha-0-max-y-x-alpha-0-，-alpha-是正常数，也是超参数。那么，x和y必须相差绝对值不大于-alpha-，这部分损失才为0，否则就产生损失了"><a href="#4-要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：-L-max-x-y-alpha-0-max-y-x-alpha-0-，-alpha-是正常数，也是超参数。那么，x和y必须相差绝对值不大于-alpha-，这部分损失才为0，否则就产生损失了" class="headerlink" title="4.要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：$L&#x3D;max(x-y+\alpha,0)+max(y-x+\alpha,0)$，$\alpha$是正常数，也是超参数。那么，x和y必须相差绝对值不大于$\alpha$，这部分损失才为0，否则就产生损失了"></a>4.要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：$L&#x3D;max(x-y+\alpha,0)+max(y-x+\alpha,0)$，$\alpha$是正常数，也是超参数。那么，x和y必须相差绝对值不大于$\alpha$，这部分损失才为0，否则就产生损失了</h3><h3 id="5-这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific-feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID-2；如果它是infrared的图像，则ID-ID-2-1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific-feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific-feature"><a href="#5-这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific-feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID-2；如果它是infrared的图像，则ID-ID-2-1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific-feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific-feature" class="headerlink" title="5.这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID * 2；如果它是infrared的图像，则ID&#x3D;ID * 2 + 1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific feature"></a>5.这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID * 2；如果它是infrared的图像，则ID&#x3D;ID * 2 + 1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific feature</h3><h1 id="Spectral-Enhancement-and-Pseudo-Anchor-Guidance-for-Infrared-Visible-Person-Re-Identification"><a href="#Spectral-Enhancement-and-Pseudo-Anchor-Guidance-for-Infrared-Visible-Person-Re-Identification" class="headerlink" title="Spectral Enhancement and Pseudo-Anchor Guidance  for Infrared-Visible Person Re-Identification"></a>Spectral Enhancement and Pseudo-Anchor Guidance  for Infrared-Visible Person Re-Identification</h1><h2 id="arxiv-2025-1-2"><a href="#arxiv-2025-1-2" class="headerlink" title="arxiv, 2025.1.2"></a>arxiv, 2025.1.2</h2><h2 id="这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral-gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）"><a href="#这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral-gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）" class="headerlink" title="这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）"></a>这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）</h2><h2 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h2><h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><h2 id="分析：第一步是生成一个Semantically-Enhanced-Grey-Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically-Enhanced-Grey-Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇，-就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific-feature第三步是PABA-loss，即Pseudo-Anchor-guided-Bidirectional-Aggregation-Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征-F-shared-seg-和-F-shared-ir-，先进行一个分块，分成N块，之后对每个块都施加PABA-loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet-loss，这里增加了跨模态的考虑；而相比于cross-center-loss，这里的改进在于分了块，且是对每个块都施加PABA-loss，故更加细粒度（这里其实要结合公式来看才清楚）除了上面提到的PABA-loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared-feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）"><a href="#分析：第一步是生成一个Semantically-Enhanced-Grey-Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically-Enhanced-Grey-Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇，-就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific-feature第三步是PABA-loss，即Pseudo-Anchor-guided-Bidirectional-Aggregation-Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征-F-shared-seg-和-F-shared-ir-，先进行一个分块，分成N块，之后对每个块都施加PABA-loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet-loss，这里增加了跨模态的考虑；而相比于cross-center-loss，这里的改进在于分了块，且是对每个块都施加PABA-loss，故更加细粒度（这里其实要结合公式来看才清楚）除了上面提到的PABA-loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared-feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）" class="headerlink" title="分析：第一步是生成一个Semantically Enhanced Grey Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically Enhanced Grey Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇， 就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific feature第三步是PABA loss，即Pseudo Anchor-guided Bidirectional Aggregation Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征$F_{shared}^{seg}$和$F_{shared}^{ir}$，先进行一个分块，分成N块，之后对每个块都施加PABA loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet loss，这里增加了跨模态的考虑；而相比于cross-center loss，这里的改进在于分了块，且是对每个块都施加PABA loss，故更加细粒度（这里其实要结合公式来看才清楚）除了上面提到的PABA loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）"></a>分析：第一步是生成一个Semantically Enhanced Grey Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically Enhanced Grey Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了<br><br>第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇， 就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific feature<br><br>第三步是PABA loss，即Pseudo Anchor-guided Bidirectional Aggregation Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征$F_{shared}^{seg}$和$F_{shared}^{ir}$，先进行一个分块，分成N块，之后对每个块都施加PABA loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet loss，这里增加了跨模态的考虑；而相比于cross-center loss，这里的改进在于分了块，且是对每个块都施加PABA loss，故更加细粒度（这里其实要结合公式来看才清楚）<br>除了上面提到的PABA loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）</h2><h1 id="Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification"><a href="#Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification"></a>Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification</h1><h2 id="arxiv，2024-12-11"><a href="#arxiv，2024-12-11" class="headerlink" title="arxiv，2024.12.11"></a>arxiv，2024.12.11</h2><h2 id="这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由"><a href="#这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由" class="headerlink" title="这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由"></a>这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由</h2><h2 id="insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）"><a href="#insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）" class="headerlink" title="insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）"></a>insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）</h2><h2 id="动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人-llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）"><a href="#动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人-llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）" class="headerlink" title="动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人&#x2F;llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）"></a>动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人&#x2F;llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）</h2><h2 id="方法：-1"><a href="#方法：-1" class="headerlink" title="方法："></a>方法：</h2><h3 id="提出一个EEES框架，包含三个部分，各司其职第一个是ESE模块（Explicit-Semantics-Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息"><a href="#提出一个EEES框架，包含三个部分，各司其职第一个是ESE模块（Explicit-Semantics-Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息" class="headerlink" title="提出一个EEES框架，包含三个部分，各司其职第一个是ESE模块（Explicit Semantics Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息"></a>提出一个EEES框架，包含三个部分，各司其职<br>第一个是ESE模块（Explicit Semantics Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息</h3><h3 id="第二个是CVSC（Cross-View-Semantics-Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共-M-1-个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是训练时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下："><a href="#第二个是CVSC（Cross-View-Semantics-Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共-M-1-个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是训练时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下：" class="headerlink" title="第二个是CVSC（Cross-View Semantics Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共$M+1$个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是训练时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下："></a>第二个是CVSC（Cross-View Semantics Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共$M+1$个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息<br>然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是<strong>训练</strong>时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下：<img src="/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1/%E6%B3%9B%E8%AF%BB/pic/distillation_loss.png" alt="|500"></h3><h3 id="第三个是CMSP模块（Cross-Modality-Semantics-Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp-loss，公式如下：-L-cmsp-frac-1-N-sum-i-1-N-d-i-vv-d-i-vr-2-frac-1-N-sum-i-1-N-d-i-rr-d-i-rv-2-其中，-d-i-vv-lVert-f-i-v-t-i-v-rVert-2-，-d-i-vr-lVert-f-i-v-t-i-r-rVert-2-，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧"><a href="#第三个是CMSP模块（Cross-Modality-Semantics-Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp-loss，公式如下：-L-cmsp-frac-1-N-sum-i-1-N-d-i-vv-d-i-vr-2-frac-1-N-sum-i-1-N-d-i-rr-d-i-rv-2-其中，-d-i-vv-lVert-f-i-v-t-i-v-rVert-2-，-d-i-vr-lVert-f-i-v-t-i-r-rVert-2-，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧" class="headerlink" title="第三个是CMSP模块（Cross-Modality Semantics Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp loss，公式如下：$$L_{cmsp}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{vv}-d_{i}^{vr})}^{2}+\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{rr}-d_{i}^{rv})}^{2}$$其中，$d_{i}^{vv}&#x3D;\lVert f_{i}^{v}-t_{i}^{v}\rVert_{2}$，$d_{i}^{vr}&#x3D;\lVert f_{i}^{v}-t_{i}^{r}\rVert_{2}$，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧"></a>第三个是CMSP模块（Cross-Modality Semantics Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp loss，公式如下：$$L_{cmsp}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{vv}-d_{i}^{vr})}^{2}+\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{rr}-d_{i}^{rv})}^{2}$$<br>其中，$d_{i}^{vv}&#x3D;\lVert f_{i}^{v}-t_{i}^{v}\rVert_{2}$，$d_{i}^{vr}&#x3D;\lVert f_{i}^{v}-t_{i}^{r}\rVert_{2}$，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧</h3><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。"><a href="#第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。" class="headerlink" title="第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。"></a>第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。</h3><h3 id="第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光-红外，图像-文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）"><a href="#第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光-红外，图像-文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）" class="headerlink" title="第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光&#x2F;红外，图像&#x2F;文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）"></a>第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光&#x2F;红外，图像&#x2F;文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）</h3><h3 id="第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp-loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的-一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字-图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。"><a href="#第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp-loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的-一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字-图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。" class="headerlink" title="第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的&#x2F;一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字&#x2F;图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。"></a>第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的&#x2F;一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字&#x2F;图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。</h3><h1 id="Prototype-Driven-Multi-Feature-Generation-for-Visible-Infrared-Person-Re-identification"><a href="#Prototype-Driven-Multi-Feature-Generation-for-Visible-Infrared-Person-Re-identification" class="headerlink" title="Prototype-Driven Multi-Feature Generation for  Visible-Infrared Person Re-identification"></a>Prototype-Driven Multi-Feature Generation for  Visible-Infrared Person Re-identification</h1><h2 id="ICASSP，2024-9-9"><a href="#ICASSP，2024-9-9" class="headerlink" title="ICASSP，2024.9.9"></a>ICASSP，2024.9.9</h2><h2 id="动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了"><a href="#动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了" class="headerlink" title="动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了"></a>动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了</h2><h2 id="insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性"><a href="#insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性" class="headerlink" title="insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性"></a>insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性</h2><h2 id="动机：-1"><a href="#动机：-1" class="headerlink" title="动机："></a>动机：</h2><h2 id="方法：-2"><a href="#方法：-2" class="headerlink" title="方法："></a>方法：</h2><h3 id="提出的PDM框架，主要是两个组成部分，分别是Multi-Feature-Generation-Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype-Learning-Moduel（PLM）"><a href="#提出的PDM框架，主要是两个组成部分，分别是Multi-Feature-Generation-Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype-Learning-Moduel（PLM）" class="headerlink" title="提出的PDM框架，主要是两个组成部分，分别是Multi-Feature Generation Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype Learning Moduel（PLM）"></a>提出的PDM框架，主要是两个组成部分，分别是Multi-Feature Generation Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype Learning Moduel（PLM）</h3><h3 id="MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation-convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided-Pair-Mining-Loss"><a href="#MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation-convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided-Pair-Mining-Loss" class="headerlink" title="MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided Pair Mining Loss"></a>MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided Pair Mining Loss</h3><h3 id="PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）"><a href="#PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）" class="headerlink" title="PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）"></a>PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）</h3><h3 id="此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine-Heterogeneity-Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center-Seperation-Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）"><a href="#此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine-Heterogeneity-Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center-Seperation-Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）" class="headerlink" title="此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine Heterogeneity Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center Seperation Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）"></a>此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine Heterogeneity Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center Seperation Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）</h3><h1 id="Parameter-Hierarchical-Optimization-for-Visible-Infrared-Person-Re-Identification"><a href="#Parameter-Hierarchical-Optimization-for-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Parameter Hierarchical Optimization for  Visible-Infrared Person Re-Identification"></a>Parameter Hierarchical Optimization for  Visible-Infrared Person Re-Identification</h1><h2 id="arxiv，2024-4-11"><a href="#arxiv，2024-4-11" class="headerlink" title="arxiv，2024.4.11"></a>arxiv，2024.4.11</h2><h2 id="好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外-可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没"><a href="#好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外-可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没" class="headerlink" title="好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外&#x2F;可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没"></a>好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外&#x2F;可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没</h2><h1 id="Bidirectional-Multi-Step-Domain-Generalization-for-Visible-Infrared-Person-Re-Identification"><a href="#Bidirectional-Multi-Step-Domain-Generalization-for-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification"></a>Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification</h1><h2 id="arxiv，2024-3-16"><a href="#arxiv，2024-3-16" class="headerlink" title="arxiv，2024.3.16"></a>arxiv，2024.3.16</h2><h2 id="这篇论文想解决-改善的点在于，大部分方法都是把vis和ir-images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional-Multi-Step-Domain-Generalization）主要由两部分组成，一是part-prototype-alignment-learning（负责提取出局部的身体部位信息）；二是bidirectional-multi-step-learning（通过多步学习，从红外和可见光分别出发，以减小modality-gap）"><a href="#这篇论文想解决-改善的点在于，大部分方法都是把vis和ir-images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional-Multi-Step-Domain-Generalization）主要由两部分组成，一是part-prototype-alignment-learning（负责提取出局部的身体部位信息）；二是bidirectional-multi-step-learning（通过多步学习，从红外和可见光分别出发，以减小modality-gap）" class="headerlink" title="这篇论文想解决&#x2F;改善的点在于，大部分方法都是把vis和ir images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional Multi-Step Domain Generalization）主要由两部分组成，一是part prototype alignment learning（负责提取出局部的身体部位信息）；二是bidirectional multi-step learning（通过多步学习，从红外和可见光分别出发，以减小modality gap）"></a>这篇论文想解决&#x2F;改善的点在于，大部分方法都是把vis和ir images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional Multi-Step Domain Generalization）主要由两部分组成，一是part prototype alignment learning（负责提取出局部的身体部位信息）；二是bidirectional multi-step learning（通过多步学习，从红外和可见光分别出发，以减小modality gap）</h2><h2 id="insight"><a href="#insight" class="headerlink" title="insight"></a>insight</h2><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="part-prototype-alignment-learning主要由三个部分组成："><a href="#part-prototype-alignment-learning主要由三个部分组成：" class="headerlink" title="part prototype alignment learning主要由三个部分组成："></a>part prototype alignment learning主要由三个部分组成：</h3><h4 id="1-prototype-discovery（这个部分是用来获取身体部位细节信息的，并且要在feature-map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是"><a href="#1-prototype-discovery（这个部分是用来获取身体部位细节信息的，并且要在feature-map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是" class="headerlink" title="1.prototype discovery（这个部分是用来获取身体部位细节信息的，并且要在feature map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是"></a>1.prototype discovery（这个部分是用来获取身体部位细节信息的，并且要在feature map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是</h4><h1 id="CLIP-Driven-Semantic-Discovery-Network-for-Visible-Infrared-Person-Re-Identification"><a href="#CLIP-Driven-Semantic-Discovery-Network-for-Visible-Infrared-Person-Re-Identification" class="headerlink" title="CLIP-Driven Semantic Discovery Network for  Visible-Infrared Person Re-Identification"></a>CLIP-Driven Semantic Discovery Network for  Visible-Infrared Person Re-Identification</h1><h2 id="TMM，2024-1-11"><a href="#TMM，2024-1-11" class="headerlink" title="TMM，2024.1.11"></a>TMM，2024.1.11</h2><h2 id="这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点-出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification）的出发点有点类似"><a href="#这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点-出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification）的出发点有点类似" class="headerlink" title="这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点&#x2F;出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification）的出发点有点类似"></a>这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点&#x2F;出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification）的出发点有点类似</h2>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/01/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/RAG/Retrieval-Augmented%20Generation%20for%20%20AI-Generated%20Content%20%20A%20Survey/" rel="prev" title="">
      <i class="fa fa-chevron-left"></i> 
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/02/03/hello-world/" rel="next" title="Hello World">
      Hello World <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#From-Cross-Modal-to-Mixed-Modal-Visible-Infrared-Re-Identification"><span class="nav-number">1.</span> <span class="nav-text">From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv-2025-1-23"><span class="nav-number">1.1.</span> <span class="nav-text">arxiv 2025.1.23</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E4%B8%BB%E8%A6%81%E6%98%AF%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AAvi-reid%E4%B8%AD%E7%9A%84%E6%96%B0%E7%9A%84%E5%9C%BA%E6%99%AF%E5%90%A7%EF%BC%8C%E5%B0%B1%E6%98%AFgallery-images%E4%B8%AD%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%AD%A2%E4%B8%80%E7%A7%8D%E6%A8%A1%E6%80%81%E3%80%82"><span class="nav-number">1.2.</span> <span class="nav-text">这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery images中有两种模态的图像，而不止一种模态。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA%EF%BC%9A%E7%BB%8F%E5%85%B8%E7%9A%84vi-reid%EF%BC%8C%E5%AE%9E%E9%AA%8C%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E9%83%BD%E6%98%AFquery%E5%92%8Cgallery%E6%A8%A1%E6%80%81%E4%B8%8D%E5%90%8C%E3%80%82%E9%82%A3%E4%B9%88%EF%BC%8C%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E4%B8%80%E7%A7%8Dvisible-infrared-infrared-visible%E7%9A%84%E5%8C%B9%E9%85%8D%E3%80%82%E5%AF%B9%E5%BA%94%E5%88%B0%E5%AE%9E%E9%99%85%E6%83%85%E5%86%B5%EF%BC%8C%E5%8F%AF%E8%83%BD%E5%B0%B1%E6%98%AF%E7%99%BD%E5%A4%A9%E7%85%A7%E7%89%87-%E6%99%9A%E4%B8%8A%E7%85%A7%E7%89%87-%E6%99%9A%E4%B8%8A%E7%85%A7%E7%89%87-%E7%99%BD%E5%A4%A9%E7%85%A7%E7%89%87%E3%80%82%E4%BD%86%E6%9B%B4%E5%AE%9E%E9%99%85%E7%9A%84%E6%83%85%E5%86%B5%EF%BC%8C%E5%BA%94%E8%AF%A5%E6%98%AF%EF%BC%8C%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%8F%AF%E8%83%BD%E6%97%A2%E6%9C%89%E5%8F%AF%E8%A7%81%E5%85%89%E7%85%A7%E7%89%87%EF%BC%8C%E4%B9%9F%E6%9C%89%E7%BA%A2%E5%A4%96%E7%85%A7%E7%89%87%EF%BC%8C%E6%89%80%E4%BB%A5gallery%E4%B8%AD%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%96%87%E4%B8%AD%E6%8F%90%E5%88%B0%E7%9A%84mixed-modal%E7%9A%84setting%E3%80%82%E8%BF%99%E4%BC%9A%E5%B8%A6%E6%9D%A5%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%EF%BC%8C%E5%B0%B1%E6%98%AF%EF%BC%8Cgallery%E4%B8%AD%E5%90%8C%E4%B8%80%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E7%9A%84domain-gap%E6%98%BE%E7%84%B6%E4%BC%9A%E6%AF%94%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84domain-gap%E8%A6%81%E5%B0%8F%EF%BC%8C%E7%84%B6%E8%80%8C%E5%85%B6%E4%B8%AD%E5%90%8C%E4%B8%80%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8D%B4%E6%9C%89%E5%8F%AF%E8%83%BD%E6%98%AF%E5%B1%9E%E4%BA%8E%E4%B8%8D%E5%90%8Cid%E7%9A%84%EF%BC%8C%E8%BF%99%E5%8F%AF%E8%83%BD%E5%AF%BC%E8%87%B4%E5%8C%B9%E9%85%8D%E9%94%99%E8%AF%AF%E3%80%82%E4%B8%BE%E4%B8%AA%E5%85%B7%E4%BD%93%E4%BE%8B%E5%AD%90%EF%BC%8Cgallery%E4%B8%AD%E6%9C%89id-1%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%EF%BC%8C%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%92%8C%E4%B8%80%E4%B8%AAid-2%E7%9A%84%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E3%80%82%E7%8E%B0%E5%9C%A8%E6%9C%89%E4%B8%80%E4%B8%AAid-2%E7%9A%84%E7%BA%A2%E5%A4%96query%E3%80%82%E5%A6%82%E6%9E%9C%E7%89%B9%E5%BE%81%E5%AD%A6%E7%9A%84%E4%B8%8D%E5%A4%9F%E5%A5%BD%EF%BC%8C%E8%BF%9B%E8%A1%8C%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%BE%88%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%8A%8Aquery%E5%8C%B9%E9%85%8D%E5%88%B0id-1%E7%9A%84%E9%82%A3%E4%B8%AA%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E4%B8%8A%E4%BA%86%EF%BC%8C%E6%AF%95%E7%AB%9F%E9%83%BD%E6%98%AF%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%EF%BC%8C%E6%9C%AC%E8%BA%AB%E7%9A%84gap%E5%B0%B1%E6%AF%94%E8%BE%83%E5%B0%8F%EF%BC%8C%E8%80%8C%E7%BA%A2%E5%A4%96%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E4%B9%8B%E9%97%B4%E7%9A%84gap%E5%8F%AF%E5%B0%B1%E5%A4%A7%E4%BA%86%E3%80%82%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E5%B0%B1%E6%98%AF%E6%83%B3%E8%A7%A3%E5%86%B3%E8%BF%99%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="nav-number">1.3.</span> <span class="nav-text">动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared&#x2F;infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片&#x2F;晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain gap显然会比不同模态的domain gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id 1的一个红外图像，一个可见光图像，和一个id 2的可见光图像。现在有一个id 2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id 1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。这篇论文就是想解决这个问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%EF%BC%9A%E5%AE%83%E7%9A%84%E5%A4%A7%E8%87%B4%E6%80%9D%E6%83%B3%E6%98%AF%EF%BC%8C%E6%97%A2%E7%84%B6gallery%E4%B8%AD%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%8C%E9%82%A3%E4%B9%88%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%E5%B0%B1%E6%97%A2%E8%A6%81%E7%94%A8%E5%88%B0modality-shared-feature%EF%BC%8C%E4%B9%9F%E8%A6%81%E7%94%A8%E5%88%B0modality-specific-feature%E3%80%82%E6%96%87%E7%AB%A0%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95%EF%BC%8C%E6%8A%8A%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3%E5%88%B0%E4%B8%A4%E4%B8%AA%E6%AD%A3%E4%BA%A4%E7%9A%84%E5%AD%90%E7%A9%BA%E9%97%B4%E4%B8%AD%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%BB%A3%E8%A1%A8shared-feature%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%BB%A3%E8%A1%A8specific-feature%EF%BC%88%E5%A5%BD%E7%BB%8F%E5%85%B8%E7%9A%84%E5%81%9A%E6%B3%95%E5%92%8C%E8%AE%B2%E6%95%85%E4%BA%8B%E7%9A%84%E5%A5%97%E8%B7%AF%E3%80%82%E3%80%82%E6%9C%89%E6%B2%A1%E6%9C%89%E4%BB%80%E4%B9%88%E6%89%8B%E6%AE%B5%E8%83%BD%E9%AA%8C%E8%AF%81%E3%80%81%E5%8F%AF%E8%A7%86%E5%8C%96%E8%BF%99%E4%BF%A9%E7%A1%AE%E5%AE%9E%E6%AD%A3%E4%BA%A4%EF%BC%8C%E7%A1%AE%E5%AE%9E%E4%BB%A3%E8%A1%A8%E4%BA%86shared-feature%E5%92%8Cspecific-feature%E5%91%A2%EF%BC%9F%EF%BC%89"><span class="nav-number">1.4.</span> <span class="nav-text">方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared feature，也要用到modality-specific feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared feature，一个代表specific feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared feature和specific feature呢？）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%88%E8%AF%9D%E8%AF%B4%E5%9B%9E%E6%9D%A5%EF%BC%8C%E5%9C%A8%E7%BB%8F%E5%85%B8%E7%9A%84vi-reid%E4%B8%AD%EF%BC%8C%E7%9C%9F%E7%9A%84%E6%9C%89%E5%BF%85%E8%A6%81%E7%94%A8%E5%88%B0modality-specific-feature%E5%90%97%EF%BC%9F%E5%9C%A8%E7%BB%8F%E5%85%B8%E8%AE%BE%E5%AE%9A%E4%B8%8B%EF%BC%8C%E4%B8%8D%E6%98%AFquery%E5%92%8Cgallery%E4%B8%80%E5%AE%9A%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E5%90%97%EF%BC%9F%E9%82%A3modality-specific-feature%EF%BC%8C%E5%8F%AA%E6%9C%89%E4%B8%80%E6%96%B9%E6%9C%89%EF%BC%8C%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%E7%94%A8%E4%B8%8D%E4%B8%8A%E5%95%8A%EF%BC%9F%E4%BA%8B%E5%AE%9E%E4%B8%8A%EF%BC%8C%E4%BA%BA%E6%9D%A5%E8%BF%9B%E8%A1%8C%E7%BA%A2%E5%A4%96%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E7%9A%84%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E4%B9%9F%E6%98%AF%E9%80%9A%E8%BF%87%E7%A5%9E%E6%80%81%E5%92%8C%E8%84%B8%E6%9D%A5%E5%88%A4%E6%96%AD%E7%9A%84%E5%90%A7%EF%BC%8C%E6%9C%8D%E8%A3%85%E9%82%A3%E4%BA%9B%E5%9F%BA%E6%9C%AC%E7%94%A8%E4%B8%8D%E4%B8%8A%E5%95%8A%EF%BC%88%E6%8D%A2%E8%A3%85%E8%AE%BE%E5%AE%9A%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B9%9F%E6%98%AF%E8%BF%99%E4%B8%AA%E6%80%9D%E8%B7%AF%EF%BC%9F%EF%BC%89"><span class="nav-number">1.4.0.1.</span> <span class="nav-text">（话说回来，在经典的vi-reid中，真的有必要用到modality-specific feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90%EF%BC%9A%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E7%94%A8%E5%88%B0%E4%B8%80%E4%BA%9B%E4%BA%92%E4%BF%A1%E6%81%AF%E7%9A%84%E5%85%AC%E5%BC%8F%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%8E%A8%E5%AF%BC%E5%92%8C%E9%98%90%E8%BF%B0%E3%80%82%E5%9B%A0%E4%B8%BA%E8%A6%81%E7%94%A8%E5%AD%A6%E5%88%B0%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E7%9A%84embedding%E6%9D%A5%E5%88%A4%E6%96%ADid%EF%BC%8C%E6%89%80%E4%BB%A5%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96embedding%E5%92%8Cid%E4%B9%8B%E9%97%B4%E7%9A%84%E6%A0%87%E7%AD%BE%E3%80%82%E8%80%8C%E8%80%83%E8%99%91%E5%88%B0%E7%8E%B0%E5%9C%A8%E7%9A%84mixed-modality-scenario%EF%BC%8C%E9%9C%80%E8%A6%81%E6%8A%8Aembedding%E6%8B%86%E5%88%86%E6%88%90%E4%B8%A4%E5%9D%97%EF%BC%8C%E5%B0%B1%E6%98%AF%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84modality-shared%E5%92%8C-specific-feature%EF%BC%88%E6%88%96%E8%80%85%E8%AF%B4%E5%8E%9F%E6%96%87%E7%9A%84modality-erased-related-feature%EF%BC%89%E3%80%82%E4%B8%8B%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%80%8E%E4%B9%88%E6%8A%8A%E8%BF%99%E4%B8%A4%E9%83%A8%E5%88%86%E5%AD%A6%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82%E8%AE%BA%E6%96%87%E6%8A%8A%E8%BF%99%E9%83%A8%E5%88%86%E8%BD%AC%E5%8C%96%E6%88%90%E4%B8%80%E4%B8%AA%E5%BE%85%E4%BC%98%E5%8C%96%E7%9A%84%E5%BC%8F%E5%AD%90%EF%BC%9A-max-Z-m-e-Z-m-r-MI-Z-m-e-Z-m-r-Y-s-t-MI-Z-m-e-Z-m-r-0-MI-Z-m-e-M-0-and-MI-Z-m-r-Y-M-0-%E4%B8%8A%E9%9D%A2%E7%9A%84-Z-m-e-%E4%BB%A3%E8%A1%A8%E6%A8%A1%E6%80%81m%E4%B8%8B%E7%9A%84modality-erased-feature%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E4%B8%8E%E6%A8%A1%E6%80%81%E6%97%A0%E5%85%B3%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%9B-Z-m-r-%E4%BB%A3%E8%A1%A8%E6%A8%A1%E6%80%81m%E4%B8%8B%E7%9A%84modality-related-feature%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E4%B8%8E%E6%A8%A1%E6%80%81%E7%9B%B8%E5%85%B3%E7%9A%84%E7%89%B9%E5%BE%81%E3%80%82%E6%A8%A1%E6%80%81m%E5%8F%AF%E8%83%BD%E6%98%AFvisible%EF%BC%8C%E4%B9%9F%E5%8F%AF%E8%83%BD%E6%98%AFinfrared%E3%80%82%E4%B8%89%E4%B8%AA%E7%BA%A6%E6%9D%9F%E4%B9%9F%E6%98%AF%E4%BF%9D%E8%AF%81%E5%AD%A6%E5%88%B0%E7%9A%84%E7%89%B9%E5%BE%81%E6%9C%89%E6%95%88%E7%9A%84%E5%85%B3%E9%94%AE%E3%80%82%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%BA%A6%E6%9D%9F%EF%BC%8C%E6%98%AF%E6%83%B3%E8%AE%A9modality-erased-feature%E4%B8%8Emodality-related-feature%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B%E3%80%82%E8%BF%99%E4%B8%AA%E6%98%AF%E5%90%88%E7%90%86%E7%9A%84%E3%80%82%E6%AF%95%E7%AB%9F%E7%90%86%E8%AE%BA%E4%B8%8A%E8%BF%99%E4%B8%A4%E9%83%A8%E5%88%86%E7%9A%84%E4%BA%A4%E9%9B%86%E6%98%AF%E7%A9%BA%E9%9B%86%EF%BC%8C%E5%B9%B6%E9%9B%86%E5%B0%B1%E6%98%AF%E6%89%80%E6%9C%89%E7%9A%84%E7%89%B9%E5%BE%81%E4%BF%A1%E6%81%AF%E4%BA%86%E3%80%82%E7%AC%AC%E4%BA%8C%E4%B8%AA%E7%BA%A6%E6%9D%9F%EF%BC%8C%E6%98%AF%E5%9B%A0%E4%B8%BA-Z-m-e-%E4%BB%A3%E8%A1%A8modality-erased-feature%EF%BC%8C%E6%89%80%E4%BB%A5%E7%94%A8%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E5%BA%94%E8%AF%A5%E6%8E%A8%E6%96%AD%E4%B8%8D%E5%87%BA%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%EF%BC%8C%E6%89%80%E4%BB%A5%E5%AE%83%E5%BA%94%E8%AF%A5%E4%B8%8E%E6%A8%A1%E6%80%81%E7%8B%AC%E7%AB%8B%EF%BC%8C%E4%B9%9F%E5%90%88%E7%90%86%E3%80%82%E7%AC%AC%E4%B8%89%E4%B8%AA%E7%BA%A6%E6%9D%9F%EF%BC%8C%E6%B2%A1%E6%90%9E%E6%98%8E%E7%99%BD%E5%AE%83%E6%83%B3%E5%B9%B2%E4%BB%80%E4%B9%88%E3%80%82%E6%8C%89%E5%8E%9F%E8%AE%BA%E6%96%87%EF%BC%8C%E5%AE%83%E6%98%AF%E5%B8%8C%E6%9C%9B-Z-m-r-%E4%B8%8D%E5%8C%85%E5%90%ABID%E7%9B%B8%E5%85%B3%E7%9A%84%E4%BF%A1%E6%81%AF%E3%80%82%E4%BD%86%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%99%E6%A0%B7%EF%BC%9F%E5%A6%82%E6%9E%9C%E7%9C%9F%E7%9A%84%E6%BB%A1%E8%B6%B3%E4%BA%86%E8%BF%99%E4%B8%AA%E7%BA%A6%E6%9D%9F%EF%BC%8C-Z-m-r-%E5%B0%B1%E6%98%AF%E5%8F%AA%E8%83%BD%E7%94%A8%E6%9D%A5%E6%8E%A8%E6%96%AD%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%EF%BC%8C%E4%B8%8D%E8%83%BD%E7%94%A8%E6%9D%A5%E6%8E%A8%E6%96%ADID%E4%BF%A1%E6%81%AF%E3%80%82%E9%82%A3%E5%AE%83%E5%92%8C-Z-m-e-%E7%BB%84%E5%90%88%E8%B5%B7%E6%9D%A5%E7%94%A8%EF%BC%8C%E5%8F%88%E6%98%AF%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%9F%E8%BF%98%E6%98%AF%E8%AF%B4%EF%BC%8C%E5%AE%83%E6%98%AF%E5%BD%BB%E5%BA%95%E7%9A%84%E6%8A%8A%E6%A8%A1%E6%80%81%E5%92%8CID%EF%BC%88%E8%AF%AD%E4%B9%89%EF%BC%89%E4%BF%A1%E6%81%AF%E7%BB%99%E5%88%86%E5%BC%80%E4%BA%86%EF%BC%8C%E5%88%86%E5%88%B0%E4%B8%A4%E4%B8%AA%E7%89%B9%E5%BE%81%E9%87%8C%EF%BC%9F%E6%89%80%E4%BB%A5%E6%8E%A8%E6%96%AD%E7%9A%84%E6%97%B6%E5%80%99%E6%98%AF%E6%80%8E%E4%B9%88%E6%8E%A8%E6%96%AD%E7%9A%84%EF%BC%9F%E6%98%AF%E8%AF%B4%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%98%AF%E5%90%8C%E4%B8%80%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E6%9D%A5%E5%8C%B9%E9%85%8D%EF%BC%8C%E5%88%99%E7%94%A8%E7%9A%84%E7%89%B9%E5%BE%81%E6%98%AF%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84modality-erased-feature%E5%92%8Cmodality-related-feature%E6%8B%BC%E6%8E%A5%E8%B5%B7%E6%9D%A5%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E5%BE%81%EF%BC%9B%E5%A6%82%E6%9E%9C%E6%98%AF%E8%B7%A8%E6%A8%A1%E6%80%81%E7%9A%84%E5%8C%B9%E9%85%8D%EF%BC%8C%E5%88%99%E5%8F%AA%E7%94%A8modality-erased-feature%EF%BC%88%E4%BD%86%E8%BF%99%E6%A0%B7%E4%B8%8D%E6%98%AF%E5%BE%88%E9%BA%BB%E7%83%A6%E5%90%97%EF%BC%9F%E5%BE%97%E5%86%8D%E7%9C%8B%E7%9C%8B%E4%BB%A3%E7%A0%81%E4%BA%86%E5%8F%AF%E8%83%BD%EF%BC%89%E6%80%BB%E8%80%8C%E8%A8%80%E4%B9%8B%EF%BC%8C%E5%BE%97%E5%88%B0%E4%BA%86%E4%B8%8A%E9%9D%A2%E7%9A%84%E5%BE%85%E4%BC%98%E5%8C%96%E5%BC%8F%E5%AD%90%EF%BC%8C%E4%B8%8B%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%8A%8A%E5%AE%83%E8%BD%AC%E5%8C%96%E6%88%90%E4%B8%80%E4%B8%AA%E8%BF%91%E4%BC%BC%E7%9A%84%E5%BC%8F%E5%AD%90%EF%BC%8C%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%8E%BB%E5%88%BB%E7%94%BB%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%8F%98%E6%88%90%E5%8F%AF%E4%BB%A5%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82%E5%BE%97%E5%88%B0%E7%BB%93%E6%9E%9C%E5%A6%82%E4%B8%8B%EF%BC%9A%E4%B8%94%E9%92%88%E5%AF%B9%E5%9B%9B%E4%B8%AA%E9%83%A8%E5%88%86%EF%BC%8C%E9%83%BD%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%90%84%E8%87%AA%E7%9A%84%E4%BC%98%E5%8C%96"><span class="nav-number">1.5.</span> <span class="nav-text">分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific feature（或者说原文的modality-erased&#x2F;-related feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：$$\max_{Z_{m}^{e},Z_{m}^{r}}{MI(Z_{m}^{e},Z_{m}^{r};Y)}\ \ s.t.MI(Z_{m}^{e},Z_{m}^{r})&#x3D;0,MI(Z_{m}^{e};M)&#x3D;0,and\ MI(Z_{m}^{r};Y|M)&#x3D;0$$上面的$Z_{m}^{e}$代表模态m下的modality-erased feature，也就是与模态无关的特征；$Z_{m}^{r}$代表模态m下的modality-related feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased feature与modality-related feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为$Z_{m}^{e}$代表modality-erased feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望$Z_{m}^{r}$不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，$Z_{m}^{r}$就是只能用来推断模态信息，不能用来推断ID信息。那它和$Z_{m}^{e}$组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased feature和modality-related feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased feature（但这样不是很麻烦吗？得再看看代码了可能）总而言之，得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题。得到结果如下：且针对四个部分，都进行了各自的优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7%EF%BC%9A%E8%99%BD%E7%84%B6%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E4%B9%8D%E4%B8%80%E7%9C%8B%E5%BE%88%E7%83%82%EF%BC%8C%E7%89%B9%E5%88%AB%E6%98%AF%E6%91%98%E8%A6%81%E9%82%A3%E9%87%8C%E8%BF%98%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%90%8D%E5%AD%97%E6%89%93%E9%94%99%E4%BA%86%EF%BC%8C%E6%9B%B4%E6%98%AF%E9%99%8D%E4%BD%8E%E5%8D%B0%E8%B1%A1%E5%88%86%EF%BC%8C%E5%8F%AF%E6%98%AF%E7%9C%8B%E5%AE%8C%E4%BB%A5%E5%90%8E%EF%BC%8C%E4%B8%80%E6%9D%A5%EF%BC%8C%E5%AE%83%E6%8F%90%E5%87%BA%E7%9A%84%E6%96%B0%E5%9C%BA%E6%99%AF%E4%B9%9F%E4%B8%8D%E7%AE%97%E7%A9%BA%E4%B8%AD%E6%A5%BC%E9%98%81%EF%BC%8C%E7%A1%AE%E5%AE%9E%E6%98%AF%E6%9B%B4%E6%9C%89%E5%AE%9E%E9%99%85%E6%84%8F%E4%B9%89%E7%9A%84%EF%BC%9B%E4%BA%8C%E6%9D%A5%EF%BC%8C%E5%AE%83%E6%8F%90%E5%87%BA%E6%96%B9%E6%B3%95%E4%BB%A5%E5%90%8E%EF%BC%8C%E5%AF%B9%E6%AF%8F%E4%B8%80%E9%83%A8%E5%88%86%E7%9A%84%E5%88%BB%E7%94%BB%E5%92%8C%E9%98%90%E8%BF%B0%E9%83%BD%E8%BF%98%E7%AE%97%E5%90%88%E7%90%86%EF%BC%8C%E6%9C%89%E4%B8%80%E7%82%B9%E5%A8%93%E5%A8%93%E9%81%93%E6%9D%A5%E7%9A%84%E6%84%9F%E8%A7%89%EF%BC%8C%E9%83%BD%E6%98%AF%E5%B0%B1%E4%BA%8B%E8%AE%BA%E4%BA%8B%E7%9A%84%E5%88%86%E6%9E%90"><span class="nav-number">1.6.</span> <span class="nav-text">评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#trick%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="nav-number">1.7.</span> <span class="nav-text">trick总结：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%83%B3%E8%AE%A9%E4%B8%A4%E4%B8%AA%E5%90%91%E9%87%8F%E5%B0%BD%E5%8F%AF%E8%83%BD%E7%8B%AC%E7%AB%8B%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%94%A8%E4%BA%92%E4%BF%A1%E6%81%AF%E4%B8%BA0%E6%9D%A5%E8%BF%9B%E8%A1%8C%E7%90%86%E8%AE%BA%E4%B8%8A%E7%9A%84%E5%88%BB%E7%94%BB%EF%BC%88%E8%99%BD%E8%AF%B4%E5%85%B6%E5%AE%9E%E5%8F%AF%E8%83%BD%E6%98%AF%E6%9C%89%E5%A4%B1%E5%81%8F%E9%A2%87%E7%9A%84%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%E6%AD%A3%E4%BA%A4%E6%8D%9F%E5%A4%B1%E6%9D%A5%E8%BF%91%E4%BC%BC%E4%BB%A3%E6%9B%BF"><span class="nav-number">1.7.1.</span> <span class="nav-text">1.想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%A6%82%E6%9E%9C%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96%E6%9F%90%E4%B8%AA%E4%BA%92%E4%BF%A1%E6%81%AF%EF%BC%8C%E9%82%A3%E4%B9%88%E5%8F%AF%E4%BB%A5%E4%BB%8E%E8%AF%AD%E4%B9%89%E4%B8%8A%E5%8E%BB%E7%90%86%E8%A7%A3%EF%BC%8C%E7%BB%99%E5%87%BA%E7%9B%B8%E5%BA%94%E7%9A%84%E6%8D%9F%E5%A4%B1%E3%80%82%E6%AF%94%E5%A6%82%E8%AE%BA%E6%96%87%E9%87%8C%E6%8F%90%E5%88%B0%E7%9A%84%EF%BC%8C%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96-MI-Z-m-e-Y-%EF%BC%8C%E5%8D%B3%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96modality-erased-feature%E4%B8%8E%E6%A0%87%E7%AD%BE%E5%8F%98%E9%87%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BA%92%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%85%B6%E5%AE%9E%E7%9B%AE%E7%9A%84%E5%B0%B1%E6%98%AF%E8%AE%A9modality-erased-feature%E8%83%BD%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8F%AD%E7%A4%BA%E5%87%BAID%E4%BF%A1%E6%81%AF%EF%BC%8C%E6%89%80%E4%BB%A5%E7%94%A8%E4%B8%80%E4%B8%AA%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%B0%B1%E6%98%AF%E7%90%86%E6%89%80%E5%BA%94%E5%BD%93%E7%9A%84%E4%BA%86%EF%BC%88%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E6%98%AF%E5%AF%B9%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E7%BB%99%E5%87%BA%E4%B8%80%E4%B8%AA%E7%90%86%E8%AE%BA%E4%B8%8A%E7%9A%84%E8%A7%A3%E9%87%8A%EF%BC%89"><span class="nav-number">1.7.2.</span> <span class="nav-text">2.如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化$MI(Z_{m}^{e};Y)$，即要最大化modality-erased feature与标签变量之间的互信息，其实目的就是让modality-erased feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E8%A6%81%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%B7%B7%E6%B7%86%E6%9F%90%E5%87%A0%E4%B8%AA%E9%83%A8%E5%88%86%EF%BC%88%E8%BF%99%E5%8F%AF%E8%83%BD%E6%98%AF%E5%87%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%E7%9A%84%E8%80%83%E8%99%91%EF%BC%89%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%EF%BC%8C%E4%BD%86%E5%8A%A0%E4%B8%8A%E6%A2%AF%E5%BA%A6%E5%8F%8D%E8%BD%AC%E5%B1%82"><span class="nav-number">1.7.3.</span> <span class="nav-text">3.要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E8%A6%81%E7%BA%A6%E6%9D%9F%E6%9F%90%E4%B8%A4%E4%B8%AA%E9%87%8Fx%EF%BC%8Cy%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E7%BB%99%E5%87%BA%E8%BF%99%E4%B8%AA%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%8D%9F%E5%A4%B1%EF%BC%9A-L-max-x-y-alpha-0-max-y-x-alpha-0-%EF%BC%8C-alpha-%E6%98%AF%E6%AD%A3%E5%B8%B8%E6%95%B0%EF%BC%8C%E4%B9%9F%E6%98%AF%E8%B6%85%E5%8F%82%E6%95%B0%E3%80%82%E9%82%A3%E4%B9%88%EF%BC%8Cx%E5%92%8Cy%E5%BF%85%E9%A1%BB%E7%9B%B8%E5%B7%AE%E7%BB%9D%E5%AF%B9%E5%80%BC%E4%B8%8D%E5%A4%A7%E4%BA%8E-alpha-%EF%BC%8C%E8%BF%99%E9%83%A8%E5%88%86%E6%8D%9F%E5%A4%B1%E6%89%8D%E4%B8%BA0%EF%BC%8C%E5%90%A6%E5%88%99%E5%B0%B1%E4%BA%A7%E7%94%9F%E6%8D%9F%E5%A4%B1%E4%BA%86"><span class="nav-number">1.7.4.</span> <span class="nav-text">4.要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：$L&#x3D;max(x-y+\alpha,0)+max(y-x+\alpha,0)$，$\alpha$是正常数，也是超参数。那么，x和y必须相差绝对值不大于$\alpha$，这部分损失才为0，否则就产生损失了</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E8%BF%99%E4%B8%AAtrick%E7%9B%AE%E5%89%8D%E7%9C%8B%E4%B8%8B%E6%9D%A5%E5%A5%BD%E5%83%8F%E8%BF%98%E6%98%AF%E6%AF%94%E8%BE%83%E5%B1%80%E9%99%90%E7%9A%84%EF%BC%8C%E5%B0%B1%E6%98%AF%E9%92%88%E5%AF%B9%E8%B7%A8%E6%A8%A1%E6%80%81%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E7%9A%84%E6%97%B6%E5%80%99%E6%89%8D%E4%BC%9A%E7%94%A8%E5%88%B0%E3%80%82%E3%80%82%E3%80%82%E5%B0%B1%E6%98%AF%EF%BC%8C%E5%A6%82%E6%9E%9C%E8%A6%81%E5%AD%A6%E4%B9%A0modality-specific-feature%EF%BC%8C%E4%B8%94%E6%AF%8F%E4%B8%AAID%E9%83%BD%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%88%99%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E6%8A%8A%E6%AF%8F%E4%B8%80%E4%B8%AAID%E7%BB%99%E2%80%9Cdoubled%E2%80%9D%EF%BC%8C%E6%AF%94%E5%A6%82%EF%BC%8C%E5%A6%82%E6%9E%9C%E5%AE%83%E6%98%AFvisible%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%88%99ID-2%EF%BC%9B%E5%A6%82%E6%9E%9C%E5%AE%83%E6%98%AFinfrared%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%88%99ID-ID-2-1%E3%80%82%E5%9F%BA%E4%BA%8E%E8%BF%99%E4%B8%80%E5%A5%97%E6%96%B0%E7%9A%84%E6%A0%87%E7%AD%BE%EF%BC%8C%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%EF%BC%8C%E5%B9%B6%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E6%9D%A5%E4%BC%98%E5%8C%96%E3%80%82%E8%BF%99%E9%87%8C%E4%B9%8B%E6%89%80%E4%BB%A5%E8%83%BD%E8%B5%B7%E5%88%B0%E6%95%88%E6%9E%9C%EF%BC%8C%E6%98%AF%E5%9B%A0%E4%B8%BA%EF%BC%8C%E6%88%91%E4%BB%AC%E6%8A%8A%E5%90%8C%E4%B8%80ID%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%99%E8%A7%86%E4%B8%BA%E4%B8%8D%E5%90%8C%E6%A0%87%E7%AD%BE%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%BF%9B%E8%A1%8C%E6%9B%B4%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E5%88%86%E7%B1%BB%E3%80%82%E6%A8%A1%E5%9E%8B%E5%A6%82%E6%9E%9C%E8%A6%81%E6%AD%A3%E7%A1%AE%E5%88%86%E7%B1%BB%EF%BC%8C%E5%B0%B1%E5%BF%85%E9%A1%BB%E5%AD%A6%E4%BC%9A%E6%8D%95%E8%8E%B7%E5%90%8C%E4%B8%80ID%E4%B8%8Bvisible%E5%92%8Cinfrared%E5%9B%BE%E5%83%8F%E7%9A%84%E5%90%84%E8%87%AA%E7%9A%84%E3%80%81%E6%9C%89%E5%88%A4%E5%88%AB%E5%8A%9B%E7%9A%84%E7%89%B9%E5%BE%81%E3%80%82%E8%BF%99%E6%A0%B7%E6%A8%A1%E5%9E%8B%E5%B0%B1%E8%83%BD%E5%AD%A6%E4%BC%9A%E6%8F%90%E5%8F%96%E5%87%BAmodality-specific-feature%E3%80%82%E5%BD%93%E7%84%B6%EF%BC%8C%E8%BF%99%E4%B8%AAtrick%E4%B9%9F%E5%B9%B6%E4%B8%8D%E5%94%AF%E4%B8%80%EF%BC%8C%E8%82%AF%E5%AE%9A%E6%98%AF%E8%BF%98%E6%9C%89%E5%88%AB%E7%9A%84%E6%96%B9%E6%B3%95%E7%9A%84%E3%80%82%E5%B0%B1%E6%AF%94%E5%A6%82%EF%BC%8C%E5%86%8D%E7%94%A8%E4%B8%AA%E6%8D%9F%E5%A4%B1%EF%BC%8C%E7%BA%A6%E6%9D%9F%E5%90%8C%E4%B8%80ID%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E8%A6%81%E5%B0%BD%E5%8F%AF%E8%83%BD%E8%BF%9C%E7%A6%BB%EF%BC%88%E6%9C%89%E7%82%B9%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%84%8F%E6%80%9D%EF%BC%9A%E9%80%9A%E8%BF%87%E8%AE%A9%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%88%AB%E7%9A%84%E6%95%B0%E6%8D%AE%E5%B0%BD%E5%8F%AF%E8%83%BD%E8%BF%9C%E7%A6%BB%EF%BC%8C%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%BD%9C%E7%A7%BB%E9%BB%98%E5%8C%96%E5%9C%B0%E5%AD%A6%E4%BC%9A%E4%BA%86%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%EF%BC%89%EF%BC%8C%E5%BA%94%E8%AF%A5%E4%B9%9F%E8%83%BD%E5%AD%A6%E4%BC%9A%E6%8F%90%E5%8F%96modality-specific-feature"><span class="nav-number">1.7.5.</span> <span class="nav-text">5.这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID * 2；如果它是infrared的图像，则ID&#x3D;ID * 2 + 1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific feature</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spectral-Enhancement-and-Pseudo-Anchor-Guidance-for-Infrared-Visible-Person-Re-Identification"><span class="nav-number">2.</span> <span class="nav-text">Spectral Enhancement and Pseudo-Anchor Guidance  for Infrared-Visible Person Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv-2025-1-2"><span class="nav-number">2.1.</span> <span class="nav-text">arxiv, 2025.1.2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E4%B8%BB%E8%A6%81%E6%98%AF%E4%BB%8E%E8%B0%B1%E5%9F%9F%E7%9A%84%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%BC%93%E8%A7%A3%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%B7%A8%E5%A4%A7%E5%B7%AE%E5%BC%82%EF%BC%88%E4%BD%86%E5%AE%83%E5%8F%AA%E6%98%AF%E7%9B%B4%E6%8E%A5%E6%8C%87%E5%87%BA%E8%BF%99%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9C%89%E5%B7%A8%E5%A4%A7%E7%9A%84spectral-gap%EF%BC%8C%E5%8D%B4%E5%B9%B6%E6%B2%A1%E6%9C%89%E5%88%86%E6%9E%90%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E8%BF%99%E7%A7%8Dgap%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%E6%88%91%E4%BB%AC%E6%80%8E%E4%B9%88%E7%A1%AE%E8%AE%A4%E8%BF%99%E7%A7%8Dgap%E7%A1%AE%E5%AE%9E%E5%AD%98%E5%9C%A8%EF%BC%8C%E8%80%8C%E4%B8%8D%E5%8F%AA%E6%98%AF%E8%AF%B4%E8%AF%B4%E8%80%8C%E5%B7%B2%E3%80%82%E6%9C%89%E7%A7%8D%E6%8B%BF%E5%88%B0%E8%B0%B1%E5%9F%9F%E6%96%B9%E6%B3%95%E5%B0%B1%E6%9D%A5%E8%AF%95%E4%B8%80%E4%B8%8B%E7%9A%84%E6%84%9F%E8%A7%89%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA%EF%BC%9A"><span class="nav-number">2.3.</span> <span class="nav-text">动机：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">2.4.</span> <span class="nav-text">方法：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90%EF%BC%9A%E7%AC%AC%E4%B8%80%E6%AD%A5%E6%98%AF%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AASemantically-Enhanced-Grey-Images%EF%BC%88%E5%A7%91%E4%B8%94%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E6%98%AF%E4%B8%80%E7%A7%8D%E6%8B%89%E8%BF%91%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E5%8F%AA%E4%B8%8D%E8%BF%87%E6%98%AF%E4%BB%8E%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E5%87%BA%E5%8F%91%EF%BC%8C%E5%BE%80%E7%BA%A2%E5%A4%96%E4%B8%8A%E9%9D%A0%E6%8B%A2%E3%80%82%E4%B9%9F%E5%90%88%E7%90%86%EF%BC%8C%E6%AF%95%E7%AB%9F%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E4%BF%A1%E6%81%AF%E6%9B%B4%E5%A4%9A%EF%BC%8C%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E4%BF%A1%E6%81%AF%E6%9B%B4%E5%B0%91%EF%BC%8C%E4%BB%8E%E4%BF%A1%E6%81%AF%E6%9B%B4%E5%A4%9A%E7%9A%84%E5%BE%80%E4%BF%A1%E6%81%AF%E6%9B%B4%E5%B0%91%E7%9A%84%E5%8F%98%E6%8D%A2%E6%98%AF%E6%AF%94%E8%BE%83%E5%AE%B9%E6%98%93%E7%9A%84%EF%BC%89%E3%80%82%E5%AE%83%E4%B8%BB%E8%A6%81%E5%81%9A%E7%9A%84%E5%85%B6%E5%AE%9E%E6%98%AF%EF%BC%8C%E5%9C%A8%E5%BE%88%E5%B8%B8%E8%A7%81%E7%9A%84%E7%81%B0%E5%BA%A6%E6%98%A0%E5%B0%84%E4%B9%8B%E4%BD%99%EF%BC%8C%E5%8A%A0%E4%B8%8A%E4%BA%86%E9%AB%98%E9%A2%91%E4%BF%A1%E6%81%AF%E3%80%82%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%AE%83%E4%BC%9A%E5%85%88%E5%AF%B9%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%EF%BC%8C%E6%8F%90%E5%8F%96%E5%AE%83%E7%9A%84%E9%A2%91%E5%9F%9F%E4%BF%A1%E6%81%AF%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%E5%85%B6%E4%B8%AD%E7%9A%84%E9%A2%91%E7%8E%87%E6%88%90%E5%88%86%E8%BF%9B%E8%A1%8C%E5%82%85%E9%87%8C%E5%8F%B6%E9%80%86%E5%8F%98%E6%8D%A2%EF%BC%88%E8%BF%99%E9%87%8C%E6%98%AF%E4%B8%BA%E4%BA%86%E6%8F%90%E5%8F%96%E8%BD%AE%E5%BB%93%E4%BF%A1%E6%81%AF%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E5%B0%86%E9%80%86%E5%8F%98%E6%8D%A2%E7%9A%84%E7%BB%93%E6%9E%9C%E4%B8%8E%E7%81%B0%E5%BA%A6%E6%98%A0%E5%B0%84%E7%9A%84%E7%BB%93%E6%9E%9C%E7%9B%B8%E5%8A%A0%EF%BC%8C%E5%BE%97%E5%88%B0Semantically-Enhanced-Grey-Images%EF%BC%88%E4%B8%8B%E7%AE%80%E7%A7%B0SEG%E5%9B%BE%E5%83%8F%EF%BC%89%E3%80%82btw%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%96%B9%E6%B3%95%E5%80%92%E6%98%AF%E5%8F%AF%E4%BB%A5%E5%80%9F%E9%89%B4%E4%B8%80%E4%B8%8B%EF%BC%8C%E6%84%9F%E8%A7%89%E8%BF%98%E6%98%AF%E8%A1%8C%E5%BE%97%E9%80%9A%E7%9A%84%EF%BC%8C%E8%B5%B7%E7%A0%81%E5%8F%AF%E4%BB%A5%E8%AF%95%E8%AF%95%E7%94%A8%E5%88%B0%E6%88%91%E4%BB%AC%E7%9A%84%E6%96%B9%E6%B3%95%E9%87%8C%E3%80%82%E5%B0%B1%E6%98%AF%E5%8A%A0%E4%B8%8A%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%8F%AF%E8%83%BD%E8%AE%A1%E7%AE%97%E9%87%8F%E5%A4%A7%E4%BA%86%E7%AC%AC%E4%BA%8C%E6%AD%A5%E6%98%AF%E5%AF%B9%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E5%92%8CSEG%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%EF%BC%8C%E4%B9%9F%E6%98%AF%E4%B8%BA%E4%BA%86%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%BC%A9%E5%B0%8F%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%EF%BC%88%E5%9B%A0%E4%B8%BA%E6%9C%80%E5%90%8E%E8%BF%98%E6%98%AF%E8%A6%81%E7%94%A8%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E7%9A%84%EF%BC%89%E3%80%82%E8%BF%99%E9%87%8C%E5%85%B6%E5%AE%9E%E5%B9%B3%E5%B9%B3%E6%97%A0%E5%A5%87%EF%BC%8C-%E5%B0%B1%E6%98%AF%E7%94%A8ResNet-50%E7%9A%84%E5%89%8D%E4%B8%89%E5%9D%97%EF%BC%88%E5%85%B1%E4%BA%AB%E6%9D%83%E9%87%8D%EF%BC%89%E6%9D%A5%E6%8F%90%E5%8F%96%E5%85%B1%E5%90%8C%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E4%B9%8B%E5%90%8E%E5%8F%88%E7%94%A8ResNet-50%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E5%9D%97%EF%BC%88%E4%B8%8D%E5%90%8C%E6%9D%83%E9%87%8D%EF%BC%89%EF%BC%8C%E6%9D%A5%E5%88%86%E5%88%AB%E6%8F%90%E5%8F%96modality-specific-feature%E7%AC%AC%E4%B8%89%E6%AD%A5%E6%98%AFPABA-loss%EF%BC%8C%E5%8D%B3Pseudo-Anchor-guided-Bidirectional-Aggregation-Loss%E3%80%82%E5%AE%83%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%EF%BC%8C%E5%AF%B9%E4%BA%8E%E5%89%8D%E9%9D%A2%E7%9A%84%E5%85%B1%E5%90%8C%E7%89%B9%E5%BE%81%EF%BC%8C%E5%B8%8C%E6%9C%9B%E5%AE%83%E4%BB%AC%E8%83%BD%E6%9B%B4%E5%8A%A0%E5%85%BC%E5%AE%B9%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%B0%BD%E5%8F%AF%E8%83%BD%E7%9A%84%E5%8F%98%E6%8D%A2%E5%88%B0%E4%B8%80%E4%B8%AA%E7%A9%BA%E9%97%B4%E9%87%8C%EF%BC%8C%E4%BD%86%E4%B9%9F%E4%B8%8D%E8%83%BD%E5%B0%B1%E5%A4%B1%E5%8E%BB%E5%88%A4%E6%96%AD%E5%8A%9B%E4%BA%86%E3%80%82%E8%80%8C%E6%89%80%E8%B0%93Anchor-guided%EF%BC%8C%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E7%94%A8%E2%80%9C%E5%90%91Anchor%E9%9D%A0%E6%8B%A2%E2%80%9D%E6%9D%A5%E5%AE%9E%E7%8E%B0%E4%B8%A4%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%9A%84%E9%9D%A0%E6%8B%A2%E3%80%82%E4%B8%BE%E4%B8%AA%E5%85%B7%E4%BD%93%E4%BE%8B%E5%AD%90%EF%BC%8C%E6%88%91%E6%9C%89%E5%90%8C%E4%B8%80ID%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E8%8B%A5%E5%B9%B2%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%88%91%E5%B8%8C%E6%9C%9B%E6%8A%8A%E5%AE%83%E4%BB%AC%E5%8F%98%E6%8D%A2%E5%88%B0%E4%B8%80%E4%B8%AA%E7%BB%9F%E4%B8%80%E7%9A%84%E7%A9%BA%E9%97%B4%E9%87%8C%EF%BC%8C%E5%88%99%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%EF%BC%8C%E6%B1%82%E4%B8%80%E4%B8%8B%E5%8F%AF%E8%A7%81%E5%85%89%E7%9A%84%E4%B8%80%E4%B8%AAAnchor%EF%BC%88%E6%AF%94%E5%A6%82%EF%BC%8C%E6%89%80%E6%9C%89%E5%8F%AF%E8%A7%81%E5%85%89%E6%95%B0%E6%8D%AE%E7%9A%84%E5%9D%87%E5%80%BC%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E6%8B%89%E8%BF%91%E6%89%80%E6%9C%89%E7%BA%A2%E5%A4%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%BF%99%E4%B8%AAAnchor%E7%9A%84%E8%B7%9D%E7%A6%BB%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%AE%9E%E7%8E%B0%E7%BA%A2%E5%A4%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E5%8F%AF%E8%A7%81%E5%85%89%E6%95%B0%E6%8D%AE%E7%9A%84%E9%9D%A0%E6%8B%A2%E3%80%82%E8%AF%B4%E5%9B%9E%E8%BF%99%E9%87%8C%EF%BC%8C%E5%AE%83%E6%98%AF%E5%AF%B9%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84%E5%85%B1%E5%90%8C%E7%89%B9%E5%BE%81-F-shared-seg-%E5%92%8C-F-shared-ir-%EF%BC%8C%E5%85%88%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E5%88%86%E5%9D%97%EF%BC%8C%E5%88%86%E6%88%90N%E5%9D%97%EF%BC%8C%E4%B9%8B%E5%90%8E%E5%AF%B9%E6%AF%8F%E4%B8%AA%E5%9D%97%E9%83%BD%E6%96%BD%E5%8A%A0PABA-loss%EF%BC%8C%E6%8B%89%E8%BF%91%E5%90%8C%E4%B8%80ID%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9A%84%E8%B7%9D%E7%A6%BB%EF%BC%8C%E5%90%8C%E6%97%B6%E6%8B%89%E8%BF%9C%E4%B8%8D%E5%90%8CID%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9A%84%E8%B7%9D%E7%A6%BB%E3%80%82%E7%9B%B8%E6%AF%94%E4%BA%8E%E5%B8%B8%E8%A7%81%E7%9A%84triplet-loss%EF%BC%8C%E8%BF%99%E9%87%8C%E5%A2%9E%E5%8A%A0%E4%BA%86%E8%B7%A8%E6%A8%A1%E6%80%81%E7%9A%84%E8%80%83%E8%99%91%EF%BC%9B%E8%80%8C%E7%9B%B8%E6%AF%94%E4%BA%8Ecross-center-loss%EF%BC%8C%E8%BF%99%E9%87%8C%E7%9A%84%E6%94%B9%E8%BF%9B%E5%9C%A8%E4%BA%8E%E5%88%86%E4%BA%86%E5%9D%97%EF%BC%8C%E4%B8%94%E6%98%AF%E5%AF%B9%E6%AF%8F%E4%B8%AA%E5%9D%97%E9%83%BD%E6%96%BD%E5%8A%A0PABA-loss%EF%BC%8C%E6%95%85%E6%9B%B4%E5%8A%A0%E7%BB%86%E7%B2%92%E5%BA%A6%EF%BC%88%E8%BF%99%E9%87%8C%E5%85%B6%E5%AE%9E%E8%A6%81%E7%BB%93%E5%90%88%E5%85%AC%E5%BC%8F%E6%9D%A5%E7%9C%8B%E6%89%8D%E6%B8%85%E6%A5%9A%EF%BC%89%E9%99%A4%E4%BA%86%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84PABA-loss%EF%BC%8C%E7%BB%8F%E5%85%B8%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%BD%93%E7%84%B6%E4%B9%9F%E6%98%AF%E4%B8%8D%E8%83%BD%E5%B0%91%E7%9A%84%E3%80%82%E9%92%88%E5%AF%B9specific%E5%92%8Cshared-feature%E9%83%BD%E6%9C%89%E8%BF%99%E4%B8%AA%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E3%80%82%E4%B8%89%E9%83%A8%E5%88%86%E6%8D%9F%E5%A4%B1%E5%8A%A0%E6%9D%83%E6%B1%82%E5%92%8C%E5%8D%B3%E5%BE%97%E5%88%B0%E6%80%BB%E7%9A%84%E6%8D%9F%E5%A4%B1%EF%BC%88%E6%9D%83%E9%87%8D%E6%98%AF%E8%B6%85%E5%8F%82%EF%BC%89"><span class="nav-number">2.5.</span> <span class="nav-text">分析：第一步是生成一个Semantically Enhanced Grey Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically Enhanced Grey Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇， 就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific feature第三步是PABA loss，即Pseudo Anchor-guided Bidirectional Aggregation Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征$F_{shared}^{seg}$和$F_{shared}^{ir}$，先进行一个分块，分成N块，之后对每个块都施加PABA loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet loss，这里增加了跨模态的考虑；而相比于cross-center loss，这里的改进在于分了块，且是对每个块都施加PABA loss，故更加细粒度（这里其实要结合公式来看才清楚）除了上面提到的PABA loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification"><span class="nav-number">3.</span> <span class="nav-text">Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv%EF%BC%8C2024-12-11"><span class="nav-number">3.1.</span> <span class="nav-text">arxiv，2024.12.11</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E7%9A%84%E6%96%B9%E6%B3%95%E7%9C%8B%E8%B5%B7%E6%9D%A5%E6%9C%89%E7%82%B9%E5%A4%8D%E6%9D%82%EF%BC%8C%E7%84%B6%E5%90%8E%E5%8A%A8%E6%9C%BA%E4%B8%8A%E7%9C%8B%EF%BC%8C%E6%84%9F%E8%A7%89%E4%B9%9F%E6%9C%89%E4%BA%9B%E7%89%B5%E5%BC%BA%EF%BC%8C%E6%9B%B4%E5%A4%9A%E6%98%AF%E4%B8%BA%E4%BA%86%E7%94%A8%E4%B8%8A%E7%8E%B0%E5%9C%A8%E5%BE%88%E7%81%AB%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C%E6%89%80%E4%BB%A5%E7%A1%AC%E5%87%91%E4%BA%86%E4%B8%AA%E7%90%86%E7%94%B1"><span class="nav-number">3.2.</span> <span class="nav-text">这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#insight%EF%BC%9A%E5%88%A9%E7%94%A8%E4%BA%86%E4%B8%80%E4%B8%8Btext%EF%BC%8C%E7%94%A8%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%E6%9D%A5%E7%9B%B4%E6%8E%A5%E8%A1%A5%E5%85%85%E4%B8%80%E9%83%A8%E5%88%86%E5%9B%BE%E5%83%8F%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%AF%94%E5%8D%95%E7%BA%AF%E7%94%A8%E5%9B%BE%E5%83%8Fencoder%E6%9D%A5%E6%8F%90%E5%8F%96%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E8%A6%81%E5%A5%BD%E4%B8%80%E7%82%B9%EF%BC%9B%E6%8F%90%E5%87%BA%E8%8B%A5%E5%B9%B2%E6%8D%9F%E5%A4%B1%E6%9D%A5%E5%88%A9%E7%94%A8%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%EF%BC%88%E4%BD%86%E6%98%AF%E5%90%A6%E5%90%88%E7%90%86%E6%9C%89%E5%BE%85%E8%80%83%E5%AF%9F%EF%BC%8C%E8%87%B3%E5%B0%91%E7%9C%8B%E5%AE%83%E7%9A%84%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%EF%BC%8C%E8%AF%81%E6%98%8E%E4%BA%86%E8%BF%99%E9%83%A8%E5%88%86%E6%98%AF%E6%9C%89%E6%95%88%E7%9A%84%EF%BC%89"><span class="nav-number">3.3.</span> <span class="nav-text">insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA%EF%BC%9A%E6%96%87%E7%AB%A0%E6%8C%87%E5%87%BA%EF%BC%8C%E7%8E%B0%E5%9C%A8%E7%9A%84VI-ReID%E6%96%B9%E6%B3%95%E6%9C%89%E4%B8%A4%E5%A4%A7%E7%B1%BB%EF%BC%8C%E4%B8%80%E7%B1%BB%E6%98%AF%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E6%95%88%E6%9E%9C%E4%B8%8D%E5%A4%9F%E5%A5%BD%EF%BC%88%E4%B8%AA%E4%BA%BA%E4%B8%8D%E5%A4%AA%E4%BA%86%E8%A7%A3%EF%BC%89%EF%BC%8C%E4%B8%80%E7%B1%BB%E6%98%AF%E5%9F%BA%E4%BA%8E%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E7%94%A8%E6%A8%A1%E5%9E%8B%E6%8A%8A%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8Cembed%EF%BC%8C%E7%94%A8embedding%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%8C%B9%E9%85%8D%EF%BC%8C%E9%80%9A%E5%B8%B8%E9%9C%80%E8%A6%81%E5%90%84%E7%A7%8D%E9%AD%94%E6%94%B9%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E3%80%82%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E7%9A%84%E6%95%88%E6%9E%9C%E4%BC%9A%E6%9B%B4%E5%A5%BD%EF%BC%8C%E4%B9%9F%E6%9B%B4%E4%B8%BB%E6%B5%81%E3%80%82%E8%80%8C%E6%96%87%E7%AB%A0%E6%8C%87%E5%87%BA%EF%BC%8C%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E4%BB%85%E5%88%A9%E7%94%A8%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%AF%B9%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84%E6%8F%90%E5%8F%96%E4%B8%8D%E5%A4%9F%EF%BC%8C%E5%9B%A0%E6%AD%A4%E5%B8%8C%E6%9C%9B%E5%8A%A0%E5%85%A5%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%EF%BC%8C%E6%9D%A5%E5%B8%AE%E5%8A%A9%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%8F%90%E5%8F%96%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%E3%80%82%EF%BC%88%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B8%80%E7%82%B9%EF%BC%8C%E8%AE%A9%E6%88%91%E6%84%9F%E8%A7%89%E7%95%A5%E6%9C%89%E7%82%B9%E7%89%B5%E5%BC%BA%E3%80%82%E5%AE%83%E6%89%80%E8%B0%93%E7%9A%84%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%EF%BC%8C%E4%B9%9F%E6%98%AF%E4%BA%BA-llm%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E7%9A%84%E6%8F%8F%E8%BF%B0%EF%BC%8C%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E6%9C%AC%E8%BA%AB%E5%B0%B1%E6%98%AF%E6%9D%A5%E8%87%AA%E5%9B%BE%E5%83%8F%E7%9A%84%E3%80%82%E8%87%B3%E4%BA%8E%E8%AF%B4%E6%8F%90%E5%8F%96%E7%9A%84%E4%B8%8D%E5%A4%9F%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E6%98%AF%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E4%B8%8D%E5%A4%9F%E5%A5%BD%E3%80%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%A4%9F%E5%A4%9A%E7%AD%89%E9%97%AE%E9%A2%98%E5%AF%BC%E8%87%B4%E7%9A%84%E3%80%82%E5%8A%A0%E5%85%A5%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%EF%BC%8C%E6%9B%B4%E5%A4%9A%E5%8F%AF%E8%83%BD%E6%98%AF%E5%B8%8C%E6%9C%9B%E8%83%BD%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%83%BD%E2%80%9C%E6%AF%94%E8%BE%83%E5%AE%B9%E6%98%93%E2%80%9D%E5%9C%B0%E5%AD%A6%E5%88%B0%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%EF%BC%89%EF%BC%88%E4%B8%8D%E8%BF%87%E8%80%83%E8%99%91%E5%88%B0%E7%8E%B0%E5%9C%A8reid%E7%9A%84%E9%AA%A8%E5%B9%B2%E5%9F%BA%E6%9C%AC%E9%83%BD%E6%98%AF%E4%B8%80%E4%B8%AAResNet50%EF%BC%8C%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F%E5%BA%94%E8%AF%A5%E6%98%AF%E6%9C%89%E4%B8%8A%E9%99%90%E7%9A%84%EF%BC%8C%E9%82%A3%E4%B9%88%E7%94%A8%E6%96%87%E5%AD%97embedding%E6%9D%A5%E8%BE%85%E5%8A%A9%E4%B8%80%E4%B8%8B%E4%B9%9F%E6%9C%89%E9%81%93%E7%90%86%EF%BC%89"><span class="nav-number">3.4.</span> <span class="nav-text">动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人&#x2F;llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%EF%BC%9A-1"><span class="nav-number">3.5.</span> <span class="nav-text">方法：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AAEEES%E6%A1%86%E6%9E%B6%EF%BC%8C%E5%8C%85%E5%90%AB%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%EF%BC%8C%E5%90%84%E5%8F%B8%E5%85%B6%E8%81%8C%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%98%AFESE%E6%A8%A1%E5%9D%97%EF%BC%88Explicit-Semantics-Embedding%EF%BC%89%EF%BC%8C%E5%AE%83%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%EF%BC%8C%E5%88%A9%E7%94%A8%E6%96%87%E6%9C%AC%EF%BC%8C%E8%AE%A9%E5%9B%BE%E5%83%8Fembedding%E5%AD%A6%E5%88%B0%E6%9B%B4%E5%A4%9A%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E3%80%82%E4%B8%BB%E8%A6%81%E5%81%9A%E7%9A%84%E6%98%AF%E5%BC%95%E5%85%A5vllm%EF%BC%8C%E4%B8%BA%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%8B%89%E8%BF%91%E5%9B%BE%E5%83%8Fembedding%E5%92%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%E7%9A%84embedding%EF%BC%8C%E8%AE%A9%E5%9B%BE%E5%83%8Fembedding%E5%AD%A6%E5%88%B0%E6%96%87%E6%9C%AC%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF"><span class="nav-number">3.5.1.</span> <span class="nav-text">提出一个EEES框架，包含三个部分，各司其职第一个是ESE模块（Explicit Semantics Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E4%B8%AA%E6%98%AFCVSC%EF%BC%88Cross-View-Semantics-Compensation%EF%BC%89%EF%BC%8C%E5%AE%83%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%EF%BC%8C%E5%88%A9%E7%94%A8%E5%A4%9A%E8%A7%86%E8%A7%92%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%8C%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%83%BD%E6%8F%90%E5%8F%96%E5%87%BA%E6%9B%B4%E5%8A%A0%E4%B8%B0%E5%AF%8C%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E3%80%82%E4%B8%BE%E4%B8%AA%E4%BE%8B%E5%AD%90%EF%BC%8C%E5%AF%B9%E4%BA%8E%E5%90%8C%E4%B8%80%E4%B8%AA%E4%BA%BA%EF%BC%8C%E4%B8%80%E4%B8%AA%E6%91%84%E5%83%8F%E5%A4%B4%EF%BC%88%E8%A7%86%E8%A7%92%EF%BC%89%E6%8B%8D%E5%88%B0%E7%9A%84%E7%85%A7%E7%89%87%E6%89%80%E8%95%B4%E5%90%AB%E7%9A%84%E4%BF%A1%E6%81%AF%EF%BC%8C%E7%BB%88%E5%BD%92%E6%98%AF%E6%9C%89%E9%99%90%E7%9A%84%E3%80%82%E5%8F%AF%E8%83%BD%E8%BF%99%E4%B8%AA%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E7%85%A7%E7%89%87%E8%83%BD%E6%8B%8D%E5%88%B0%E8%BA%AB%E6%9D%90%E8%BD%AE%E5%BB%93%EF%BC%8C%E8%84%B8%E5%8D%B4%E8%A2%AB%E9%81%AE%E4%BD%8F%E4%BA%86%EF%BC%9B%E8%BF%99%E6%97%B6%E5%A6%82%E6%9E%9C%E8%83%BD%E7%94%A8%E5%88%B0%E5%85%B6%E5%AE%83%E8%A7%86%E8%A7%92%E4%B8%8B%E8%BF%99%E4%B8%AA%E4%BA%BA%E7%9A%84%E7%85%A7%E7%89%87%EF%BC%8C%E5%8F%AF%E8%83%BD%E5%B0%B1%E8%83%BD%E8%8E%B7%E5%BE%97%E8%84%B8%E9%83%A8%E4%BF%A1%E6%81%AF%E4%BA%86%E3%80%82%E7%BB%BC%E5%90%88%E8%B5%B7%E6%9D%A5%EF%BC%8C%E5%B0%B1%E8%83%BD%E6%8F%90%E5%8F%96%E5%87%BA%E8%BF%99%E4%B8%AA%E4%BA%BA%E7%9A%84%E6%9B%B4%E5%A4%9A%E7%89%B9%E5%BE%81%E3%80%82%E6%89%80%E4%BB%A5%EF%BC%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%AF%B9%E4%BA%8E%E4%B8%80%E4%B8%AAbatch%E9%87%8C%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%9B%BE%E5%83%8F%EF%BC%8C%E8%BF%99%E5%A5%97%E6%96%B9%E6%B3%95%E9%83%BD%E4%BC%9A%E5%8E%BB%E5%88%A9%E7%94%A8%E5%90%8CID%E5%9C%A8%E5%85%B6%E5%AE%83%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E3%80%82%E8%80%8C%E5%85%B7%E4%BD%93%E7%9A%84%E5%88%A9%E7%94%A8%E6%96%B9%E6%B3%95%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%9C%A8%E8%AE%AD%E7%BB%83%E6%97%B6%EF%BC%8C%E5%AF%B9%E4%B8%80%E4%B8%AAbatch%E4%B8%AD%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%9B%BE%E5%83%8F%EF%BC%8C%E9%83%BD%E5%8E%BB%E9%9A%8F%E6%9C%BA%E5%8F%96M%E4%B8%AA%E5%90%8CID%E7%9A%84%EF%BC%8C%E4%B8%8D%E5%90%8C%E6%91%84%E5%83%8F%E5%A4%B4%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%8C%E7%84%B6%E5%90%8E%E6%B1%82%E4%B8%AA%E5%B9%B3%E5%9D%87%E5%80%BC%EF%BC%88%E5%85%B1-M-1-%E4%B8%AA%E5%9B%BE%E5%83%8F%EF%BC%89%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E8%AE%A4%E4%B8%BA%E8%BF%99%E4%B8%AA%E5%9D%87%E5%80%BC%E5%9B%BE%E5%83%8F%E9%87%8C%E8%95%B4%E5%90%AB%E4%BA%86%E5%A4%9A%E8%A7%86%E8%A7%92%E7%9A%84%E4%BF%A1%E6%81%AF%E3%80%82%E5%AF%B9%E4%BA%8E%E6%96%87%E6%9C%AC%EF%BC%8C%E4%B9%9F%E6%98%AF%E8%BF%9B%E8%A1%8C%E7%B1%BB%E4%BC%BC%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%BE%97%E5%88%B0%E8%95%B4%E5%90%AB%E4%BA%86%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%E7%9A%84%E6%96%87%E6%9C%ACembedding%EF%BC%88%E5%BD%93%E7%84%B6%EF%BC%8C%E9%83%BD%E5%BF%85%E9%A1%BB%E6%98%AF%E5%90%8C%E6%A8%A1%E6%80%81%E4%B8%8B%E7%9A%84%E3%80%82%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E8%A6%81%E5%8E%BB%E6%89%BE%E5%90%8CID%E7%9A%84%E4%B8%8D%E5%90%8C%E6%91%84%E5%83%8F%E5%A4%B4%E4%B8%8B%E7%9A%84%E5%85%B6%E5%AE%83%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E3%80%82%E5%85%B6%E4%BD%99%E7%B1%BB%E4%BC%BC%EF%BC%89%E3%80%82%E4%B9%8B%E5%90%8E%E4%BC%9A%E7%94%A8%E4%B8%80%E4%B8%AA%E5%AF%B9%E9%BD%90%E6%8D%9F%E5%A4%B1%EF%BC%8C%E5%8E%BB%E5%8F%8C%E5%90%91%E7%BA%A6%E6%9D%9F%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8Fembedding%E5%92%8C%E6%96%87%E5%AD%97embedding%E3%80%82%E6%AF%95%E7%AB%9F%EF%BC%8C%E5%8D%B3%E4%BD%BF%E6%98%AF%E7%BB%BC%E5%90%88%E4%BA%86%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%AF%B9%E4%BA%8E%E5%90%8C%E4%B8%80%E4%B8%AA%E4%BA%BA%E7%9A%84%E5%9B%BE%E5%83%8Fembedding%E5%92%8C%E6%96%87%E5%AD%97embedding%E4%BE%9D%E7%84%B6%E5%BA%94%E8%AF%A5%E5%B0%BD%E5%8F%AF%E8%83%BD%E7%9B%B8%E8%BF%91%EF%BC%8C%E7%A1%AE%E4%BF%9D%E5%9B%BE%E5%83%8F%E5%AD%A6%E5%88%B0%E6%9B%B4%E5%A4%9A%E5%90%88%E7%90%86%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%84%B6%E8%80%8C%EF%BC%8C%E5%AE%9E%E9%99%85%E6%B5%8B%E8%AF%95%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%88%91%E4%BB%AC%E9%83%BD%E6%98%AF%E6%8B%BF%E5%88%B0%E4%B8%80%E4%B8%AA%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%B0%B1%E8%A6%81%E5%8E%BB%E8%BF%9B%E8%A1%8C%E5%8C%B9%E9%85%8D%E4%BA%86%E3%80%82%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E6%B2%A1%E6%9C%89%E5%A4%9A%E8%A7%86%E8%A7%92%E7%9A%84%E4%BF%A1%E6%81%AF%E5%8F%AF%E4%BB%A5%E5%88%A9%E7%94%A8%EF%BC%88%E4%B8%8A%E9%9D%A2%E4%B9%9F%E6%8F%90%E5%88%B0%E4%BA%86%EF%BC%8C%E6%98%AF%E8%AE%AD%E7%BB%83%E6%97%B6%E5%8F%AF%E4%BB%A5%E8%BF%99%E6%A0%B7%E5%88%A9%E7%94%A8%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%E3%80%82%E9%82%A3%E4%B8%BA%E4%BA%86%E5%BC%A5%E8%A1%A5%E8%BF%99%E4%B8%80%E7%BC%BA%E7%82%B9%EF%BC%8C%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E8%92%B8%E9%A6%8F%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%AE%A9%E6%AF%8F%E4%B8%AA%E5%9B%BE%E5%83%8Fembedding%E5%8E%BB%E9%80%BC%E8%BF%91%E5%AE%83%E5%AF%B9%E5%BA%94%E7%9A%84%E9%82%A3%E4%B8%AA%E5%A4%9A%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E5%9D%87%E5%80%BCembedding%EF%BC%8C%E6%96%87%E5%AD%97embedding%E7%B1%BB%E4%BC%BC%E3%80%82%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E9%83%BD%E8%BF%9B%E8%A1%8C%E8%BF%99%E4%B8%AA%E6%93%8D%E4%BD%9C%E3%80%82%E6%89%80%E4%BB%A5%E8%BF%99%E4%B8%AA%E8%92%B8%E9%A6%8F%E6%8D%9F%E5%A4%B1%E5%B0%B1%E6%98%AF%E5%9B%9B%E9%A1%B9%E4%BA%86%E3%80%82%E5%85%AC%E5%BC%8F%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="nav-number">3.5.2.</span> <span class="nav-text">第二个是CVSC（Cross-View Semantics Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共$M+1$个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是训练时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E4%B8%AA%E6%98%AFCMSP%E6%A8%A1%E5%9D%97%EF%BC%88Cross-Modality-Semantics-Purification%EF%BC%89%EF%BC%8C%E5%AE%83%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%EF%BC%8C%E9%81%BF%E5%85%8D%E5%AD%A6%E5%88%B0%E7%9A%84%E8%AF%AD%E4%B9%89%E5%8F%91%E7%94%9F%E5%86%B2%E7%AA%81%E3%80%82%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%90%8C%E4%B8%80%E4%B8%AAID%E4%BC%9A%E6%9C%89%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%EF%BC%8C%E4%B9%9F%E4%BC%9A%E6%9C%89%E5%AF%B9%E5%BA%94%E7%9A%84%E7%81%B0%E5%BA%A6%E5%9B%BE%E7%9A%84%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E3%80%82%E8%80%8C%E8%BF%99%E4%B8%A4%E7%A7%8D%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%EF%BC%8C%E5%8F%AF%E8%83%BD%E5%8F%91%E7%94%9F%E5%86%B2%E7%AA%81%E3%80%82%E6%AF%94%E5%A6%82%EF%BC%8Cllm%E9%92%88%E5%AF%B9%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%8F%AF%E8%83%BD%E5%8F%AF%E4%BB%A5%E5%87%86%E7%A1%AE%E5%9C%B0%E6%8F%8F%E8%BF%B0%E5%87%BA%E4%BA%BA%E7%A9%BF%E7%9A%84%E8%A1%A3%E6%9C%8D%E7%9A%84%E9%A2%9C%E8%89%B2%EF%BC%8C%E4%B9%9F%E8%AE%B8%E6%98%AF%E8%93%9D%E8%89%B2%EF%BC%9B%E8%80%8C%E6%8B%BF%E5%88%B0%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%85%A8%E9%83%A8%E9%83%BD%E6%98%AF%E4%BC%9A%E7%9A%84%EF%BC%8C%E5%AE%83%E5%8F%AF%E8%83%BD%E5%B0%B1%E4%BC%9A%E8%BE%93%E5%87%BA%E2%80%9C%E4%BA%BA%E7%A9%BF%E7%9A%84%E8%A1%A3%E6%9C%8D%E7%9A%84%E9%A2%9C%E8%89%B2%E6%98%AF%E7%81%B0%E7%9A%84%E2%80%9D%EF%BC%8C%E8%BF%99%E6%97%A0%E7%96%91%E4%BC%9A%E9%80%A0%E6%88%90%E7%9F%9B%E7%9B%BE%E3%80%82%E6%89%80%E4%BB%A5%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%EF%BC%8C%E5%AF%B9%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%EF%BC%8C%E5%8F%AF%E4%BB%A5%E4%B8%8D%E5%90%8C%EF%BC%88%E4%BA%92%E8%A1%A5%E5%BD%93%E7%84%B6%E6%98%AF%E5%A5%BD%E7%8E%B0%E8%B1%A1%EF%BC%89%EF%BC%8C%E4%BD%86%E4%B8%8D%E8%83%BD%E7%9F%9B%E7%9B%BE%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AAcmsp-loss%EF%BC%8C%E5%85%AC%E5%BC%8F%E5%A6%82%E4%B8%8B%EF%BC%9A-L-cmsp-frac-1-N-sum-i-1-N-d-i-vv-d-i-vr-2-frac-1-N-sum-i-1-N-d-i-rr-d-i-rv-2-%E5%85%B6%E4%B8%AD%EF%BC%8C-d-i-vv-lVert-f-i-v-t-i-v-rVert-2-%EF%BC%8C-d-i-vr-lVert-f-i-v-t-i-r-rVert-2-%EF%BC%8C%E5%85%B6%E4%BD%99%E5%AE%9A%E4%B9%89%E7%B1%BB%E4%BC%BC%E3%80%82%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%EF%BC%8C%E8%AE%A9%E8%BF%99%E4%B8%AA%E6%8D%9F%E5%A4%B1%E7%BA%A6%E6%9D%9F%E5%90%8C%E4%B8%80ID%E4%B8%8B%E7%9A%84%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E8%A6%81%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%E5%AE%83%E7%9A%84%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%EF%BC%88%E8%87%B3%E4%BA%8E%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A8%A1%E5%9E%8B%E4%B8%8D%E4%BC%9A%E5%A1%8C%E7%BC%A9%EF%BC%8C%E7%9B%B4%E6%8E%A5%E8%AE%A9%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E4%B8%8E%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E5%AE%8C%E5%85%A8%E4%B8%80%E6%A0%B7%EF%BC%8C%E5%8F%AF%E8%83%BD%E6%98%AF%E5%9B%A0%E4%B8%BA%E5%89%8D%E9%9D%A2%E7%9A%84%E5%AF%B9%E6%AF%94%E6%8D%9F%E5%A4%B1%E3%80%81%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E7%BA%A6%E6%9D%9F%E4%BA%86%E8%BF%99%E4%BA%9B%E6%96%87%E5%AD%97embedding%EF%BC%8C%E8%AE%A9%E5%AE%83%E4%BB%AC%E5%BF%85%E9%A1%BB%E6%9C%89%E5%90%88%E7%90%86%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E5%90%A7"><span class="nav-number">3.5.3.</span> <span class="nav-text">第三个是CMSP模块（Cross-Modality Semantics Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp loss，公式如下：$$L_{cmsp}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{vv}-d_{i}^{vr})}^{2}+\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{rr}-d_{i}^{rv})}^{2}$$其中，$d_{i}^{vv}&#x3D;\lVert f_{i}^{v}-t_{i}^{v}\rVert_{2}$，$d_{i}^{vr}&#x3D;\lVert f_{i}^{v}-t_{i}^{r}\rVert_{2}$，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-number">3.6.</span> <span class="nav-text">分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E6%98%AF%E7%A8%80%E6%9D%BE%E5%B9%B3%E5%B8%B8%E7%9A%84%EF%BC%8C%E4%BD%86%E5%A5%BD%E5%83%8F%E4%B9%9F%E5%B0%B1%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E7%9C%8B%E8%B5%B7%E6%9D%A5%E9%9D%A0%E7%82%B9%E8%B0%B1%E3%80%82%E3%80%82"><span class="nav-number">3.6.1.</span> <span class="nav-text">第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E6%83%B3%E6%B3%95%E6%98%AF%E6%8C%BA%E5%A5%BD%E7%9A%84%EF%BC%8C%E4%BD%86%E4%B8%80%E6%9D%A5%EF%BC%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%87%AD%E4%BB%80%E4%B9%88%E8%AF%B4%E6%B1%82%E4%B8%AA%E5%9D%87%E5%80%BC%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%B0%B1%E8%95%B4%E5%90%AB%E4%BA%86%E5%A4%9A%E8%A7%86%E8%A7%92%E7%9A%84%E4%BF%A1%E6%81%AF%EF%BC%9F%E6%88%96%E8%80%85%E8%AF%B4%EF%BC%8C%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E6%98%AF%E4%B8%8D%E6%98%AF%E5%A4%AA%E7%AE%80%E5%8D%95%E7%B2%97%E6%9A%B4%E4%BA%86%EF%BC%9F%E8%BF%98%E6%9C%89%EF%BC%8C%E6%B5%8B%E8%AF%95%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E9%82%A3%E4%B8%AA%E8%92%B8%E9%A6%8F%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E5%B8%8C%E6%9C%9B%E5%8D%95%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84embedding%E8%83%BD%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%E5%A4%9A%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84embedding%EF%BC%88%E5%8F%AF%E8%A7%81%E5%85%89-%E7%BA%A2%E5%A4%96%EF%BC%8C%E5%9B%BE%E5%83%8F-%E6%96%87%E5%AD%97%EF%BC%89%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E7%AB%9F%E7%84%B6%E6%98%AF%E5%B8%8C%E6%9C%9B%E6%A8%A1%E5%9E%8B%E6%8B%BF%E5%88%B0%E4%B8%80%E4%B8%AA%E5%8D%95%E8%A7%86%E8%A7%92%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%B0%B1%E8%83%BD%E2%80%9C%E8%84%91%E8%A1%A5%E2%80%9D%E5%87%BA%E5%A4%9A%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E4%BF%A1%E6%81%AF%EF%BC%9F%E8%BF%99%E5%90%88%E7%90%86%E5%90%97%EF%BC%9F%E5%8F%AA%E8%83%BD%E8%AF%B4%EF%BC%8C%E6%9C%89%E4%B8%80%E4%BA%9B%E5%90%88%E7%90%86%E4%B9%8B%E5%A4%84%EF%BC%88%E6%AF%95%E7%AB%9F%E4%BA%BA%E6%8B%BF%E5%88%B0%E4%B8%80%E5%BC%A0%E5%9B%BE%E5%83%8F%EF%BC%8C%E7%A1%AE%E5%AE%9E%E8%83%BD%E9%9D%A0%E7%A9%BA%E9%97%B4%E6%83%B3%E8%B1%A1%E5%8A%9B%E8%84%91%E8%A1%A5%E5%87%BA%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF%EF%BC%89%EF%BC%8C%E4%BD%86%E4%B9%9F%E6%9C%89%E4%BA%9B%E4%B8%8D%E5%90%88%E7%90%86%E4%B9%8B%E5%A4%84%EF%BC%88%E6%AF%94%E5%A6%82%E4%B8%8A%E9%9D%A2%E7%9A%84%E8%84%B8%E9%83%A8%E9%81%AE%E6%8C%A1%E4%BE%8B%E5%AD%90%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%98%AF%E6%97%A0%E8%AE%BA%E5%A6%82%E4%BD%95%E4%B9%9F%E4%B8%8D%E8%83%BD%E8%84%91%E8%A1%A5%E5%87%BA%E6%9D%A5%E7%9A%84%E3%80%82%E9%80%BC%E8%BF%AB%E6%A8%A1%E5%9E%8B%E5%8E%BB%E5%AD%A6%E8%BF%99%E4%B8%AA%EF%BC%8C%E4%B9%9F%E6%98%AF%E4%B8%8D%E5%90%88%E7%90%86%E7%9A%84%EF%BC%8C%E9%9A%BE%E8%AF%B4%E6%9C%80%E5%90%8E%E5%AD%A6%E5%87%BA%E6%9D%A5%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E5%8F%AF%E8%83%BD%E8%BF%87%E6%8B%9F%E5%90%88%E4%BA%86%EF%BC%89"><span class="nav-number">3.6.2.</span> <span class="nav-text">第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光&#x2F;红外，图像&#x2F;文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%8C%E5%90%8C%E6%A0%B7%EF%BC%8C%E5%87%BA%E5%8F%91%E7%82%B9%E5%80%92%E4%B9%9F%E6%B2%A1%E9%94%99%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B8%AA%E6%8D%9F%E5%A4%B1%EF%BC%8C%E7%9C%8B%E8%B5%B7%E6%9D%A5%E6%9C%89%E4%BA%9B%E5%A5%87%E6%80%AA%E3%80%82%E4%BB%A5cmsp-loss%E7%9A%84%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%BA%E4%BE%8B%EF%BC%8C%E5%85%B6%E5%AE%9E%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E8%AF%B4%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%8D%9F%E5%A4%B1%E6%98%AF%E5%9C%A8%E4%BB%A5%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%9A%84embedding%E4%B8%BA%E9%94%9A%E7%82%B9%EF%BC%8C%E8%AE%A9%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%87%E5%AD%97embedding%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%87%E5%AD%97embedding%E8%B7%9D%E7%A6%BB%E8%BF%99%E4%B8%AA%E9%94%9A%E7%82%B9%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%E3%80%82%E6%88%91%E7%8C%9C%E6%83%B3%E4%BD%9C%E8%80%85%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%EF%BC%8C%E8%AE%A9%E4%B8%A4%E7%A7%8D%E6%96%87%E5%AD%97embedding%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%EF%BC%8C%E8%BF%99%E6%A0%B7%E5%B0%B1%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%AF%B4%E8%AE%A9%E6%96%87%E5%AD%97embedding%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8F%90%E5%8F%96%E5%87%BA%E5%85%B1%E6%80%A7%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%EF%BC%8C%E8%80%8C%E4%B8%8D%E4%BC%9A%E5%8E%BB%E6%8F%90%E5%8F%96%E5%87%BA%E7%9F%9B%E7%9B%BE%E7%9A%84-%E4%B8%80%E6%96%B9%E7%8B%AC%E6%9C%89%E7%9A%84%E4%BF%A1%E6%81%AF%EF%BC%88%E5%A6%82%E6%9E%9C%E7%9C%9F%E6%98%AF%E8%BF%99%E6%A0%B7%EF%BC%8C%E5%AE%83%E5%B0%B1%E5%BA%94%E8%AF%A5%E7%9B%B4%E6%8E%A5%E7%BA%A6%E6%9D%9F%E4%B8%A4%E4%B8%AA%E6%96%87%E5%AD%97embedding%E4%BA%86%EF%BC%8C%E6%B2%A1%E5%BF%85%E8%A6%81%E6%8E%BA%E5%92%8C%E4%B8%8A%E5%9B%BE%E5%83%8Fembedding%EF%BC%9F%E4%BD%86%E5%87%A0%E4%BD%95%E4%B8%8A%E7%9C%8B%EF%BC%8C%E8%A6%81%E8%AE%A9%E6%8D%9F%E5%A4%B1%E6%9C%80%E5%B0%8F%EF%BC%8C%E5%AE%83%E4%BB%AC%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%9C%A8%E4%B8%80%E4%B8%AA%E4%BB%A5%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E4%B8%BA%E2%80%9C%E5%9C%86%E5%BF%83%E2%80%9D%E7%9A%84%E6%9F%90%E4%B8%AA%E2%80%9C%E5%9C%86%E2%80%9D%E4%B8%8A%EF%BC%88%E5%8F%AA%E4%B8%8D%E8%BF%87%E6%98%AF%E9%AB%98%E7%BB%B4%E7%9A%84%EF%BC%89%E3%80%82%E6%AD%A4%E6%97%B6%E5%AE%83%E4%BB%AC%E6%9C%AA%E5%BF%85%E5%B0%B1%E8%B7%9D%E7%A6%BB%E5%BE%88%E8%BF%91%E3%80%82%E8%BF%99%E6%A0%B7%E5%BE%97%E5%88%B0%E7%9A%84%E4%B8%A4%E7%A7%8Dembedding%E6%9C%89%E4%BB%80%E4%B9%88%E5%90%AB%E4%B9%89%E5%91%A2%EF%BC%9F%E4%B8%8D%E8%83%BD%E8%AE%A4%E4%B8%BA%E8%AF%B4%EF%BC%8C%E5%9C%A8%E5%90%8C%E4%B8%80%E4%B8%AA%E5%9C%86%E5%91%A8%E4%B8%8A%EF%BC%8C%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E5%B0%B1%E4%B8%8D%E7%9F%9B%E7%9B%BE%E4%BA%86%E5%90%A7%EF%BC%9F%E6%9C%89%E7%82%B9%E6%80%AA%E3%80%82%E6%88%96%E8%80%85%E6%88%91%E4%BB%AC%E8%AE%A4%E4%B8%BA%EF%BC%8C%E6%96%87%E5%AD%97embedding%E4%B8%8D%E5%8A%A8%EF%BC%8C%E4%BC%98%E5%8C%96%E7%9A%84%E6%98%AF%E5%9B%BE%E5%83%8Fembedding%EF%BC%9F%E8%BF%99%E6%A0%B7%E6%9C%80%E7%90%86%E6%83%B3%E7%9A%84%E6%83%85%E5%86%B5%E6%98%AF%EF%BC%8C%E5%9B%BE%E5%83%8Fembedding%E8%90%BD%E5%9C%A8%E4%BA%86%E4%B8%A4%E7%A7%8D%E6%96%87%E5%AD%97embedding%E7%9A%84%E2%80%9C%E4%B8%AD%E5%9E%82%E7%BA%BF%E2%80%9D%E4%B8%8A%E4%BA%86%E3%80%82%E8%BF%99%E6%A0%B7%E5%B0%B1%E8%83%BD%E8%AE%A4%E4%B8%BA%E5%9B%BE%E5%83%8Fembedding%E6%B2%A1%E6%9C%89%E5%AD%A6%E5%88%B0%E4%BB%80%E4%B9%88%E7%9F%9B%E7%9B%BE%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E5%90%97%EF%BC%9F%E8%BF%98%E6%98%AF%E6%B2%A1%E9%81%93%E7%90%86%E5%95%8A%E3%80%82%E5%8F%88%E6%88%96%E8%80%85%E4%B8%8A%E9%9D%A2%E7%9A%84%E5%88%86%E6%9E%90%E9%83%BD%E6%98%AF%E5%81%87%E5%AE%9A%E4%BA%86%E4%B8%80%E7%A7%8Dembedding%E6%98%AF%E4%B8%8D%E5%8A%A8%E7%9A%84%EF%BC%88%E6%96%87%E5%AD%97-%E5%9B%BE%E5%83%8F%EF%BC%89%EF%BC%8C%E5%AE%9E%E9%99%85%E6%98%AF%E4%B8%80%E8%B5%B7%E4%BC%98%E5%8C%96%E7%9A%84%E3%80%82%E9%82%A3%E5%AE%83%E6%83%B3%E4%BC%98%E5%8C%96%E4%BB%80%E4%B9%88%E5%91%A2%EF%BC%9F%E8%BF%98%E6%98%AF%E4%B8%8D%E5%A4%AA%E6%98%8E%E7%99%BD%E3%80%82%E3%80%82"><span class="nav-number">3.6.3.</span> <span class="nav-text">第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的&#x2F;一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字&#x2F;图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Prototype-Driven-Multi-Feature-Generation-for-Visible-Infrared-Person-Re-identification"><span class="nav-number">4.</span> <span class="nav-text">Prototype-Driven Multi-Feature Generation for  Visible-Infrared Person Re-identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ICASSP%EF%BC%8C2024-9-9"><span class="nav-number">4.1.</span> <span class="nav-text">ICASSP，2024.9.9</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA%E5%85%B6%E5%AE%9E%E6%84%9F%E8%A7%89%E4%B8%8D%E5%A4%AA%E6%98%8E%E7%A1%AE%EF%BC%8C%E4%BC%BC%E4%B9%8E%E6%98%AF%E9%AD%94%E6%94%B9%E5%AE%8C%E6%9C%89%E6%95%88%E6%9E%9C%E4%BA%86%EF%BC%8C%E5%B0%B1%E5%BC%80%E5%A7%8B%E8%AE%B2%E6%95%85%E4%BA%8B%E4%BA%86"><span class="nav-number">4.2.</span> <span class="nav-text">动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#insight%EF%BC%9A%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AAMFGM%E6%9D%A5%E8%8E%B7%E5%8F%96%E5%A4%9A%E6%A0%B7%E7%89%B9%E5%BE%81%EF%BC%88%E4%BD%86%E5%85%B6%E5%AE%9E%E4%B8%8D%E6%98%AF%E5%BE%88%E6%96%B0%E5%A5%87%E4%BA%86%EF%BC%8C%E8%B7%9FDEEN%E5%B7%AE%E4%B8%8D%E5%A4%9A%E7%9A%84%E6%80%9D%E8%B7%AF%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%A4%9A%E5%87%A0%E4%B8%AA%E5%88%86%E6%94%AF%E5%8A%A0%E4%B8%8A%E4%B8%80%E4%BA%9B%E6%8D%9F%E5%A4%B1%E7%BA%A6%E6%9D%9F%EF%BC%8C%E8%AE%A9%E8%BF%99%E4%BA%9B%E7%94%9F%E6%88%90%E7%9A%84%E7%89%B9%E5%BE%81%E4%B8%8D%E5%A4%AA%E4%B8%80%E6%A0%B7%EF%BC%89%EF%BC%9B%E7%94%A8%E4%B8%80%E4%B8%AAprototype%E6%9D%A5%E6%8F%90%E5%8F%96%E7%89%B9%E5%BE%81%EF%BC%88%E6%84%9F%E8%A7%89%E6%9C%AC%E8%B4%A8%E5%8F%AA%E6%98%AF1%C3%971%E7%9A%84%E5%8D%B7%E7%A7%AF%EF%BC%89%EF%BC%8C%E8%B7%A8%E6%A8%A1%E6%80%81%E7%94%A8%E5%90%8C%E4%B8%80%E5%A5%97%E6%A8%A1%E6%9D%BF%EF%BC%8C%E7%84%B6%E5%90%8E%E5%8F%88%E7%94%A8%E4%B8%80%E4%B8%AA%E6%8D%9F%E5%A4%B1%E6%9D%A5%E7%BA%A6%E6%9D%9F%E6%A8%A1%E6%9D%BF%E6%8F%90%E5%8F%96%E4%B8%8D%E5%90%8C%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%88%E5%92%8C%E4%B8%8A%E9%9D%A2%E7%BA%A6%E6%9D%9F%E7%94%9F%E6%88%90%E7%89%B9%E5%BE%81%E7%9A%84%E5%B7%AE%E4%B8%8D%E5%A4%9A%EF%BC%8C%E8%BF%99%E9%87%8C%E5%8F%AA%E6%98%AF%E6%8D%A2%E4%BA%86%E4%B8%AAcos%E7%9A%84%E5%A5%97%E5%AD%90%EF%BC%89%EF%BC%8C%E6%8B%89%E8%BF%91%E5%90%8CID%E7%89%B9%E5%BE%81%EF%BC%8C%E6%8B%89%E8%BF%9C%E4%B8%8D%E5%90%8CID%E7%89%B9%E5%BE%81%EF%BC%8C%E6%9D%A5%E7%A1%AE%E4%BF%9D%E7%89%B9%E5%BE%81%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7"><span class="nav-number">4.3.</span> <span class="nav-text">insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA%EF%BC%9A-1"><span class="nav-number">4.4.</span> <span class="nav-text">动机：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%EF%BC%9A-2"><span class="nav-number">4.5.</span> <span class="nav-text">方法：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E5%87%BA%E7%9A%84PDM%E6%A1%86%E6%9E%B6%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E4%B8%A4%E4%B8%AA%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86%EF%BC%8C%E5%88%86%E5%88%AB%E6%98%AFMulti-Feature-Generation-Module%EF%BC%88MFGM%EF%BC%89%EF%BC%88%E7%94%A8%E4%BA%8E%E7%94%9F%E6%88%90%E6%9B%B4%E5%A4%9A%E5%A4%9A%E6%A0%B7%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E5%8F%82%E8%80%83DEEN%EF%BC%89%E5%92%8CPrototype-Learning-Moduel%EF%BC%88PLM%EF%BC%89"><span class="nav-number">4.5.1.</span> <span class="nav-text">提出的PDM框架，主要是两个组成部分，分别是Multi-Feature Generation Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype Learning Moduel（PLM）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MFGM%E6%84%9F%E8%A7%89%E7%9C%9F%E7%9A%84%E5%92%8CDEEN%E5%BE%88%E5%83%8F%EF%BC%8C%E9%83%BD%E6%98%AF%E7%94%A8%E5%87%A0%E4%B8%AA%E5%88%86%E6%94%AF%EF%BC%8C%E7%94%9F%E6%88%90%E5%A4%9A%E5%87%A0%E4%B8%AA%E7%89%B9%E5%BE%81%EF%BC%8C%E7%84%B6%E5%90%8E%E5%88%86%E6%94%AF%E4%B8%8A%E9%83%BD%E6%9C%89dilation-convolution%EF%BC%8C%E4%B9%8B%E5%90%8E%E5%8F%88%E7%94%A8%E4%B8%80%E4%BA%9B%E6%8D%9F%E5%A4%B1%E6%9D%A5%E6%8A%8A%E6%9F%90%E4%BA%9B%E7%89%B9%E5%BE%81%E6%8B%89%E8%BF%9C%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E2%80%9C%E5%A4%9A%E6%A0%B7%E5%8C%96%E2%80%9D%E7%89%B9%E5%BE%81%EF%BC%88%E8%BF%99%E9%87%8C%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%B0%B1%E6%98%AF%E6%96%87%E4%B8%AD%E7%9A%84Center-guided-Pair-Mining-Loss"><span class="nav-number">4.5.2.</span> <span class="nav-text">MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided Pair Mining Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PLM%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%8F%AF%E8%83%BD%E6%98%AF%E6%88%91%E8%AE%BA%E6%96%87%E7%9C%8B%E5%B0%91%E4%BA%86%EF%BC%8C%E7%9B%B8%E5%AF%B9%E6%B2%A1%E9%82%A3%E4%B9%88%E5%B8%B8%E8%A7%81%EF%BC%8C%E4%BD%86%E5%AE%83%E8%BF%99%E9%87%8C%E6%8F%90%E5%87%BAprototype%E4%B9%9F%E4%B8%8D%E6%98%AF%E4%B8%80%E8%88%AC%E7%9A%84%E4%B8%BA%E4%BA%86%E4%BF%9D%E7%95%99%E7%9F%A5%E8%AF%86%EF%BC%8C%E8%80%8C%E6%98%AF%E4%B8%BA%E4%BA%86%E6%8F%90%E5%8F%96%E7%9F%A5%E8%AF%86%E3%80%82%E5%AE%83%E7%9A%84%E5%A4%A7%E6%A6%82%E6%80%9D%E6%83%B3%E5%92%8CCNN%E9%87%8C%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%B7%AE%E4%B8%8D%E5%A4%9A%E5%90%A7%EF%BC%8C%E5%9B%A0%E4%B8%BA%E5%AF%B9%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E4%B8%80%E7%A7%8D%E7%90%86%E8%A7%A3%E5%B0%B1%E6%98%AF%EF%BC%8C%E5%AE%83%E5%AD%A6%E5%88%B0%E4%BA%86%E4%B8%80%E4%BA%9B%E7%89%B9%E5%AE%9A%E7%9A%84%E6%A8%A1%E5%BC%8F%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%EF%BC%9B%E8%BF%99%E9%87%8C%E6%8F%90%E5%8F%96%E7%9F%A5%E8%AF%86%E4%B9%9F%E6%98%AF%E7%B1%BB%E4%BC%BC%E7%9A%84%EF%BC%8C%E7%94%A8%E4%B8%80%E7%BB%84%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84prototype%EF%BC%88%E7%B1%BB%E6%AF%94%E5%8D%B7%E7%A7%AF%E6%A0%B8%EF%BC%89%EF%BC%8C%E6%9D%A5%E5%92%8C%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E5%93%88%E8%BE%BE%E7%8E%9B%E7%A7%AF%EF%BC%8C%E5%86%8D%E6%B1%82%E5%9D%87%E5%80%BC%EF%BC%88%E7%B1%BB%E6%AF%94%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%EF%BC%89%EF%BC%8C%E5%B9%B6%E4%B8%94%E6%98%AF%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%94%A8%E5%90%8C%E4%B8%80%E7%BB%84prototype%EF%BC%8C%E6%89%80%E4%BB%A5%E4%BB%A5%E6%AD%A4%E6%9D%A5%E4%BF%83%E8%BF%9B%E8%B7%A8%E6%A8%A1%E6%80%81%E7%9A%84%E5%85%B1%E6%80%A7%E7%9F%A5%E8%AF%86%E6%8F%90%E5%8F%96%EF%BC%88%E4%BD%86%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81prototype%E6%8F%90%E5%8F%96%E7%9A%84%E5%B0%B1%E4%B8%80%E5%AE%9A%E6%98%AF%E5%B1%80%E9%83%A8%E4%BF%A1%E6%81%AF%E5%91%A2%EF%BC%8C%E4%BB%85%E4%BB%85%E6%98%AF%E5%9B%A0%E4%B8%BAprototype%E7%9A%84%E5%BD%A2%E7%8A%B6%E6%98%AFc%EF%BC%88channel%EF%BC%89%E7%BB%B4%E7%9A%84%E5%90%91%E9%87%8F%E5%90%97%EF%BC%8C%E8%BF%99%E6%A0%B7%E7%9A%84%E8%AF%9D%E5%80%92%E6%98%AF%E8%83%BD%E7%A8%8D%E5%BE%AE%E8%A7%A3%E9%87%8A%E4%B8%80%E4%B8%8B%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%9B%B4%E6%8E%A5%E7%94%A8%E4%B8%80%E4%B8%AACNN%E4%BA%86%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%E8%BF%99%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E4%B8%80%E7%A7%8D%E7%89%B9%E6%AE%8A%E7%9A%84CNN%EF%BC%8C%E5%8F%AA%E4%B8%8D%E8%BF%87%E6%98%AF1%C3%971%E5%8D%B7%E7%A7%AF%E4%BA%86%E3%80%82%E8%BF%99%E4%B9%88%E8%AF%B4%E6%9D%A5%EF%BC%8C%E5%85%B6%E5%AE%9E%E8%BF%98%E6%98%AF%E8%AE%B2%E6%95%85%E4%BA%8B%E5%95%8A%E3%80%82%E3%80%82%EF%BC%89"><span class="nav-number">4.5.3.</span> <span class="nav-text">PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A4%E5%A4%96%EF%BC%8C%E4%B8%BA%E4%BA%86%E4%BF%9D%E8%AF%81%E5%AE%83%E7%9A%84%E4%B8%8D%E5%90%8Cprototype%E6%8F%90%E5%8F%96%E5%87%BA%E7%9A%84%E7%89%B9%E5%BE%81%E4%B9%9F%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%A4%9A%E6%A0%B7%EF%BC%8C%E6%8F%90%E5%87%BA%E4%BA%86Cosine-Heterogeneity-Loss%EF%BC%88%E5%85%B6%E5%AE%9E%E5%BE%88%E8%80%81%E5%A5%97%E4%BA%86%EF%BC%8C%E8%B7%9F%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB%E5%B7%AE%E4%B8%8D%E5%A4%9A%EF%BC%8C%E5%8F%AA%E4%B8%8D%E8%BF%87%E8%BF%99%E9%87%8C%E6%8D%A2%E6%88%90%E4%BA%86%E7%94%A8cos%EF%BC%89%EF%BC%8C%E8%BF%98%E6%9C%89%E4%B8%80%E4%B8%AADual-Center-Seperation-Loss%EF%BC%88%E8%BF%99%E4%B8%AA%E6%8D%9F%E5%A4%B1%E4%BC%BC%E4%B9%8E%E5%B9%B6%E6%B2%A1%E6%9C%89%E8%AF%B4%E6%98%8E%E5%AE%83%E8%A7%A3%E5%86%B3%E4%BA%86%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%8C%E6%84%9F%E8%A7%89%E5%BE%88%E5%8F%AF%E8%83%BD%E5%8F%AA%E6%98%AF%E4%B8%BA%E4%BA%86%E6%8F%90%E7%82%B9%E8%80%8C%E5%B7%B2%E3%80%82%E6%9C%AC%E8%BA%AB%E5%AE%83%E7%9A%84%E6%80%9D%E6%83%B3%E4%B9%9F%E5%BE%88%E5%B8%B8%E8%A7%81%E4%BA%86%EF%BC%8C%E5%B0%B1%E6%98%AF%E6%8D%A2%E4%B8%AA%E6%B3%95%E5%AD%90%E8%AE%A9%E5%90%8Cid%E7%9A%84%E7%89%B9%E5%BE%81%E8%BF%91%E4%B8%80%E7%82%B9%EF%BC%8C%E4%B8%8D%E5%90%8Cid%E7%9A%84%E7%89%B9%E5%BE%81%E8%BF%9C%E4%B8%80%E7%82%B9%EF%BC%89"><span class="nav-number">4.5.4.</span> <span class="nav-text">此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine Heterogeneity Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center Seperation Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Parameter-Hierarchical-Optimization-for-Visible-Infrared-Person-Re-Identification"><span class="nav-number">5.</span> <span class="nav-text">Parameter Hierarchical Optimization for  Visible-Infrared Person Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv%EF%BC%8C2024-4-11"><span class="nav-number">5.1.</span> <span class="nav-text">arxiv，2024.4.11</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A5%BD%E5%83%8F%E7%A1%AE%E5%AE%9E%E6%9C%89%E7%82%B9%E6%96%B0%E5%A5%87%EF%BC%9F%E6%8A%8A%E5%8F%82%E6%95%B0%E5%88%92%E5%88%86%E4%B8%BA%E4%B8%A4%E7%A7%8D%E7%B1%BB%E5%9E%8B%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%98%AF%E6%AD%A3%E5%B8%B8%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%94%A8%E6%A2%AF%E5%BA%A6%E5%8F%8D%E4%BC%A0%E6%9D%A5%E4%BC%98%E5%8C%96%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%98%AF%E5%9F%BA%E4%BA%8E%E4%B8%80%E4%BA%9B%E8%A7%84%E5%88%99%E6%9D%A5%E4%BC%98%E5%8C%96%EF%BC%88%E5%BA%94%E8%AF%A5%E6%98%AF%E6%AF%94%E8%BE%83%E4%BC%A0%E7%BB%9F%E7%9A%84%E9%82%A3%E7%A7%8D%EF%BC%89%EF%BC%8C%E4%BB%A5%E6%AD%A4%E6%9D%A5%E5%87%8F%E5%B0%91%E9%9C%80%E8%A6%81%E6%A2%AF%E5%BA%A6%E5%8F%8D%E4%BC%A0%E6%9D%A5%E4%BC%98%E5%8C%96%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%88%E8%BF%99%E4%B8%AA%E6%98%AF%E5%8F%AF%E4%BB%A5%E5%87%8F%E5%B0%91%E4%B8%80%E4%BA%9B%E8%AE%A1%E7%AE%97%E9%87%8F%EF%BC%8C%E4%BD%86%E5%85%B6%E5%AE%83%E5%A5%BD%E5%A4%84%E5%91%A2%EF%BC%9F%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E6%9D%A5%E4%BC%98%E5%8C%96%EF%BC%8C%E6%95%88%E6%9E%9C%E6%9C%89%E4%BF%9D%E8%AF%81%E5%90%97%EF%BC%9F%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%EF%BC%89%E3%80%82%E8%BF%98%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AASAS%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%8A%8A%E7%BA%A2%E5%A4%96-%E5%8F%AF%E8%A7%81%E5%85%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BA%92%E8%BD%AC%E5%8C%96%E3%80%82%E8%BF%99%E4%B8%AA%E5%80%92%E6%98%AF%E5%8F%AF%E4%BB%A5%E7%9C%8B%E7%9C%8B%E3%80%82%E6%9C%80%E5%90%8E%E8%BF%98%E6%9C%89%E4%B8%80%E4%B8%AA%E4%B8%80%E8%87%B4%E6%80%A7%E5%AD%A6%E4%B9%A0%EF%BC%8C%E4%BC%B0%E6%91%B8%E7%9D%80%E6%98%AF%E6%8F%90%E5%87%BA%E6%9F%90%E4%B8%AA%E6%8D%9F%E5%A4%B1%EF%BC%8C%E5%8F%88%E6%98%AF%E6%8B%89%E8%BF%91%E5%90%8CID%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB%E3%80%82%E3%80%82%E3%80%82%E5%9B%9E%E5%A4%B4%E7%9C%8B%E7%9C%8B%E7%8C%9C%E5%AF%B9%E6%B2%A1"><span class="nav-number">5.2.</span> <span class="nav-text">好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外&#x2F;可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bidirectional-Multi-Step-Domain-Generalization-for-Visible-Infrared-Person-Re-Identification"><span class="nav-number">6.</span> <span class="nav-text">Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv%EF%BC%8C2024-3-16"><span class="nav-number">6.1.</span> <span class="nav-text">arxiv，2024.3.16</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%83%B3%E8%A7%A3%E5%86%B3-%E6%94%B9%E5%96%84%E7%9A%84%E7%82%B9%E5%9C%A8%E4%BA%8E%EF%BC%8C%E5%A4%A7%E9%83%A8%E5%88%86%E6%96%B9%E6%B3%95%E9%83%BD%E6%98%AF%E6%8A%8Avis%E5%92%8Cir-images%E6%8A%95%E5%BD%B1%E5%88%B0%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%A9%BA%E9%97%B4%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%E8%BF%99%E4%B8%AA%E7%A9%BA%E9%97%B4%E9%87%8C%E7%9A%84embedding%E8%BF%9B%E8%A1%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%8C%B9%E9%85%8D%EF%BC%88%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E5%8F%AB%E6%89%80%E8%B0%93%E2%80%9C%E5%8D%95%E4%B8%AD%E9%97%B4%E5%9F%9F%E7%94%9F%E6%88%90%E2%80%9D%EF%BC%89%EF%BC%8C%E8%80%8C%E8%AE%BA%E6%96%87%E6%8F%90%E5%88%B0%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%96%B9%E6%B3%95%E6%8F%90%E5%8F%96%E7%9A%84%E4%BF%A1%E6%81%AF%E4%B8%8D%E5%A4%9F%E5%A5%BD%EF%BC%8C%E5%9B%A0%E4%B8%BA%E5%AE%83%E5%8F%AF%E8%83%BD%E6%8F%90%E5%8F%96%E5%87%BA%E4%BA%86%E4%B8%80%E4%BA%9B%E5%85%AC%E5%85%B1%E7%9A%84%E8%83%8C%E6%99%AF%E4%BF%A1%E6%81%AF%EF%BC%8C%E8%80%8C%E8%BF%99%E9%83%A8%E5%88%86%E4%BF%A1%E6%81%AF%E6%98%AF%E6%B2%A1%E7%94%A8%E7%9A%84%EF%BC%88%E6%89%80%E4%BB%A5%E4%BB%8E%E7%BB%93%E6%9E%9C%E4%B8%8A%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%8D%95%E5%8D%95%E6%8A%95%E5%BD%B1%E5%88%B0%E5%90%8C%E4%B8%80%E7%A9%BA%E9%97%B4%EF%BC%8C%E6%95%88%E6%9E%9C%E4%B8%8D%E5%A4%9F%E5%A5%BD%EF%BC%9B%E8%87%B3%E4%BA%8E%E8%AF%B4%E6%8F%90%E5%8F%96%E5%87%BA%E8%83%8C%E6%99%AF%E4%BF%A1%E6%81%AF%EF%BC%8C%E4%B8%8D%E5%A5%BD%E5%88%A4%E6%96%AD%E6%98%AF%E8%AE%B2%E6%95%85%E4%BA%8B%E8%BF%98%E6%98%AF%E8%AF%B4%E7%9C%9F%E7%9A%84%E3%80%82%E4%B8%BB%E8%A6%81%E6%B2%A1%E6%9C%89%E5%AE%9E%E9%AA%8C%E6%9D%A5%E8%BE%85%E4%BD%90%E9%AA%8C%E8%AF%81%EF%BC%89%E3%80%82%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E7%9A%84%E6%96%B9%E6%B3%95BMDG%EF%BC%88Bidirectional-Multi-Step-Domain-Generalization%EF%BC%89%E4%B8%BB%E8%A6%81%E7%94%B1%E4%B8%A4%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90%EF%BC%8C%E4%B8%80%E6%98%AFpart-prototype-alignment-learning%EF%BC%88%E8%B4%9F%E8%B4%A3%E6%8F%90%E5%8F%96%E5%87%BA%E5%B1%80%E9%83%A8%E7%9A%84%E8%BA%AB%E4%BD%93%E9%83%A8%E4%BD%8D%E4%BF%A1%E6%81%AF%EF%BC%89%EF%BC%9B%E4%BA%8C%E6%98%AFbidirectional-multi-step-learning%EF%BC%88%E9%80%9A%E8%BF%87%E5%A4%9A%E6%AD%A5%E5%AD%A6%E4%B9%A0%EF%BC%8C%E4%BB%8E%E7%BA%A2%E5%A4%96%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E5%88%86%E5%88%AB%E5%87%BA%E5%8F%91%EF%BC%8C%E4%BB%A5%E5%87%8F%E5%B0%8Fmodality-gap%EF%BC%89"><span class="nav-number">6.2.</span> <span class="nav-text">这篇论文想解决&#x2F;改善的点在于，大部分方法都是把vis和ir images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional Multi-Step Domain Generalization）主要由两部分组成，一是part prototype alignment learning（负责提取出局部的身体部位信息）；二是bidirectional multi-step learning（通过多步学习，从红外和可见光分别出发，以减小modality gap）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#insight"><span class="nav-number">6.3.</span> <span class="nav-text">insight</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA"><span class="nav-number">6.4.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">6.5.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#part-prototype-alignment-learning%E4%B8%BB%E8%A6%81%E7%94%B1%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90%EF%BC%9A"><span class="nav-number">6.5.1.</span> <span class="nav-text">part prototype alignment learning主要由三个部分组成：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-prototype-discovery%EF%BC%88%E8%BF%99%E4%B8%AA%E9%83%A8%E5%88%86%E6%98%AF%E7%94%A8%E6%9D%A5%E8%8E%B7%E5%8F%96%E8%BA%AB%E4%BD%93%E9%83%A8%E4%BD%8D%E7%BB%86%E8%8A%82%E4%BF%A1%E6%81%AF%E7%9A%84%EF%BC%8C%E5%B9%B6%E4%B8%94%E8%A6%81%E5%9C%A8feature-map%E4%B8%8A%E6%8C%96%E6%8E%98%E5%87%BA%E5%85%B7%E6%9C%89%E5%88%A4%E5%88%AB%E6%80%A7%E7%9A%84%E4%BD%8D%E7%BD%AE%EF%BC%89%E3%80%82%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%AE%83%E7%9A%84%E4%B8%80%E4%B8%AAprototype%E7%9A%84%E5%BD%A2%E7%8A%B6%E6%98%AF"><span class="nav-number">6.5.1.1.</span> <span class="nav-text">1.prototype discovery（这个部分是用来获取身体部位细节信息的，并且要在feature map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CLIP-Driven-Semantic-Discovery-Network-for-Visible-Infrared-Person-Re-Identification"><span class="nav-number">7.</span> <span class="nav-text">CLIP-Driven Semantic Discovery Network for  Visible-Infrared Person Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TMM%EF%BC%8C2024-1-11"><span class="nav-number">7.1.</span> <span class="nav-text">TMM，2024.1.11</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E5%8F%88%E6%98%AF%E5%BC%95%E5%85%A5clip%EF%BC%88%E5%A5%BD%E5%A4%9A%E6%96%87%E7%AB%A0%E5%BC%95%E5%85%A5%E4%BA%86clip%EF%BC%8C%E7%9C%8B%E6%9D%A5%E5%8D%95%E7%BA%AF%E5%81%9Avi-reid%EF%BC%8C%E8%BF%98%E5%9C%A8%E9%AD%94%E6%94%B9%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%8F%AF%E8%83%BD%E4%B8%8A%E9%99%90%E4%B8%8D%E5%A4%AA%E9%AB%98%E4%BA%86%EF%BC%8C%E8%80%8C%E4%B8%94%E5%A4%AA%E5%8D%B7%E4%BA%86%EF%BC%8C%E5%8F%88%E4%B8%8D%E5%88%9B%E6%96%B0%EF%BC%9B%E6%89%80%E4%BB%A5%E8%AF%95%E7%9D%80%E5%BC%95%E5%85%A5clip%EF%BC%8C%E6%8F%90%E9%AB%98%E8%83%BD%E5%8A%9B%EF%BC%8C%E4%B9%9F%E2%80%9C%E6%98%BE%E5%BE%97%E2%80%9D%E5%88%9B%E6%96%B0%E4%B8%80%E7%82%B9%E3%80%82%E6%96%87%E7%AB%A0%E7%9A%84%E5%A4%A7%E4%BD%93%E7%9A%84%E4%BA%AE%E7%82%B9-%E5%87%BA%E5%8F%91%E7%82%B9%E5%9C%A8%E4%BA%8E%EF%BC%8C%E5%88%A9%E7%94%A8clip%E7%9A%84%E8%8E%B7%E5%8F%96%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84%E8%83%BD%E5%8A%9B%EF%BC%8C%E6%8F%90%E5%8F%96%E4%B8%80%E4%BA%9B%E9%AB%98%E5%B1%82%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%EF%BC%8C%E7%94%A8%E6%9D%A5%E8%BE%85%E5%8A%A9%E6%A3%80%E7%B4%A2%E5%9B%BE%E5%83%8F%E3%80%82%E8%BF%99%E4%B8%AA%E5%92%8C%E5%89%8D%E9%9D%A2EEES%E9%82%A3%E7%AF%87%EF%BC%88Embedding-and-Enriching-Explicit-Semantics-for-Visible-Infrared-Person-Re-Identification%EF%BC%89%E7%9A%84%E5%87%BA%E5%8F%91%E7%82%B9%E6%9C%89%E7%82%B9%E7%B1%BB%E4%BC%BC"><span class="nav-number">7.2.</span> <span class="nav-text">这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点&#x2F;出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding and Enriching Explicit Semantics for Visible-Infrared Person Re-Identification）的出发点有点类似</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">134</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
