<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification arxiv 2025.1.23 这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery images中有两种模态的图像，而不止一种模态。 动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种vi">
<meta property="og:type" content="article">
<meta property="og:title" content="bluemouse&#39;s blog">
<meta property="og:url" content="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="From Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification arxiv 2025.1.23 这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery images中有两种模态的图像，而不止一种模态。 动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种vi">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-01-29T16:25:39.695Z">
<meta property="article:modified_time" content="2025-02-09T18:31:27.826Z">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title> | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/01/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/cv/Object_Re-Identification/VI-ReID/%E6%B3%9B%E8%AF%BB/%E6%B3%9B%E8%AF%BB%EF%BC%88VI-ReID%EF%BC%89%E2%80%94%E2%80%942025.1.29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-30 00:25:39" itemprop="dateCreated datePublished" datetime="2025-01-30T00:25:39+08:00">2025-01-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-02-10 02:31:27" itemprop="dateModified" datetime="2025-02-10T02:31:27+08:00">2025-02-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1
id="from-cross-modal-to-mixed-modal-visible-infrared-re-identification">From
Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification</h1>
<h2 id="arxiv-2025.1.23">arxiv 2025.1.23</h2>
<h2
id="这篇论文主要是提出一个vi-reid中的新的场景吧就是gallery-images中有两种模态的图像而不止一种模态">这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery
images中有两种模态的图像，而不止一种模态。</h2>
<h2
id="动机经典的vi-reid实验的时候都是query和gallery模态不同那么其实就是一种visible-infraredinfrared-visible的匹配对应到实际情况可能就是白天照片-晚上照片晚上照片-白天照片但更实际的情况应该是匹配的时候可能既有可见光照片也有红外照片所以gallery中有两种模态的图像也就是文中提到的mixed-modal的setting这会带来一个问题就是gallery中同一模态的图像的domain-gap显然会比不同模态的domain-gap要小然而其中同一模态的图像却有可能是属于不同id的这可能导致匹配错误举个具体例子gallery中有id-1的一个红外图像一个可见光图像和一个id-2的可见光图像现在有一个id-2的红外query如果特征学的不够好进行匹配的时候很可能会把query匹配到id-1的那个红外图像上了毕竟都是红外图像本身的gap就比较小而红外和可见光之间的gap可就大了-这篇论文就是想解决这个问题">动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared/infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片/晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain
gap显然会比不同模态的domain
gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id
1的一个红外图像，一个可见光图像，和一个id 2的可见光图像。现在有一个id
2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id
1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。<br>这篇论文就是想解决这个问题</h2>
<h2
id="方法它的大致思想是既然gallery中有两种模态的图像那么匹配的时候就既要用到modality-shared-feature也要用到modality-specific-feature文章提出一种方法把特征分解到两个正交的子空间中一个代表shared-feature一个代表specific-feature好经典的做法和讲故事的套路有没有什么手段能验证可视化这俩确实正交确实代表了shared-feature和specific-feature呢">方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared
feature，也要用到modality-specific
feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared
feature，一个代表specific
feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared
feature和specific feature呢？）</h2>
<h4
id="话说回来在经典的vi-reid中真的有必要用到modality-specific-feature吗在经典设定下不是query和gallery一定不同模态吗那modality-specific-feature只有一方有匹配的时候用不上啊事实上人来进行红外和可见光的匹配的时候也是通过神态和脸来判断的吧服装那些基本用不上啊换装设定是不是也是这个思路">（话说回来，在经典的vi-reid中，真的有必要用到modality-specific
feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific
feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）</h4>
<h2
id="分析这篇文章用到一些互信息的公式来进行推导和阐述因为要用学到的模型生成的embedding来判断id所以要最大化embedding和id之间的标签而考虑到现在的mixed-modality-scenario需要把embedding拆分成两块就是上面提到的modality-shared和-specific-feature或者说原文的modality-erased-related-feature下面就是怎么把这两部分学到的问题论文把这部分转化成一个待优化的式子max_z_mez_mrmiz_mez_mry-s.t.miz_mez_mr0miz_mem0and-miz_mrym0-上面的z_me代表模态m下的modality-erased-feature也就是与模态无关的特征z_mr代表模态m下的modality-related-feature也就是与模态相关的特征模态m可能是visible也可能是infrared三个约束也是保证学到的特征有效的关键第一个约束是想让modality-erased-feature与modality-related-feature相互独立这个是合理的毕竟理论上这两部分的交集是空集并集就是所有的特征信息了第二个约束是因为z_me代表modality-erased-feature所以用这个特征应该推断不出模态信息所以它应该与模态独立也合理第三个约束没搞明白它想干什么按原论文它是希望z_mr不包含id相关的信息但为什么要这样如果真的满足了这个约束z_mr就是只能用来推断模态信息不能用来推断id信息那它和z_me组合起来用又是为什么还是说它是彻底的把模态和id语义信息给分开了分到两个特征里所以推断的时候是怎么推断的是说如果是同一模态的图像来匹配则用的特征是上面提到的modality-erased-feature和modality-related-feature拼接起来的一个特征如果是跨模态的匹配则只用modality-erased-feature但这样不是很麻烦吗得再看看代码了可能-总而言之得到了上面的待优化式子下面就是把它转化成一个近似的式子用损失去刻画从而变成可以用深度学习解决的问题得到结果如下500-且针对四个部分都进行了各自的优化">分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality
scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific
feature（或者说原文的modality-erased/-related
feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：<span
class="math display">\[\max_{Z_{m}^{e},Z_{m}^{r}}\{MI(Z_{m}^{e},Z_{m}^{r};Y)\}\
\ s.t.MI(Z_{m}^{e},Z_{m}^{r})=0,MI(Z_{m}^{e};M)=0,and\
MI(Z_{m}^{r};Y|M)=0\]</span><br>上面的<span
class="math inline">\(Z_{m}^{e}\)</span>代表模态m下的modality-erased
feature，也就是与模态无关的特征；<span
class="math inline">\(Z_{m}^{r}\)</span>代表模态m下的modality-related
feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased
feature与modality-related
feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为<span
class="math inline">\(Z_{m}^{e}\)</span>代表modality-erased
feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望<span
class="math inline">\(Z_{m}^{r}\)</span>不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，<span
class="math inline">\(Z_{m}^{r}\)</span>就是只能用来推断模态信息，不能用来推断ID信息。那它和<span
class="math inline">\(Z_{m}^{e}\)</span>组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased
feature和modality-related
feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased
feature（但这样不是很麻烦吗？得再看看代码了可能）<br>总而言之，<strong>得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题</strong>。得到结果如下：<img
src="../泛读/pic/cross-modal.png"
alt="|500" /><br>且针对四个部分，都进行了各自的优化</h2>
<h2
id="评价虽然这篇论文乍一看很烂特别是摘要那里还把数据集的名字打错了更是降低印象分可是看完以后一来它提出的新场景也不算空中楼阁确实是更有实际意义的二来它提出方法以后对每一部分的刻画和阐述都还算合理有一点娓娓道来的感觉都是就事论事的分析">评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析</h2>
<h2 id="trick总结">trick总结：</h2>
<h3
id="想让两个向量尽可能独立可以用互信息为0来进行理论上的刻画虽说其实可能是有失偏颇的然后用正交损失来近似代替">1.想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替</h3>
<h3
id="如果要最大化某个互信息那么可以从语义上去理解给出相应的损失比如论文里提到的要最大化miz_mey即要最大化modality-erased-feature与标签变量之间的互信息其实目的就是让modality-erased-feature能尽可能揭示出id信息所以用一个交叉熵的分类损失就是理所应当的了可以认为是对常见的交叉熵分类损失给出一个理论上的解释">2.如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化<span
class="math inline">\(MI(Z_{m}^{e};Y)\)</span>，即要最大化modality-erased
feature与标签变量之间的互信息，其实目的就是让modality-erased
feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）</h3>
<h3
id="要让模型混淆某几个部分这可能是出于模型设计的考虑可以考虑进行分类但加上梯度反转层">3.要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层</h3>
<h3
id="要约束某两个量xy尽可能接近可以考虑给出这个形式的损失lmaxx-yalpha0maxy-xalpha0alpha是正常数也是超参数那么x和y必须相差绝对值不大于alpha这部分损失才为0否则就产生损失了">4.要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：<span
class="math inline">\(L=max(x-y+\alpha,0)+max(y-x+\alpha,0)\)</span>，<span
class="math inline">\(\alpha\)</span>是正常数，也是超参数。那么，x和y必须相差绝对值不大于<span
class="math inline">\(\alpha\)</span>，这部分损失才为0，否则就产生损失了</h3>
<h3
id="这个trick目前看下来好像还是比较局限的就是针对跨模态行人重识别的时候才会用到就是如果要学习modality-specific-feature且每个id都有两种模态的数据则可以考虑把每一个id给doubled比如如果它是visible的图像则id-2如果它是infrared的图像则idid-2-1基于这一套新的标签进行分类并用交叉熵分类损失来优化这里之所以能起到效果是因为我们把同一id的不同模态的数据给视为不同标签相当于进行更细粒度的分类模型如果要正确分类就必须学会捕获同一id下visible和infrared图像的各自的有判别力的特征这样模型就能学会提取出modality-specific-feature当然这个trick也并不唯一肯定是还有别的方法的就比如再用个损失约束同一id的不同模态的数据要尽可能远离有点对比学习的意思通过让不同类别的数据尽可能远离让模型潜移默化地学会了语义信息应该也能学会提取modality-specific-feature">5.这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific
feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID
* 2；如果它是infrared的图像，则ID=ID * 2 +
1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific
feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific
feature</h3>
<h1
id="spectral-enhancement-and-pseudo-anchor-guidance-for-infrared-visible-person-re-identification">Spectral
Enhancement and Pseudo-Anchor Guidance for Infrared-Visible Person
Re-Identification</h1>
<h2 id="arxiv-2025.1.2">arxiv, 2025.1.2</h2>
<h2
id="这篇文章主要是从谱域的角度来缓解两种模态的巨大差异但它只是直接指出这两种模态的数据有巨大的spectral-gap却并没有分析为什么有这种gap或者说我们怎么确认这种gap确实存在而不只是说说而已有种拿到谱域方法就来试一下的感觉">这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral
gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）</h2>
<h2 id="动机">动机：</h2>
<h2 id="方法">方法：</h2>
<h2
id="分析第一步是生成一个semantically-enhanced-grey-images姑且可以认为是一种拉近两种模态的数据的方法只不过是从可见光图像出发往红外上靠拢也合理毕竟可见光图像信息更多红外图像信息更少从信息更多的往信息更少的变换是比较容易的它主要做的其实是在很常见的灰度映射之余加上了高频信息具体来说它会先对可见光图像进行一个傅里叶变换提取它的频域信息然后用其中的频率成分进行傅里叶逆变换这里是为了提取轮廓信息然后将逆变换的结果与灰度映射的结果相加得到semantically-enhanced-grey-images下简称seg图像btw这个方法倒是可以借鉴一下感觉还是行得通的起码可以试试用到我们的方法里就是加上傅里叶变换之后可能计算量大了-第二步是对红外图像和seg图像进行进一步的特征提取也是为了进一步缩小两种模态的数据之间的差异因为最后还是要用特征进行相似度计算的这里其实平平无奇-就是用resnet-50的前三块共享权重来提取共同的特征之后又用resnet-50的最后一个块不同权重来分别提取modality-specific-feature-第三步是paba-loss即pseudo-anchor-guided-bidirectional-aggregation-loss它的目的是对于前面的共同特征希望它们能更加兼容也就是尽可能的变换到一个空间里但也不能就失去判断力了而所谓anchor-guided其实就是说用向anchor靠拢来实现两种数据的靠拢举个具体例子我有同一id的两种模态的若干数据我希望把它们变换到一个统一的空间里则可以考虑求一下可见光的一个anchor比如所有可见光数据的均值然后拉近所有红外数据与这个anchor的距离从而实现红外数据与可见光数据的靠拢说回这里它是对上面提到的共同特征f_sharedseg和f_sharedir先进行一个分块分成n块之后对每个块都施加paba-loss拉近同一id的不同模态的数据的距离同时拉远不同id的不同模态的数据的距离相比于常见的triplet-loss这里增加了跨模态的考虑而相比于cross-center-loss这里的改进在于分了块且是对每个块都施加paba-loss故更加细粒度这里其实要结合公式来看才清楚-除了上面提到的paba-loss经典的交叉熵分类损失当然也是不能少的针对specific和shared-feature都有这个分类损失三部分损失加权求和即得到总的损失权重是超参">分析：第一步是生成一个Semantically
Enhanced Grey
Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically
Enhanced Grey
Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了<br><br>第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇，
就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific
feature<br><br>第三步是PABA loss，即Pseudo Anchor-guided Bidirectional
Aggregation
Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征<span
class="math inline">\(F_{shared}^{seg}\)</span>和<span
class="math inline">\(F_{shared}^{ir}\)</span>，先进行一个分块，分成N块，之后对每个块都施加PABA
loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet
loss，这里增加了跨模态的考虑；而相比于cross-center
loss，这里的改进在于分了块，且是对每个块都施加PABA
loss，故更加细粒度（这里其实要结合公式来看才清楚）<br>除了上面提到的PABA
loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared
feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）</h2>
<h1
id="embedding-and-enriching-explicit-semantics-for-visible-infrared-person-re-identification">Embedding
and Enriching Explicit Semantics for Visible-Infrared Person
Re-Identification</h1>
<h2 id="arxiv2024.12.11">arxiv，2024.12.11</h2>
<h2
id="这篇文章的方法看起来有点复杂然后动机上看感觉也有些牵强更多是为了用上现在很火的大模型所以硬凑了个理由">这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由</h2>
<h2
id="insight利用了一下text用文本信息来直接补充一部分图像的语义信息可能会比单纯用图像encoder来提取语义信息要好一点提出若干损失来利用多视角信息但是否合理有待考察至少看它的消融实验结果证明了这部分是有效的">insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）</h2>
<h2
id="动机文章指出现在的vi-reid方法有两大类一类是基于生成式模型的方法效果不够好个人不太了解一类是基于判别式模型的方法也就是用模型把图像进行embed用embedding来进行匹配通常需要各种魔改网络架构这种方法的效果会更好也更主流而文章指出这种方法仅利用图像对语义信息的提取不够因此希望加入文本描述来帮助更好的提取文本信息就是这一点让我感觉略有点牵强它所谓的文本描述也是人llm基于图像进行的描述语义信息本身就是来自图像的至于说提取的不够可以认为是网络架构不够好数据不够多等问题导致的加入文本描述更多可能是希望能让模型能比较容易地学到语义信息不过考虑到现在reid的骨干基本都是一个resnet50模型容量应该是有上限的那么用文字embedding来辅助一下也有道理">动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人/llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）</h2>
<h2 id="方法-1">方法：</h2>
<h3
id="提出一个eees框架包含三个部分各司其职-第一个是ese模块explicit-semantics-embedding它的目的是利用文本让图像embedding学到更多语义信息主要做的是引入vllm为图像生成文本描述然后用对比学习拉近图像embedding和对应的文本描述的embedding让图像embedding学到文本的语义信息">提出一个EEES框架，包含三个部分，各司其职<br>第一个是ESE模块（Explicit
Semantics
Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息</h3>
<h3
id="第二个是cvsccross-view-semantics-compensation它的目的是利用多视角的图像让模型能提取出更加丰富的语义信息举个例子对于同一个人一个摄像头视角拍到的照片所蕴含的信息终归是有限的可能这个视角下的照片能拍到身材轮廓脸却被遮住了这时如果能用到其它视角下这个人的照片可能就能获得脸部信息了综合起来就能提取出这个人的更多特征所以训练的时候对于一个batch里的每个图像这套方法都会去利用同id在其它视角下的图像而具体的利用方法就是在训练时对一个batch中的每个图像都去随机取m个同id的不同摄像头下的图像然后求个平均值共m1个图像我们就认为这个均值图像里蕴含了多视角的信息对于文本也是进行类似的操作从而得到蕴含了多视角信息的文本embedding当然都必须是同模态下的就是说可见光图像要去找同id的不同摄像头下的其它可见光图像其余类似之后会用一个对齐损失去双向约束多视角信息下的图像embedding和文字embedding毕竟即使是综合了多视角信息对于同一个人的图像embedding和文字embedding依然应该尽可能相近确保图像学到更多合理的语义信息-然而实际测试的时候我们都是拿到一个图像就要去进行匹配了也就是说没有多视角的信息可以利用上面也提到了是训练时可以这样利用多视角信息那为了弥补这一缺点论文提出进行一个蒸馏就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding文字embedding类似两种模态都进行这个操作所以这个蒸馏损失就是四项了公式如下500">第二个是CVSC（Cross-View
Semantics
Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共<span
class="math inline">\(M+1\)</span>个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息<br>然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是<strong>训练</strong>时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下：<img
src="../泛读/pic/distillation_loss.png" alt="|500" /></h3>
<h3
id="第三个是cmsp模块cross-modality-semantics-purification它的目的是避免学到的语义发生冲突具体来说同一个id会有灰度图和可见光图像也会有对应的灰度图的文字描述和可见光图像的文字描述而这两种文字描述可能发生冲突比如llm针对可见光图像可能可以准确地描述出人穿的衣服的颜色也许是蓝色而拿到灰度图像全部都是会的它可能就会输出人穿的衣服的颜色是灰的这无疑会造成矛盾所以我们希望对两种模态的文字描述可以不同互补当然是好现象但不能矛盾因此论文提出一个cmsp-loss公式如下l_cmspfrac1nsum_i1nd_ivv-d_ivr2frac1nsum_i1nd_irr-d_irv2-其中d_ivvlvert-f_iv-t_ivrvert_2d_ivrlvert-f_iv-t_irrvert_2其余定义类似可以看到我们的目的是让这个损失约束同一id下的红外图像文字描述要尽可能接近它的可见光图像的文字描述至于为什么模型不会塌缩直接让红外图像文字描述与可见光图像文字描述完全一样可能是因为前面的对比损失分类损失约束了这些文字embedding让它们必须有合理的语义信息吧">第三个是CMSP模块（Cross-Modality
Semantics
Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp
loss，公式如下：<span
class="math display">\[L_{cmsp}=\frac{1}{N}\sum_{i=1}^{N}{(d_{i}^{vv}-d_{i}^{vr})}^{2}+\frac{1}{N}\sum_{i=1}^{N}{(d_{i}^{rr}-d_{i}^{rv})}^{2}\]</span><br>其中，<span
class="math inline">\(d_{i}^{vv}=\lVert
f_{i}^{v}-t_{i}^{v}\rVert_{2}\)</span>，<span
class="math inline">\(d_{i}^{vr}=\lVert
f_{i}^{v}-t_{i}^{r}\rVert_{2}\)</span>，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧</h3>
<h2 id="分析">分析</h2>
<h3
id="第一部分是稀松平常的但好像也就第一部分看起来靠点谱">第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。</h3>
<h3
id="第二部分想法是挺好的但一来训练的时候凭什么说求个均值图像就蕴含了多视角的信息或者说这种方法是不是太简单粗暴了还有测试的时候那个蒸馏就是说希望单视角下的embedding能尽可能接近多视角下的embedding可见光红外图像文字也就是说我们竟然是希望模型拿到一个单视角的图像就能脑补出多视角下的图像信息这合理吗只能说有一些合理之处毕竟人拿到一张图像确实能靠空间想象力脑补出一些东西但也有些不合理之处比如上面的脸部遮挡例子这个是无论如何也不能脑补出来的逼迫模型去学这个也是不合理的难说最后学出来的是什么可能过拟合了">第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光/红外，图像/文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）</h3>
<h3
id="第三部分同样出发点倒也没错就是这个损失看起来有些奇怪以cmsp-loss的第一部分为例其实我们可以说这个损失是在以可见光图像的embedding为锚点让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近我猜想作者的目的是让两种文字embedding尽可能接近这样就相当于说让文字embedding尽可能提取出共性的语义信息而不会去提取出矛盾的一方独有的信息如果真是这样它就应该直接约束两个文字embedding了没必要掺和上图像embedding但几何上看要让损失最小它们也可以在一个以可见光图像为圆心的某个圆上只不过是高维的此时它们未必就距离很近这样得到的两种embedding有什么含义呢不能认为说在同一个圆周上语义信息就不矛盾了吧有点怪或者我们认为文字embedding不动优化的是图像embedding这样最理想的情况是图像embedding落在了两种文字embedding的中垂线上了这样就能认为图像embedding没有学到什么矛盾的语义信息吗还是没道理啊又或者上面的分析都是假定了一种embedding是不动的文字图像实际是一起优化的那它想优化什么呢还是不太明白">第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp
loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的/一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字/图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。</h3>
<h1
id="prototype-driven-multi-feature-generation-for-visible-infrared-person-re-identification">Prototype-Driven
Multi-Feature Generation for Visible-Infrared Person
Re-identification</h1>
<h2 id="icassp2024.9.9">ICASSP，2024.9.9</h2>
<h2
id="动机其实感觉不太明确似乎是魔改完有效果了就开始讲故事了">动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了</h2>
<h2
id="insight提出一个mfgm来获取多样特征但其实不是很新奇了跟deen差不多的思路就是多几个分支加上一些损失约束让这些生成的特征不太一样用一个prototype来提取特征感觉本质只是11的卷积跨模态用同一套模板然后又用一个损失来约束模板提取不同的特征和上面约束生成特征的差不多这里只是换了个cos的套子拉近同id特征拉远不同id特征来确保特征的语义信息的正确性">insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性</h2>
<h2 id="动机-1">动机：</h2>
<h2 id="方法-2">方法：</h2>
<h3
id="提出的pdm框架主要是两个组成部分分别是multi-feature-generation-modulemfgm用于生成更多多样的特征参考deen和prototype-learning-moduelplm">提出的PDM框架，主要是两个组成部分，分别是Multi-Feature
Generation Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype
Learning Moduel（PLM）</h3>
<h3
id="mfgm感觉真的和deen很像都是用几个分支生成多几个特征然后分支上都有dilation-convolution之后又用一些损失来把某些特征拉远以实现多样化特征这里的损失就是文中的center-guided-pair-mining-loss">MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation
convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided
Pair Mining Loss</h3>
<h3
id="plm的话可能是我论文看少了相对没那么常见但它这里提出prototype也不是一般的为了保留知识而是为了提取知识它的大概思想和cnn里的卷积核差不多吧因为对卷积核的一种理解就是它学到了一些特定的模式然后用卷积操作来进行模式匹配这里提取知识也是类似的用一组可学习的prototype类比卷积核来和图像进行哈达玛积再求均值类比卷积操作并且是两种模态的数据用同一组prototype所以以此来促进跨模态的共性知识提取但怎么保证prototype提取的就一定是局部信息呢仅仅是因为prototype的形状是cchannel维的向量吗这样的话倒是能稍微解释一下为什么不直接用一个cnn了或者说这其实就是一种特殊的cnn只不过是11卷积了这么说来其实还是讲故事啊">PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）</h3>
<h3
id="此外为了保证它的不同prototype提取出的特征也尽可能多样提出了cosine-heterogeneity-loss其实很老套了跟欧氏距离差不多只不过这里换成了用cos还有一个dual-center-seperation-loss这个损失似乎并没有说明它解决了什么问题感觉很可能只是为了提点而已本身它的思想也很常见了就是换个法子让同id的特征近一点不同id的特征远一点">此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine
Heterogeneity
Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center
Seperation
Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）</h3>
<h1
id="parameter-hierarchical-optimization-for-visible-infrared-person-re-identification">Parameter
Hierarchical Optimization for Visible-Infrared Person
Re-Identification</h1>
<h2 id="arxiv2024.4.11">arxiv，2024.4.11</h2>
<h2
id="好像确实有点新奇把参数划分为两种类型一种是正常在深度学习中用梯度反传来优化一种是基于一些规则来优化应该是比较传统的那种以此来减少需要梯度反传来优化的参数这个是可以减少一些计算量但其它好处呢基于规则来优化效果有保证吗原理是什么还提出一个sas可以把红外可见光两种模态的图像相互转化这个倒是可以看看最后还有一个一致性学习估摸着是提出某个损失又是拉近同id的不同模态的特征之间的距离回头看看猜对没">好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外/可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没</h2>
<h1
id="bidirectional-multi-step-domain-generalization-for-visible-infrared-person-re-identification">Bidirectional
Multi-Step Domain Generalization for Visible-Infrared Person
Re-Identification</h1>
<h2 id="arxiv2024.3.16">arxiv，2024.3.16</h2>
<h2
id="这篇论文想解决改善的点在于大部分方法都是把vis和ir-images投影到一个公共空间然后用这个空间里的embedding进行相似度匹配这种方法叫所谓单中间域生成而论文提到这个方法提取的信息不够好因为它可能提取出了一些公共的背景信息而这部分信息是没用的所以从结果上来说就是单单投影到同一空间效果不够好至于说提取出背景信息不好判断是讲故事还是说真的主要没有实验来辅佐验证论文提出的方法bmdgbidirectional-multi-step-domain-generalization主要由两部分组成一是part-prototype-alignment-learning负责提取出局部的身体部位信息二是bidirectional-multi-step-learning通过多步学习从红外和可见光分别出发以减小modality-gap">这篇论文想解决/改善的点在于，大部分方法都是把vis和ir
images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional
Multi-Step Domain Generalization）主要由两部分组成，一是part prototype
alignment learning（负责提取出局部的身体部位信息）；二是bidirectional
multi-step
learning（通过多步学习，从红外和可见光分别出发，以减小modality
gap）</h2>
<h2 id="insight">insight</h2>
<h2 id="动机-2">动机</h2>
<h2 id="方法-3">方法</h2>
<h3 id="part-prototype-alignment-learning主要由三个部分组成">part
prototype alignment learning主要由三个部分组成：</h3>
<h4
id="prototype-discovery这个部分是用来获取身体部位细节信息的并且要在feature-map上挖掘出具有判别性的位置具体来说它的一个prototype的形状是">1.prototype
discovery（这个部分是用来获取身体部位细节信息的，并且要在feature
map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是</h4>
<h1
id="clip-driven-semantic-discovery-network-for-visible-infrared-person-re-identification">CLIP-Driven
Semantic Discovery Network for Visible-Infrared Person
Re-Identification</h1>
<h2 id="tmm2024.1.11">TMM，2024.1.11</h2>
<h2
id="这篇文章又是引入clip好多文章引入了clip看来单纯做vi-reid还在魔改网络的话可能上限不太高了而且太卷了又不创新所以试着引入clip提高能力也显得创新一点文章的大体的亮点出发点在于利用clip的获取图像语义信息的能力提取一些高层语义信息用来辅助检索图像这个和前面eees那篇embedding-and-enriching-explicit-semantics-for-visible-infrared-person-re-identification的出发点有点类似">这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点/出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding
and Enriching Explicit Semantics for Visible-Infrared Person
Re-Identification）的出发点有点类似</h2>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/01/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/RAG/Retrieval-Augmented%20Generation%20for%20%20AI-Generated%20Content%20%20A%20Survey/" rel="prev" title="">
      <i class="fa fa-chevron-left"></i> 
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/02/03/hello-world/" rel="next" title="Hello World">
      Hello World <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#from-cross-modal-to-mixed-modal-visible-infrared-re-identification"><span class="nav-number">1.</span> <span class="nav-text">From
Cross-Modal to Mixed-Modal Visible-Infrared Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv-2025.1.23"><span class="nav-number">1.1.</span> <span class="nav-text">arxiv 2025.1.23</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E4%B8%BB%E8%A6%81%E6%98%AF%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AAvi-reid%E4%B8%AD%E7%9A%84%E6%96%B0%E7%9A%84%E5%9C%BA%E6%99%AF%E5%90%A7%E5%B0%B1%E6%98%AFgallery-images%E4%B8%AD%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E8%80%8C%E4%B8%8D%E6%AD%A2%E4%B8%80%E7%A7%8D%E6%A8%A1%E6%80%81"><span class="nav-number">1.2.</span> <span class="nav-text">这篇论文主要是提出一个vi-reid中的新的场景吧，就是gallery
images中有两种模态的图像，而不止一种模态。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA%E7%BB%8F%E5%85%B8%E7%9A%84vi-reid%E5%AE%9E%E9%AA%8C%E7%9A%84%E6%97%B6%E5%80%99%E9%83%BD%E6%98%AFquery%E5%92%8Cgallery%E6%A8%A1%E6%80%81%E4%B8%8D%E5%90%8C%E9%82%A3%E4%B9%88%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E4%B8%80%E7%A7%8Dvisible-infraredinfrared-visible%E7%9A%84%E5%8C%B9%E9%85%8D%E5%AF%B9%E5%BA%94%E5%88%B0%E5%AE%9E%E9%99%85%E6%83%85%E5%86%B5%E5%8F%AF%E8%83%BD%E5%B0%B1%E6%98%AF%E7%99%BD%E5%A4%A9%E7%85%A7%E7%89%87-%E6%99%9A%E4%B8%8A%E7%85%A7%E7%89%87%E6%99%9A%E4%B8%8A%E7%85%A7%E7%89%87-%E7%99%BD%E5%A4%A9%E7%85%A7%E7%89%87%E4%BD%86%E6%9B%B4%E5%AE%9E%E9%99%85%E7%9A%84%E6%83%85%E5%86%B5%E5%BA%94%E8%AF%A5%E6%98%AF%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%E5%8F%AF%E8%83%BD%E6%97%A2%E6%9C%89%E5%8F%AF%E8%A7%81%E5%85%89%E7%85%A7%E7%89%87%E4%B9%9F%E6%9C%89%E7%BA%A2%E5%A4%96%E7%85%A7%E7%89%87%E6%89%80%E4%BB%A5gallery%E4%B8%AD%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%96%87%E4%B8%AD%E6%8F%90%E5%88%B0%E7%9A%84mixed-modal%E7%9A%84setting%E8%BF%99%E4%BC%9A%E5%B8%A6%E6%9D%A5%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%E5%B0%B1%E6%98%AFgallery%E4%B8%AD%E5%90%8C%E4%B8%80%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E7%9A%84domain-gap%E6%98%BE%E7%84%B6%E4%BC%9A%E6%AF%94%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84domain-gap%E8%A6%81%E5%B0%8F%E7%84%B6%E8%80%8C%E5%85%B6%E4%B8%AD%E5%90%8C%E4%B8%80%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8D%B4%E6%9C%89%E5%8F%AF%E8%83%BD%E6%98%AF%E5%B1%9E%E4%BA%8E%E4%B8%8D%E5%90%8Cid%E7%9A%84%E8%BF%99%E5%8F%AF%E8%83%BD%E5%AF%BC%E8%87%B4%E5%8C%B9%E9%85%8D%E9%94%99%E8%AF%AF%E4%B8%BE%E4%B8%AA%E5%85%B7%E4%BD%93%E4%BE%8B%E5%AD%90gallery%E4%B8%AD%E6%9C%89id-1%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E5%92%8C%E4%B8%80%E4%B8%AAid-2%E7%9A%84%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%8E%B0%E5%9C%A8%E6%9C%89%E4%B8%80%E4%B8%AAid-2%E7%9A%84%E7%BA%A2%E5%A4%96query%E5%A6%82%E6%9E%9C%E7%89%B9%E5%BE%81%E5%AD%A6%E7%9A%84%E4%B8%8D%E5%A4%9F%E5%A5%BD%E8%BF%9B%E8%A1%8C%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%E5%BE%88%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%8A%8Aquery%E5%8C%B9%E9%85%8D%E5%88%B0id-1%E7%9A%84%E9%82%A3%E4%B8%AA%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E4%B8%8A%E4%BA%86%E6%AF%95%E7%AB%9F%E9%83%BD%E6%98%AF%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E6%9C%AC%E8%BA%AB%E7%9A%84gap%E5%B0%B1%E6%AF%94%E8%BE%83%E5%B0%8F%E8%80%8C%E7%BA%A2%E5%A4%96%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E4%B9%8B%E9%97%B4%E7%9A%84gap%E5%8F%AF%E5%B0%B1%E5%A4%A7%E4%BA%86-%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E5%B0%B1%E6%98%AF%E6%83%B3%E8%A7%A3%E5%86%B3%E8%BF%99%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="nav-number">1.3.</span> <span class="nav-text">动机：经典的vi-reid，实验的时候，都是query和gallery模态不同。那么，其实就是一种visible-infrared&#x2F;infrared-visible的匹配。对应到实际情况，可能就是白天照片-晚上照片&#x2F;晚上照片-白天照片。但更实际的情况，应该是，匹配的时候，可能既有可见光照片，也有红外照片，所以gallery中有两种模态的图像，也就是文中提到的mixed-modal的setting。这会带来一个问题，就是，gallery中同一模态的图像的domain
gap显然会比不同模态的domain
gap要小，然而其中同一模态的图像却有可能是属于不同id的，这可能导致匹配错误。举个具体例子，gallery中有id
1的一个红外图像，一个可见光图像，和一个id 2的可见光图像。现在有一个id
2的红外query。如果特征学的不够好，进行匹配的时候，很可能会把query匹配到id
1的那个红外图像上了，毕竟都是红外图像，本身的gap就比较小，而红外和可见光之间的gap可就大了。这篇论文就是想解决这个问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E5%AE%83%E7%9A%84%E5%A4%A7%E8%87%B4%E6%80%9D%E6%83%B3%E6%98%AF%E6%97%A2%E7%84%B6gallery%E4%B8%AD%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E9%82%A3%E4%B9%88%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%E5%B0%B1%E6%97%A2%E8%A6%81%E7%94%A8%E5%88%B0modality-shared-feature%E4%B9%9F%E8%A6%81%E7%94%A8%E5%88%B0modality-specific-feature%E6%96%87%E7%AB%A0%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95%E6%8A%8A%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3%E5%88%B0%E4%B8%A4%E4%B8%AA%E6%AD%A3%E4%BA%A4%E7%9A%84%E5%AD%90%E7%A9%BA%E9%97%B4%E4%B8%AD%E4%B8%80%E4%B8%AA%E4%BB%A3%E8%A1%A8shared-feature%E4%B8%80%E4%B8%AA%E4%BB%A3%E8%A1%A8specific-feature%E5%A5%BD%E7%BB%8F%E5%85%B8%E7%9A%84%E5%81%9A%E6%B3%95%E5%92%8C%E8%AE%B2%E6%95%85%E4%BA%8B%E7%9A%84%E5%A5%97%E8%B7%AF%E6%9C%89%E6%B2%A1%E6%9C%89%E4%BB%80%E4%B9%88%E6%89%8B%E6%AE%B5%E8%83%BD%E9%AA%8C%E8%AF%81%E5%8F%AF%E8%A7%86%E5%8C%96%E8%BF%99%E4%BF%A9%E7%A1%AE%E5%AE%9E%E6%AD%A3%E4%BA%A4%E7%A1%AE%E5%AE%9E%E4%BB%A3%E8%A1%A8%E4%BA%86shared-feature%E5%92%8Cspecific-feature%E5%91%A2"><span class="nav-number">1.4.</span> <span class="nav-text">方法：它的大致思想是，既然gallery中有两种模态的图像，那么匹配的时候就既要用到modality-shared
feature，也要用到modality-specific
feature。文章提出一种方法，把特征分解到两个正交的子空间中，一个代表shared
feature，一个代表specific
feature（好经典的做法和讲故事的套路。。有没有什么手段能验证、可视化这俩确实正交，确实代表了shared
feature和specific feature呢？）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%9D%E8%AF%B4%E5%9B%9E%E6%9D%A5%E5%9C%A8%E7%BB%8F%E5%85%B8%E7%9A%84vi-reid%E4%B8%AD%E7%9C%9F%E7%9A%84%E6%9C%89%E5%BF%85%E8%A6%81%E7%94%A8%E5%88%B0modality-specific-feature%E5%90%97%E5%9C%A8%E7%BB%8F%E5%85%B8%E8%AE%BE%E5%AE%9A%E4%B8%8B%E4%B8%8D%E6%98%AFquery%E5%92%8Cgallery%E4%B8%80%E5%AE%9A%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E5%90%97%E9%82%A3modality-specific-feature%E5%8F%AA%E6%9C%89%E4%B8%80%E6%96%B9%E6%9C%89%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%E7%94%A8%E4%B8%8D%E4%B8%8A%E5%95%8A%E4%BA%8B%E5%AE%9E%E4%B8%8A%E4%BA%BA%E6%9D%A5%E8%BF%9B%E8%A1%8C%E7%BA%A2%E5%A4%96%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E7%9A%84%E5%8C%B9%E9%85%8D%E7%9A%84%E6%97%B6%E5%80%99%E4%B9%9F%E6%98%AF%E9%80%9A%E8%BF%87%E7%A5%9E%E6%80%81%E5%92%8C%E8%84%B8%E6%9D%A5%E5%88%A4%E6%96%AD%E7%9A%84%E5%90%A7%E6%9C%8D%E8%A3%85%E9%82%A3%E4%BA%9B%E5%9F%BA%E6%9C%AC%E7%94%A8%E4%B8%8D%E4%B8%8A%E5%95%8A%E6%8D%A2%E8%A3%85%E8%AE%BE%E5%AE%9A%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B9%9F%E6%98%AF%E8%BF%99%E4%B8%AA%E6%80%9D%E8%B7%AF"><span class="nav-number">1.4.0.1.</span> <span class="nav-text">（话说回来，在经典的vi-reid中，真的有必要用到modality-specific
feature吗？在经典设定下，不是query和gallery一定不同模态吗？那modality-specific
feature，只有一方有，匹配的时候用不上啊？事实上，人来进行红外和可见光的匹配的时候，也是通过神态和脸来判断的吧，服装那些基本用不上啊（换装设定是不是也是这个思路？）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E7%94%A8%E5%88%B0%E4%B8%80%E4%BA%9B%E4%BA%92%E4%BF%A1%E6%81%AF%E7%9A%84%E5%85%AC%E5%BC%8F%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%8E%A8%E5%AF%BC%E5%92%8C%E9%98%90%E8%BF%B0%E5%9B%A0%E4%B8%BA%E8%A6%81%E7%94%A8%E5%AD%A6%E5%88%B0%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E7%9A%84embedding%E6%9D%A5%E5%88%A4%E6%96%ADid%E6%89%80%E4%BB%A5%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96embedding%E5%92%8Cid%E4%B9%8B%E9%97%B4%E7%9A%84%E6%A0%87%E7%AD%BE%E8%80%8C%E8%80%83%E8%99%91%E5%88%B0%E7%8E%B0%E5%9C%A8%E7%9A%84mixed-modality-scenario%E9%9C%80%E8%A6%81%E6%8A%8Aembedding%E6%8B%86%E5%88%86%E6%88%90%E4%B8%A4%E5%9D%97%E5%B0%B1%E6%98%AF%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84modality-shared%E5%92%8C-specific-feature%E6%88%96%E8%80%85%E8%AF%B4%E5%8E%9F%E6%96%87%E7%9A%84modality-erased-related-feature%E4%B8%8B%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%80%8E%E4%B9%88%E6%8A%8A%E8%BF%99%E4%B8%A4%E9%83%A8%E5%88%86%E5%AD%A6%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%BA%E6%96%87%E6%8A%8A%E8%BF%99%E9%83%A8%E5%88%86%E8%BD%AC%E5%8C%96%E6%88%90%E4%B8%80%E4%B8%AA%E5%BE%85%E4%BC%98%E5%8C%96%E7%9A%84%E5%BC%8F%E5%AD%90max_z_mez_mrmiz_mez_mry-s.t.miz_mez_mr0miz_mem0and-miz_mrym0-%E4%B8%8A%E9%9D%A2%E7%9A%84z_me%E4%BB%A3%E8%A1%A8%E6%A8%A1%E6%80%81m%E4%B8%8B%E7%9A%84modality-erased-feature%E4%B9%9F%E5%B0%B1%E6%98%AF%E4%B8%8E%E6%A8%A1%E6%80%81%E6%97%A0%E5%85%B3%E7%9A%84%E7%89%B9%E5%BE%81z_mr%E4%BB%A3%E8%A1%A8%E6%A8%A1%E6%80%81m%E4%B8%8B%E7%9A%84modality-related-feature%E4%B9%9F%E5%B0%B1%E6%98%AF%E4%B8%8E%E6%A8%A1%E6%80%81%E7%9B%B8%E5%85%B3%E7%9A%84%E7%89%B9%E5%BE%81%E6%A8%A1%E6%80%81m%E5%8F%AF%E8%83%BD%E6%98%AFvisible%E4%B9%9F%E5%8F%AF%E8%83%BD%E6%98%AFinfrared%E4%B8%89%E4%B8%AA%E7%BA%A6%E6%9D%9F%E4%B9%9F%E6%98%AF%E4%BF%9D%E8%AF%81%E5%AD%A6%E5%88%B0%E7%9A%84%E7%89%B9%E5%BE%81%E6%9C%89%E6%95%88%E7%9A%84%E5%85%B3%E9%94%AE%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%BA%A6%E6%9D%9F%E6%98%AF%E6%83%B3%E8%AE%A9modality-erased-feature%E4%B8%8Emodality-related-feature%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B%E8%BF%99%E4%B8%AA%E6%98%AF%E5%90%88%E7%90%86%E7%9A%84%E6%AF%95%E7%AB%9F%E7%90%86%E8%AE%BA%E4%B8%8A%E8%BF%99%E4%B8%A4%E9%83%A8%E5%88%86%E7%9A%84%E4%BA%A4%E9%9B%86%E6%98%AF%E7%A9%BA%E9%9B%86%E5%B9%B6%E9%9B%86%E5%B0%B1%E6%98%AF%E6%89%80%E6%9C%89%E7%9A%84%E7%89%B9%E5%BE%81%E4%BF%A1%E6%81%AF%E4%BA%86%E7%AC%AC%E4%BA%8C%E4%B8%AA%E7%BA%A6%E6%9D%9F%E6%98%AF%E5%9B%A0%E4%B8%BAz_me%E4%BB%A3%E8%A1%A8modality-erased-feature%E6%89%80%E4%BB%A5%E7%94%A8%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E5%BA%94%E8%AF%A5%E6%8E%A8%E6%96%AD%E4%B8%8D%E5%87%BA%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E6%89%80%E4%BB%A5%E5%AE%83%E5%BA%94%E8%AF%A5%E4%B8%8E%E6%A8%A1%E6%80%81%E7%8B%AC%E7%AB%8B%E4%B9%9F%E5%90%88%E7%90%86%E7%AC%AC%E4%B8%89%E4%B8%AA%E7%BA%A6%E6%9D%9F%E6%B2%A1%E6%90%9E%E6%98%8E%E7%99%BD%E5%AE%83%E6%83%B3%E5%B9%B2%E4%BB%80%E4%B9%88%E6%8C%89%E5%8E%9F%E8%AE%BA%E6%96%87%E5%AE%83%E6%98%AF%E5%B8%8C%E6%9C%9Bz_mr%E4%B8%8D%E5%8C%85%E5%90%ABid%E7%9B%B8%E5%85%B3%E7%9A%84%E4%BF%A1%E6%81%AF%E4%BD%86%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%99%E6%A0%B7%E5%A6%82%E6%9E%9C%E7%9C%9F%E7%9A%84%E6%BB%A1%E8%B6%B3%E4%BA%86%E8%BF%99%E4%B8%AA%E7%BA%A6%E6%9D%9Fz_mr%E5%B0%B1%E6%98%AF%E5%8F%AA%E8%83%BD%E7%94%A8%E6%9D%A5%E6%8E%A8%E6%96%AD%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E4%B8%8D%E8%83%BD%E7%94%A8%E6%9D%A5%E6%8E%A8%E6%96%ADid%E4%BF%A1%E6%81%AF%E9%82%A3%E5%AE%83%E5%92%8Cz_me%E7%BB%84%E5%90%88%E8%B5%B7%E6%9D%A5%E7%94%A8%E5%8F%88%E6%98%AF%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E6%98%AF%E8%AF%B4%E5%AE%83%E6%98%AF%E5%BD%BB%E5%BA%95%E7%9A%84%E6%8A%8A%E6%A8%A1%E6%80%81%E5%92%8Cid%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%BB%99%E5%88%86%E5%BC%80%E4%BA%86%E5%88%86%E5%88%B0%E4%B8%A4%E4%B8%AA%E7%89%B9%E5%BE%81%E9%87%8C%E6%89%80%E4%BB%A5%E6%8E%A8%E6%96%AD%E7%9A%84%E6%97%B6%E5%80%99%E6%98%AF%E6%80%8E%E4%B9%88%E6%8E%A8%E6%96%AD%E7%9A%84%E6%98%AF%E8%AF%B4%E5%A6%82%E6%9E%9C%E6%98%AF%E5%90%8C%E4%B8%80%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E6%9D%A5%E5%8C%B9%E9%85%8D%E5%88%99%E7%94%A8%E7%9A%84%E7%89%B9%E5%BE%81%E6%98%AF%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84modality-erased-feature%E5%92%8Cmodality-related-feature%E6%8B%BC%E6%8E%A5%E8%B5%B7%E6%9D%A5%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E5%BE%81%E5%A6%82%E6%9E%9C%E6%98%AF%E8%B7%A8%E6%A8%A1%E6%80%81%E7%9A%84%E5%8C%B9%E9%85%8D%E5%88%99%E5%8F%AA%E7%94%A8modality-erased-feature%E4%BD%86%E8%BF%99%E6%A0%B7%E4%B8%8D%E6%98%AF%E5%BE%88%E9%BA%BB%E7%83%A6%E5%90%97%E5%BE%97%E5%86%8D%E7%9C%8B%E7%9C%8B%E4%BB%A3%E7%A0%81%E4%BA%86%E5%8F%AF%E8%83%BD-%E6%80%BB%E8%80%8C%E8%A8%80%E4%B9%8B%E5%BE%97%E5%88%B0%E4%BA%86%E4%B8%8A%E9%9D%A2%E7%9A%84%E5%BE%85%E4%BC%98%E5%8C%96%E5%BC%8F%E5%AD%90%E4%B8%8B%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%8A%8A%E5%AE%83%E8%BD%AC%E5%8C%96%E6%88%90%E4%B8%80%E4%B8%AA%E8%BF%91%E4%BC%BC%E7%9A%84%E5%BC%8F%E5%AD%90%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%8E%BB%E5%88%BB%E7%94%BB%E4%BB%8E%E8%80%8C%E5%8F%98%E6%88%90%E5%8F%AF%E4%BB%A5%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98%E5%BE%97%E5%88%B0%E7%BB%93%E6%9E%9C%E5%A6%82%E4%B8%8B500-%E4%B8%94%E9%92%88%E5%AF%B9%E5%9B%9B%E4%B8%AA%E9%83%A8%E5%88%86%E9%83%BD%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%90%84%E8%87%AA%E7%9A%84%E4%BC%98%E5%8C%96"><span class="nav-number">1.5.</span> <span class="nav-text">分析：这篇文章用到一些互信息的公式来进行推导和阐述。因为要用学到的模型生成的embedding来判断id，所以要最大化embedding和id之间的标签。而考虑到现在的mixed-modality
scenario，需要把embedding拆分成两块，就是上面提到的modality-shared和-specific
feature（或者说原文的modality-erased&#x2F;-related
feature）。下面就是怎么把这两部分学到的问题。论文把这部分转化成一个待优化的式子：\[\max_{Z_{m}^{e},Z_{m}^{r}}\{MI(Z_{m}^{e},Z_{m}^{r};Y)\}\
\ s.t.MI(Z_{m}^{e},Z_{m}^{r})&#x3D;0,MI(Z_{m}^{e};M)&#x3D;0,and\
MI(Z_{m}^{r};Y|M)&#x3D;0\]上面的\(Z_{m}^{e}\)代表模态m下的modality-erased
feature，也就是与模态无关的特征；\(Z_{m}^{r}\)代表模态m下的modality-related
feature，也就是与模态相关的特征。模态m可能是visible，也可能是infrared。三个约束也是保证学到的特征有效的关键。第一个约束，是想让modality-erased
feature与modality-related
feature相互独立。这个是合理的。毕竟理论上这两部分的交集是空集，并集就是所有的特征信息了。第二个约束，是因为\(Z_{m}^{e}\)代表modality-erased
feature，所以用这个特征应该推断不出模态信息，所以它应该与模态独立，也合理。第三个约束，没搞明白它想干什么。按原论文，它是希望\(Z_{m}^{r}\)不包含ID相关的信息。但为什么要这样？如果真的满足了这个约束，\(Z_{m}^{r}\)就是只能用来推断模态信息，不能用来推断ID信息。那它和\(Z_{m}^{e}\)组合起来用，又是为什么？还是说，它是彻底的把模态和ID（语义）信息给分开了，分到两个特征里？所以推断的时候是怎么推断的？是说，如果是同一模态的图像来匹配，则用的特征是上面提到的modality-erased
feature和modality-related
feature拼接起来的一个特征；如果是跨模态的匹配，则只用modality-erased
feature（但这样不是很麻烦吗？得再看看代码了可能）总而言之，得到了上面的待优化式子，下面就是把它转化成一个近似的式子，用损失去刻画，从而变成可以用深度学习解决的问题。得到结果如下：且针对四个部分，都进行了各自的优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7%E8%99%BD%E7%84%B6%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E4%B9%8D%E4%B8%80%E7%9C%8B%E5%BE%88%E7%83%82%E7%89%B9%E5%88%AB%E6%98%AF%E6%91%98%E8%A6%81%E9%82%A3%E9%87%8C%E8%BF%98%E6%8A%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%90%8D%E5%AD%97%E6%89%93%E9%94%99%E4%BA%86%E6%9B%B4%E6%98%AF%E9%99%8D%E4%BD%8E%E5%8D%B0%E8%B1%A1%E5%88%86%E5%8F%AF%E6%98%AF%E7%9C%8B%E5%AE%8C%E4%BB%A5%E5%90%8E%E4%B8%80%E6%9D%A5%E5%AE%83%E6%8F%90%E5%87%BA%E7%9A%84%E6%96%B0%E5%9C%BA%E6%99%AF%E4%B9%9F%E4%B8%8D%E7%AE%97%E7%A9%BA%E4%B8%AD%E6%A5%BC%E9%98%81%E7%A1%AE%E5%AE%9E%E6%98%AF%E6%9B%B4%E6%9C%89%E5%AE%9E%E9%99%85%E6%84%8F%E4%B9%89%E7%9A%84%E4%BA%8C%E6%9D%A5%E5%AE%83%E6%8F%90%E5%87%BA%E6%96%B9%E6%B3%95%E4%BB%A5%E5%90%8E%E5%AF%B9%E6%AF%8F%E4%B8%80%E9%83%A8%E5%88%86%E7%9A%84%E5%88%BB%E7%94%BB%E5%92%8C%E9%98%90%E8%BF%B0%E9%83%BD%E8%BF%98%E7%AE%97%E5%90%88%E7%90%86%E6%9C%89%E4%B8%80%E7%82%B9%E5%A8%93%E5%A8%93%E9%81%93%E6%9D%A5%E7%9A%84%E6%84%9F%E8%A7%89%E9%83%BD%E6%98%AF%E5%B0%B1%E4%BA%8B%E8%AE%BA%E4%BA%8B%E7%9A%84%E5%88%86%E6%9E%90"><span class="nav-number">1.6.</span> <span class="nav-text">评价：虽然这篇论文乍一看很烂，特别是摘要那里还把数据集的名字打错了，更是降低印象分，可是看完以后，一来，它提出的新场景也不算空中楼阁，确实是更有实际意义的；二来，它提出方法以后，对每一部分的刻画和阐述都还算合理，有一点娓娓道来的感觉，都是就事论事的分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#trick%E6%80%BB%E7%BB%93"><span class="nav-number">1.7.</span> <span class="nav-text">trick总结：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%83%B3%E8%AE%A9%E4%B8%A4%E4%B8%AA%E5%90%91%E9%87%8F%E5%B0%BD%E5%8F%AF%E8%83%BD%E7%8B%AC%E7%AB%8B%E5%8F%AF%E4%BB%A5%E7%94%A8%E4%BA%92%E4%BF%A1%E6%81%AF%E4%B8%BA0%E6%9D%A5%E8%BF%9B%E8%A1%8C%E7%90%86%E8%AE%BA%E4%B8%8A%E7%9A%84%E5%88%BB%E7%94%BB%E8%99%BD%E8%AF%B4%E5%85%B6%E5%AE%9E%E5%8F%AF%E8%83%BD%E6%98%AF%E6%9C%89%E5%A4%B1%E5%81%8F%E9%A2%87%E7%9A%84%E7%84%B6%E5%90%8E%E7%94%A8%E6%AD%A3%E4%BA%A4%E6%8D%9F%E5%A4%B1%E6%9D%A5%E8%BF%91%E4%BC%BC%E4%BB%A3%E6%9B%BF"><span class="nav-number">1.7.1.</span> <span class="nav-text">1.想让两个向量尽可能独立，可以用互信息为0来进行理论上的刻画（虽说其实可能是有失偏颇的），然后用正交损失来近似代替</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E6%9E%9C%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96%E6%9F%90%E4%B8%AA%E4%BA%92%E4%BF%A1%E6%81%AF%E9%82%A3%E4%B9%88%E5%8F%AF%E4%BB%A5%E4%BB%8E%E8%AF%AD%E4%B9%89%E4%B8%8A%E5%8E%BB%E7%90%86%E8%A7%A3%E7%BB%99%E5%87%BA%E7%9B%B8%E5%BA%94%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%AF%94%E5%A6%82%E8%AE%BA%E6%96%87%E9%87%8C%E6%8F%90%E5%88%B0%E7%9A%84%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96miz_mey%E5%8D%B3%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96modality-erased-feature%E4%B8%8E%E6%A0%87%E7%AD%BE%E5%8F%98%E9%87%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BA%92%E4%BF%A1%E6%81%AF%E5%85%B6%E5%AE%9E%E7%9B%AE%E7%9A%84%E5%B0%B1%E6%98%AF%E8%AE%A9modality-erased-feature%E8%83%BD%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8F%AD%E7%A4%BA%E5%87%BAid%E4%BF%A1%E6%81%AF%E6%89%80%E4%BB%A5%E7%94%A8%E4%B8%80%E4%B8%AA%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%B0%B1%E6%98%AF%E7%90%86%E6%89%80%E5%BA%94%E5%BD%93%E7%9A%84%E4%BA%86%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E6%98%AF%E5%AF%B9%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E7%BB%99%E5%87%BA%E4%B8%80%E4%B8%AA%E7%90%86%E8%AE%BA%E4%B8%8A%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="nav-number">1.7.2.</span> <span class="nav-text">2.如果要最大化某个互信息，那么可以从语义上去理解，给出相应的损失。比如论文里提到的，要最大化\(MI(Z_{m}^{e};Y)\)，即要最大化modality-erased
feature与标签变量之间的互信息，其实目的就是让modality-erased
feature能尽可能揭示出ID信息，所以用一个交叉熵的分类损失就是理所应当的了（可以认为是对常见的交叉熵分类损失给出一个理论上的解释）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A6%81%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%B7%B7%E6%B7%86%E6%9F%90%E5%87%A0%E4%B8%AA%E9%83%A8%E5%88%86%E8%BF%99%E5%8F%AF%E8%83%BD%E6%98%AF%E5%87%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%E7%9A%84%E8%80%83%E8%99%91%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%E4%BD%86%E5%8A%A0%E4%B8%8A%E6%A2%AF%E5%BA%A6%E5%8F%8D%E8%BD%AC%E5%B1%82"><span class="nav-number">1.7.3.</span> <span class="nav-text">3.要让模型混淆某几个部分（这可能是出于模型设计的考虑），可以考虑进行分类，但加上梯度反转层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A6%81%E7%BA%A6%E6%9D%9F%E6%9F%90%E4%B8%A4%E4%B8%AA%E9%87%8Fxy%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E7%BB%99%E5%87%BA%E8%BF%99%E4%B8%AA%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%8D%9F%E5%A4%B1lmaxx-yalpha0maxy-xalpha0alpha%E6%98%AF%E6%AD%A3%E5%B8%B8%E6%95%B0%E4%B9%9F%E6%98%AF%E8%B6%85%E5%8F%82%E6%95%B0%E9%82%A3%E4%B9%88x%E5%92%8Cy%E5%BF%85%E9%A1%BB%E7%9B%B8%E5%B7%AE%E7%BB%9D%E5%AF%B9%E5%80%BC%E4%B8%8D%E5%A4%A7%E4%BA%8Ealpha%E8%BF%99%E9%83%A8%E5%88%86%E6%8D%9F%E5%A4%B1%E6%89%8D%E4%B8%BA0%E5%90%A6%E5%88%99%E5%B0%B1%E4%BA%A7%E7%94%9F%E6%8D%9F%E5%A4%B1%E4%BA%86"><span class="nav-number">1.7.4.</span> <span class="nav-text">4.要约束某两个量x，y尽可能接近，可以考虑给出这个形式的损失：\(L&#x3D;max(x-y+\alpha,0)+max(y-x+\alpha,0)\)，\(\alpha\)是正常数，也是超参数。那么，x和y必须相差绝对值不大于\(\alpha\)，这部分损失才为0，否则就产生损失了</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%99%E4%B8%AAtrick%E7%9B%AE%E5%89%8D%E7%9C%8B%E4%B8%8B%E6%9D%A5%E5%A5%BD%E5%83%8F%E8%BF%98%E6%98%AF%E6%AF%94%E8%BE%83%E5%B1%80%E9%99%90%E7%9A%84%E5%B0%B1%E6%98%AF%E9%92%88%E5%AF%B9%E8%B7%A8%E6%A8%A1%E6%80%81%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E7%9A%84%E6%97%B6%E5%80%99%E6%89%8D%E4%BC%9A%E7%94%A8%E5%88%B0%E5%B0%B1%E6%98%AF%E5%A6%82%E6%9E%9C%E8%A6%81%E5%AD%A6%E4%B9%A0modality-specific-feature%E4%B8%94%E6%AF%8F%E4%B8%AAid%E9%83%BD%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%99%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E6%8A%8A%E6%AF%8F%E4%B8%80%E4%B8%AAid%E7%BB%99doubled%E6%AF%94%E5%A6%82%E5%A6%82%E6%9E%9C%E5%AE%83%E6%98%AFvisible%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%99id-2%E5%A6%82%E6%9E%9C%E5%AE%83%E6%98%AFinfrared%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%99idid-2-1%E5%9F%BA%E4%BA%8E%E8%BF%99%E4%B8%80%E5%A5%97%E6%96%B0%E7%9A%84%E6%A0%87%E7%AD%BE%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%E5%B9%B6%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E6%9D%A5%E4%BC%98%E5%8C%96%E8%BF%99%E9%87%8C%E4%B9%8B%E6%89%80%E4%BB%A5%E8%83%BD%E8%B5%B7%E5%88%B0%E6%95%88%E6%9E%9C%E6%98%AF%E5%9B%A0%E4%B8%BA%E6%88%91%E4%BB%AC%E6%8A%8A%E5%90%8C%E4%B8%80id%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%99%E8%A7%86%E4%B8%BA%E4%B8%8D%E5%90%8C%E6%A0%87%E7%AD%BE%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%BF%9B%E8%A1%8C%E6%9B%B4%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%A6%82%E6%9E%9C%E8%A6%81%E6%AD%A3%E7%A1%AE%E5%88%86%E7%B1%BB%E5%B0%B1%E5%BF%85%E9%A1%BB%E5%AD%A6%E4%BC%9A%E6%8D%95%E8%8E%B7%E5%90%8C%E4%B8%80id%E4%B8%8Bvisible%E5%92%8Cinfrared%E5%9B%BE%E5%83%8F%E7%9A%84%E5%90%84%E8%87%AA%E7%9A%84%E6%9C%89%E5%88%A4%E5%88%AB%E5%8A%9B%E7%9A%84%E7%89%B9%E5%BE%81%E8%BF%99%E6%A0%B7%E6%A8%A1%E5%9E%8B%E5%B0%B1%E8%83%BD%E5%AD%A6%E4%BC%9A%E6%8F%90%E5%8F%96%E5%87%BAmodality-specific-feature%E5%BD%93%E7%84%B6%E8%BF%99%E4%B8%AAtrick%E4%B9%9F%E5%B9%B6%E4%B8%8D%E5%94%AF%E4%B8%80%E8%82%AF%E5%AE%9A%E6%98%AF%E8%BF%98%E6%9C%89%E5%88%AB%E7%9A%84%E6%96%B9%E6%B3%95%E7%9A%84%E5%B0%B1%E6%AF%94%E5%A6%82%E5%86%8D%E7%94%A8%E4%B8%AA%E6%8D%9F%E5%A4%B1%E7%BA%A6%E6%9D%9F%E5%90%8C%E4%B8%80id%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E8%A6%81%E5%B0%BD%E5%8F%AF%E8%83%BD%E8%BF%9C%E7%A6%BB%E6%9C%89%E7%82%B9%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%84%8F%E6%80%9D%E9%80%9A%E8%BF%87%E8%AE%A9%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%88%AB%E7%9A%84%E6%95%B0%E6%8D%AE%E5%B0%BD%E5%8F%AF%E8%83%BD%E8%BF%9C%E7%A6%BB%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%BD%9C%E7%A7%BB%E9%BB%98%E5%8C%96%E5%9C%B0%E5%AD%A6%E4%BC%9A%E4%BA%86%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E5%BA%94%E8%AF%A5%E4%B9%9F%E8%83%BD%E5%AD%A6%E4%BC%9A%E6%8F%90%E5%8F%96modality-specific-feature"><span class="nav-number">1.7.5.</span> <span class="nav-text">5.这个trick目前看下来好像还是比较局限的，就是针对跨模态行人重识别的时候才会用到。。。就是，如果要学习modality-specific
feature，且每个ID都有两种模态的数据，则可以考虑把每一个ID给“doubled”，比如，如果它是visible的图像，则ID
* 2；如果它是infrared的图像，则ID&#x3D;ID * 2 +
1。基于这一套新的标签，进行分类，并用交叉熵分类损失来优化。这里之所以能起到效果，是因为，我们把同一ID的不同模态的数据给视为不同标签，相当于进行更细粒度的分类。模型如果要正确分类，就必须学会捕获同一ID下visible和infrared图像的各自的、有判别力的特征。这样模型就能学会提取出modality-specific
feature。当然，这个trick也并不唯一，肯定是还有别的方法的。就比如，再用个损失，约束同一ID的不同模态的数据要尽可能远离（有点对比学习的意思：通过让不同类别的数据尽可能远离，让模型潜移默化地学会了语义信息），应该也能学会提取modality-specific
feature</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spectral-enhancement-and-pseudo-anchor-guidance-for-infrared-visible-person-re-identification"><span class="nav-number">2.</span> <span class="nav-text">Spectral
Enhancement and Pseudo-Anchor Guidance for Infrared-Visible Person
Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv-2025.1.2"><span class="nav-number">2.1.</span> <span class="nav-text">arxiv, 2025.1.2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E4%B8%BB%E8%A6%81%E6%98%AF%E4%BB%8E%E8%B0%B1%E5%9F%9F%E7%9A%84%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%BC%93%E8%A7%A3%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%B7%A8%E5%A4%A7%E5%B7%AE%E5%BC%82%E4%BD%86%E5%AE%83%E5%8F%AA%E6%98%AF%E7%9B%B4%E6%8E%A5%E6%8C%87%E5%87%BA%E8%BF%99%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9C%89%E5%B7%A8%E5%A4%A7%E7%9A%84spectral-gap%E5%8D%B4%E5%B9%B6%E6%B2%A1%E6%9C%89%E5%88%86%E6%9E%90%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E8%BF%99%E7%A7%8Dgap%E6%88%96%E8%80%85%E8%AF%B4%E6%88%91%E4%BB%AC%E6%80%8E%E4%B9%88%E7%A1%AE%E8%AE%A4%E8%BF%99%E7%A7%8Dgap%E7%A1%AE%E5%AE%9E%E5%AD%98%E5%9C%A8%E8%80%8C%E4%B8%8D%E5%8F%AA%E6%98%AF%E8%AF%B4%E8%AF%B4%E8%80%8C%E5%B7%B2%E6%9C%89%E7%A7%8D%E6%8B%BF%E5%88%B0%E8%B0%B1%E5%9F%9F%E6%96%B9%E6%B3%95%E5%B0%B1%E6%9D%A5%E8%AF%95%E4%B8%80%E4%B8%8B%E7%9A%84%E6%84%9F%E8%A7%89"><span class="nav-number">2.2.</span> <span class="nav-text">这篇文章主要是从谱域的角度来缓解两种模态的巨大差异（但它只是直接指出这两种模态的数据有巨大的spectral
gap，却并没有分析为什么有这种gap，或者说我们怎么确认这种gap确实存在，而不只是说说而已。有种拿到谱域方法就来试一下的感觉）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA"><span class="nav-number">2.3.</span> <span class="nav-text">动机：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">2.4.</span> <span class="nav-text">方法：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90%E7%AC%AC%E4%B8%80%E6%AD%A5%E6%98%AF%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AAsemantically-enhanced-grey-images%E5%A7%91%E4%B8%94%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E6%98%AF%E4%B8%80%E7%A7%8D%E6%8B%89%E8%BF%91%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E6%B3%95%E5%8F%AA%E4%B8%8D%E8%BF%87%E6%98%AF%E4%BB%8E%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E5%87%BA%E5%8F%91%E5%BE%80%E7%BA%A2%E5%A4%96%E4%B8%8A%E9%9D%A0%E6%8B%A2%E4%B9%9F%E5%90%88%E7%90%86%E6%AF%95%E7%AB%9F%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E4%BF%A1%E6%81%AF%E6%9B%B4%E5%A4%9A%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E4%BF%A1%E6%81%AF%E6%9B%B4%E5%B0%91%E4%BB%8E%E4%BF%A1%E6%81%AF%E6%9B%B4%E5%A4%9A%E7%9A%84%E5%BE%80%E4%BF%A1%E6%81%AF%E6%9B%B4%E5%B0%91%E7%9A%84%E5%8F%98%E6%8D%A2%E6%98%AF%E6%AF%94%E8%BE%83%E5%AE%B9%E6%98%93%E7%9A%84%E5%AE%83%E4%B8%BB%E8%A6%81%E5%81%9A%E7%9A%84%E5%85%B6%E5%AE%9E%E6%98%AF%E5%9C%A8%E5%BE%88%E5%B8%B8%E8%A7%81%E7%9A%84%E7%81%B0%E5%BA%A6%E6%98%A0%E5%B0%84%E4%B9%8B%E4%BD%99%E5%8A%A0%E4%B8%8A%E4%BA%86%E9%AB%98%E9%A2%91%E4%BF%A1%E6%81%AF%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%E5%AE%83%E4%BC%9A%E5%85%88%E5%AF%B9%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E6%8F%90%E5%8F%96%E5%AE%83%E7%9A%84%E9%A2%91%E5%9F%9F%E4%BF%A1%E6%81%AF%E7%84%B6%E5%90%8E%E7%94%A8%E5%85%B6%E4%B8%AD%E7%9A%84%E9%A2%91%E7%8E%87%E6%88%90%E5%88%86%E8%BF%9B%E8%A1%8C%E5%82%85%E9%87%8C%E5%8F%B6%E9%80%86%E5%8F%98%E6%8D%A2%E8%BF%99%E9%87%8C%E6%98%AF%E4%B8%BA%E4%BA%86%E6%8F%90%E5%8F%96%E8%BD%AE%E5%BB%93%E4%BF%A1%E6%81%AF%E7%84%B6%E5%90%8E%E5%B0%86%E9%80%86%E5%8F%98%E6%8D%A2%E7%9A%84%E7%BB%93%E6%9E%9C%E4%B8%8E%E7%81%B0%E5%BA%A6%E6%98%A0%E5%B0%84%E7%9A%84%E7%BB%93%E6%9E%9C%E7%9B%B8%E5%8A%A0%E5%BE%97%E5%88%B0semantically-enhanced-grey-images%E4%B8%8B%E7%AE%80%E7%A7%B0seg%E5%9B%BE%E5%83%8Fbtw%E8%BF%99%E4%B8%AA%E6%96%B9%E6%B3%95%E5%80%92%E6%98%AF%E5%8F%AF%E4%BB%A5%E5%80%9F%E9%89%B4%E4%B8%80%E4%B8%8B%E6%84%9F%E8%A7%89%E8%BF%98%E6%98%AF%E8%A1%8C%E5%BE%97%E9%80%9A%E7%9A%84%E8%B5%B7%E7%A0%81%E5%8F%AF%E4%BB%A5%E8%AF%95%E8%AF%95%E7%94%A8%E5%88%B0%E6%88%91%E4%BB%AC%E7%9A%84%E6%96%B9%E6%B3%95%E9%87%8C%E5%B0%B1%E6%98%AF%E5%8A%A0%E4%B8%8A%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E5%90%8E%E5%8F%AF%E8%83%BD%E8%AE%A1%E7%AE%97%E9%87%8F%E5%A4%A7%E4%BA%86-%E7%AC%AC%E4%BA%8C%E6%AD%A5%E6%98%AF%E5%AF%B9%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E5%92%8Cseg%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B9%9F%E6%98%AF%E4%B8%BA%E4%BA%86%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%BC%A9%E5%B0%8F%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%E5%9B%A0%E4%B8%BA%E6%9C%80%E5%90%8E%E8%BF%98%E6%98%AF%E8%A6%81%E7%94%A8%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E7%9A%84%E8%BF%99%E9%87%8C%E5%85%B6%E5%AE%9E%E5%B9%B3%E5%B9%B3%E6%97%A0%E5%A5%87-%E5%B0%B1%E6%98%AF%E7%94%A8resnet-50%E7%9A%84%E5%89%8D%E4%B8%89%E5%9D%97%E5%85%B1%E4%BA%AB%E6%9D%83%E9%87%8D%E6%9D%A5%E6%8F%90%E5%8F%96%E5%85%B1%E5%90%8C%E7%9A%84%E7%89%B9%E5%BE%81%E4%B9%8B%E5%90%8E%E5%8F%88%E7%94%A8resnet-50%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E5%9D%97%E4%B8%8D%E5%90%8C%E6%9D%83%E9%87%8D%E6%9D%A5%E5%88%86%E5%88%AB%E6%8F%90%E5%8F%96modality-specific-feature-%E7%AC%AC%E4%B8%89%E6%AD%A5%E6%98%AFpaba-loss%E5%8D%B3pseudo-anchor-guided-bidirectional-aggregation-loss%E5%AE%83%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E5%AF%B9%E4%BA%8E%E5%89%8D%E9%9D%A2%E7%9A%84%E5%85%B1%E5%90%8C%E7%89%B9%E5%BE%81%E5%B8%8C%E6%9C%9B%E5%AE%83%E4%BB%AC%E8%83%BD%E6%9B%B4%E5%8A%A0%E5%85%BC%E5%AE%B9%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%B0%BD%E5%8F%AF%E8%83%BD%E7%9A%84%E5%8F%98%E6%8D%A2%E5%88%B0%E4%B8%80%E4%B8%AA%E7%A9%BA%E9%97%B4%E9%87%8C%E4%BD%86%E4%B9%9F%E4%B8%8D%E8%83%BD%E5%B0%B1%E5%A4%B1%E5%8E%BB%E5%88%A4%E6%96%AD%E5%8A%9B%E4%BA%86%E8%80%8C%E6%89%80%E8%B0%93anchor-guided%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E8%AF%B4%E7%94%A8%E5%90%91anchor%E9%9D%A0%E6%8B%A2%E6%9D%A5%E5%AE%9E%E7%8E%B0%E4%B8%A4%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%9A%84%E9%9D%A0%E6%8B%A2%E4%B8%BE%E4%B8%AA%E5%85%B7%E4%BD%93%E4%BE%8B%E5%AD%90%E6%88%91%E6%9C%89%E5%90%8C%E4%B8%80id%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E8%8B%A5%E5%B9%B2%E6%95%B0%E6%8D%AE%E6%88%91%E5%B8%8C%E6%9C%9B%E6%8A%8A%E5%AE%83%E4%BB%AC%E5%8F%98%E6%8D%A2%E5%88%B0%E4%B8%80%E4%B8%AA%E7%BB%9F%E4%B8%80%E7%9A%84%E7%A9%BA%E9%97%B4%E9%87%8C%E5%88%99%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E6%B1%82%E4%B8%80%E4%B8%8B%E5%8F%AF%E8%A7%81%E5%85%89%E7%9A%84%E4%B8%80%E4%B8%AAanchor%E6%AF%94%E5%A6%82%E6%89%80%E6%9C%89%E5%8F%AF%E8%A7%81%E5%85%89%E6%95%B0%E6%8D%AE%E7%9A%84%E5%9D%87%E5%80%BC%E7%84%B6%E5%90%8E%E6%8B%89%E8%BF%91%E6%89%80%E6%9C%89%E7%BA%A2%E5%A4%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%BF%99%E4%B8%AAanchor%E7%9A%84%E8%B7%9D%E7%A6%BB%E4%BB%8E%E8%80%8C%E5%AE%9E%E7%8E%B0%E7%BA%A2%E5%A4%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E5%8F%AF%E8%A7%81%E5%85%89%E6%95%B0%E6%8D%AE%E7%9A%84%E9%9D%A0%E6%8B%A2%E8%AF%B4%E5%9B%9E%E8%BF%99%E9%87%8C%E5%AE%83%E6%98%AF%E5%AF%B9%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84%E5%85%B1%E5%90%8C%E7%89%B9%E5%BE%81f_sharedseg%E5%92%8Cf_sharedir%E5%85%88%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E5%88%86%E5%9D%97%E5%88%86%E6%88%90n%E5%9D%97%E4%B9%8B%E5%90%8E%E5%AF%B9%E6%AF%8F%E4%B8%AA%E5%9D%97%E9%83%BD%E6%96%BD%E5%8A%A0paba-loss%E6%8B%89%E8%BF%91%E5%90%8C%E4%B8%80id%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%90%8C%E6%97%B6%E6%8B%89%E8%BF%9C%E4%B8%8D%E5%90%8Cid%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9A%84%E8%B7%9D%E7%A6%BB%E7%9B%B8%E6%AF%94%E4%BA%8E%E5%B8%B8%E8%A7%81%E7%9A%84triplet-loss%E8%BF%99%E9%87%8C%E5%A2%9E%E5%8A%A0%E4%BA%86%E8%B7%A8%E6%A8%A1%E6%80%81%E7%9A%84%E8%80%83%E8%99%91%E8%80%8C%E7%9B%B8%E6%AF%94%E4%BA%8Ecross-center-loss%E8%BF%99%E9%87%8C%E7%9A%84%E6%94%B9%E8%BF%9B%E5%9C%A8%E4%BA%8E%E5%88%86%E4%BA%86%E5%9D%97%E4%B8%94%E6%98%AF%E5%AF%B9%E6%AF%8F%E4%B8%AA%E5%9D%97%E9%83%BD%E6%96%BD%E5%8A%A0paba-loss%E6%95%85%E6%9B%B4%E5%8A%A0%E7%BB%86%E7%B2%92%E5%BA%A6%E8%BF%99%E9%87%8C%E5%85%B6%E5%AE%9E%E8%A6%81%E7%BB%93%E5%90%88%E5%85%AC%E5%BC%8F%E6%9D%A5%E7%9C%8B%E6%89%8D%E6%B8%85%E6%A5%9A-%E9%99%A4%E4%BA%86%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84paba-loss%E7%BB%8F%E5%85%B8%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%BD%93%E7%84%B6%E4%B9%9F%E6%98%AF%E4%B8%8D%E8%83%BD%E5%B0%91%E7%9A%84%E9%92%88%E5%AF%B9specific%E5%92%8Cshared-feature%E9%83%BD%E6%9C%89%E8%BF%99%E4%B8%AA%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E4%B8%89%E9%83%A8%E5%88%86%E6%8D%9F%E5%A4%B1%E5%8A%A0%E6%9D%83%E6%B1%82%E5%92%8C%E5%8D%B3%E5%BE%97%E5%88%B0%E6%80%BB%E7%9A%84%E6%8D%9F%E5%A4%B1%E6%9D%83%E9%87%8D%E6%98%AF%E8%B6%85%E5%8F%82"><span class="nav-number">2.5.</span> <span class="nav-text">分析：第一步是生成一个Semantically
Enhanced Grey
Images（姑且可以认为是一种拉近两种模态的数据的方法，只不过是从可见光图像出发，往红外上靠拢。也合理，毕竟可见光图像信息更多，红外图像信息更少，从信息更多的往信息更少的变换是比较容易的）。它主要做的其实是，在很常见的灰度映射之余，加上了高频信息。具体来说，它会先对可见光图像进行一个傅里叶变换，提取它的频域信息，然后用其中的频率成分进行傅里叶逆变换（这里是为了提取轮廓信息），然后将逆变换的结果与灰度映射的结果相加，得到Semantically
Enhanced Grey
Images（下简称SEG图像）。btw，这个方法倒是可以借鉴一下，感觉还是行得通的，起码可以试试用到我们的方法里。就是加上傅里叶变换之后，可能计算量大了第二步是对红外图像和SEG图像进行进一步的特征提取，也是为了进一步缩小两种模态的数据之间的差异（因为最后还是要用特征进行相似度计算的）。这里其实平平无奇，
就是用ResNet-50的前三块（共享权重）来提取共同的特征，之后又用ResNet-50的最后一个块（不同权重），来分别提取modality-specific
feature第三步是PABA loss，即Pseudo Anchor-guided Bidirectional
Aggregation
Loss。它的目的是，对于前面的共同特征，希望它们能更加兼容，也就是尽可能的变换到一个空间里，但也不能就失去判断力了。而所谓Anchor-guided，其实就是说，用“向Anchor靠拢”来实现两种数据的靠拢。举个具体例子，我有同一ID的两种模态的若干数据，我希望把它们变换到一个统一的空间里，则可以考虑，求一下可见光的一个Anchor（比如，所有可见光数据的均值），然后拉近所有红外数据与这个Anchor的距离，从而实现红外数据与可见光数据的靠拢。说回这里，它是对上面提到的共同特征\(F_{shared}^{seg}\)和\(F_{shared}^{ir}\)，先进行一个分块，分成N块，之后对每个块都施加PABA
loss，拉近同一ID的不同模态的数据的距离，同时拉远不同ID的不同模态的数据的距离。相比于常见的triplet
loss，这里增加了跨模态的考虑；而相比于cross-center
loss，这里的改进在于分了块，且是对每个块都施加PABA
loss，故更加细粒度（这里其实要结合公式来看才清楚）除了上面提到的PABA
loss，经典的交叉熵分类损失当然也是不能少的。针对specific和shared
feature都有这个分类损失。三部分损失加权求和即得到总的损失（权重是超参）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#embedding-and-enriching-explicit-semantics-for-visible-infrared-person-re-identification"><span class="nav-number">3.</span> <span class="nav-text">Embedding
and Enriching Explicit Semantics for Visible-Infrared Person
Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv2024.12.11"><span class="nav-number">3.1.</span> <span class="nav-text">arxiv，2024.12.11</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E7%9A%84%E6%96%B9%E6%B3%95%E7%9C%8B%E8%B5%B7%E6%9D%A5%E6%9C%89%E7%82%B9%E5%A4%8D%E6%9D%82%E7%84%B6%E5%90%8E%E5%8A%A8%E6%9C%BA%E4%B8%8A%E7%9C%8B%E6%84%9F%E8%A7%89%E4%B9%9F%E6%9C%89%E4%BA%9B%E7%89%B5%E5%BC%BA%E6%9B%B4%E5%A4%9A%E6%98%AF%E4%B8%BA%E4%BA%86%E7%94%A8%E4%B8%8A%E7%8E%B0%E5%9C%A8%E5%BE%88%E7%81%AB%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%89%80%E4%BB%A5%E7%A1%AC%E5%87%91%E4%BA%86%E4%B8%AA%E7%90%86%E7%94%B1"><span class="nav-number">3.2.</span> <span class="nav-text">这篇文章的方法看起来有点复杂，然后动机上看，感觉也有些牵强，更多是为了用上现在很火的大模型，所以硬凑了个理由</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#insight%E5%88%A9%E7%94%A8%E4%BA%86%E4%B8%80%E4%B8%8Btext%E7%94%A8%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%E6%9D%A5%E7%9B%B4%E6%8E%A5%E8%A1%A5%E5%85%85%E4%B8%80%E9%83%A8%E5%88%86%E5%9B%BE%E5%83%8F%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%AF%94%E5%8D%95%E7%BA%AF%E7%94%A8%E5%9B%BE%E5%83%8Fencoder%E6%9D%A5%E6%8F%90%E5%8F%96%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E8%A6%81%E5%A5%BD%E4%B8%80%E7%82%B9%E6%8F%90%E5%87%BA%E8%8B%A5%E5%B9%B2%E6%8D%9F%E5%A4%B1%E6%9D%A5%E5%88%A9%E7%94%A8%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%E4%BD%86%E6%98%AF%E5%90%A6%E5%90%88%E7%90%86%E6%9C%89%E5%BE%85%E8%80%83%E5%AF%9F%E8%87%B3%E5%B0%91%E7%9C%8B%E5%AE%83%E7%9A%84%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E8%AF%81%E6%98%8E%E4%BA%86%E8%BF%99%E9%83%A8%E5%88%86%E6%98%AF%E6%9C%89%E6%95%88%E7%9A%84"><span class="nav-number">3.3.</span> <span class="nav-text">insight：利用了一下text，用文本信息来直接补充一部分图像的语义信息，可能会比单纯用图像encoder来提取语义信息要好一点；提出若干损失来利用多视角信息（但是否合理有待考察，至少看它的消融实验结果，证明了这部分是有效的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA%E6%96%87%E7%AB%A0%E6%8C%87%E5%87%BA%E7%8E%B0%E5%9C%A8%E7%9A%84vi-reid%E6%96%B9%E6%B3%95%E6%9C%89%E4%B8%A4%E5%A4%A7%E7%B1%BB%E4%B8%80%E7%B1%BB%E6%98%AF%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95%E6%95%88%E6%9E%9C%E4%B8%8D%E5%A4%9F%E5%A5%BD%E4%B8%AA%E4%BA%BA%E4%B8%8D%E5%A4%AA%E4%BA%86%E8%A7%A3%E4%B8%80%E7%B1%BB%E6%98%AF%E5%9F%BA%E4%BA%8E%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95%E4%B9%9F%E5%B0%B1%E6%98%AF%E7%94%A8%E6%A8%A1%E5%9E%8B%E6%8A%8A%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8Cembed%E7%94%A8embedding%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%8C%B9%E9%85%8D%E9%80%9A%E5%B8%B8%E9%9C%80%E8%A6%81%E5%90%84%E7%A7%8D%E9%AD%94%E6%94%B9%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E7%9A%84%E6%95%88%E6%9E%9C%E4%BC%9A%E6%9B%B4%E5%A5%BD%E4%B9%9F%E6%9B%B4%E4%B8%BB%E6%B5%81%E8%80%8C%E6%96%87%E7%AB%A0%E6%8C%87%E5%87%BA%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E4%BB%85%E5%88%A9%E7%94%A8%E5%9B%BE%E5%83%8F%E5%AF%B9%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84%E6%8F%90%E5%8F%96%E4%B8%8D%E5%A4%9F%E5%9B%A0%E6%AD%A4%E5%B8%8C%E6%9C%9B%E5%8A%A0%E5%85%A5%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%E6%9D%A5%E5%B8%AE%E5%8A%A9%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%8F%90%E5%8F%96%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B8%80%E7%82%B9%E8%AE%A9%E6%88%91%E6%84%9F%E8%A7%89%E7%95%A5%E6%9C%89%E7%82%B9%E7%89%B5%E5%BC%BA%E5%AE%83%E6%89%80%E8%B0%93%E7%9A%84%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%E4%B9%9F%E6%98%AF%E4%BA%BAllm%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E7%9A%84%E6%8F%8F%E8%BF%B0%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E6%9C%AC%E8%BA%AB%E5%B0%B1%E6%98%AF%E6%9D%A5%E8%87%AA%E5%9B%BE%E5%83%8F%E7%9A%84%E8%87%B3%E4%BA%8E%E8%AF%B4%E6%8F%90%E5%8F%96%E7%9A%84%E4%B8%8D%E5%A4%9F%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E6%98%AF%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E4%B8%8D%E5%A4%9F%E5%A5%BD%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%A4%9F%E5%A4%9A%E7%AD%89%E9%97%AE%E9%A2%98%E5%AF%BC%E8%87%B4%E7%9A%84%E5%8A%A0%E5%85%A5%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%E6%9B%B4%E5%A4%9A%E5%8F%AF%E8%83%BD%E6%98%AF%E5%B8%8C%E6%9C%9B%E8%83%BD%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%83%BD%E6%AF%94%E8%BE%83%E5%AE%B9%E6%98%93%E5%9C%B0%E5%AD%A6%E5%88%B0%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E4%B8%8D%E8%BF%87%E8%80%83%E8%99%91%E5%88%B0%E7%8E%B0%E5%9C%A8reid%E7%9A%84%E9%AA%A8%E5%B9%B2%E5%9F%BA%E6%9C%AC%E9%83%BD%E6%98%AF%E4%B8%80%E4%B8%AAresnet50%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F%E5%BA%94%E8%AF%A5%E6%98%AF%E6%9C%89%E4%B8%8A%E9%99%90%E7%9A%84%E9%82%A3%E4%B9%88%E7%94%A8%E6%96%87%E5%AD%97embedding%E6%9D%A5%E8%BE%85%E5%8A%A9%E4%B8%80%E4%B8%8B%E4%B9%9F%E6%9C%89%E9%81%93%E7%90%86"><span class="nav-number">3.4.</span> <span class="nav-text">动机：文章指出，现在的VI-ReID方法有两大类，一类是基于生成式模型的方法，效果不够好（个人不太了解），一类是基于判别式模型的方法，也就是用模型把图像进行embed，用embedding来进行匹配，通常需要各种魔改网络架构。这种方法的效果会更好，也更主流。而文章指出，这种方法仅利用图像，对语义信息的提取不够，因此希望加入文本描述，来帮助更好的提取文本信息。（就是这一点，让我感觉略有点牵强。它所谓的文本描述，也是人&#x2F;llm基于图像进行的描述，语义信息本身就是来自图像的。至于说提取的不够，可以认为是网络架构不够好、数据不够多等问题导致的。加入文本描述，更多可能是希望能让模型能“比较容易”地学到语义信息）（不过考虑到现在reid的骨干基本都是一个ResNet50，模型容量应该是有上限的，那么用文字embedding来辅助一下也有道理）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95-1"><span class="nav-number">3.5.</span> <span class="nav-text">方法：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AAeees%E6%A1%86%E6%9E%B6%E5%8C%85%E5%90%AB%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%E5%90%84%E5%8F%B8%E5%85%B6%E8%81%8C-%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%98%AFese%E6%A8%A1%E5%9D%97explicit-semantics-embedding%E5%AE%83%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E5%88%A9%E7%94%A8%E6%96%87%E6%9C%AC%E8%AE%A9%E5%9B%BE%E5%83%8Fembedding%E5%AD%A6%E5%88%B0%E6%9B%B4%E5%A4%9A%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E4%B8%BB%E8%A6%81%E5%81%9A%E7%9A%84%E6%98%AF%E5%BC%95%E5%85%A5vllm%E4%B8%BA%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%E7%84%B6%E5%90%8E%E7%94%A8%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%8B%89%E8%BF%91%E5%9B%BE%E5%83%8Fembedding%E5%92%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%E7%9A%84embedding%E8%AE%A9%E5%9B%BE%E5%83%8Fembedding%E5%AD%A6%E5%88%B0%E6%96%87%E6%9C%AC%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF"><span class="nav-number">3.5.1.</span> <span class="nav-text">提出一个EEES框架，包含三个部分，各司其职第一个是ESE模块（Explicit
Semantics
Embedding），它的目的是，利用文本，让图像embedding学到更多语义信息。主要做的是引入vllm，为图像生成文本描述，然后用对比学习，拉近图像embedding和对应的文本描述的embedding，让图像embedding学到文本的语义信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E4%B8%AA%E6%98%AFcvsccross-view-semantics-compensation%E5%AE%83%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E5%88%A9%E7%94%A8%E5%A4%9A%E8%A7%86%E8%A7%92%E7%9A%84%E5%9B%BE%E5%83%8F%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%83%BD%E6%8F%90%E5%8F%96%E5%87%BA%E6%9B%B4%E5%8A%A0%E4%B8%B0%E5%AF%8C%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E4%B8%BE%E4%B8%AA%E4%BE%8B%E5%AD%90%E5%AF%B9%E4%BA%8E%E5%90%8C%E4%B8%80%E4%B8%AA%E4%BA%BA%E4%B8%80%E4%B8%AA%E6%91%84%E5%83%8F%E5%A4%B4%E8%A7%86%E8%A7%92%E6%8B%8D%E5%88%B0%E7%9A%84%E7%85%A7%E7%89%87%E6%89%80%E8%95%B4%E5%90%AB%E7%9A%84%E4%BF%A1%E6%81%AF%E7%BB%88%E5%BD%92%E6%98%AF%E6%9C%89%E9%99%90%E7%9A%84%E5%8F%AF%E8%83%BD%E8%BF%99%E4%B8%AA%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E7%85%A7%E7%89%87%E8%83%BD%E6%8B%8D%E5%88%B0%E8%BA%AB%E6%9D%90%E8%BD%AE%E5%BB%93%E8%84%B8%E5%8D%B4%E8%A2%AB%E9%81%AE%E4%BD%8F%E4%BA%86%E8%BF%99%E6%97%B6%E5%A6%82%E6%9E%9C%E8%83%BD%E7%94%A8%E5%88%B0%E5%85%B6%E5%AE%83%E8%A7%86%E8%A7%92%E4%B8%8B%E8%BF%99%E4%B8%AA%E4%BA%BA%E7%9A%84%E7%85%A7%E7%89%87%E5%8F%AF%E8%83%BD%E5%B0%B1%E8%83%BD%E8%8E%B7%E5%BE%97%E8%84%B8%E9%83%A8%E4%BF%A1%E6%81%AF%E4%BA%86%E7%BB%BC%E5%90%88%E8%B5%B7%E6%9D%A5%E5%B0%B1%E8%83%BD%E6%8F%90%E5%8F%96%E5%87%BA%E8%BF%99%E4%B8%AA%E4%BA%BA%E7%9A%84%E6%9B%B4%E5%A4%9A%E7%89%B9%E5%BE%81%E6%89%80%E4%BB%A5%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%E5%AF%B9%E4%BA%8E%E4%B8%80%E4%B8%AAbatch%E9%87%8C%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%9B%BE%E5%83%8F%E8%BF%99%E5%A5%97%E6%96%B9%E6%B3%95%E9%83%BD%E4%BC%9A%E5%8E%BB%E5%88%A9%E7%94%A8%E5%90%8Cid%E5%9C%A8%E5%85%B6%E5%AE%83%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E8%80%8C%E5%85%B7%E4%BD%93%E7%9A%84%E5%88%A9%E7%94%A8%E6%96%B9%E6%B3%95%E5%B0%B1%E6%98%AF%E5%9C%A8%E8%AE%AD%E7%BB%83%E6%97%B6%E5%AF%B9%E4%B8%80%E4%B8%AAbatch%E4%B8%AD%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%9B%BE%E5%83%8F%E9%83%BD%E5%8E%BB%E9%9A%8F%E6%9C%BA%E5%8F%96m%E4%B8%AA%E5%90%8Cid%E7%9A%84%E4%B8%8D%E5%90%8C%E6%91%84%E5%83%8F%E5%A4%B4%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E7%84%B6%E5%90%8E%E6%B1%82%E4%B8%AA%E5%B9%B3%E5%9D%87%E5%80%BC%E5%85%B1m1%E4%B8%AA%E5%9B%BE%E5%83%8F%E6%88%91%E4%BB%AC%E5%B0%B1%E8%AE%A4%E4%B8%BA%E8%BF%99%E4%B8%AA%E5%9D%87%E5%80%BC%E5%9B%BE%E5%83%8F%E9%87%8C%E8%95%B4%E5%90%AB%E4%BA%86%E5%A4%9A%E8%A7%86%E8%A7%92%E7%9A%84%E4%BF%A1%E6%81%AF%E5%AF%B9%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B9%9F%E6%98%AF%E8%BF%9B%E8%A1%8C%E7%B1%BB%E4%BC%BC%E7%9A%84%E6%93%8D%E4%BD%9C%E4%BB%8E%E8%80%8C%E5%BE%97%E5%88%B0%E8%95%B4%E5%90%AB%E4%BA%86%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%E7%9A%84%E6%96%87%E6%9C%ACembedding%E5%BD%93%E7%84%B6%E9%83%BD%E5%BF%85%E9%A1%BB%E6%98%AF%E5%90%8C%E6%A8%A1%E6%80%81%E4%B8%8B%E7%9A%84%E5%B0%B1%E6%98%AF%E8%AF%B4%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E8%A6%81%E5%8E%BB%E6%89%BE%E5%90%8Cid%E7%9A%84%E4%B8%8D%E5%90%8C%E6%91%84%E5%83%8F%E5%A4%B4%E4%B8%8B%E7%9A%84%E5%85%B6%E5%AE%83%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E5%85%B6%E4%BD%99%E7%B1%BB%E4%BC%BC%E4%B9%8B%E5%90%8E%E4%BC%9A%E7%94%A8%E4%B8%80%E4%B8%AA%E5%AF%B9%E9%BD%90%E6%8D%9F%E5%A4%B1%E5%8E%BB%E5%8F%8C%E5%90%91%E7%BA%A6%E6%9D%9F%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8Fembedding%E5%92%8C%E6%96%87%E5%AD%97embedding%E6%AF%95%E7%AB%9F%E5%8D%B3%E4%BD%BF%E6%98%AF%E7%BB%BC%E5%90%88%E4%BA%86%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%E5%AF%B9%E4%BA%8E%E5%90%8C%E4%B8%80%E4%B8%AA%E4%BA%BA%E7%9A%84%E5%9B%BE%E5%83%8Fembedding%E5%92%8C%E6%96%87%E5%AD%97embedding%E4%BE%9D%E7%84%B6%E5%BA%94%E8%AF%A5%E5%B0%BD%E5%8F%AF%E8%83%BD%E7%9B%B8%E8%BF%91%E7%A1%AE%E4%BF%9D%E5%9B%BE%E5%83%8F%E5%AD%A6%E5%88%B0%E6%9B%B4%E5%A4%9A%E5%90%88%E7%90%86%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF-%E7%84%B6%E8%80%8C%E5%AE%9E%E9%99%85%E6%B5%8B%E8%AF%95%E7%9A%84%E6%97%B6%E5%80%99%E6%88%91%E4%BB%AC%E9%83%BD%E6%98%AF%E6%8B%BF%E5%88%B0%E4%B8%80%E4%B8%AA%E5%9B%BE%E5%83%8F%E5%B0%B1%E8%A6%81%E5%8E%BB%E8%BF%9B%E8%A1%8C%E5%8C%B9%E9%85%8D%E4%BA%86%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%E6%B2%A1%E6%9C%89%E5%A4%9A%E8%A7%86%E8%A7%92%E7%9A%84%E4%BF%A1%E6%81%AF%E5%8F%AF%E4%BB%A5%E5%88%A9%E7%94%A8%E4%B8%8A%E9%9D%A2%E4%B9%9F%E6%8F%90%E5%88%B0%E4%BA%86%E6%98%AF%E8%AE%AD%E7%BB%83%E6%97%B6%E5%8F%AF%E4%BB%A5%E8%BF%99%E6%A0%B7%E5%88%A9%E7%94%A8%E5%A4%9A%E8%A7%86%E8%A7%92%E4%BF%A1%E6%81%AF%E9%82%A3%E4%B8%BA%E4%BA%86%E5%BC%A5%E8%A1%A5%E8%BF%99%E4%B8%80%E7%BC%BA%E7%82%B9%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E8%92%B8%E9%A6%8F%E5%B0%B1%E6%98%AF%E8%AE%A9%E6%AF%8F%E4%B8%AA%E5%9B%BE%E5%83%8Fembedding%E5%8E%BB%E9%80%BC%E8%BF%91%E5%AE%83%E5%AF%B9%E5%BA%94%E7%9A%84%E9%82%A3%E4%B8%AA%E5%A4%9A%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E5%9D%87%E5%80%BCembedding%E6%96%87%E5%AD%97embedding%E7%B1%BB%E4%BC%BC%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E9%83%BD%E8%BF%9B%E8%A1%8C%E8%BF%99%E4%B8%AA%E6%93%8D%E4%BD%9C%E6%89%80%E4%BB%A5%E8%BF%99%E4%B8%AA%E8%92%B8%E9%A6%8F%E6%8D%9F%E5%A4%B1%E5%B0%B1%E6%98%AF%E5%9B%9B%E9%A1%B9%E4%BA%86%E5%85%AC%E5%BC%8F%E5%A6%82%E4%B8%8B500"><span class="nav-number">3.5.2.</span> <span class="nav-text">第二个是CVSC（Cross-View
Semantics
Compensation），它的目的是，利用多视角的图像，让模型能提取出更加丰富的语义信息。举个例子，对于同一个人，一个摄像头（视角）拍到的照片所蕴含的信息，终归是有限的。可能这个视角下的照片能拍到身材轮廓，脸却被遮住了；这时如果能用到其它视角下这个人的照片，可能就能获得脸部信息了。综合起来，就能提取出这个人的更多特征。所以，训练的时候，对于一个batch里的每个图像，这套方法都会去利用同ID在其它视角下的图像。而具体的利用方法，就是在训练时，对一个batch中的每个图像，都去随机取M个同ID的，不同摄像头下的图像，然后求个平均值（共\(M+1\)个图像），我们就认为这个均值图像里蕴含了多视角的信息。对于文本，也是进行类似的操作，从而得到蕴含了多视角信息的文本embedding（当然，都必须是同模态下的。就是说，可见光图像要去找同ID的不同摄像头下的其它可见光图像。其余类似）。之后会用一个对齐损失，去双向约束多视角信息下的图像embedding和文字embedding。毕竟，即使是综合了多视角信息，对于同一个人的图像embedding和文字embedding依然应该尽可能相近，确保图像学到更多合理的语义信息然而，实际测试的时候，我们都是拿到一个图像，就要去进行匹配了。也就是说，没有多视角的信息可以利用（上面也提到了，是训练时可以这样利用多视角信息。那为了弥补这一缺点，论文提出进行一个蒸馏，就是让每个图像embedding去逼近它对应的那个多视角下的均值embedding，文字embedding类似。两种模态都进行这个操作。所以这个蒸馏损失就是四项了。公式如下：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E4%B8%AA%E6%98%AFcmsp%E6%A8%A1%E5%9D%97cross-modality-semantics-purification%E5%AE%83%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E9%81%BF%E5%85%8D%E5%AD%A6%E5%88%B0%E7%9A%84%E8%AF%AD%E4%B9%89%E5%8F%91%E7%94%9F%E5%86%B2%E7%AA%81%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%E5%90%8C%E4%B8%80%E4%B8%AAid%E4%BC%9A%E6%9C%89%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E4%B9%9F%E4%BC%9A%E6%9C%89%E5%AF%B9%E5%BA%94%E7%9A%84%E7%81%B0%E5%BA%A6%E5%9B%BE%E7%9A%84%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E8%80%8C%E8%BF%99%E4%B8%A4%E7%A7%8D%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E5%8F%AF%E8%83%BD%E5%8F%91%E7%94%9F%E5%86%B2%E7%AA%81%E6%AF%94%E5%A6%82llm%E9%92%88%E5%AF%B9%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E5%8F%AF%E8%83%BD%E5%8F%AF%E4%BB%A5%E5%87%86%E7%A1%AE%E5%9C%B0%E6%8F%8F%E8%BF%B0%E5%87%BA%E4%BA%BA%E7%A9%BF%E7%9A%84%E8%A1%A3%E6%9C%8D%E7%9A%84%E9%A2%9C%E8%89%B2%E4%B9%9F%E8%AE%B8%E6%98%AF%E8%93%9D%E8%89%B2%E8%80%8C%E6%8B%BF%E5%88%B0%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E5%85%A8%E9%83%A8%E9%83%BD%E6%98%AF%E4%BC%9A%E7%9A%84%E5%AE%83%E5%8F%AF%E8%83%BD%E5%B0%B1%E4%BC%9A%E8%BE%93%E5%87%BA%E4%BA%BA%E7%A9%BF%E7%9A%84%E8%A1%A3%E6%9C%8D%E7%9A%84%E9%A2%9C%E8%89%B2%E6%98%AF%E7%81%B0%E7%9A%84%E8%BF%99%E6%97%A0%E7%96%91%E4%BC%9A%E9%80%A0%E6%88%90%E7%9F%9B%E7%9B%BE%E6%89%80%E4%BB%A5%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E5%AF%B9%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E5%8F%AF%E4%BB%A5%E4%B8%8D%E5%90%8C%E4%BA%92%E8%A1%A5%E5%BD%93%E7%84%B6%E6%98%AF%E5%A5%BD%E7%8E%B0%E8%B1%A1%E4%BD%86%E4%B8%8D%E8%83%BD%E7%9F%9B%E7%9B%BE%E5%9B%A0%E6%AD%A4%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AAcmsp-loss%E5%85%AC%E5%BC%8F%E5%A6%82%E4%B8%8Bl_cmspfrac1nsum_i1nd_ivv-d_ivr2frac1nsum_i1nd_irr-d_irv2-%E5%85%B6%E4%B8%ADd_ivvlvert-f_iv-t_ivrvert_2d_ivrlvert-f_iv-t_irrvert_2%E5%85%B6%E4%BD%99%E5%AE%9A%E4%B9%89%E7%B1%BB%E4%BC%BC%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E6%88%91%E4%BB%AC%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E8%AE%A9%E8%BF%99%E4%B8%AA%E6%8D%9F%E5%A4%B1%E7%BA%A6%E6%9D%9F%E5%90%8C%E4%B8%80id%E4%B8%8B%E7%9A%84%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E8%A6%81%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%E5%AE%83%E7%9A%84%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E8%87%B3%E4%BA%8E%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A8%A1%E5%9E%8B%E4%B8%8D%E4%BC%9A%E5%A1%8C%E7%BC%A9%E7%9B%B4%E6%8E%A5%E8%AE%A9%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E4%B8%8E%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E5%AE%8C%E5%85%A8%E4%B8%80%E6%A0%B7%E5%8F%AF%E8%83%BD%E6%98%AF%E5%9B%A0%E4%B8%BA%E5%89%8D%E9%9D%A2%E7%9A%84%E5%AF%B9%E6%AF%94%E6%8D%9F%E5%A4%B1%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E7%BA%A6%E6%9D%9F%E4%BA%86%E8%BF%99%E4%BA%9B%E6%96%87%E5%AD%97embedding%E8%AE%A9%E5%AE%83%E4%BB%AC%E5%BF%85%E9%A1%BB%E6%9C%89%E5%90%88%E7%90%86%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E5%90%A7"><span class="nav-number">3.5.3.</span> <span class="nav-text">第三个是CMSP模块（Cross-Modality
Semantics
Purification），它的目的是，避免学到的语义发生冲突。具体来说，同一个ID会有灰度图和可见光图像，也会有对应的灰度图的文字描述和可见光图像的文字描述。而这两种文字描述，可能发生冲突。比如，llm针对可见光图像，可能可以准确地描述出人穿的衣服的颜色，也许是蓝色；而拿到灰度图像，全部都是会的，它可能就会输出“人穿的衣服的颜色是灰的”，这无疑会造成矛盾。所以我们希望，对两种模态的文字描述，可以不同（互补当然是好现象），但不能矛盾。因此，论文提出一个cmsp
loss，公式如下：\[L_{cmsp}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{vv}-d_{i}^{vr})}^{2}+\frac{1}{N}\sum_{i&#x3D;1}^{N}{(d_{i}^{rr}-d_{i}^{rv})}^{2}\]其中，\(d_{i}^{vv}&#x3D;\lVert
f_{i}^{v}-t_{i}^{v}\rVert_{2}\)，\(d_{i}^{vr}&#x3D;\lVert
f_{i}^{v}-t_{i}^{r}\rVert_{2}\)，其余定义类似。可以看到，我们的目的是，让这个损失约束同一ID下的红外图像文字描述要尽可能接近它的可见光图像的文字描述（至于为什么模型不会塌缩，直接让红外图像文字描述与可见光图像文字描述完全一样，可能是因为前面的对比损失、分类损失约束了这些文字embedding，让它们必须有合理的语义信息吧</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-number">3.6.</span> <span class="nav-text">分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E6%98%AF%E7%A8%80%E6%9D%BE%E5%B9%B3%E5%B8%B8%E7%9A%84%E4%BD%86%E5%A5%BD%E5%83%8F%E4%B9%9F%E5%B0%B1%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E7%9C%8B%E8%B5%B7%E6%9D%A5%E9%9D%A0%E7%82%B9%E8%B0%B1"><span class="nav-number">3.6.1.</span> <span class="nav-text">第一部分是稀松平常的，但好像也就第一部分看起来靠点谱。。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E6%83%B3%E6%B3%95%E6%98%AF%E6%8C%BA%E5%A5%BD%E7%9A%84%E4%BD%86%E4%B8%80%E6%9D%A5%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%E5%87%AD%E4%BB%80%E4%B9%88%E8%AF%B4%E6%B1%82%E4%B8%AA%E5%9D%87%E5%80%BC%E5%9B%BE%E5%83%8F%E5%B0%B1%E8%95%B4%E5%90%AB%E4%BA%86%E5%A4%9A%E8%A7%86%E8%A7%92%E7%9A%84%E4%BF%A1%E6%81%AF%E6%88%96%E8%80%85%E8%AF%B4%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E6%98%AF%E4%B8%8D%E6%98%AF%E5%A4%AA%E7%AE%80%E5%8D%95%E7%B2%97%E6%9A%B4%E4%BA%86%E8%BF%98%E6%9C%89%E6%B5%8B%E8%AF%95%E7%9A%84%E6%97%B6%E5%80%99%E9%82%A3%E4%B8%AA%E8%92%B8%E9%A6%8F%E5%B0%B1%E6%98%AF%E8%AF%B4%E5%B8%8C%E6%9C%9B%E5%8D%95%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84embedding%E8%83%BD%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%E5%A4%9A%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84embedding%E5%8F%AF%E8%A7%81%E5%85%89%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E6%96%87%E5%AD%97%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%E6%88%91%E4%BB%AC%E7%AB%9F%E7%84%B6%E6%98%AF%E5%B8%8C%E6%9C%9B%E6%A8%A1%E5%9E%8B%E6%8B%BF%E5%88%B0%E4%B8%80%E4%B8%AA%E5%8D%95%E8%A7%86%E8%A7%92%E7%9A%84%E5%9B%BE%E5%83%8F%E5%B0%B1%E8%83%BD%E8%84%91%E8%A1%A5%E5%87%BA%E5%A4%9A%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E4%BF%A1%E6%81%AF%E8%BF%99%E5%90%88%E7%90%86%E5%90%97%E5%8F%AA%E8%83%BD%E8%AF%B4%E6%9C%89%E4%B8%80%E4%BA%9B%E5%90%88%E7%90%86%E4%B9%8B%E5%A4%84%E6%AF%95%E7%AB%9F%E4%BA%BA%E6%8B%BF%E5%88%B0%E4%B8%80%E5%BC%A0%E5%9B%BE%E5%83%8F%E7%A1%AE%E5%AE%9E%E8%83%BD%E9%9D%A0%E7%A9%BA%E9%97%B4%E6%83%B3%E8%B1%A1%E5%8A%9B%E8%84%91%E8%A1%A5%E5%87%BA%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF%E4%BD%86%E4%B9%9F%E6%9C%89%E4%BA%9B%E4%B8%8D%E5%90%88%E7%90%86%E4%B9%8B%E5%A4%84%E6%AF%94%E5%A6%82%E4%B8%8A%E9%9D%A2%E7%9A%84%E8%84%B8%E9%83%A8%E9%81%AE%E6%8C%A1%E4%BE%8B%E5%AD%90%E8%BF%99%E4%B8%AA%E6%98%AF%E6%97%A0%E8%AE%BA%E5%A6%82%E4%BD%95%E4%B9%9F%E4%B8%8D%E8%83%BD%E8%84%91%E8%A1%A5%E5%87%BA%E6%9D%A5%E7%9A%84%E9%80%BC%E8%BF%AB%E6%A8%A1%E5%9E%8B%E5%8E%BB%E5%AD%A6%E8%BF%99%E4%B8%AA%E4%B9%9F%E6%98%AF%E4%B8%8D%E5%90%88%E7%90%86%E7%9A%84%E9%9A%BE%E8%AF%B4%E6%9C%80%E5%90%8E%E5%AD%A6%E5%87%BA%E6%9D%A5%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%E5%8F%AF%E8%83%BD%E8%BF%87%E6%8B%9F%E5%90%88%E4%BA%86"><span class="nav-number">3.6.2.</span> <span class="nav-text">第二部分想法是挺好的，但一来，训练的时候，凭什么说求个均值图像，就蕴含了多视角的信息？或者说，这种方法是不是太简单粗暴了？还有，测试的时候，那个蒸馏，就是说，希望单视角下的embedding能尽可能接近多视角下的embedding（可见光&#x2F;红外，图像&#x2F;文字），也就是说，我们竟然是希望模型拿到一个单视角的图像，就能“脑补”出多视角下的图像信息？这合理吗？只能说，有一些合理之处（毕竟人拿到一张图像，确实能靠空间想象力脑补出一些东西），但也有些不合理之处（比如上面的脸部遮挡例子，这个是无论如何也不能脑补出来的。逼迫模型去学这个，也是不合理的，难说最后学出来的是什么，可能过拟合了）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E5%90%8C%E6%A0%B7%E5%87%BA%E5%8F%91%E7%82%B9%E5%80%92%E4%B9%9F%E6%B2%A1%E9%94%99%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B8%AA%E6%8D%9F%E5%A4%B1%E7%9C%8B%E8%B5%B7%E6%9D%A5%E6%9C%89%E4%BA%9B%E5%A5%87%E6%80%AA%E4%BB%A5cmsp-loss%E7%9A%84%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%BA%E4%BE%8B%E5%85%B6%E5%AE%9E%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E8%AF%B4%E8%BF%99%E4%B8%AA%E6%8D%9F%E5%A4%B1%E6%98%AF%E5%9C%A8%E4%BB%A5%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%9A%84embedding%E4%B8%BA%E9%94%9A%E7%82%B9%E8%AE%A9%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%87%E5%AD%97embedding%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%87%E5%AD%97embedding%E8%B7%9D%E7%A6%BB%E8%BF%99%E4%B8%AA%E9%94%9A%E7%82%B9%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%E6%88%91%E7%8C%9C%E6%83%B3%E4%BD%9C%E8%80%85%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E8%AE%A9%E4%B8%A4%E7%A7%8D%E6%96%87%E5%AD%97embedding%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8E%A5%E8%BF%91%E8%BF%99%E6%A0%B7%E5%B0%B1%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%AF%B4%E8%AE%A9%E6%96%87%E5%AD%97embedding%E5%B0%BD%E5%8F%AF%E8%83%BD%E6%8F%90%E5%8F%96%E5%87%BA%E5%85%B1%E6%80%A7%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E8%80%8C%E4%B8%8D%E4%BC%9A%E5%8E%BB%E6%8F%90%E5%8F%96%E5%87%BA%E7%9F%9B%E7%9B%BE%E7%9A%84%E4%B8%80%E6%96%B9%E7%8B%AC%E6%9C%89%E7%9A%84%E4%BF%A1%E6%81%AF%E5%A6%82%E6%9E%9C%E7%9C%9F%E6%98%AF%E8%BF%99%E6%A0%B7%E5%AE%83%E5%B0%B1%E5%BA%94%E8%AF%A5%E7%9B%B4%E6%8E%A5%E7%BA%A6%E6%9D%9F%E4%B8%A4%E4%B8%AA%E6%96%87%E5%AD%97embedding%E4%BA%86%E6%B2%A1%E5%BF%85%E8%A6%81%E6%8E%BA%E5%92%8C%E4%B8%8A%E5%9B%BE%E5%83%8Fembedding%E4%BD%86%E5%87%A0%E4%BD%95%E4%B8%8A%E7%9C%8B%E8%A6%81%E8%AE%A9%E6%8D%9F%E5%A4%B1%E6%9C%80%E5%B0%8F%E5%AE%83%E4%BB%AC%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%9C%A8%E4%B8%80%E4%B8%AA%E4%BB%A5%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E4%B8%BA%E5%9C%86%E5%BF%83%E7%9A%84%E6%9F%90%E4%B8%AA%E5%9C%86%E4%B8%8A%E5%8F%AA%E4%B8%8D%E8%BF%87%E6%98%AF%E9%AB%98%E7%BB%B4%E7%9A%84%E6%AD%A4%E6%97%B6%E5%AE%83%E4%BB%AC%E6%9C%AA%E5%BF%85%E5%B0%B1%E8%B7%9D%E7%A6%BB%E5%BE%88%E8%BF%91%E8%BF%99%E6%A0%B7%E5%BE%97%E5%88%B0%E7%9A%84%E4%B8%A4%E7%A7%8Dembedding%E6%9C%89%E4%BB%80%E4%B9%88%E5%90%AB%E4%B9%89%E5%91%A2%E4%B8%8D%E8%83%BD%E8%AE%A4%E4%B8%BA%E8%AF%B4%E5%9C%A8%E5%90%8C%E4%B8%80%E4%B8%AA%E5%9C%86%E5%91%A8%E4%B8%8A%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E5%B0%B1%E4%B8%8D%E7%9F%9B%E7%9B%BE%E4%BA%86%E5%90%A7%E6%9C%89%E7%82%B9%E6%80%AA%E6%88%96%E8%80%85%E6%88%91%E4%BB%AC%E8%AE%A4%E4%B8%BA%E6%96%87%E5%AD%97embedding%E4%B8%8D%E5%8A%A8%E4%BC%98%E5%8C%96%E7%9A%84%E6%98%AF%E5%9B%BE%E5%83%8Fembedding%E8%BF%99%E6%A0%B7%E6%9C%80%E7%90%86%E6%83%B3%E7%9A%84%E6%83%85%E5%86%B5%E6%98%AF%E5%9B%BE%E5%83%8Fembedding%E8%90%BD%E5%9C%A8%E4%BA%86%E4%B8%A4%E7%A7%8D%E6%96%87%E5%AD%97embedding%E7%9A%84%E4%B8%AD%E5%9E%82%E7%BA%BF%E4%B8%8A%E4%BA%86%E8%BF%99%E6%A0%B7%E5%B0%B1%E8%83%BD%E8%AE%A4%E4%B8%BA%E5%9B%BE%E5%83%8Fembedding%E6%B2%A1%E6%9C%89%E5%AD%A6%E5%88%B0%E4%BB%80%E4%B9%88%E7%9F%9B%E7%9B%BE%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E5%90%97%E8%BF%98%E6%98%AF%E6%B2%A1%E9%81%93%E7%90%86%E5%95%8A%E5%8F%88%E6%88%96%E8%80%85%E4%B8%8A%E9%9D%A2%E7%9A%84%E5%88%86%E6%9E%90%E9%83%BD%E6%98%AF%E5%81%87%E5%AE%9A%E4%BA%86%E4%B8%80%E7%A7%8Dembedding%E6%98%AF%E4%B8%8D%E5%8A%A8%E7%9A%84%E6%96%87%E5%AD%97%E5%9B%BE%E5%83%8F%E5%AE%9E%E9%99%85%E6%98%AF%E4%B8%80%E8%B5%B7%E4%BC%98%E5%8C%96%E7%9A%84%E9%82%A3%E5%AE%83%E6%83%B3%E4%BC%98%E5%8C%96%E4%BB%80%E4%B9%88%E5%91%A2%E8%BF%98%E6%98%AF%E4%B8%8D%E5%A4%AA%E6%98%8E%E7%99%BD"><span class="nav-number">3.6.3.</span> <span class="nav-text">第三部分，同样，出发点倒也没错，就是这个损失，看起来有些奇怪。以cmsp
loss的第一部分为例，其实我们可以说，这个损失是在以可见光图像的embedding为锚点，让红外图像的文字embedding和可见光图像的文字embedding距离这个锚点的距离尽可能接近。我猜想作者的目的是，让两种文字embedding尽可能接近，这样就相当于说让文字embedding尽可能提取出共性的语义信息，而不会去提取出矛盾的&#x2F;一方独有的信息（如果真是这样，它就应该直接约束两个文字embedding了，没必要掺和上图像embedding？但几何上看，要让损失最小，它们也可以在一个以可见光图像为“圆心”的某个“圆”上（只不过是高维的）。此时它们未必就距离很近。这样得到的两种embedding有什么含义呢？不能认为说，在同一个圆周上，语义信息就不矛盾了吧？有点怪。或者我们认为，文字embedding不动，优化的是图像embedding？这样最理想的情况是，图像embedding落在了两种文字embedding的“中垂线”上了。这样就能认为图像embedding没有学到什么矛盾的语义信息吗？还是没道理啊。又或者上面的分析都是假定了一种embedding是不动的（文字&#x2F;图像），实际是一起优化的。那它想优化什么呢？还是不太明白。。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#prototype-driven-multi-feature-generation-for-visible-infrared-person-re-identification"><span class="nav-number">4.</span> <span class="nav-text">Prototype-Driven
Multi-Feature Generation for Visible-Infrared Person
Re-identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#icassp2024.9.9"><span class="nav-number">4.1.</span> <span class="nav-text">ICASSP，2024.9.9</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA%E5%85%B6%E5%AE%9E%E6%84%9F%E8%A7%89%E4%B8%8D%E5%A4%AA%E6%98%8E%E7%A1%AE%E4%BC%BC%E4%B9%8E%E6%98%AF%E9%AD%94%E6%94%B9%E5%AE%8C%E6%9C%89%E6%95%88%E6%9E%9C%E4%BA%86%E5%B0%B1%E5%BC%80%E5%A7%8B%E8%AE%B2%E6%95%85%E4%BA%8B%E4%BA%86"><span class="nav-number">4.2.</span> <span class="nav-text">动机其实感觉不太明确，似乎是魔改完有效果了，就开始讲故事了</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#insight%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AAmfgm%E6%9D%A5%E8%8E%B7%E5%8F%96%E5%A4%9A%E6%A0%B7%E7%89%B9%E5%BE%81%E4%BD%86%E5%85%B6%E5%AE%9E%E4%B8%8D%E6%98%AF%E5%BE%88%E6%96%B0%E5%A5%87%E4%BA%86%E8%B7%9Fdeen%E5%B7%AE%E4%B8%8D%E5%A4%9A%E7%9A%84%E6%80%9D%E8%B7%AF%E5%B0%B1%E6%98%AF%E5%A4%9A%E5%87%A0%E4%B8%AA%E5%88%86%E6%94%AF%E5%8A%A0%E4%B8%8A%E4%B8%80%E4%BA%9B%E6%8D%9F%E5%A4%B1%E7%BA%A6%E6%9D%9F%E8%AE%A9%E8%BF%99%E4%BA%9B%E7%94%9F%E6%88%90%E7%9A%84%E7%89%B9%E5%BE%81%E4%B8%8D%E5%A4%AA%E4%B8%80%E6%A0%B7%E7%94%A8%E4%B8%80%E4%B8%AAprototype%E6%9D%A5%E6%8F%90%E5%8F%96%E7%89%B9%E5%BE%81%E6%84%9F%E8%A7%89%E6%9C%AC%E8%B4%A8%E5%8F%AA%E6%98%AF11%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%B7%A8%E6%A8%A1%E6%80%81%E7%94%A8%E5%90%8C%E4%B8%80%E5%A5%97%E6%A8%A1%E6%9D%BF%E7%84%B6%E5%90%8E%E5%8F%88%E7%94%A8%E4%B8%80%E4%B8%AA%E6%8D%9F%E5%A4%B1%E6%9D%A5%E7%BA%A6%E6%9D%9F%E6%A8%A1%E6%9D%BF%E6%8F%90%E5%8F%96%E4%B8%8D%E5%90%8C%E7%9A%84%E7%89%B9%E5%BE%81%E5%92%8C%E4%B8%8A%E9%9D%A2%E7%BA%A6%E6%9D%9F%E7%94%9F%E6%88%90%E7%89%B9%E5%BE%81%E7%9A%84%E5%B7%AE%E4%B8%8D%E5%A4%9A%E8%BF%99%E9%87%8C%E5%8F%AA%E6%98%AF%E6%8D%A2%E4%BA%86%E4%B8%AAcos%E7%9A%84%E5%A5%97%E5%AD%90%E6%8B%89%E8%BF%91%E5%90%8Cid%E7%89%B9%E5%BE%81%E6%8B%89%E8%BF%9C%E4%B8%8D%E5%90%8Cid%E7%89%B9%E5%BE%81%E6%9D%A5%E7%A1%AE%E4%BF%9D%E7%89%B9%E5%BE%81%E7%9A%84%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7"><span class="nav-number">4.3.</span> <span class="nav-text">insight：提出一个MFGM来获取多样特征（但其实不是很新奇了，跟DEEN差不多的思路，就是多几个分支加上一些损失约束，让这些生成的特征不太一样）；用一个prototype来提取特征（感觉本质只是1×1的卷积），跨模态用同一套模板，然后又用一个损失来约束模板提取不同的特征（和上面约束生成特征的差不多，这里只是换了个cos的套子），拉近同ID特征，拉远不同ID特征，来确保特征的语义信息的正确性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA-1"><span class="nav-number">4.4.</span> <span class="nav-text">动机：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95-2"><span class="nav-number">4.5.</span> <span class="nav-text">方法：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E5%87%BA%E7%9A%84pdm%E6%A1%86%E6%9E%B6%E4%B8%BB%E8%A6%81%E6%98%AF%E4%B8%A4%E4%B8%AA%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86%E5%88%86%E5%88%AB%E6%98%AFmulti-feature-generation-modulemfgm%E7%94%A8%E4%BA%8E%E7%94%9F%E6%88%90%E6%9B%B4%E5%A4%9A%E5%A4%9A%E6%A0%B7%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%82%E8%80%83deen%E5%92%8Cprototype-learning-moduelplm"><span class="nav-number">4.5.1.</span> <span class="nav-text">提出的PDM框架，主要是两个组成部分，分别是Multi-Feature
Generation Module（MFGM）（用于生成更多多样的特征，参考DEEN）和Prototype
Learning Moduel（PLM）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mfgm%E6%84%9F%E8%A7%89%E7%9C%9F%E7%9A%84%E5%92%8Cdeen%E5%BE%88%E5%83%8F%E9%83%BD%E6%98%AF%E7%94%A8%E5%87%A0%E4%B8%AA%E5%88%86%E6%94%AF%E7%94%9F%E6%88%90%E5%A4%9A%E5%87%A0%E4%B8%AA%E7%89%B9%E5%BE%81%E7%84%B6%E5%90%8E%E5%88%86%E6%94%AF%E4%B8%8A%E9%83%BD%E6%9C%89dilation-convolution%E4%B9%8B%E5%90%8E%E5%8F%88%E7%94%A8%E4%B8%80%E4%BA%9B%E6%8D%9F%E5%A4%B1%E6%9D%A5%E6%8A%8A%E6%9F%90%E4%BA%9B%E7%89%B9%E5%BE%81%E6%8B%89%E8%BF%9C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%A0%B7%E5%8C%96%E7%89%B9%E5%BE%81%E8%BF%99%E9%87%8C%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%B0%B1%E6%98%AF%E6%96%87%E4%B8%AD%E7%9A%84center-guided-pair-mining-loss"><span class="nav-number">4.5.2.</span> <span class="nav-text">MFGM感觉真的和DEEN很像，都是用几个分支，生成多几个特征，然后分支上都有dilation
convolution，之后又用一些损失来把某些特征拉远，以实现“多样化”特征（这里的损失就是文中的Center-guided
Pair Mining Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#plm%E7%9A%84%E8%AF%9D%E5%8F%AF%E8%83%BD%E6%98%AF%E6%88%91%E8%AE%BA%E6%96%87%E7%9C%8B%E5%B0%91%E4%BA%86%E7%9B%B8%E5%AF%B9%E6%B2%A1%E9%82%A3%E4%B9%88%E5%B8%B8%E8%A7%81%E4%BD%86%E5%AE%83%E8%BF%99%E9%87%8C%E6%8F%90%E5%87%BAprototype%E4%B9%9F%E4%B8%8D%E6%98%AF%E4%B8%80%E8%88%AC%E7%9A%84%E4%B8%BA%E4%BA%86%E4%BF%9D%E7%95%99%E7%9F%A5%E8%AF%86%E8%80%8C%E6%98%AF%E4%B8%BA%E4%BA%86%E6%8F%90%E5%8F%96%E7%9F%A5%E8%AF%86%E5%AE%83%E7%9A%84%E5%A4%A7%E6%A6%82%E6%80%9D%E6%83%B3%E5%92%8Ccnn%E9%87%8C%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%B7%AE%E4%B8%8D%E5%A4%9A%E5%90%A7%E5%9B%A0%E4%B8%BA%E5%AF%B9%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E4%B8%80%E7%A7%8D%E7%90%86%E8%A7%A3%E5%B0%B1%E6%98%AF%E5%AE%83%E5%AD%A6%E5%88%B0%E4%BA%86%E4%B8%80%E4%BA%9B%E7%89%B9%E5%AE%9A%E7%9A%84%E6%A8%A1%E5%BC%8F%E7%84%B6%E5%90%8E%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%E8%BF%99%E9%87%8C%E6%8F%90%E5%8F%96%E7%9F%A5%E8%AF%86%E4%B9%9F%E6%98%AF%E7%B1%BB%E4%BC%BC%E7%9A%84%E7%94%A8%E4%B8%80%E7%BB%84%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84prototype%E7%B1%BB%E6%AF%94%E5%8D%B7%E7%A7%AF%E6%A0%B8%E6%9D%A5%E5%92%8C%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E5%93%88%E8%BE%BE%E7%8E%9B%E7%A7%AF%E5%86%8D%E6%B1%82%E5%9D%87%E5%80%BC%E7%B1%BB%E6%AF%94%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E5%B9%B6%E4%B8%94%E6%98%AF%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%94%A8%E5%90%8C%E4%B8%80%E7%BB%84prototype%E6%89%80%E4%BB%A5%E4%BB%A5%E6%AD%A4%E6%9D%A5%E4%BF%83%E8%BF%9B%E8%B7%A8%E6%A8%A1%E6%80%81%E7%9A%84%E5%85%B1%E6%80%A7%E7%9F%A5%E8%AF%86%E6%8F%90%E5%8F%96%E4%BD%86%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81prototype%E6%8F%90%E5%8F%96%E7%9A%84%E5%B0%B1%E4%B8%80%E5%AE%9A%E6%98%AF%E5%B1%80%E9%83%A8%E4%BF%A1%E6%81%AF%E5%91%A2%E4%BB%85%E4%BB%85%E6%98%AF%E5%9B%A0%E4%B8%BAprototype%E7%9A%84%E5%BD%A2%E7%8A%B6%E6%98%AFcchannel%E7%BB%B4%E7%9A%84%E5%90%91%E9%87%8F%E5%90%97%E8%BF%99%E6%A0%B7%E7%9A%84%E8%AF%9D%E5%80%92%E6%98%AF%E8%83%BD%E7%A8%8D%E5%BE%AE%E8%A7%A3%E9%87%8A%E4%B8%80%E4%B8%8B%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%9B%B4%E6%8E%A5%E7%94%A8%E4%B8%80%E4%B8%AAcnn%E4%BA%86%E6%88%96%E8%80%85%E8%AF%B4%E8%BF%99%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E4%B8%80%E7%A7%8D%E7%89%B9%E6%AE%8A%E7%9A%84cnn%E5%8F%AA%E4%B8%8D%E8%BF%87%E6%98%AF11%E5%8D%B7%E7%A7%AF%E4%BA%86%E8%BF%99%E4%B9%88%E8%AF%B4%E6%9D%A5%E5%85%B6%E5%AE%9E%E8%BF%98%E6%98%AF%E8%AE%B2%E6%95%85%E4%BA%8B%E5%95%8A"><span class="nav-number">4.5.3.</span> <span class="nav-text">PLM的话，可能是我论文看少了，相对没那么常见，但它这里提出prototype也不是一般的为了保留知识，而是为了提取知识。它的大概思想和CNN里的卷积核差不多吧，因为对卷积核的一种理解就是，它学到了一些特定的模式，然后用卷积操作来进行模式匹配；这里提取知识也是类似的，用一组可学习的prototype（类比卷积核），来和图像进行哈达玛积，再求均值（类比卷积操作），并且是两种模态的数据用同一组prototype，所以以此来促进跨模态的共性知识提取（但怎么保证prototype提取的就一定是局部信息呢，仅仅是因为prototype的形状是c（channel）维的向量吗，这样的话倒是能稍微解释一下为什么不直接用一个CNN了，或者说这其实就是一种特殊的CNN，只不过是1×1卷积了。这么说来，其实还是讲故事啊。。）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A4%E5%A4%96%E4%B8%BA%E4%BA%86%E4%BF%9D%E8%AF%81%E5%AE%83%E7%9A%84%E4%B8%8D%E5%90%8Cprototype%E6%8F%90%E5%8F%96%E5%87%BA%E7%9A%84%E7%89%B9%E5%BE%81%E4%B9%9F%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%A4%9A%E6%A0%B7%E6%8F%90%E5%87%BA%E4%BA%86cosine-heterogeneity-loss%E5%85%B6%E5%AE%9E%E5%BE%88%E8%80%81%E5%A5%97%E4%BA%86%E8%B7%9F%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB%E5%B7%AE%E4%B8%8D%E5%A4%9A%E5%8F%AA%E4%B8%8D%E8%BF%87%E8%BF%99%E9%87%8C%E6%8D%A2%E6%88%90%E4%BA%86%E7%94%A8cos%E8%BF%98%E6%9C%89%E4%B8%80%E4%B8%AAdual-center-seperation-loss%E8%BF%99%E4%B8%AA%E6%8D%9F%E5%A4%B1%E4%BC%BC%E4%B9%8E%E5%B9%B6%E6%B2%A1%E6%9C%89%E8%AF%B4%E6%98%8E%E5%AE%83%E8%A7%A3%E5%86%B3%E4%BA%86%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%E6%84%9F%E8%A7%89%E5%BE%88%E5%8F%AF%E8%83%BD%E5%8F%AA%E6%98%AF%E4%B8%BA%E4%BA%86%E6%8F%90%E7%82%B9%E8%80%8C%E5%B7%B2%E6%9C%AC%E8%BA%AB%E5%AE%83%E7%9A%84%E6%80%9D%E6%83%B3%E4%B9%9F%E5%BE%88%E5%B8%B8%E8%A7%81%E4%BA%86%E5%B0%B1%E6%98%AF%E6%8D%A2%E4%B8%AA%E6%B3%95%E5%AD%90%E8%AE%A9%E5%90%8Cid%E7%9A%84%E7%89%B9%E5%BE%81%E8%BF%91%E4%B8%80%E7%82%B9%E4%B8%8D%E5%90%8Cid%E7%9A%84%E7%89%B9%E5%BE%81%E8%BF%9C%E4%B8%80%E7%82%B9"><span class="nav-number">4.5.4.</span> <span class="nav-text">此外，为了保证它的不同prototype提取出的特征也尽可能多样，提出了Cosine
Heterogeneity
Loss（其实很老套了，跟欧氏距离差不多，只不过这里换成了用cos），还有一个Dual-Center
Seperation
Loss（这个损失似乎并没有说明它解决了什么问题，感觉很可能只是为了提点而已。本身它的思想也很常见了，就是换个法子让同id的特征近一点，不同id的特征远一点）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#parameter-hierarchical-optimization-for-visible-infrared-person-re-identification"><span class="nav-number">5.</span> <span class="nav-text">Parameter
Hierarchical Optimization for Visible-Infrared Person
Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv2024.4.11"><span class="nav-number">5.1.</span> <span class="nav-text">arxiv，2024.4.11</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A5%BD%E5%83%8F%E7%A1%AE%E5%AE%9E%E6%9C%89%E7%82%B9%E6%96%B0%E5%A5%87%E6%8A%8A%E5%8F%82%E6%95%B0%E5%88%92%E5%88%86%E4%B8%BA%E4%B8%A4%E7%A7%8D%E7%B1%BB%E5%9E%8B%E4%B8%80%E7%A7%8D%E6%98%AF%E6%AD%A3%E5%B8%B8%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%94%A8%E6%A2%AF%E5%BA%A6%E5%8F%8D%E4%BC%A0%E6%9D%A5%E4%BC%98%E5%8C%96%E4%B8%80%E7%A7%8D%E6%98%AF%E5%9F%BA%E4%BA%8E%E4%B8%80%E4%BA%9B%E8%A7%84%E5%88%99%E6%9D%A5%E4%BC%98%E5%8C%96%E5%BA%94%E8%AF%A5%E6%98%AF%E6%AF%94%E8%BE%83%E4%BC%A0%E7%BB%9F%E7%9A%84%E9%82%A3%E7%A7%8D%E4%BB%A5%E6%AD%A4%E6%9D%A5%E5%87%8F%E5%B0%91%E9%9C%80%E8%A6%81%E6%A2%AF%E5%BA%A6%E5%8F%8D%E4%BC%A0%E6%9D%A5%E4%BC%98%E5%8C%96%E7%9A%84%E5%8F%82%E6%95%B0%E8%BF%99%E4%B8%AA%E6%98%AF%E5%8F%AF%E4%BB%A5%E5%87%8F%E5%B0%91%E4%B8%80%E4%BA%9B%E8%AE%A1%E7%AE%97%E9%87%8F%E4%BD%86%E5%85%B6%E5%AE%83%E5%A5%BD%E5%A4%84%E5%91%A2%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E6%9D%A5%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C%E6%9C%89%E4%BF%9D%E8%AF%81%E5%90%97%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%E8%BF%98%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AAsas%E5%8F%AF%E4%BB%A5%E6%8A%8A%E7%BA%A2%E5%A4%96%E5%8F%AF%E8%A7%81%E5%85%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E7%9A%84%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BA%92%E8%BD%AC%E5%8C%96%E8%BF%99%E4%B8%AA%E5%80%92%E6%98%AF%E5%8F%AF%E4%BB%A5%E7%9C%8B%E7%9C%8B%E6%9C%80%E5%90%8E%E8%BF%98%E6%9C%89%E4%B8%80%E4%B8%AA%E4%B8%80%E8%87%B4%E6%80%A7%E5%AD%A6%E4%B9%A0%E4%BC%B0%E6%91%B8%E7%9D%80%E6%98%AF%E6%8F%90%E5%87%BA%E6%9F%90%E4%B8%AA%E6%8D%9F%E5%A4%B1%E5%8F%88%E6%98%AF%E6%8B%89%E8%BF%91%E5%90%8Cid%E7%9A%84%E4%B8%8D%E5%90%8C%E6%A8%A1%E6%80%81%E7%9A%84%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%9B%9E%E5%A4%B4%E7%9C%8B%E7%9C%8B%E7%8C%9C%E5%AF%B9%E6%B2%A1"><span class="nav-number">5.2.</span> <span class="nav-text">好像确实有点新奇？把参数划分为两种类型，一种是正常在深度学习中用梯度反传来优化，一种是基于一些规则来优化（应该是比较传统的那种），以此来减少需要梯度反传来优化的参数（这个是可以减少一些计算量，但其它好处呢？基于规则来优化，效果有保证吗？原理是什么？）。还提出一个SAS，可以把红外&#x2F;可见光两种模态的图像相互转化。这个倒是可以看看。最后还有一个一致性学习，估摸着是提出某个损失，又是拉近同ID的不同模态的特征之间的距离。。。回头看看猜对没</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bidirectional-multi-step-domain-generalization-for-visible-infrared-person-re-identification"><span class="nav-number">6.</span> <span class="nav-text">Bidirectional
Multi-Step Domain Generalization for Visible-Infrared Person
Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#arxiv2024.3.16"><span class="nav-number">6.1.</span> <span class="nav-text">arxiv，2024.3.16</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%83%B3%E8%A7%A3%E5%86%B3%E6%94%B9%E5%96%84%E7%9A%84%E7%82%B9%E5%9C%A8%E4%BA%8E%E5%A4%A7%E9%83%A8%E5%88%86%E6%96%B9%E6%B3%95%E9%83%BD%E6%98%AF%E6%8A%8Avis%E5%92%8Cir-images%E6%8A%95%E5%BD%B1%E5%88%B0%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%A9%BA%E9%97%B4%E7%84%B6%E5%90%8E%E7%94%A8%E8%BF%99%E4%B8%AA%E7%A9%BA%E9%97%B4%E9%87%8C%E7%9A%84embedding%E8%BF%9B%E8%A1%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%8C%B9%E9%85%8D%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E5%8F%AB%E6%89%80%E8%B0%93%E5%8D%95%E4%B8%AD%E9%97%B4%E5%9F%9F%E7%94%9F%E6%88%90%E8%80%8C%E8%AE%BA%E6%96%87%E6%8F%90%E5%88%B0%E8%BF%99%E4%B8%AA%E6%96%B9%E6%B3%95%E6%8F%90%E5%8F%96%E7%9A%84%E4%BF%A1%E6%81%AF%E4%B8%8D%E5%A4%9F%E5%A5%BD%E5%9B%A0%E4%B8%BA%E5%AE%83%E5%8F%AF%E8%83%BD%E6%8F%90%E5%8F%96%E5%87%BA%E4%BA%86%E4%B8%80%E4%BA%9B%E5%85%AC%E5%85%B1%E7%9A%84%E8%83%8C%E6%99%AF%E4%BF%A1%E6%81%AF%E8%80%8C%E8%BF%99%E9%83%A8%E5%88%86%E4%BF%A1%E6%81%AF%E6%98%AF%E6%B2%A1%E7%94%A8%E7%9A%84%E6%89%80%E4%BB%A5%E4%BB%8E%E7%BB%93%E6%9E%9C%E4%B8%8A%E6%9D%A5%E8%AF%B4%E5%B0%B1%E6%98%AF%E5%8D%95%E5%8D%95%E6%8A%95%E5%BD%B1%E5%88%B0%E5%90%8C%E4%B8%80%E7%A9%BA%E9%97%B4%E6%95%88%E6%9E%9C%E4%B8%8D%E5%A4%9F%E5%A5%BD%E8%87%B3%E4%BA%8E%E8%AF%B4%E6%8F%90%E5%8F%96%E5%87%BA%E8%83%8C%E6%99%AF%E4%BF%A1%E6%81%AF%E4%B8%8D%E5%A5%BD%E5%88%A4%E6%96%AD%E6%98%AF%E8%AE%B2%E6%95%85%E4%BA%8B%E8%BF%98%E6%98%AF%E8%AF%B4%E7%9C%9F%E7%9A%84%E4%B8%BB%E8%A6%81%E6%B2%A1%E6%9C%89%E5%AE%9E%E9%AA%8C%E6%9D%A5%E8%BE%85%E4%BD%90%E9%AA%8C%E8%AF%81%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E7%9A%84%E6%96%B9%E6%B3%95bmdgbidirectional-multi-step-domain-generalization%E4%B8%BB%E8%A6%81%E7%94%B1%E4%B8%A4%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90%E4%B8%80%E6%98%AFpart-prototype-alignment-learning%E8%B4%9F%E8%B4%A3%E6%8F%90%E5%8F%96%E5%87%BA%E5%B1%80%E9%83%A8%E7%9A%84%E8%BA%AB%E4%BD%93%E9%83%A8%E4%BD%8D%E4%BF%A1%E6%81%AF%E4%BA%8C%E6%98%AFbidirectional-multi-step-learning%E9%80%9A%E8%BF%87%E5%A4%9A%E6%AD%A5%E5%AD%A6%E4%B9%A0%E4%BB%8E%E7%BA%A2%E5%A4%96%E5%92%8C%E5%8F%AF%E8%A7%81%E5%85%89%E5%88%86%E5%88%AB%E5%87%BA%E5%8F%91%E4%BB%A5%E5%87%8F%E5%B0%8Fmodality-gap"><span class="nav-number">6.2.</span> <span class="nav-text">这篇论文想解决&#x2F;改善的点在于，大部分方法都是把vis和ir
images投影到一个公共空间，然后用这个空间里的embedding进行相似度匹配（这种方法叫所谓“单中间域生成”），而论文提到，这个方法提取的信息不够好，因为它可能提取出了一些公共的背景信息，而这部分信息是没用的（所以从结果上来说，就是单单投影到同一空间，效果不够好；至于说提取出背景信息，不好判断是讲故事还是说真的。主要没有实验来辅佐验证）。论文提出的方法BMDG（Bidirectional
Multi-Step Domain Generalization）主要由两部分组成，一是part prototype
alignment learning（负责提取出局部的身体部位信息）；二是bidirectional
multi-step
learning（通过多步学习，从红外和可见光分别出发，以减小modality
gap）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#insight"><span class="nav-number">6.3.</span> <span class="nav-text">insight</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA-2"><span class="nav-number">6.4.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95-3"><span class="nav-number">6.5.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#part-prototype-alignment-learning%E4%B8%BB%E8%A6%81%E7%94%B1%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90"><span class="nav-number">6.5.1.</span> <span class="nav-text">part
prototype alignment learning主要由三个部分组成：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#prototype-discovery%E8%BF%99%E4%B8%AA%E9%83%A8%E5%88%86%E6%98%AF%E7%94%A8%E6%9D%A5%E8%8E%B7%E5%8F%96%E8%BA%AB%E4%BD%93%E9%83%A8%E4%BD%8D%E7%BB%86%E8%8A%82%E4%BF%A1%E6%81%AF%E7%9A%84%E5%B9%B6%E4%B8%94%E8%A6%81%E5%9C%A8feature-map%E4%B8%8A%E6%8C%96%E6%8E%98%E5%87%BA%E5%85%B7%E6%9C%89%E5%88%A4%E5%88%AB%E6%80%A7%E7%9A%84%E4%BD%8D%E7%BD%AE%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%E5%AE%83%E7%9A%84%E4%B8%80%E4%B8%AAprototype%E7%9A%84%E5%BD%A2%E7%8A%B6%E6%98%AF"><span class="nav-number">6.5.1.1.</span> <span class="nav-text">1.prototype
discovery（这个部分是用来获取身体部位细节信息的，并且要在feature
map上挖掘出具有判别性的位置）。具体来说，它的一个prototype的形状是</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#clip-driven-semantic-discovery-network-for-visible-infrared-person-re-identification"><span class="nav-number">7.</span> <span class="nav-text">CLIP-Driven
Semantic Discovery Network for Visible-Infrared Person
Re-Identification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tmm2024.1.11"><span class="nav-number">7.1.</span> <span class="nav-text">TMM，2024.1.11</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E5%8F%88%E6%98%AF%E5%BC%95%E5%85%A5clip%E5%A5%BD%E5%A4%9A%E6%96%87%E7%AB%A0%E5%BC%95%E5%85%A5%E4%BA%86clip%E7%9C%8B%E6%9D%A5%E5%8D%95%E7%BA%AF%E5%81%9Avi-reid%E8%BF%98%E5%9C%A8%E9%AD%94%E6%94%B9%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AF%9D%E5%8F%AF%E8%83%BD%E4%B8%8A%E9%99%90%E4%B8%8D%E5%A4%AA%E9%AB%98%E4%BA%86%E8%80%8C%E4%B8%94%E5%A4%AA%E5%8D%B7%E4%BA%86%E5%8F%88%E4%B8%8D%E5%88%9B%E6%96%B0%E6%89%80%E4%BB%A5%E8%AF%95%E7%9D%80%E5%BC%95%E5%85%A5clip%E6%8F%90%E9%AB%98%E8%83%BD%E5%8A%9B%E4%B9%9F%E6%98%BE%E5%BE%97%E5%88%9B%E6%96%B0%E4%B8%80%E7%82%B9%E6%96%87%E7%AB%A0%E7%9A%84%E5%A4%A7%E4%BD%93%E7%9A%84%E4%BA%AE%E7%82%B9%E5%87%BA%E5%8F%91%E7%82%B9%E5%9C%A8%E4%BA%8E%E5%88%A9%E7%94%A8clip%E7%9A%84%E8%8E%B7%E5%8F%96%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84%E8%83%BD%E5%8A%9B%E6%8F%90%E5%8F%96%E4%B8%80%E4%BA%9B%E9%AB%98%E5%B1%82%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%94%A8%E6%9D%A5%E8%BE%85%E5%8A%A9%E6%A3%80%E7%B4%A2%E5%9B%BE%E5%83%8F%E8%BF%99%E4%B8%AA%E5%92%8C%E5%89%8D%E9%9D%A2eees%E9%82%A3%E7%AF%87embedding-and-enriching-explicit-semantics-for-visible-infrared-person-re-identification%E7%9A%84%E5%87%BA%E5%8F%91%E7%82%B9%E6%9C%89%E7%82%B9%E7%B1%BB%E4%BC%BC"><span class="nav-number">7.2.</span> <span class="nav-text">这篇文章又是引入clip（好多文章引入了clip，看来单纯做vi-reid，还在魔改网络的话，可能上限不太高了，而且太卷了，又不创新；所以试着引入clip，提高能力，也“显得”创新一点。文章的大体的亮点&#x2F;出发点在于，利用clip的获取图像语义信息的能力，提取一些高层语义信息，用来辅助检索图像。这个和前面EEES那篇（Embedding
and Enriching Explicit Semantics for Visible-Infrared Person
Re-Identification）的出发点有点类似</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">96</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
