{
	"nodes":[
		{"id":"31cb8a1129818cf9","type":"text","text":"general rag challenge-insight tree","x":-220,"y":-40,"width":269,"height":70},
		{"id":"63c687a0cb3d4771","type":"text","text":"cha：对检索到的文档过于敏感","x":127,"y":-189,"width":250,"height":60},
		{"id":"17136bf525a77b39","type":"text","text":"表现在，一旦我们检索的top-k文档里，无关的比较多，那即使里面是有真正有用的，llm也很有可能认不出来、利用不上。因此，我们会想方设法提高检索质量，还有各种的后处理（比如rerank）来提高结果质量。又比如，我们对检索到的结果也会进行顺序上的调整。但仔细想，这有点没道理。如果一个模型真的足够强，那它应该有能够辨认哪些是有用的，哪些是没用的的能力。起码一个人肯定是有这个能力的，只不过人可能在效率上不如llm而已（读得没那么快）","x":440,"y":-369,"width":380,"height":420},
		{"id":"18f9bb8c6bd35029","type":"text","text":"但这个问题，更根本一点讲，其实是跟llm的机制有关。对顺序敏感，那已经引起了注意，一方面有在实践中利用这个bias提高效果的，也有尝试改掉这个问题的，还有从理论角度研究为什么会产生这个问题的。当然也可以归咎到文本能力不够，但这只是一个方面而已。context window长了，依然有这个顺序上的bias\n好像从rag的角度上看，也只能尽量提高检索的效果，保证最后喂到llm的文档列表是符合它的bias的","x":860,"y":-353,"width":400,"height":389},
		{"id":"d7a079b4c5882e65","type":"text","text":"cha：如何更好检索","x":127,"y":200,"width":250,"height":60},
		{"id":"fc945753ea676ce7","type":"text","text":"insight：对query进行分解，然后利用这些sub-query进行检索，扩大检索面（Question Decomposition for Retrieval-Augmented Generation，acl 25 workshop）","x":440,"y":140,"width":380,"height":90},
		{"id":"8ac1c30f0df6f109","x":860,"y":133,"width":400,"height":105,"type":"text","text":"扩大知识面之后，每个sub-query对应可能有k个检索文档。我们要从中进行选择，可以考虑建模成一个多臂老虎机（MAB），每个sub-query对应的文档集合相关的概率可以被建模成一个分布（这个分布可以在探索的过程中不断被优化完善）（paper：Query Decomposition for RAG: Balancing Exploration-Exploitation）"}
	],
	"edges":[]
}