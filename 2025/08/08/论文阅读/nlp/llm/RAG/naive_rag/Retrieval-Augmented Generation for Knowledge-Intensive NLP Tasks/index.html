<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Venue：NIPS 20 date：2021-04-12 动机： 语言模型的幻觉问题是一直存在的；同时，语言模型它是通过把知识编码到它的参数里来进行记忆的，因此知识的修改&#x2F;更新是很麻烦的，需要微调。故，考虑引入外部知识库，来进行辅助。一来可以减少幻觉，二来可以外部知识库的更新还是方便的，如果有新的更改，那么对外部知识库（例如维基百科）进行操作就行，就不需要微调模型了。这一通操作就是所谓re">
<meta property="og:type" content="article">
<meta property="og:title" content="Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks">
<meta property="og:url" content="https://blueeemouse.github.io/2025/08/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/naive_rag/Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="Venue：NIPS 20 date：2021-04-12 动机： 语言模型的幻觉问题是一直存在的；同时，语言模型它是通过把知识编码到它的参数里来进行记忆的，因此知识的修改&#x2F;更新是很麻烦的，需要微调。故，考虑引入外部知识库，来进行辅助。一来可以减少幻觉，二来可以外部知识库的更新还是方便的，如果有新的更改，那么对外部知识库（例如维基百科）进行操作就行，就不需要微调模型了。这一通操作就是所谓re">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blueeemouse.github.io/images/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks/framework.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks/RAG-token.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks/RAG-seq.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks/RAG-token-next-token.png">
<meta property="article:published_time" content="2025-08-08T15:36:00.000Z">
<meta property="article:modified_time" content="2025-08-18T16:59:34.930Z">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blueeemouse.github.io/images/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks/framework.png">

<link rel="canonical" href="https://blueeemouse.github.io/2025/08/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/naive_rag/Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/08/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/naive_rag/Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-08-08 23:36:00" itemprop="dateCreated datePublished" datetime="2025-08-08T23:36:00+08:00">2025-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-19 00:59:34" itemprop="dateModified" datetime="2025-08-19T00:59:34+08:00">2025-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/naive-rag/" itemprop="url" rel="index"><span itemprop="name">naive rag</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="venuenips-20">Venue：NIPS 20</h1>
<h1 id="date2021-04-12">date：2021-04-12</h1>
<h1 id="动机">动机：</h1>
<h2 id="语言模型的幻觉问题是一直存在的同时语言模型它是通过把知识编码到它的参数里来进行记忆的因此知识的修改更新是很麻烦的需要微调故考虑引入外部知识库来进行辅助一来可以减少幻觉二来可以外部知识库的更新还是方便的如果有新的更改那么对外部知识库例如维基百科进行操作就行就不需要微调模型了这一通操作就是所谓retrieval-augmented检索增强">语言模型的幻觉问题是一直存在的；同时，语言模型它是通过把知识编码到它的参数里来进行记忆的，因此知识的修改/更新是很麻烦的，需要微调。故，考虑引入外部知识库，来进行辅助。一来可以减少幻觉，二来可以外部知识库的更新还是方便的，如果有新的更改，那么对外部知识库（例如维基百科）进行操作就行，就不需要微调模型了。这一通操作就是所谓retrieval
augmented，检索增强</h2>
<h2 id="但是此前的研究主要是聚焦于把检索增强操作应用到抽取型任务中而忽略了把它应用到生成型任务中论文主要就是填补这个空缺将检索增强操作应用到语言模型的一般生成上从而缓解幻觉">但是，此前的研究主要是聚焦于把检索增强操作应用到抽取型任务中，而忽略了把它应用到生成型任务中。论文主要就是填补这个空缺，将检索增强操作应用到语言模型的一般生成上，从而缓解幻觉</h2>
<h1 id="insight">insight：</h1>
<h2 id="感觉主要是在当时能想到把检索增强应用到生成型任务上了算是弥补了当时的一个空缺吧有点开山之作的意思毕竟万引了但是现在有了llm以后所有的nlp任务几乎都被建模成了生成型的任务并且一个llm就能完成大部分所以现在一提起检索增强应该立马想到的都是检索增强生成也即rag吧但似乎retrieval-augmented这个概念并不是一经提出就是配套于generation的啊">感觉主要是在当时能想到把检索增强应用到生成型任务上了？算是弥补了当时的一个空缺吧（有点开山之作的意思？毕竟万引了）。但是现在有了llm以后，所有的nlp任务几乎都被建模成了生成型的任务，并且一个llm就能完成大部分，所以现在一提起检索增强，应该立马想到的都是检索增强生成，也即RAG吧……但似乎Retrieval
Augmented这个概念并不是一经提出就是配套于Generation的啊</h2>
<h1 id="contribution">contribution：</h1>
<h2 id="提出了一套框架可以端到端训练把检索增强操作应用到生成式任务上了并且训练好的retriever包含一个query-encoder以及一个passage-encoder是可以迁移的">提出了一套框架，可以端到端训练，把检索增强操作应用到生成式任务上了；并且训练好的retriever（包含一个query
encoder以及一个passage encoder）是可以迁移的</h2>
<span id="more"></span>
<h1 id="method">method：</h1>
<h2 id="大体的框架就是这样的">大体的框架就是这样的</h2>
<h2 id="section"><img src="/images/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks/framework.png"></h2>
<h2 id="流程也比较简单有一套retriever包含一个query-encoder和document-encoder可以根据输入检索出知识库里相关的文档检索得到top-k相关的文档后利用这些文档用generator进行最终的生成下面分步讲一下">流程也比较简单。有一套retriever（包含一个query
encoder和document
encoder），可以根据输入检索出知识库里相关的文档；检索得到top-K相关的文档后，利用这些文档，用generator进行最终的生成。下面分步讲一下</h2>
<h2 id="retriever">1. retriever</h2>
<h3 id="这里的设计follow的是dpr里的设计也就是设计了一个bi-encoder即上文提到的query-encoder和document-encoderquery-encoder负责把query编码成一个稠密的向量所谓稠密的向量是和稀疏向量对应的稀疏向量指的是类似于tf-idf方法得到的那种向量也就是统计输入的某一些特征得到的那么稠密向量相对的指的就是现在神经网络编码得到的若干维的向量每个维度有特定的潜在含义向量整体也有一定的语义只不过可能人类无法读懂但模型可以读懂因此简单来说稠密向量重语义而稀疏向量一般是通过词重叠来检索仅适用于简单的关键词匹配document-encoder也类似负责把document编码成稠密向量根据query进行检索的依据则是document-vector和query-vector的相似性常用向量内积来衡量论文的确也是用的这种方法">这里的设计follow的是<a href="%5Bhttps://arxiv.org/abs/2004.04906%5D(https://arxiv.org/abs/2004.04906).">DPR</a>里的设计，也就是设计了一个bi-encoder，即上文提到的query
encoder和document encoder。query
encoder负责把query编码成一个稠密的向量（所谓稠密的向量，是和稀疏向量对应的。稀疏向量指的是类似于TF-IDF方法得到的那种向量，也就是统计输入的某一些特征得到的。那么稠密向量，相对的，指的就是现在神经网络编码得到的若干维的向量，每个维度有特定的潜在含义，向量整体也有一定的语义（只不过可能人类无法读懂，但模型可以读懂）。因此简单来说，稠密向量重语义，而稀疏向量一般是通过词重叠来检索，仅适用于简单的关键词匹配）；document
encoder也类似，负责把document编码成稠密向量。根据query进行检索的依据，则是document
vector和query
vector的相似性（常用向量内积来衡量。论文的确也是用的这种方法）</h3>
<h3 id="因为论文是要检索出可能性top-k高的document如果挨个拿query-vector和document-vector进行相似性计算无疑计算量是极大的因为我们是把外部知识库转化成dense-vector稠密向量的而外部知识库一般是非常大的所以这里会用一些现有的近似方法来做到在sub-linear的时间复杂度内解决这类问题其实有一个名称叫maximum-inner-product-search-mips-problem">因为论文是要检索出可能性top-K高的document，如果挨个拿query
vector和document
vector进行相似性计算，无疑计算量是极大的（因为我们是把外部知识库转化成dense
vector稠密向量的，而外部知识库一般是非常大的）。所以这里会用一些现有的近似方法来做到，在sub-linear的时间复杂度内解决。这类问题其实有一个名称，叫Maximum
Inner Product Search (MIPS) problem</h3>
<h2 id="generator">2. generator</h2>
<h3 id="没什么太多好说的按理说随便一个encoder-decoder模型都是可以的论文选用的是bart-large400m参数这在当时应该算是很好很大的模型了">没什么太多好说的，按理说随便一个encoder-decoder模型都是可以的。论文选用的是bart-large，400M参数（这在当时应该算是很好很大的模型了）</h3>
<h3 id="关于检索到的文档z和原始输入x之间的融合论文仅仅是把二者concat起来在token的维度上bart是支持变长序列的处理的前提是不要超过长度限制别忘了batch-input的形状一般是batch-seq_len-embed_dim">关于检索到的文档z和原始输入x之间的融合，论文仅仅是把二者concat起来（在token的维度上。bart是支持变长序列的处理的，前提是不要超过长度限制。别忘了batch
input的形状一般是<span class="math inline">\((batch, seq\_len,
embed\_dim)\)</span>）</h3>
<h2 id="training">3. training</h2>
<h3 id="这套模型的训练完全是端到端的所以关于retriever部分我们不会给出应该检索哪些文档而是全都让模型自己学习这里怀疑之所以能做到不学崩可能是因为retriever初始化的时候用的权重效果还不错以及不知道具体训练的时候有没有什么trick也不得不说端到端训练在某种程度上是方便的">这套模型的训练，完全是端到端的。所以关于retriever部分，我们不会给出“应该检索哪些文档”，而是全都让模型自己学习（这里怀疑，之所以能做到不学崩，可能是因为retriever初始化的时候用的权重效果还不错？以及不知道具体训练的时候有没有什么trick。也不得不说，端到端训练，在某种程度上是方便的）</h3>
<h3 id="训练的目标则是给定训练数据对也即x_iy_i这样的数据我们要最大化py_ix_i就是说要最大化似然函数而实际待优化的函数形式则形如-sum_jlogpy_jx_j优化器用的是adam">训练的目标，则是给定训练数据对（也即<span class="math inline">\((x_{i},y_{i})\)</span>这样的数据），我们要最大化<span class="math inline">\(p(y_{i}|x_{i})\)</span>，就是说要最大化似然函数。而实际待优化的函数形式则形如<span class="math inline">\(-\sum_{j}log(p(y_{j}|x_{j}))\)</span>；优化器用的是Adam</h3>
<h3 id="另外有一个小点是训练时论文说明并不会更新document-encoder也就是把它冻结了这主要是因为上文提到的在sub-linear时间复杂度内近似解决搜索出top-k个document的方法它需要我们事先处理得到外部知识的vector然后把它们进行某种排列如果document-encoder更新了但我们不更新之前得到的知识库的vector那就会出现对不上的情况document-encoder都更新了它输出的vector也是会更新的而如果我们决定要更新document-vector那就意味着需要再次遍历知识库并更新这就带来了很大的计算负担故论文考虑不更新document-encoder当然硬是要更新的话间隔若干epoch再更新也不是不可以考虑或者用上其它的trick不过看论文的意思好像是即使更新了document-encoder效果也没有好上特别多">另外有一个小点是，训练时，论文说明，并不会更新document
encoder，也就是把它冻结了。这主要是因为，上文提到的在sub-linear时间复杂度内近似解决搜索出top-K个document的方法，它需要我们事先处理得到外部知识的vector，然后把它们进行某种排列。如果document
encoder更新了，但我们不更新之前得到的知识库的vector，那就会出现对不上的情况（document
encoder都更新了，它输出的vector也是会更新的）；而如果我们决定要更新document
vector，那就意味着需要再次遍历知识库并更新，这就带来了很大的计算负担。故论文考虑不更新document
encoder。当然，硬是要更新的话，间隔若干epoch再更新，也不是不可以考虑；或者用上其它的trick。不过看论文的意思，好像是即使更新了document
encoder，效果也没有好上特别多？</h3>
<h2 id="解码">4. 解码</h2>
<h3 id="解码算是比较关键的一个部分也是比较复杂的一个部分论文里考虑了两种解码方式一种是rag-sequence一种是rag-token前者的含义是我们解码生成seq的时候仅依赖于一篇document全程都是后者则表示解码生成seq的时候生成每个token时它依赖的document都可以是不同的-显然前者计算量会小一些后者按理说上限会高一些但计算量也会更大一些">解码算是比较关键的一个部分，也是比较复杂的一个部分。论文里考虑了两种解码方式，一种是RAG-sequence，一种是RAG-token。前者的含义是，我们解码生成seq的时候，仅依赖于一篇document（全程都是）；后者则表示，解码生成seq的时候，生成每个token时，它依赖的document都可以是不同的<br>显然，前者计算量会小一些，后者按理说上限会高一些，但计算量也会更大一些</h3>
<h4 id="section-1"><img src="/images/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks/RAG-token.png"></h4>
<h4 id="上为rag-token的解码的公式可以看到每一步解码生成一个token的时候都是会对我们检索到的top-k个document进行marginalization">上为RAG-token的解码的公式。可以看到，每一步解码生成一个token的时候，都是会对我们检索到的top-k个document进行marginalization</h4>
<h4 id="section-2"><img src="/images/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks/RAG-seq.png"></h4>
<h4 id="这个则是rag-sequence的解码生成公式此处我们虽然也会对document进行边缘化但每次一旦选定一个document以后我们就会用它来生成一个完整的序列">这个则是RAG-sequence的解码生成公式。此处我们虽然也会对document进行边缘化，但每次一旦选定一个document以后，我们就会用它来生成一个完整的序列</h4>
<h3 id="rag-token">RAG-token</h3>
<h4 id="整体的rag-token方式的解码公式其实如上所示这里进一步细化一下我们给出每一步也就是生成下一个token的公式但这里我们其实只是知道了下一个token的概率分布那具体下一个token取哪个呢这里就涉及到一些搜索的策略了而此处论文采用的是beam-searchbeam-search也即束搜索它在第一步采取概率top-k高的结果此时就得到k个候选序列只不过它们都是单个token的序列这个k是超参数之后基于这k个序列进一步的进行解码然后从这些所有的候选的2-token的序列中挑选出top-k高的概率的序列作为最终得到的2-token的序列的结果后面的解码以此类推所以可以看到beam-search的关键在于每一步都挑选出概率top-k高的序列这样既保证了合理性也有一定的多样性">整体的RAG-token方式的解码公式，其实如上所示。这里进一步细化一下，我们给出每一步（也就是生成下一个token）的公式：<img src="/images/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks/RAG-token-next-token.png">但这里我们其实只是知道了下一个token的概率分布。那具体下一个token取哪个呢？这里就涉及到一些搜索的策略了。而此处论文采用的是beam
search（beam
search，也即束搜索，它在第一步采取概率top-k高的结果（此时就得到k个候选序列，只不过它们都是单个token的序列。这个k是超参数），之后基于这k个序列进一步的进行解码，然后从这些所有的候选的2-token的序列中，挑选出top-k高的概率的序列，作为最终得到的2-token的序列的结果；后面的解码以此类推。所以可以看到，beam
search的关键在于，每一步都挑选出概率top-k高的序列，这样既保证了合理性，也有一定的多样性）</h4>
<h3 id="rag-sequence">RAG-sequence</h3>
<h4 id="根据上面的rag-sequence的解码公式可以看到我们生成的概率不能分解为每个token的生成概率之积最直观的看最外层是一个求和符号那基本就意味着这个概率表达式不能分解为乘积的形式而从公式的含义上看这个解码公式是对document进行边缘化我们遍历所有的document假设用它进行生成得到一个变量乘上检索到这个document的概率作为权重最后再求和得到概率这样从语义上看也不能分解成token概率之积因为这里是序列概率之和-这带来的问题是我们不能直接用一个beam-search来解码得到最终结果了上面的beam-search的应用它是在解码得到下一个token的时候挑出出现概率top-k的序列但此处我们的解码方式就不是per-token的因此不能直接用标准的beam-search">根据上面的RAG-sequence的解码公式，可以看到，我们生成的概率不能分解为每个token的生成概率之积（最直观的，看最外层，是一个求和符号，那基本就意味着这个概率表达式不能分解为乘积的形式。而从公式的含义上看，这个解码公式是对document进行边缘化，我们遍历所有的document，假设用它进行生成，得到一个变量，乘上检索到这个document的概率作为权重，最后再求和，得到概率。这样从语义上看，也不能分解成token概率之积，因为这里是序列概率之和）<br>这带来的问题是，我们不能直接用一个beam
search来解码得到最终结果了（上面的beam
search的应用，它是在解码得到下一个token的时候挑出出现概率top-k的序列。但此处我们的解码方式就不是per
token的，因此不能直接用标准的beam search）</h4>
<h4 id="那么针对这种情况一种可以考虑的解码方式是thorough-decoding而论文的解码策略则是基于此的一种变体下面先讲讲什么是thorough-decoding">那么针对这种情况，一种可以考虑的解码方式是Thorough
Decoding。而论文的解码策略则是基于此的一种变体。下面先讲讲什么是Thorough
Decoding</h4>
<h5 id="这个解码策略它是从document出发再进行beam-search也就是我们遍历所有的document对每个document我们假设检索到的是它然后进行生成此处的生成是beam-search不妨假设我们的beam-search是取top-k高的序列则每个document都能对应生成k个序列并且每个序列都有一个概率记为p_thetayxz按理说假设一共n个document则此时会得到nk个序列只不过这里面可能会有一些重复的我们把重复的去掉即可但得到的概率p_thetayxz需要保留后面有用-此时这些去重后的序列它们放到一起组成了一个候选序列的集合论文中称为set-of-hypothesis我们认为这些序列就是出现概率相对较高的最终的结果应该是这些序列里出现概率最高的那个那就需要算出这些候选序列的出现概率怎么算呢就是对document进行边缘化但这里应该是用到所有的document吧那上面我们都是遍历所有的document这里也应该用所有的document进行边缘化才对啊不应该再用top-k的document了吧可能得看一下代码了论文里这一块写得不够清楚我们对每个候选序列算出它们的出现概率取最大概率对应的序列即可-然而此处又可能有一个问题计算一个序列出现的概率需要得到在每个document下解码出这个序列的概率而这个序列能被我们计算它的概率就说明它一定出现在了set-of-hypothesis里了所以它一定是由某个document解码得到的故我们一定已经有一个p_thetayxz_i可是我们要的却是所有的p_thetayxz_j-z_jin-z-所以有可能某些document下解码得到该序列的概率我们没有那怎么办就得算一下了即我们前向算一下在当前的documentz_i下解码得到当前序列y的概率这就是thorough-decoding">这个解码策略，它是从document出发，再进行beam
search。也就是，我们遍历所有的document，对每个document，我们假设检索到的是它，然后进行生成（此处的生成是beam
search）。不妨假设我们的beam
search是取top-k高的序列，则每个document都能对应生成k个序列（并且每个序列都有一个概率，记为<span class="math inline">\(p_{\theta}(y|x,z)\)</span>）。按理说，假设一共n个document，则此时会得到nk个序列。只不过这里面可能会有一些重复的，我们把重复的去掉即可（但得到的概率<span class="math inline">\(p_{\theta}(y|x,z)\)</span>需要保留，后面有用）<br>此时这些去重后的序列，它们放到一起，组成了一个候选序列的集合，论文中称为set
of
hypothesis。我们认为，这些序列就是出现概率相对较高的，最终的结果应该是这些序列里出现概率最高的那个。那就需要算出这些候选序列的出现概率。怎么算呢？就是对document进行边缘化<strong><em>（但这里应该是用到所有的document吧？那上面我们都是遍历所有的document，这里也应该用所有的document进行边缘化才对啊？不应该再用top-k的document了吧？可能得看一下代码了，论文里这一块写得不够清楚）</em></strong>，我们对每个候选序列，算出它们的出现概率，取最大概率对应的序列即可<br>然而此处又可能有一个问题。计算一个序列出现的概率，需要得到在每个document下，解码出这个序列的概率。而这个序列能被我们计算它的概率，就说明它一定出现在了set
of
hypothesis里了，所以它一定是由某个document解码得到的，故我们一定已经有一个<span class="math inline">\(p_{\theta}(y|x,z_{i})\)</span>，可是我们要的却是所有的<span class="math inline">\(p_{\theta}(y|x,z_{j}),\ \ \ z_{j}\in
Z\)</span><br>所以有可能某些document下，解码得到该序列的概率我们没有。那怎么办？就得算一下了，即我们前向算一下在当前的document<span class="math inline">\(z_{i}\)</span>下，解码得到当前序列y的概率。这就是Thorough
Decoding</h5>
<h4 id="我们发现thorough-decoding中可能会需要补充算一些序列的解码概率并且set-of-hypothesis一般还比较大这就导致需要补充算的概率可能还不少无疑会增加很多计算量故论文的fast-decoding就对此作出了一些改进简单来说在上面我们如果遇到p_thetayxz_i没算出来会重新算一下但在fast-decoding中我们直接把这个概率近似为0直观上可以这样理解在document-z_i中进行beam-search的时候竟然没搜索出当前的这个序列y那某种程度上就表明了基于这个document想解码出当前序列y其实是比较难的概率是比较小的那么为了方便计算不妨将这个概率近似为0-由此少了很多的计算量至于效果论文应该是有实验过发现两种在结果上差不太多但是速度上fast-decoding还是快了比较多所以最终才采取的这个解码策略吧本质上这就是一个质量和速度的trade-off">我们发现，Thorough
Decoding中，可能会需要补充算一些序列的解码概率。并且set of
hypothesis一般还比较大，这就导致需要补充算的概率可能还不少，无疑会增加很多计算量。故论文的Fast
Decoding就对此作出了一些改进。简单来说，在上面，我们如果遇到<span class="math inline">\(p_{\theta}(y|x,z_{i})\)</span>没算出来，会重新算一下；但在Fast
Decoding中，我们直接把这个概率近似为0（直观上可以这样理解：在document
<span class="math inline">\(z_{i}\)</span>中进行beam
search的时候，竟然没搜索出当前的这个序列y，那某种程度上就表明了，基于这个document，想解码出当前序列y，其实是比较难的，概率是比较小的。那么，为了方便计算，不妨将这个概率近似为0）<br>由此少了很多的计算量。至于效果，论文应该是有实验过，发现两种在结果上差不太多，但是速度上Fast
Decoding还是快了比较多，所以最终才采取的这个解码策略吧。本质上，这就是一个质量和速度的trade-off</h4>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/08/06/algo/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E7%BD%91%E6%A0%BC%E5%9B%BEDP/%E5%9F%BA%E7%A1%80%E7%B1%BB/%E7%BB%9F%E8%AE%A1%E5%BC%82%E6%88%96%E5%80%BC%E4%B8%BA%E7%BB%99%E5%AE%9A%E5%80%BC%E7%9A%84%E8%B7%AF%E5%BE%84%E6%95%B0%E7%9B%AE/" rel="prev" title="统计异或值为给定值的路径数目">
      <i class="fa fa-chevron-left"></i> 统计异或值为给定值的路径数目
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/08/11/algo/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E8%83%8C%E5%8C%85/0-1%E8%83%8C%E5%8C%85/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/" rel="next" title="分割等和子集">
      分割等和子集 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#venuenips-20"><span class="nav-number">1.</span> <span class="nav-text">Venue：NIPS 20</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#date2021-04-12"><span class="nav-number">2.</span> <span class="nav-text">date：2021-04-12</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA"><span class="nav-number">3.</span> <span class="nav-text">动机：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BB%E8%A7%89%E9%97%AE%E9%A2%98%E6%98%AF%E4%B8%80%E7%9B%B4%E5%AD%98%E5%9C%A8%E7%9A%84%E5%90%8C%E6%97%B6%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AE%83%E6%98%AF%E9%80%9A%E8%BF%87%E6%8A%8A%E7%9F%A5%E8%AF%86%E7%BC%96%E7%A0%81%E5%88%B0%E5%AE%83%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8C%E6%9D%A5%E8%BF%9B%E8%A1%8C%E8%AE%B0%E5%BF%86%E7%9A%84%E5%9B%A0%E6%AD%A4%E7%9F%A5%E8%AF%86%E7%9A%84%E4%BF%AE%E6%94%B9%E6%9B%B4%E6%96%B0%E6%98%AF%E5%BE%88%E9%BA%BB%E7%83%A6%E7%9A%84%E9%9C%80%E8%A6%81%E5%BE%AE%E8%B0%83%E6%95%85%E8%80%83%E8%99%91%E5%BC%95%E5%85%A5%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E6%9D%A5%E8%BF%9B%E8%A1%8C%E8%BE%85%E5%8A%A9%E4%B8%80%E6%9D%A5%E5%8F%AF%E4%BB%A5%E5%87%8F%E5%B0%91%E5%B9%BB%E8%A7%89%E4%BA%8C%E6%9D%A5%E5%8F%AF%E4%BB%A5%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E7%9A%84%E6%9B%B4%E6%96%B0%E8%BF%98%E6%98%AF%E6%96%B9%E4%BE%BF%E7%9A%84%E5%A6%82%E6%9E%9C%E6%9C%89%E6%96%B0%E7%9A%84%E6%9B%B4%E6%94%B9%E9%82%A3%E4%B9%88%E5%AF%B9%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E4%BE%8B%E5%A6%82%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E8%BF%9B%E8%A1%8C%E6%93%8D%E4%BD%9C%E5%B0%B1%E8%A1%8C%E5%B0%B1%E4%B8%8D%E9%9C%80%E8%A6%81%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E4%BA%86%E8%BF%99%E4%B8%80%E9%80%9A%E6%93%8D%E4%BD%9C%E5%B0%B1%E6%98%AF%E6%89%80%E8%B0%93retrieval-augmented%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA"><span class="nav-number">3.1.</span> <span class="nav-text">语言模型的幻觉问题是一直存在的；同时，语言模型它是通过把知识编码到它的参数里来进行记忆的，因此知识的修改&#x2F;更新是很麻烦的，需要微调。故，考虑引入外部知识库，来进行辅助。一来可以减少幻觉，二来可以外部知识库的更新还是方便的，如果有新的更改，那么对外部知识库（例如维基百科）进行操作就行，就不需要微调模型了。这一通操作就是所谓retrieval
augmented，检索增强</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%86%E6%98%AF%E6%AD%A4%E5%89%8D%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%BB%E8%A6%81%E6%98%AF%E8%81%9A%E7%84%A6%E4%BA%8E%E6%8A%8A%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E6%93%8D%E4%BD%9C%E5%BA%94%E7%94%A8%E5%88%B0%E6%8A%BD%E5%8F%96%E5%9E%8B%E4%BB%BB%E5%8A%A1%E4%B8%AD%E8%80%8C%E5%BF%BD%E7%95%A5%E4%BA%86%E6%8A%8A%E5%AE%83%E5%BA%94%E7%94%A8%E5%88%B0%E7%94%9F%E6%88%90%E5%9E%8B%E4%BB%BB%E5%8A%A1%E4%B8%AD%E8%AE%BA%E6%96%87%E4%B8%BB%E8%A6%81%E5%B0%B1%E6%98%AF%E5%A1%AB%E8%A1%A5%E8%BF%99%E4%B8%AA%E7%A9%BA%E7%BC%BA%E5%B0%86%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E6%93%8D%E4%BD%9C%E5%BA%94%E7%94%A8%E5%88%B0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%80%E8%88%AC%E7%94%9F%E6%88%90%E4%B8%8A%E4%BB%8E%E8%80%8C%E7%BC%93%E8%A7%A3%E5%B9%BB%E8%A7%89"><span class="nav-number">3.2.</span> <span class="nav-text">但是，此前的研究主要是聚焦于把检索增强操作应用到抽取型任务中，而忽略了把它应用到生成型任务中。论文主要就是填补这个空缺，将检索增强操作应用到语言模型的一般生成上，从而缓解幻觉</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#insight"><span class="nav-number">4.</span> <span class="nav-text">insight：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%84%9F%E8%A7%89%E4%B8%BB%E8%A6%81%E6%98%AF%E5%9C%A8%E5%BD%93%E6%97%B6%E8%83%BD%E6%83%B3%E5%88%B0%E6%8A%8A%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E5%BA%94%E7%94%A8%E5%88%B0%E7%94%9F%E6%88%90%E5%9E%8B%E4%BB%BB%E5%8A%A1%E4%B8%8A%E4%BA%86%E7%AE%97%E6%98%AF%E5%BC%A5%E8%A1%A5%E4%BA%86%E5%BD%93%E6%97%B6%E7%9A%84%E4%B8%80%E4%B8%AA%E7%A9%BA%E7%BC%BA%E5%90%A7%E6%9C%89%E7%82%B9%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C%E7%9A%84%E6%84%8F%E6%80%9D%E6%AF%95%E7%AB%9F%E4%B8%87%E5%BC%95%E4%BA%86%E4%BD%86%E6%98%AF%E7%8E%B0%E5%9C%A8%E6%9C%89%E4%BA%86llm%E4%BB%A5%E5%90%8E%E6%89%80%E6%9C%89%E7%9A%84nlp%E4%BB%BB%E5%8A%A1%E5%87%A0%E4%B9%8E%E9%83%BD%E8%A2%AB%E5%BB%BA%E6%A8%A1%E6%88%90%E4%BA%86%E7%94%9F%E6%88%90%E5%9E%8B%E7%9A%84%E4%BB%BB%E5%8A%A1%E5%B9%B6%E4%B8%94%E4%B8%80%E4%B8%AAllm%E5%B0%B1%E8%83%BD%E5%AE%8C%E6%88%90%E5%A4%A7%E9%83%A8%E5%88%86%E6%89%80%E4%BB%A5%E7%8E%B0%E5%9C%A8%E4%B8%80%E6%8F%90%E8%B5%B7%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E5%BA%94%E8%AF%A5%E7%AB%8B%E9%A9%AC%E6%83%B3%E5%88%B0%E7%9A%84%E9%83%BD%E6%98%AF%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%E4%B9%9F%E5%8D%B3rag%E5%90%A7%E4%BD%86%E4%BC%BC%E4%B9%8Eretrieval-augmented%E8%BF%99%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%B9%B6%E4%B8%8D%E6%98%AF%E4%B8%80%E7%BB%8F%E6%8F%90%E5%87%BA%E5%B0%B1%E6%98%AF%E9%85%8D%E5%A5%97%E4%BA%8Egeneration%E7%9A%84%E5%95%8A"><span class="nav-number">4.1.</span> <span class="nav-text">感觉主要是在当时能想到把检索增强应用到生成型任务上了？算是弥补了当时的一个空缺吧（有点开山之作的意思？毕竟万引了）。但是现在有了llm以后，所有的nlp任务几乎都被建模成了生成型的任务，并且一个llm就能完成大部分，所以现在一提起检索增强，应该立马想到的都是检索增强生成，也即RAG吧……但似乎Retrieval
Augmented这个概念并不是一经提出就是配套于Generation的啊</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#contribution"><span class="nav-number">5.</span> <span class="nav-text">contribution：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E5%A5%97%E6%A1%86%E6%9E%B6%E5%8F%AF%E4%BB%A5%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83%E6%8A%8A%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E6%93%8D%E4%BD%9C%E5%BA%94%E7%94%A8%E5%88%B0%E7%94%9F%E6%88%90%E5%BC%8F%E4%BB%BB%E5%8A%A1%E4%B8%8A%E4%BA%86%E5%B9%B6%E4%B8%94%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84retriever%E5%8C%85%E5%90%AB%E4%B8%80%E4%B8%AAquery-encoder%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%B8%AApassage-encoder%E6%98%AF%E5%8F%AF%E4%BB%A5%E8%BF%81%E7%A7%BB%E7%9A%84"><span class="nav-number">5.1.</span> <span class="nav-text">提出了一套框架，可以端到端训练，把检索增强操作应用到生成式任务上了；并且训练好的retriever（包含一个query
encoder以及一个passage encoder）是可以迁移的</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#method"><span class="nav-number">6.</span> <span class="nav-text">method：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E4%BD%93%E7%9A%84%E6%A1%86%E6%9E%B6%E5%B0%B1%E6%98%AF%E8%BF%99%E6%A0%B7%E7%9A%84"><span class="nav-number">6.1.</span> <span class="nav-text">大体的框架就是这样的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#section"><span class="nav-number">6.2.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E7%A8%8B%E4%B9%9F%E6%AF%94%E8%BE%83%E7%AE%80%E5%8D%95%E6%9C%89%E4%B8%80%E5%A5%97retriever%E5%8C%85%E5%90%AB%E4%B8%80%E4%B8%AAquery-encoder%E5%92%8Cdocument-encoder%E5%8F%AF%E4%BB%A5%E6%A0%B9%E6%8D%AE%E8%BE%93%E5%85%A5%E6%A3%80%E7%B4%A2%E5%87%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E9%87%8C%E7%9B%B8%E5%85%B3%E7%9A%84%E6%96%87%E6%A1%A3%E6%A3%80%E7%B4%A2%E5%BE%97%E5%88%B0top-k%E7%9B%B8%E5%85%B3%E7%9A%84%E6%96%87%E6%A1%A3%E5%90%8E%E5%88%A9%E7%94%A8%E8%BF%99%E4%BA%9B%E6%96%87%E6%A1%A3%E7%94%A8generator%E8%BF%9B%E8%A1%8C%E6%9C%80%E7%BB%88%E7%9A%84%E7%94%9F%E6%88%90%E4%B8%8B%E9%9D%A2%E5%88%86%E6%AD%A5%E8%AE%B2%E4%B8%80%E4%B8%8B"><span class="nav-number">6.3.</span> <span class="nav-text">流程也比较简单。有一套retriever（包含一个query
encoder和document
encoder），可以根据输入检索出知识库里相关的文档；检索得到top-K相关的文档后，利用这些文档，用generator进行最终的生成。下面分步讲一下</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retriever"><span class="nav-number">6.4.</span> <span class="nav-text">1. retriever</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%99%E9%87%8C%E7%9A%84%E8%AE%BE%E8%AE%A1follow%E7%9A%84%E6%98%AFdpr%E9%87%8C%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AE%BE%E8%AE%A1%E4%BA%86%E4%B8%80%E4%B8%AAbi-encoder%E5%8D%B3%E4%B8%8A%E6%96%87%E6%8F%90%E5%88%B0%E7%9A%84query-encoder%E5%92%8Cdocument-encoderquery-encoder%E8%B4%9F%E8%B4%A3%E6%8A%8Aquery%E7%BC%96%E7%A0%81%E6%88%90%E4%B8%80%E4%B8%AA%E7%A8%A0%E5%AF%86%E7%9A%84%E5%90%91%E9%87%8F%E6%89%80%E8%B0%93%E7%A8%A0%E5%AF%86%E7%9A%84%E5%90%91%E9%87%8F%E6%98%AF%E5%92%8C%E7%A8%80%E7%96%8F%E5%90%91%E9%87%8F%E5%AF%B9%E5%BA%94%E7%9A%84%E7%A8%80%E7%96%8F%E5%90%91%E9%87%8F%E6%8C%87%E7%9A%84%E6%98%AF%E7%B1%BB%E4%BC%BC%E4%BA%8Etf-idf%E6%96%B9%E6%B3%95%E5%BE%97%E5%88%B0%E7%9A%84%E9%82%A3%E7%A7%8D%E5%90%91%E9%87%8F%E4%B9%9F%E5%B0%B1%E6%98%AF%E7%BB%9F%E8%AE%A1%E8%BE%93%E5%85%A5%E7%9A%84%E6%9F%90%E4%B8%80%E4%BA%9B%E7%89%B9%E5%BE%81%E5%BE%97%E5%88%B0%E7%9A%84%E9%82%A3%E4%B9%88%E7%A8%A0%E5%AF%86%E5%90%91%E9%87%8F%E7%9B%B8%E5%AF%B9%E7%9A%84%E6%8C%87%E7%9A%84%E5%B0%B1%E6%98%AF%E7%8E%B0%E5%9C%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A0%81%E5%BE%97%E5%88%B0%E7%9A%84%E8%8B%A5%E5%B9%B2%E7%BB%B4%E7%9A%84%E5%90%91%E9%87%8F%E6%AF%8F%E4%B8%AA%E7%BB%B4%E5%BA%A6%E6%9C%89%E7%89%B9%E5%AE%9A%E7%9A%84%E6%BD%9C%E5%9C%A8%E5%90%AB%E4%B9%89%E5%90%91%E9%87%8F%E6%95%B4%E4%BD%93%E4%B9%9F%E6%9C%89%E4%B8%80%E5%AE%9A%E7%9A%84%E8%AF%AD%E4%B9%89%E5%8F%AA%E4%B8%8D%E8%BF%87%E5%8F%AF%E8%83%BD%E4%BA%BA%E7%B1%BB%E6%97%A0%E6%B3%95%E8%AF%BB%E6%87%82%E4%BD%86%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E8%AF%BB%E6%87%82%E5%9B%A0%E6%AD%A4%E7%AE%80%E5%8D%95%E6%9D%A5%E8%AF%B4%E7%A8%A0%E5%AF%86%E5%90%91%E9%87%8F%E9%87%8D%E8%AF%AD%E4%B9%89%E8%80%8C%E7%A8%80%E7%96%8F%E5%90%91%E9%87%8F%E4%B8%80%E8%88%AC%E6%98%AF%E9%80%9A%E8%BF%87%E8%AF%8D%E9%87%8D%E5%8F%A0%E6%9D%A5%E6%A3%80%E7%B4%A2%E4%BB%85%E9%80%82%E7%94%A8%E4%BA%8E%E7%AE%80%E5%8D%95%E7%9A%84%E5%85%B3%E9%94%AE%E8%AF%8D%E5%8C%B9%E9%85%8Ddocument-encoder%E4%B9%9F%E7%B1%BB%E4%BC%BC%E8%B4%9F%E8%B4%A3%E6%8A%8Adocument%E7%BC%96%E7%A0%81%E6%88%90%E7%A8%A0%E5%AF%86%E5%90%91%E9%87%8F%E6%A0%B9%E6%8D%AEquery%E8%BF%9B%E8%A1%8C%E6%A3%80%E7%B4%A2%E7%9A%84%E4%BE%9D%E6%8D%AE%E5%88%99%E6%98%AFdocument-vector%E5%92%8Cquery-vector%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%B8%B8%E7%94%A8%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E6%9D%A5%E8%A1%A1%E9%87%8F%E8%AE%BA%E6%96%87%E7%9A%84%E7%A1%AE%E4%B9%9F%E6%98%AF%E7%94%A8%E7%9A%84%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95"><span class="nav-number">6.4.1.</span> <span class="nav-text">这里的设计follow的是DPR里的设计，也就是设计了一个bi-encoder，即上文提到的query
encoder和document encoder。query
encoder负责把query编码成一个稠密的向量（所谓稠密的向量，是和稀疏向量对应的。稀疏向量指的是类似于TF-IDF方法得到的那种向量，也就是统计输入的某一些特征得到的。那么稠密向量，相对的，指的就是现在神经网络编码得到的若干维的向量，每个维度有特定的潜在含义，向量整体也有一定的语义（只不过可能人类无法读懂，但模型可以读懂）。因此简单来说，稠密向量重语义，而稀疏向量一般是通过词重叠来检索，仅适用于简单的关键词匹配）；document
encoder也类似，负责把document编码成稠密向量。根据query进行检索的依据，则是document
vector和query
vector的相似性（常用向量内积来衡量。论文的确也是用的这种方法）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%A0%E4%B8%BA%E8%AE%BA%E6%96%87%E6%98%AF%E8%A6%81%E6%A3%80%E7%B4%A2%E5%87%BA%E5%8F%AF%E8%83%BD%E6%80%A7top-k%E9%AB%98%E7%9A%84document%E5%A6%82%E6%9E%9C%E6%8C%A8%E4%B8%AA%E6%8B%BFquery-vector%E5%92%8Cdocument-vector%E8%BF%9B%E8%A1%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E8%AE%A1%E7%AE%97%E6%97%A0%E7%96%91%E8%AE%A1%E7%AE%97%E9%87%8F%E6%98%AF%E6%9E%81%E5%A4%A7%E7%9A%84%E5%9B%A0%E4%B8%BA%E6%88%91%E4%BB%AC%E6%98%AF%E6%8A%8A%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E8%BD%AC%E5%8C%96%E6%88%90dense-vector%E7%A8%A0%E5%AF%86%E5%90%91%E9%87%8F%E7%9A%84%E8%80%8C%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E4%B8%80%E8%88%AC%E6%98%AF%E9%9D%9E%E5%B8%B8%E5%A4%A7%E7%9A%84%E6%89%80%E4%BB%A5%E8%BF%99%E9%87%8C%E4%BC%9A%E7%94%A8%E4%B8%80%E4%BA%9B%E7%8E%B0%E6%9C%89%E7%9A%84%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95%E6%9D%A5%E5%81%9A%E5%88%B0%E5%9C%A8sub-linear%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%86%85%E8%A7%A3%E5%86%B3%E8%BF%99%E7%B1%BB%E9%97%AE%E9%A2%98%E5%85%B6%E5%AE%9E%E6%9C%89%E4%B8%80%E4%B8%AA%E5%90%8D%E7%A7%B0%E5%8F%ABmaximum-inner-product-search-mips-problem"><span class="nav-number">6.4.2.</span> <span class="nav-text">因为论文是要检索出可能性top-K高的document，如果挨个拿query
vector和document
vector进行相似性计算，无疑计算量是极大的（因为我们是把外部知识库转化成dense
vector稠密向量的，而外部知识库一般是非常大的）。所以这里会用一些现有的近似方法来做到，在sub-linear的时间复杂度内解决。这类问题其实有一个名称，叫Maximum
Inner Product Search (MIPS) problem</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generator"><span class="nav-number">6.5.</span> <span class="nav-text">2. generator</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B2%A1%E4%BB%80%E4%B9%88%E5%A4%AA%E5%A4%9A%E5%A5%BD%E8%AF%B4%E7%9A%84%E6%8C%89%E7%90%86%E8%AF%B4%E9%9A%8F%E4%BE%BF%E4%B8%80%E4%B8%AAencoder-decoder%E6%A8%A1%E5%9E%8B%E9%83%BD%E6%98%AF%E5%8F%AF%E4%BB%A5%E7%9A%84%E8%AE%BA%E6%96%87%E9%80%89%E7%94%A8%E7%9A%84%E6%98%AFbart-large400m%E5%8F%82%E6%95%B0%E8%BF%99%E5%9C%A8%E5%BD%93%E6%97%B6%E5%BA%94%E8%AF%A5%E7%AE%97%E6%98%AF%E5%BE%88%E5%A5%BD%E5%BE%88%E5%A4%A7%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BA%86"><span class="nav-number">6.5.1.</span> <span class="nav-text">没什么太多好说的，按理说随便一个encoder-decoder模型都是可以的。论文选用的是bart-large，400M参数（这在当时应该算是很好很大的模型了）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E6%A3%80%E7%B4%A2%E5%88%B0%E7%9A%84%E6%96%87%E6%A1%A3z%E5%92%8C%E5%8E%9F%E5%A7%8B%E8%BE%93%E5%85%A5x%E4%B9%8B%E9%97%B4%E7%9A%84%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E4%BB%85%E4%BB%85%E6%98%AF%E6%8A%8A%E4%BA%8C%E8%80%85concat%E8%B5%B7%E6%9D%A5%E5%9C%A8token%E7%9A%84%E7%BB%B4%E5%BA%A6%E4%B8%8Abart%E6%98%AF%E6%94%AF%E6%8C%81%E5%8F%98%E9%95%BF%E5%BA%8F%E5%88%97%E7%9A%84%E5%A4%84%E7%90%86%E7%9A%84%E5%89%8D%E6%8F%90%E6%98%AF%E4%B8%8D%E8%A6%81%E8%B6%85%E8%BF%87%E9%95%BF%E5%BA%A6%E9%99%90%E5%88%B6%E5%88%AB%E5%BF%98%E4%BA%86batch-input%E7%9A%84%E5%BD%A2%E7%8A%B6%E4%B8%80%E8%88%AC%E6%98%AFbatch-seq_len-embed_dim"><span class="nav-number">6.5.2.</span> <span class="nav-text">关于检索到的文档z和原始输入x之间的融合，论文仅仅是把二者concat起来（在token的维度上。bart是支持变长序列的处理的，前提是不要超过长度限制。别忘了batch
input的形状一般是\((batch, seq\_len,
embed\_dim)\)）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#training"><span class="nav-number">6.6.</span> <span class="nav-text">3. training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%99%E5%A5%97%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E5%AE%8C%E5%85%A8%E6%98%AF%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%89%80%E4%BB%A5%E5%85%B3%E4%BA%8Eretriever%E9%83%A8%E5%88%86%E6%88%91%E4%BB%AC%E4%B8%8D%E4%BC%9A%E7%BB%99%E5%87%BA%E5%BA%94%E8%AF%A5%E6%A3%80%E7%B4%A2%E5%93%AA%E4%BA%9B%E6%96%87%E6%A1%A3%E8%80%8C%E6%98%AF%E5%85%A8%E9%83%BD%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%87%AA%E5%B7%B1%E5%AD%A6%E4%B9%A0%E8%BF%99%E9%87%8C%E6%80%80%E7%96%91%E4%B9%8B%E6%89%80%E4%BB%A5%E8%83%BD%E5%81%9A%E5%88%B0%E4%B8%8D%E5%AD%A6%E5%B4%A9%E5%8F%AF%E8%83%BD%E6%98%AF%E5%9B%A0%E4%B8%BAretriever%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E6%97%B6%E5%80%99%E7%94%A8%E7%9A%84%E6%9D%83%E9%87%8D%E6%95%88%E6%9E%9C%E8%BF%98%E4%B8%8D%E9%94%99%E4%BB%A5%E5%8F%8A%E4%B8%8D%E7%9F%A5%E9%81%93%E5%85%B7%E4%BD%93%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%E6%9C%89%E6%B2%A1%E6%9C%89%E4%BB%80%E4%B9%88trick%E4%B9%9F%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83%E5%9C%A8%E6%9F%90%E7%A7%8D%E7%A8%8B%E5%BA%A6%E4%B8%8A%E6%98%AF%E6%96%B9%E4%BE%BF%E7%9A%84"><span class="nav-number">6.6.1.</span> <span class="nav-text">这套模型的训练，完全是端到端的。所以关于retriever部分，我们不会给出“应该检索哪些文档”，而是全都让模型自己学习（这里怀疑，之所以能做到不学崩，可能是因为retriever初始化的时候用的权重效果还不错？以及不知道具体训练的时候有没有什么trick。也不得不说，端到端训练，在某种程度上是方便的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%9A%84%E7%9B%AE%E6%A0%87%E5%88%99%E6%98%AF%E7%BB%99%E5%AE%9A%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%AF%B9%E4%B9%9F%E5%8D%B3x_iy_i%E8%BF%99%E6%A0%B7%E7%9A%84%E6%95%B0%E6%8D%AE%E6%88%91%E4%BB%AC%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96py_ix_i%E5%B0%B1%E6%98%AF%E8%AF%B4%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%E8%80%8C%E5%AE%9E%E9%99%85%E5%BE%85%E4%BC%98%E5%8C%96%E7%9A%84%E5%87%BD%E6%95%B0%E5%BD%A2%E5%BC%8F%E5%88%99%E5%BD%A2%E5%A6%82-sum_jlogpy_jx_j%E4%BC%98%E5%8C%96%E5%99%A8%E7%94%A8%E7%9A%84%E6%98%AFadam"><span class="nav-number">6.6.2.</span> <span class="nav-text">训练的目标，则是给定训练数据对（也即\((x_{i},y_{i})\)这样的数据），我们要最大化\(p(y_{i}|x_{i})\)，就是说要最大化似然函数。而实际待优化的函数形式则形如\(-\sum_{j}log(p(y_{j}|x_{j}))\)；优化器用的是Adam</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%A6%E5%A4%96%E6%9C%89%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%82%B9%E6%98%AF%E8%AE%AD%E7%BB%83%E6%97%B6%E8%AE%BA%E6%96%87%E8%AF%B4%E6%98%8E%E5%B9%B6%E4%B8%8D%E4%BC%9A%E6%9B%B4%E6%96%B0document-encoder%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%8A%8A%E5%AE%83%E5%86%BB%E7%BB%93%E4%BA%86%E8%BF%99%E4%B8%BB%E8%A6%81%E6%98%AF%E5%9B%A0%E4%B8%BA%E4%B8%8A%E6%96%87%E6%8F%90%E5%88%B0%E7%9A%84%E5%9C%A8sub-linear%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%86%85%E8%BF%91%E4%BC%BC%E8%A7%A3%E5%86%B3%E6%90%9C%E7%B4%A2%E5%87%BAtop-k%E4%B8%AAdocument%E7%9A%84%E6%96%B9%E6%B3%95%E5%AE%83%E9%9C%80%E8%A6%81%E6%88%91%E4%BB%AC%E4%BA%8B%E5%85%88%E5%A4%84%E7%90%86%E5%BE%97%E5%88%B0%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AF%86%E7%9A%84vector%E7%84%B6%E5%90%8E%E6%8A%8A%E5%AE%83%E4%BB%AC%E8%BF%9B%E8%A1%8C%E6%9F%90%E7%A7%8D%E6%8E%92%E5%88%97%E5%A6%82%E6%9E%9Cdocument-encoder%E6%9B%B4%E6%96%B0%E4%BA%86%E4%BD%86%E6%88%91%E4%BB%AC%E4%B8%8D%E6%9B%B4%E6%96%B0%E4%B9%8B%E5%89%8D%E5%BE%97%E5%88%B0%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E7%9A%84vector%E9%82%A3%E5%B0%B1%E4%BC%9A%E5%87%BA%E7%8E%B0%E5%AF%B9%E4%B8%8D%E4%B8%8A%E7%9A%84%E6%83%85%E5%86%B5document-encoder%E9%83%BD%E6%9B%B4%E6%96%B0%E4%BA%86%E5%AE%83%E8%BE%93%E5%87%BA%E7%9A%84vector%E4%B9%9F%E6%98%AF%E4%BC%9A%E6%9B%B4%E6%96%B0%E7%9A%84%E8%80%8C%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E5%86%B3%E5%AE%9A%E8%A6%81%E6%9B%B4%E6%96%B0document-vector%E9%82%A3%E5%B0%B1%E6%84%8F%E5%91%B3%E7%9D%80%E9%9C%80%E8%A6%81%E5%86%8D%E6%AC%A1%E9%81%8D%E5%8E%86%E7%9F%A5%E8%AF%86%E5%BA%93%E5%B9%B6%E6%9B%B4%E6%96%B0%E8%BF%99%E5%B0%B1%E5%B8%A6%E6%9D%A5%E4%BA%86%E5%BE%88%E5%A4%A7%E7%9A%84%E8%AE%A1%E7%AE%97%E8%B4%9F%E6%8B%85%E6%95%85%E8%AE%BA%E6%96%87%E8%80%83%E8%99%91%E4%B8%8D%E6%9B%B4%E6%96%B0document-encoder%E5%BD%93%E7%84%B6%E7%A1%AC%E6%98%AF%E8%A6%81%E6%9B%B4%E6%96%B0%E7%9A%84%E8%AF%9D%E9%97%B4%E9%9A%94%E8%8B%A5%E5%B9%B2epoch%E5%86%8D%E6%9B%B4%E6%96%B0%E4%B9%9F%E4%B8%8D%E6%98%AF%E4%B8%8D%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E6%88%96%E8%80%85%E7%94%A8%E4%B8%8A%E5%85%B6%E5%AE%83%E7%9A%84trick%E4%B8%8D%E8%BF%87%E7%9C%8B%E8%AE%BA%E6%96%87%E7%9A%84%E6%84%8F%E6%80%9D%E5%A5%BD%E5%83%8F%E6%98%AF%E5%8D%B3%E4%BD%BF%E6%9B%B4%E6%96%B0%E4%BA%86document-encoder%E6%95%88%E6%9E%9C%E4%B9%9F%E6%B2%A1%E6%9C%89%E5%A5%BD%E4%B8%8A%E7%89%B9%E5%88%AB%E5%A4%9A"><span class="nav-number">6.6.3.</span> <span class="nav-text">另外有一个小点是，训练时，论文说明，并不会更新document
encoder，也就是把它冻结了。这主要是因为，上文提到的在sub-linear时间复杂度内近似解决搜索出top-K个document的方法，它需要我们事先处理得到外部知识的vector，然后把它们进行某种排列。如果document
encoder更新了，但我们不更新之前得到的知识库的vector，那就会出现对不上的情况（document
encoder都更新了，它输出的vector也是会更新的）；而如果我们决定要更新document
vector，那就意味着需要再次遍历知识库并更新，这就带来了很大的计算负担。故论文考虑不更新document
encoder。当然，硬是要更新的话，间隔若干epoch再更新，也不是不可以考虑；或者用上其它的trick。不过看论文的意思，好像是即使更新了document
encoder，效果也没有好上特别多？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81"><span class="nav-number">6.7.</span> <span class="nav-text">4. 解码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E7%AE%97%E6%98%AF%E6%AF%94%E8%BE%83%E5%85%B3%E9%94%AE%E7%9A%84%E4%B8%80%E4%B8%AA%E9%83%A8%E5%88%86%E4%B9%9F%E6%98%AF%E6%AF%94%E8%BE%83%E5%A4%8D%E6%9D%82%E7%9A%84%E4%B8%80%E4%B8%AA%E9%83%A8%E5%88%86%E8%AE%BA%E6%96%87%E9%87%8C%E8%80%83%E8%99%91%E4%BA%86%E4%B8%A4%E7%A7%8D%E8%A7%A3%E7%A0%81%E6%96%B9%E5%BC%8F%E4%B8%80%E7%A7%8D%E6%98%AFrag-sequence%E4%B8%80%E7%A7%8D%E6%98%AFrag-token%E5%89%8D%E8%80%85%E7%9A%84%E5%90%AB%E4%B9%89%E6%98%AF%E6%88%91%E4%BB%AC%E8%A7%A3%E7%A0%81%E7%94%9F%E6%88%90seq%E7%9A%84%E6%97%B6%E5%80%99%E4%BB%85%E4%BE%9D%E8%B5%96%E4%BA%8E%E4%B8%80%E7%AF%87document%E5%85%A8%E7%A8%8B%E9%83%BD%E6%98%AF%E5%90%8E%E8%80%85%E5%88%99%E8%A1%A8%E7%A4%BA%E8%A7%A3%E7%A0%81%E7%94%9F%E6%88%90seq%E7%9A%84%E6%97%B6%E5%80%99%E7%94%9F%E6%88%90%E6%AF%8F%E4%B8%AAtoken%E6%97%B6%E5%AE%83%E4%BE%9D%E8%B5%96%E7%9A%84document%E9%83%BD%E5%8F%AF%E4%BB%A5%E6%98%AF%E4%B8%8D%E5%90%8C%E7%9A%84-%E6%98%BE%E7%84%B6%E5%89%8D%E8%80%85%E8%AE%A1%E7%AE%97%E9%87%8F%E4%BC%9A%E5%B0%8F%E4%B8%80%E4%BA%9B%E5%90%8E%E8%80%85%E6%8C%89%E7%90%86%E8%AF%B4%E4%B8%8A%E9%99%90%E4%BC%9A%E9%AB%98%E4%B8%80%E4%BA%9B%E4%BD%86%E8%AE%A1%E7%AE%97%E9%87%8F%E4%B9%9F%E4%BC%9A%E6%9B%B4%E5%A4%A7%E4%B8%80%E4%BA%9B"><span class="nav-number">6.7.1.</span> <span class="nav-text">解码算是比较关键的一个部分，也是比较复杂的一个部分。论文里考虑了两种解码方式，一种是RAG-sequence，一种是RAG-token。前者的含义是，我们解码生成seq的时候，仅依赖于一篇document（全程都是）；后者则表示，解码生成seq的时候，生成每个token时，它依赖的document都可以是不同的显然，前者计算量会小一些，后者按理说上限会高一些，但计算量也会更大一些</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#section-1"><span class="nav-number">6.7.1.1.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8A%E4%B8%BArag-token%E7%9A%84%E8%A7%A3%E7%A0%81%E7%9A%84%E5%85%AC%E5%BC%8F%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E6%AF%8F%E4%B8%80%E6%AD%A5%E8%A7%A3%E7%A0%81%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AAtoken%E7%9A%84%E6%97%B6%E5%80%99%E9%83%BD%E6%98%AF%E4%BC%9A%E5%AF%B9%E6%88%91%E4%BB%AC%E6%A3%80%E7%B4%A2%E5%88%B0%E7%9A%84top-k%E4%B8%AAdocument%E8%BF%9B%E8%A1%8Cmarginalization"><span class="nav-number">6.7.1.2.</span> <span class="nav-text">上为RAG-token的解码的公式。可以看到，每一步解码生成一个token的时候，都是会对我们检索到的top-k个document进行marginalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#section-2"><span class="nav-number">6.7.1.3.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E4%B8%AA%E5%88%99%E6%98%AFrag-sequence%E7%9A%84%E8%A7%A3%E7%A0%81%E7%94%9F%E6%88%90%E5%85%AC%E5%BC%8F%E6%AD%A4%E5%A4%84%E6%88%91%E4%BB%AC%E8%99%BD%E7%84%B6%E4%B9%9F%E4%BC%9A%E5%AF%B9document%E8%BF%9B%E8%A1%8C%E8%BE%B9%E7%BC%98%E5%8C%96%E4%BD%86%E6%AF%8F%E6%AC%A1%E4%B8%80%E6%97%A6%E9%80%89%E5%AE%9A%E4%B8%80%E4%B8%AAdocument%E4%BB%A5%E5%90%8E%E6%88%91%E4%BB%AC%E5%B0%B1%E4%BC%9A%E7%94%A8%E5%AE%83%E6%9D%A5%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E5%BA%8F%E5%88%97"><span class="nav-number">6.7.1.4.</span> <span class="nav-text">这个则是RAG-sequence的解码生成公式。此处我们虽然也会对document进行边缘化，但每次一旦选定一个document以后，我们就会用它来生成一个完整的序列</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-token"><span class="nav-number">6.7.2.</span> <span class="nav-text">RAG-token</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E7%9A%84rag-token%E6%96%B9%E5%BC%8F%E7%9A%84%E8%A7%A3%E7%A0%81%E5%85%AC%E5%BC%8F%E5%85%B6%E5%AE%9E%E5%A6%82%E4%B8%8A%E6%89%80%E7%A4%BA%E8%BF%99%E9%87%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%BB%86%E5%8C%96%E4%B8%80%E4%B8%8B%E6%88%91%E4%BB%AC%E7%BB%99%E5%87%BA%E6%AF%8F%E4%B8%80%E6%AD%A5%E4%B9%9F%E5%B0%B1%E6%98%AF%E7%94%9F%E6%88%90%E4%B8%8B%E4%B8%80%E4%B8%AAtoken%E7%9A%84%E5%85%AC%E5%BC%8F%E4%BD%86%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E5%85%B6%E5%AE%9E%E5%8F%AA%E6%98%AF%E7%9F%A5%E9%81%93%E4%BA%86%E4%B8%8B%E4%B8%80%E4%B8%AAtoken%E7%9A%84%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E9%82%A3%E5%85%B7%E4%BD%93%E4%B8%8B%E4%B8%80%E4%B8%AAtoken%E5%8F%96%E5%93%AA%E4%B8%AA%E5%91%A2%E8%BF%99%E9%87%8C%E5%B0%B1%E6%B6%89%E5%8F%8A%E5%88%B0%E4%B8%80%E4%BA%9B%E6%90%9C%E7%B4%A2%E7%9A%84%E7%AD%96%E7%95%A5%E4%BA%86%E8%80%8C%E6%AD%A4%E5%A4%84%E8%AE%BA%E6%96%87%E9%87%87%E7%94%A8%E7%9A%84%E6%98%AFbeam-searchbeam-search%E4%B9%9F%E5%8D%B3%E6%9D%9F%E6%90%9C%E7%B4%A2%E5%AE%83%E5%9C%A8%E7%AC%AC%E4%B8%80%E6%AD%A5%E9%87%87%E5%8F%96%E6%A6%82%E7%8E%87top-k%E9%AB%98%E7%9A%84%E7%BB%93%E6%9E%9C%E6%AD%A4%E6%97%B6%E5%B0%B1%E5%BE%97%E5%88%B0k%E4%B8%AA%E5%80%99%E9%80%89%E5%BA%8F%E5%88%97%E5%8F%AA%E4%B8%8D%E8%BF%87%E5%AE%83%E4%BB%AC%E9%83%BD%E6%98%AF%E5%8D%95%E4%B8%AAtoken%E7%9A%84%E5%BA%8F%E5%88%97%E8%BF%99%E4%B8%AAk%E6%98%AF%E8%B6%85%E5%8F%82%E6%95%B0%E4%B9%8B%E5%90%8E%E5%9F%BA%E4%BA%8E%E8%BF%99k%E4%B8%AA%E5%BA%8F%E5%88%97%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%E7%84%B6%E5%90%8E%E4%BB%8E%E8%BF%99%E4%BA%9B%E6%89%80%E6%9C%89%E7%9A%84%E5%80%99%E9%80%89%E7%9A%842-token%E7%9A%84%E5%BA%8F%E5%88%97%E4%B8%AD%E6%8C%91%E9%80%89%E5%87%BAtop-k%E9%AB%98%E7%9A%84%E6%A6%82%E7%8E%87%E7%9A%84%E5%BA%8F%E5%88%97%E4%BD%9C%E4%B8%BA%E6%9C%80%E7%BB%88%E5%BE%97%E5%88%B0%E7%9A%842-token%E7%9A%84%E5%BA%8F%E5%88%97%E7%9A%84%E7%BB%93%E6%9E%9C%E5%90%8E%E9%9D%A2%E7%9A%84%E8%A7%A3%E7%A0%81%E4%BB%A5%E6%AD%A4%E7%B1%BB%E6%8E%A8%E6%89%80%E4%BB%A5%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0beam-search%E7%9A%84%E5%85%B3%E9%94%AE%E5%9C%A8%E4%BA%8E%E6%AF%8F%E4%B8%80%E6%AD%A5%E9%83%BD%E6%8C%91%E9%80%89%E5%87%BA%E6%A6%82%E7%8E%87top-k%E9%AB%98%E7%9A%84%E5%BA%8F%E5%88%97%E8%BF%99%E6%A0%B7%E6%97%A2%E4%BF%9D%E8%AF%81%E4%BA%86%E5%90%88%E7%90%86%E6%80%A7%E4%B9%9F%E6%9C%89%E4%B8%80%E5%AE%9A%E7%9A%84%E5%A4%9A%E6%A0%B7%E6%80%A7"><span class="nav-number">6.7.2.1.</span> <span class="nav-text">整体的RAG-token方式的解码公式，其实如上所示。这里进一步细化一下，我们给出每一步（也就是生成下一个token）的公式：但这里我们其实只是知道了下一个token的概率分布。那具体下一个token取哪个呢？这里就涉及到一些搜索的策略了。而此处论文采用的是beam
search（beam
search，也即束搜索，它在第一步采取概率top-k高的结果（此时就得到k个候选序列，只不过它们都是单个token的序列。这个k是超参数），之后基于这k个序列进一步的进行解码，然后从这些所有的候选的2-token的序列中，挑选出top-k高的概率的序列，作为最终得到的2-token的序列的结果；后面的解码以此类推。所以可以看到，beam
search的关键在于，每一步都挑选出概率top-k高的序列，这样既保证了合理性，也有一定的多样性）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-sequence"><span class="nav-number">6.7.3.</span> <span class="nav-text">RAG-sequence</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E4%B8%8A%E9%9D%A2%E7%9A%84rag-sequence%E7%9A%84%E8%A7%A3%E7%A0%81%E5%85%AC%E5%BC%8F%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E6%88%91%E4%BB%AC%E7%94%9F%E6%88%90%E7%9A%84%E6%A6%82%E7%8E%87%E4%B8%8D%E8%83%BD%E5%88%86%E8%A7%A3%E4%B8%BA%E6%AF%8F%E4%B8%AAtoken%E7%9A%84%E7%94%9F%E6%88%90%E6%A6%82%E7%8E%87%E4%B9%8B%E7%A7%AF%E6%9C%80%E7%9B%B4%E8%A7%82%E7%9A%84%E7%9C%8B%E6%9C%80%E5%A4%96%E5%B1%82%E6%98%AF%E4%B8%80%E4%B8%AA%E6%B1%82%E5%92%8C%E7%AC%A6%E5%8F%B7%E9%82%A3%E5%9F%BA%E6%9C%AC%E5%B0%B1%E6%84%8F%E5%91%B3%E7%9D%80%E8%BF%99%E4%B8%AA%E6%A6%82%E7%8E%87%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%8D%E8%83%BD%E5%88%86%E8%A7%A3%E4%B8%BA%E4%B9%98%E7%A7%AF%E7%9A%84%E5%BD%A2%E5%BC%8F%E8%80%8C%E4%BB%8E%E5%85%AC%E5%BC%8F%E7%9A%84%E5%90%AB%E4%B9%89%E4%B8%8A%E7%9C%8B%E8%BF%99%E4%B8%AA%E8%A7%A3%E7%A0%81%E5%85%AC%E5%BC%8F%E6%98%AF%E5%AF%B9document%E8%BF%9B%E8%A1%8C%E8%BE%B9%E7%BC%98%E5%8C%96%E6%88%91%E4%BB%AC%E9%81%8D%E5%8E%86%E6%89%80%E6%9C%89%E7%9A%84document%E5%81%87%E8%AE%BE%E7%94%A8%E5%AE%83%E8%BF%9B%E8%A1%8C%E7%94%9F%E6%88%90%E5%BE%97%E5%88%B0%E4%B8%80%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B9%98%E4%B8%8A%E6%A3%80%E7%B4%A2%E5%88%B0%E8%BF%99%E4%B8%AAdocument%E7%9A%84%E6%A6%82%E7%8E%87%E4%BD%9C%E4%B8%BA%E6%9D%83%E9%87%8D%E6%9C%80%E5%90%8E%E5%86%8D%E6%B1%82%E5%92%8C%E5%BE%97%E5%88%B0%E6%A6%82%E7%8E%87%E8%BF%99%E6%A0%B7%E4%BB%8E%E8%AF%AD%E4%B9%89%E4%B8%8A%E7%9C%8B%E4%B9%9F%E4%B8%8D%E8%83%BD%E5%88%86%E8%A7%A3%E6%88%90token%E6%A6%82%E7%8E%87%E4%B9%8B%E7%A7%AF%E5%9B%A0%E4%B8%BA%E8%BF%99%E9%87%8C%E6%98%AF%E5%BA%8F%E5%88%97%E6%A6%82%E7%8E%87%E4%B9%8B%E5%92%8C-%E8%BF%99%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98%E6%98%AF%E6%88%91%E4%BB%AC%E4%B8%8D%E8%83%BD%E7%9B%B4%E6%8E%A5%E7%94%A8%E4%B8%80%E4%B8%AAbeam-search%E6%9D%A5%E8%A7%A3%E7%A0%81%E5%BE%97%E5%88%B0%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C%E4%BA%86%E4%B8%8A%E9%9D%A2%E7%9A%84beam-search%E7%9A%84%E5%BA%94%E7%94%A8%E5%AE%83%E6%98%AF%E5%9C%A8%E8%A7%A3%E7%A0%81%E5%BE%97%E5%88%B0%E4%B8%8B%E4%B8%80%E4%B8%AAtoken%E7%9A%84%E6%97%B6%E5%80%99%E6%8C%91%E5%87%BA%E5%87%BA%E7%8E%B0%E6%A6%82%E7%8E%87top-k%E7%9A%84%E5%BA%8F%E5%88%97%E4%BD%86%E6%AD%A4%E5%A4%84%E6%88%91%E4%BB%AC%E7%9A%84%E8%A7%A3%E7%A0%81%E6%96%B9%E5%BC%8F%E5%B0%B1%E4%B8%8D%E6%98%AFper-token%E7%9A%84%E5%9B%A0%E6%AD%A4%E4%B8%8D%E8%83%BD%E7%9B%B4%E6%8E%A5%E7%94%A8%E6%A0%87%E5%87%86%E7%9A%84beam-search"><span class="nav-number">6.7.3.1.</span> <span class="nav-text">根据上面的RAG-sequence的解码公式，可以看到，我们生成的概率不能分解为每个token的生成概率之积（最直观的，看最外层，是一个求和符号，那基本就意味着这个概率表达式不能分解为乘积的形式。而从公式的含义上看，这个解码公式是对document进行边缘化，我们遍历所有的document，假设用它进行生成，得到一个变量，乘上检索到这个document的概率作为权重，最后再求和，得到概率。这样从语义上看，也不能分解成token概率之积，因为这里是序列概率之和）这带来的问题是，我们不能直接用一个beam
search来解码得到最终结果了（上面的beam
search的应用，它是在解码得到下一个token的时候挑出出现概率top-k的序列。但此处我们的解码方式就不是per
token的，因此不能直接用标准的beam search）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%82%A3%E4%B9%88%E9%92%88%E5%AF%B9%E8%BF%99%E7%A7%8D%E6%83%85%E5%86%B5%E4%B8%80%E7%A7%8D%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E7%9A%84%E8%A7%A3%E7%A0%81%E6%96%B9%E5%BC%8F%E6%98%AFthorough-decoding%E8%80%8C%E8%AE%BA%E6%96%87%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5%E5%88%99%E6%98%AF%E5%9F%BA%E4%BA%8E%E6%AD%A4%E7%9A%84%E4%B8%80%E7%A7%8D%E5%8F%98%E4%BD%93%E4%B8%8B%E9%9D%A2%E5%85%88%E8%AE%B2%E8%AE%B2%E4%BB%80%E4%B9%88%E6%98%AFthorough-decoding"><span class="nav-number">6.7.3.2.</span> <span class="nav-text">那么针对这种情况，一种可以考虑的解码方式是Thorough
Decoding。而论文的解码策略则是基于此的一种变体。下面先讲讲什么是Thorough
Decoding</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%99%E4%B8%AA%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5%E5%AE%83%E6%98%AF%E4%BB%8Edocument%E5%87%BA%E5%8F%91%E5%86%8D%E8%BF%9B%E8%A1%8Cbeam-search%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%88%91%E4%BB%AC%E9%81%8D%E5%8E%86%E6%89%80%E6%9C%89%E7%9A%84document%E5%AF%B9%E6%AF%8F%E4%B8%AAdocument%E6%88%91%E4%BB%AC%E5%81%87%E8%AE%BE%E6%A3%80%E7%B4%A2%E5%88%B0%E7%9A%84%E6%98%AF%E5%AE%83%E7%84%B6%E5%90%8E%E8%BF%9B%E8%A1%8C%E7%94%9F%E6%88%90%E6%AD%A4%E5%A4%84%E7%9A%84%E7%94%9F%E6%88%90%E6%98%AFbeam-search%E4%B8%8D%E5%A6%A8%E5%81%87%E8%AE%BE%E6%88%91%E4%BB%AC%E7%9A%84beam-search%E6%98%AF%E5%8F%96top-k%E9%AB%98%E7%9A%84%E5%BA%8F%E5%88%97%E5%88%99%E6%AF%8F%E4%B8%AAdocument%E9%83%BD%E8%83%BD%E5%AF%B9%E5%BA%94%E7%94%9F%E6%88%90k%E4%B8%AA%E5%BA%8F%E5%88%97%E5%B9%B6%E4%B8%94%E6%AF%8F%E4%B8%AA%E5%BA%8F%E5%88%97%E9%83%BD%E6%9C%89%E4%B8%80%E4%B8%AA%E6%A6%82%E7%8E%87%E8%AE%B0%E4%B8%BAp_thetayxz%E6%8C%89%E7%90%86%E8%AF%B4%E5%81%87%E8%AE%BE%E4%B8%80%E5%85%B1n%E4%B8%AAdocument%E5%88%99%E6%AD%A4%E6%97%B6%E4%BC%9A%E5%BE%97%E5%88%B0nk%E4%B8%AA%E5%BA%8F%E5%88%97%E5%8F%AA%E4%B8%8D%E8%BF%87%E8%BF%99%E9%87%8C%E9%9D%A2%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%9C%89%E4%B8%80%E4%BA%9B%E9%87%8D%E5%A4%8D%E7%9A%84%E6%88%91%E4%BB%AC%E6%8A%8A%E9%87%8D%E5%A4%8D%E7%9A%84%E5%8E%BB%E6%8E%89%E5%8D%B3%E5%8F%AF%E4%BD%86%E5%BE%97%E5%88%B0%E7%9A%84%E6%A6%82%E7%8E%87p_thetayxz%E9%9C%80%E8%A6%81%E4%BF%9D%E7%95%99%E5%90%8E%E9%9D%A2%E6%9C%89%E7%94%A8-%E6%AD%A4%E6%97%B6%E8%BF%99%E4%BA%9B%E5%8E%BB%E9%87%8D%E5%90%8E%E7%9A%84%E5%BA%8F%E5%88%97%E5%AE%83%E4%BB%AC%E6%94%BE%E5%88%B0%E4%B8%80%E8%B5%B7%E7%BB%84%E6%88%90%E4%BA%86%E4%B8%80%E4%B8%AA%E5%80%99%E9%80%89%E5%BA%8F%E5%88%97%E7%9A%84%E9%9B%86%E5%90%88%E8%AE%BA%E6%96%87%E4%B8%AD%E7%A7%B0%E4%B8%BAset-of-hypothesis%E6%88%91%E4%BB%AC%E8%AE%A4%E4%B8%BA%E8%BF%99%E4%BA%9B%E5%BA%8F%E5%88%97%E5%B0%B1%E6%98%AF%E5%87%BA%E7%8E%B0%E6%A6%82%E7%8E%87%E7%9B%B8%E5%AF%B9%E8%BE%83%E9%AB%98%E7%9A%84%E6%9C%80%E7%BB%88%E7%9A%84%E7%BB%93%E6%9E%9C%E5%BA%94%E8%AF%A5%E6%98%AF%E8%BF%99%E4%BA%9B%E5%BA%8F%E5%88%97%E9%87%8C%E5%87%BA%E7%8E%B0%E6%A6%82%E7%8E%87%E6%9C%80%E9%AB%98%E7%9A%84%E9%82%A3%E4%B8%AA%E9%82%A3%E5%B0%B1%E9%9C%80%E8%A6%81%E7%AE%97%E5%87%BA%E8%BF%99%E4%BA%9B%E5%80%99%E9%80%89%E5%BA%8F%E5%88%97%E7%9A%84%E5%87%BA%E7%8E%B0%E6%A6%82%E7%8E%87%E6%80%8E%E4%B9%88%E7%AE%97%E5%91%A2%E5%B0%B1%E6%98%AF%E5%AF%B9document%E8%BF%9B%E8%A1%8C%E8%BE%B9%E7%BC%98%E5%8C%96%E4%BD%86%E8%BF%99%E9%87%8C%E5%BA%94%E8%AF%A5%E6%98%AF%E7%94%A8%E5%88%B0%E6%89%80%E6%9C%89%E7%9A%84document%E5%90%A7%E9%82%A3%E4%B8%8A%E9%9D%A2%E6%88%91%E4%BB%AC%E9%83%BD%E6%98%AF%E9%81%8D%E5%8E%86%E6%89%80%E6%9C%89%E7%9A%84document%E8%BF%99%E9%87%8C%E4%B9%9F%E5%BA%94%E8%AF%A5%E7%94%A8%E6%89%80%E6%9C%89%E7%9A%84document%E8%BF%9B%E8%A1%8C%E8%BE%B9%E7%BC%98%E5%8C%96%E6%89%8D%E5%AF%B9%E5%95%8A%E4%B8%8D%E5%BA%94%E8%AF%A5%E5%86%8D%E7%94%A8top-k%E7%9A%84document%E4%BA%86%E5%90%A7%E5%8F%AF%E8%83%BD%E5%BE%97%E7%9C%8B%E4%B8%80%E4%B8%8B%E4%BB%A3%E7%A0%81%E4%BA%86%E8%AE%BA%E6%96%87%E9%87%8C%E8%BF%99%E4%B8%80%E5%9D%97%E5%86%99%E5%BE%97%E4%B8%8D%E5%A4%9F%E6%B8%85%E6%A5%9A%E6%88%91%E4%BB%AC%E5%AF%B9%E6%AF%8F%E4%B8%AA%E5%80%99%E9%80%89%E5%BA%8F%E5%88%97%E7%AE%97%E5%87%BA%E5%AE%83%E4%BB%AC%E7%9A%84%E5%87%BA%E7%8E%B0%E6%A6%82%E7%8E%87%E5%8F%96%E6%9C%80%E5%A4%A7%E6%A6%82%E7%8E%87%E5%AF%B9%E5%BA%94%E7%9A%84%E5%BA%8F%E5%88%97%E5%8D%B3%E5%8F%AF-%E7%84%B6%E8%80%8C%E6%AD%A4%E5%A4%84%E5%8F%88%E5%8F%AF%E8%83%BD%E6%9C%89%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E5%BA%8F%E5%88%97%E5%87%BA%E7%8E%B0%E7%9A%84%E6%A6%82%E7%8E%87%E9%9C%80%E8%A6%81%E5%BE%97%E5%88%B0%E5%9C%A8%E6%AF%8F%E4%B8%AAdocument%E4%B8%8B%E8%A7%A3%E7%A0%81%E5%87%BA%E8%BF%99%E4%B8%AA%E5%BA%8F%E5%88%97%E7%9A%84%E6%A6%82%E7%8E%87%E8%80%8C%E8%BF%99%E4%B8%AA%E5%BA%8F%E5%88%97%E8%83%BD%E8%A2%AB%E6%88%91%E4%BB%AC%E8%AE%A1%E7%AE%97%E5%AE%83%E7%9A%84%E6%A6%82%E7%8E%87%E5%B0%B1%E8%AF%B4%E6%98%8E%E5%AE%83%E4%B8%80%E5%AE%9A%E5%87%BA%E7%8E%B0%E5%9C%A8%E4%BA%86set-of-hypothesis%E9%87%8C%E4%BA%86%E6%89%80%E4%BB%A5%E5%AE%83%E4%B8%80%E5%AE%9A%E6%98%AF%E7%94%B1%E6%9F%90%E4%B8%AAdocument%E8%A7%A3%E7%A0%81%E5%BE%97%E5%88%B0%E7%9A%84%E6%95%85%E6%88%91%E4%BB%AC%E4%B8%80%E5%AE%9A%E5%B7%B2%E7%BB%8F%E6%9C%89%E4%B8%80%E4%B8%AAp_thetayxz_i%E5%8F%AF%E6%98%AF%E6%88%91%E4%BB%AC%E8%A6%81%E7%9A%84%E5%8D%B4%E6%98%AF%E6%89%80%E6%9C%89%E7%9A%84p_thetayxz_j-z_jin-z-%E6%89%80%E4%BB%A5%E6%9C%89%E5%8F%AF%E8%83%BD%E6%9F%90%E4%BA%9Bdocument%E4%B8%8B%E8%A7%A3%E7%A0%81%E5%BE%97%E5%88%B0%E8%AF%A5%E5%BA%8F%E5%88%97%E7%9A%84%E6%A6%82%E7%8E%87%E6%88%91%E4%BB%AC%E6%B2%A1%E6%9C%89%E9%82%A3%E6%80%8E%E4%B9%88%E5%8A%9E%E5%B0%B1%E5%BE%97%E7%AE%97%E4%B8%80%E4%B8%8B%E4%BA%86%E5%8D%B3%E6%88%91%E4%BB%AC%E5%89%8D%E5%90%91%E7%AE%97%E4%B8%80%E4%B8%8B%E5%9C%A8%E5%BD%93%E5%89%8D%E7%9A%84documentz_i%E4%B8%8B%E8%A7%A3%E7%A0%81%E5%BE%97%E5%88%B0%E5%BD%93%E5%89%8D%E5%BA%8F%E5%88%97y%E7%9A%84%E6%A6%82%E7%8E%87%E8%BF%99%E5%B0%B1%E6%98%AFthorough-decoding"><span class="nav-number">6.7.3.2.1.</span> <span class="nav-text">这个解码策略，它是从document出发，再进行beam
search。也就是，我们遍历所有的document，对每个document，我们假设检索到的是它，然后进行生成（此处的生成是beam
search）。不妨假设我们的beam
search是取top-k高的序列，则每个document都能对应生成k个序列（并且每个序列都有一个概率，记为\(p_{\theta}(y|x,z)\)）。按理说，假设一共n个document，则此时会得到nk个序列。只不过这里面可能会有一些重复的，我们把重复的去掉即可（但得到的概率\(p_{\theta}(y|x,z)\)需要保留，后面有用）此时这些去重后的序列，它们放到一起，组成了一个候选序列的集合，论文中称为set
of
hypothesis。我们认为，这些序列就是出现概率相对较高的，最终的结果应该是这些序列里出现概率最高的那个。那就需要算出这些候选序列的出现概率。怎么算呢？就是对document进行边缘化（但这里应该是用到所有的document吧？那上面我们都是遍历所有的document，这里也应该用所有的document进行边缘化才对啊？不应该再用top-k的document了吧？可能得看一下代码了，论文里这一块写得不够清楚），我们对每个候选序列，算出它们的出现概率，取最大概率对应的序列即可然而此处又可能有一个问题。计算一个序列出现的概率，需要得到在每个document下，解码出这个序列的概率。而这个序列能被我们计算它的概率，就说明它一定出现在了set
of
hypothesis里了，所以它一定是由某个document解码得到的，故我们一定已经有一个\(p_{\theta}(y|x,z_{i})\)，可是我们要的却是所有的\(p_{\theta}(y|x,z_{j}),\ \ \ z_{j}\in
Z\)所以有可能某些document下，解码得到该序列的概率我们没有。那怎么办？就得算一下了，即我们前向算一下在当前的document\(z_{i}\)下，解码得到当前序列y的概率。这就是Thorough
Decoding</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0thorough-decoding%E4%B8%AD%E5%8F%AF%E8%83%BD%E4%BC%9A%E9%9C%80%E8%A6%81%E8%A1%A5%E5%85%85%E7%AE%97%E4%B8%80%E4%BA%9B%E5%BA%8F%E5%88%97%E7%9A%84%E8%A7%A3%E7%A0%81%E6%A6%82%E7%8E%87%E5%B9%B6%E4%B8%94set-of-hypothesis%E4%B8%80%E8%88%AC%E8%BF%98%E6%AF%94%E8%BE%83%E5%A4%A7%E8%BF%99%E5%B0%B1%E5%AF%BC%E8%87%B4%E9%9C%80%E8%A6%81%E8%A1%A5%E5%85%85%E7%AE%97%E7%9A%84%E6%A6%82%E7%8E%87%E5%8F%AF%E8%83%BD%E8%BF%98%E4%B8%8D%E5%B0%91%E6%97%A0%E7%96%91%E4%BC%9A%E5%A2%9E%E5%8A%A0%E5%BE%88%E5%A4%9A%E8%AE%A1%E7%AE%97%E9%87%8F%E6%95%85%E8%AE%BA%E6%96%87%E7%9A%84fast-decoding%E5%B0%B1%E5%AF%B9%E6%AD%A4%E4%BD%9C%E5%87%BA%E4%BA%86%E4%B8%80%E4%BA%9B%E6%94%B9%E8%BF%9B%E7%AE%80%E5%8D%95%E6%9D%A5%E8%AF%B4%E5%9C%A8%E4%B8%8A%E9%9D%A2%E6%88%91%E4%BB%AC%E5%A6%82%E6%9E%9C%E9%81%87%E5%88%B0p_thetayxz_i%E6%B2%A1%E7%AE%97%E5%87%BA%E6%9D%A5%E4%BC%9A%E9%87%8D%E6%96%B0%E7%AE%97%E4%B8%80%E4%B8%8B%E4%BD%86%E5%9C%A8fast-decoding%E4%B8%AD%E6%88%91%E4%BB%AC%E7%9B%B4%E6%8E%A5%E6%8A%8A%E8%BF%99%E4%B8%AA%E6%A6%82%E7%8E%87%E8%BF%91%E4%BC%BC%E4%B8%BA0%E7%9B%B4%E8%A7%82%E4%B8%8A%E5%8F%AF%E4%BB%A5%E8%BF%99%E6%A0%B7%E7%90%86%E8%A7%A3%E5%9C%A8document-z_i%E4%B8%AD%E8%BF%9B%E8%A1%8Cbeam-search%E7%9A%84%E6%97%B6%E5%80%99%E7%AB%9F%E7%84%B6%E6%B2%A1%E6%90%9C%E7%B4%A2%E5%87%BA%E5%BD%93%E5%89%8D%E7%9A%84%E8%BF%99%E4%B8%AA%E5%BA%8F%E5%88%97y%E9%82%A3%E6%9F%90%E7%A7%8D%E7%A8%8B%E5%BA%A6%E4%B8%8A%E5%B0%B1%E8%A1%A8%E6%98%8E%E4%BA%86%E5%9F%BA%E4%BA%8E%E8%BF%99%E4%B8%AAdocument%E6%83%B3%E8%A7%A3%E7%A0%81%E5%87%BA%E5%BD%93%E5%89%8D%E5%BA%8F%E5%88%97y%E5%85%B6%E5%AE%9E%E6%98%AF%E6%AF%94%E8%BE%83%E9%9A%BE%E7%9A%84%E6%A6%82%E7%8E%87%E6%98%AF%E6%AF%94%E8%BE%83%E5%B0%8F%E7%9A%84%E9%82%A3%E4%B9%88%E4%B8%BA%E4%BA%86%E6%96%B9%E4%BE%BF%E8%AE%A1%E7%AE%97%E4%B8%8D%E5%A6%A8%E5%B0%86%E8%BF%99%E4%B8%AA%E6%A6%82%E7%8E%87%E8%BF%91%E4%BC%BC%E4%B8%BA0-%E7%94%B1%E6%AD%A4%E5%B0%91%E4%BA%86%E5%BE%88%E5%A4%9A%E7%9A%84%E8%AE%A1%E7%AE%97%E9%87%8F%E8%87%B3%E4%BA%8E%E6%95%88%E6%9E%9C%E8%AE%BA%E6%96%87%E5%BA%94%E8%AF%A5%E6%98%AF%E6%9C%89%E5%AE%9E%E9%AA%8C%E8%BF%87%E5%8F%91%E7%8E%B0%E4%B8%A4%E7%A7%8D%E5%9C%A8%E7%BB%93%E6%9E%9C%E4%B8%8A%E5%B7%AE%E4%B8%8D%E5%A4%AA%E5%A4%9A%E4%BD%86%E6%98%AF%E9%80%9F%E5%BA%A6%E4%B8%8Afast-decoding%E8%BF%98%E6%98%AF%E5%BF%AB%E4%BA%86%E6%AF%94%E8%BE%83%E5%A4%9A%E6%89%80%E4%BB%A5%E6%9C%80%E7%BB%88%E6%89%8D%E9%87%87%E5%8F%96%E7%9A%84%E8%BF%99%E4%B8%AA%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5%E5%90%A7%E6%9C%AC%E8%B4%A8%E4%B8%8A%E8%BF%99%E5%B0%B1%E6%98%AF%E4%B8%80%E4%B8%AA%E8%B4%A8%E9%87%8F%E5%92%8C%E9%80%9F%E5%BA%A6%E7%9A%84trade-off"><span class="nav-number">6.7.3.3.</span> <span class="nav-text">我们发现，Thorough
Decoding中，可能会需要补充算一些序列的解码概率。并且set of
hypothesis一般还比较大，这就导致需要补充算的概率可能还不少，无疑会增加很多计算量。故论文的Fast
Decoding就对此作出了一些改进。简单来说，在上面，我们如果遇到\(p_{\theta}(y|x,z_{i})\)没算出来，会重新算一下；但在Fast
Decoding中，我们直接把这个概率近似为0（直观上可以这样理解：在document
\(z_{i}\)中进行beam
search的时候，竟然没搜索出当前的这个序列y，那某种程度上就表明了，基于这个document，想解码出当前序列y，其实是比较难的，概率是比较小的。那么，为了方便计算，不妨将这个概率近似为0）由此少了很多的计算量。至于效果，论文应该是有实验过，发现两种在结果上差不太多，但是速度上Fast
Decoding还是快了比较多，所以最终才采取的这个解码策略吧。本质上，这就是一个质量和速度的trade-off</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">232</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">76</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">104</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
