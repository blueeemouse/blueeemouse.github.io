<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Venue：arxiv date：2025-01-22 动机： 基于知识图谱的RAG是一种常用的，缓解llm幻觉和知识匮乏的问题的手段。但是，现有的KG-RAG的方法有一个问题，就是灵活性和检索质量之间通常需要进行权衡。论文提出的FRAG就是想在这两个方面都做得比较好 现有的KG-RAG方法主要有两类，一类是modular methods，它的主要特点是简单，不会用KG对llm进行微调，">
<meta property="og:type" content="article">
<meta property="og:title" content="FRAG：A Flexible Modular Framework for Retrieval-Augmented Generation based on Knowledge Graphs">
<meta property="og:url" content="https://blueeemouse.github.io/2025/08/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/GraphRAG/FRAG%EF%BC%9AA%20Flexible%20Modular%20Framework%20for%20Retrieval-Augmented%20Generation%20based%20on%20Knowledge%20Graphs/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="Venue：arxiv date：2025-01-22 动机： 基于知识图谱的RAG是一种常用的，缓解llm幻觉和知识匮乏的问题的手段。但是，现有的KG-RAG的方法有一个问题，就是灵活性和检索质量之间通常需要进行权衡。论文提出的FRAG就是想在这两个方面都做得比较好 现有的KG-RAG方法主要有两类，一类是modular methods，它的主要特点是简单，不会用KG对llm进行微调，">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-08-03T03:21:00.000Z">
<meta property="article:modified_time" content="2025-08-04T10:58:49.860Z">
<meta property="article:author" content="bluemouse">
<meta property="article:tag" content="RAG">
<meta property="article:tag" content="GraphRAG">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/2025/08/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/GraphRAG/FRAG%EF%BC%9AA%20Flexible%20Modular%20Framework%20for%20Retrieval-Augmented%20Generation%20based%20on%20Knowledge%20Graphs/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>FRAG：A Flexible Modular Framework for Retrieval-Augmented Generation based on Knowledge Graphs | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/08/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/GraphRAG/FRAG%EF%BC%9AA%20Flexible%20Modular%20Framework%20for%20Retrieval-Augmented%20Generation%20based%20on%20Knowledge%20Graphs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          FRAG：A Flexible Modular Framework for Retrieval-Augmented Generation based on Knowledge Graphs
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-08-03 11:21:00" itemprop="dateCreated datePublished" datetime="2025-08-03T11:21:00+08:00">2025-08-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-04 18:58:49" itemprop="dateModified" datetime="2025-08-04T18:58:49+08:00">2025-08-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/GraphRAG/" itemprop="url" rel="index"><span itemprop="name">GraphRAG</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="venuearxiv">Venue：arxiv</h1>
<h1 id="date2025-01-22">date：2025-01-22</h1>
<h1 id="动机">动机：</h1>
<h2 id="基于知识图谱的rag是一种常用的缓解llm幻觉和知识匮乏的问题的手段但是现有的kg-rag的方法有一个问题就是灵活性和检索质量之间通常需要进行权衡论文提出的frag就是想在这两个方面都做得比较好">基于知识图谱的RAG是一种常用的，缓解llm幻觉和知识匮乏的问题的手段。但是，现有的KG-RAG的方法有一个问题，就是灵活性和检索质量之间通常需要进行权衡。论文提出的FRAG就是想在这两个方面都做得比较好</h2>
<h4 id="现有的kg-rag方法主要有两类一类是modular-methods它的主要特点是简单不会用kg对llm进行微调因此使用更加方便但通常是用固定的策略来检索所以检索的质量一般不高">现有的KG-RAG方法主要有两类，一类是modular
methods，它的主要特点是简单，不会用KG对llm进行微调，因此使用更加方便。但通常是用固定的策略来检索，所以检索的质量一般不高</h4>
<h4 id="另一类是coupled-methods也就是耦合的方法它需要用kg对llm进行微调从而让llm学到kg里的关系但因为需要微调所以很麻烦加上用一个kg微调完之后如果换了一个kg又需要重新微调就更加麻烦了">另一类是coupled
methods，也就是耦合的方法，它需要用KG对llm进行微调，从而让llm学到KG里的关系。但因为需要微调，所以很麻烦；加上用一个KG微调完之后，如果换了一个KG，又需要重新微调，就更加麻烦了</h4>
<h1 id="insight">insight：</h1>
<h1 id="contribution">contribution：</h1>
<span id="more"></span>
<h1 id="method">method：</h1>
<h2 id="总览frag的方法它大概是由三个模块组成第一个模块是reasoning-aware-module它用于判断query是简单的还是复杂的后续根据query的难度检索策略会有所不同所以本质上这里就是一个二分类模型-第二个模块是flexible-retrieval-module它做的主要是把整个的检索流程划分成三步并且如上所述对不同难度的query有不同的检索策略为的就是提高检索的精确度-第三个模块则是reasoning-module这里做的其实就是把上面检索得到的路径以及问题本身都拿给llm辅助生成-一个可选的操作是根据llm的生成结果我们可以对前面的模块进行调整">总览FRAG的方法，它大概是由三个模块组成。第一个模块是reasoning-aware
module，它用于判断query是简单的还是复杂的，后续根据query的难度，检索策略会有所不同（所以本质上这里就是一个二分类模型）；<br>第二个模块是flexible-retrieval
module，它做的主要是把整个的检索流程划分成三步，并且如上所述，对不同难度的query有不同的检索策略，为的就是提高检索的精确度<br>第三个模块则是reasoning
module，这里做的其实就是把上面检索得到的路径以及问题本身都拿给llm，辅助生成<br>一个可选的操作是，根据llm的生成结果，我们可以对前面的模块进行调整</h2>
<h2 id="方法的亮点或者说比现有方法的优点在于两部分第一点是因为有前面的reasoning-aware-module我们对一个query会事先有一个大致的难度判断虽然未必百分百准确根据难度我们后面也会采用相应的检索策略这就减少了小题大做大题小做的风险小题大做就是说一个很简单的query一共也没几个hops但是检索时却跳了很多步导致检索得到了很多无用的信息大题小做则是指一个很复杂的query它的正确路径是很多hops的但是检索的时候却只检索很少的hops那无疑是找不到正确答案的-第二点则是我们训练reasoning-aware-module的时候用的数据集不是特定的某一个kg而是公共的kg这样是有利于提高模型的泛用性的不至于说换了一个kg就必须得再微调了">方法的亮点，或者说比现有方法的优点，在于两部分。第一点是，因为有前面的reasoning-aware
module，我们对一个query会事先有一个大致的难度判断（虽然未必百分百准确），根据难度，我们后面也会采用相应的检索策略，这就减少了小题大做、大题小做的风险（小题大做，就是说，一个很简单的query，一共也没几个hops，但是检索时却跳了很多步，导致检索得到了很多无用的信息；大题小做，则是指，一个很复杂的query，它的正确路径是很多hops的，但是检索的时候却只检索很少的hops，那无疑是找不到正确答案的）<br>第二点则是，我们训练reasoning-aware
module的时候，用的数据集不是特定的某一个KG，而是公共的KG，这样是有利于提高模型的泛用性的，不至于说换了一个KG就必须得再微调了</h2>
<h2 id="下面详细讲讲三个模块">下面详细讲讲三个模块</h2>
<h2 id="reasoning-aware-module">1. Reasoning-aware Module</h2>
<h3 id="正如上所述这个模块就是为了尽可能识别出query的复杂程度也就是要进行分类按照上面的定义如果我们能直接根据kg搜索一下当前query的路径那就可以获得该query精确的hops数了可以精确地对query分类但是这样就会对当前的kg有依赖无法适配其它的kg了">正如上所述，这个模块就是为了尽可能识别出query的复杂程度，也就是要进行分类。按照上面的定义，如果我们能直接根据KG，搜索一下当前query‘的路径，那就可以获得该query精确的hops数了，可以精确地对query分类。但是，这样就会对当前的KG有依赖，无法适配其它的KG了</h3>
<h5 id="什么意思呢举个例子如果是一个modular-method那么它在一个kg上检索的时候是采用固定的策略不会用kg对llm进行微调但是这个策略显然是在当前kg上效果好的但迁移到其它kg上则未必如果换了kg可能就要重新搜索一下换一下策略了而如果是coupled-method那它会用kg对llm进行微调那此时一旦换kg就特别麻烦了得微调">什么意思呢？举个例子，如果是一个modular
method，那么它在一个KG上检索的时候是采用固定的策略，不会用KG对llm进行微调。但是这个策略显然是在当前KG上效果好的，但迁移到其它KG上则未必，如果换了KG，可能就要重新搜索一下，换一下策略了；而如果是coupled
method，那它会用KG对llm进行微调，那此时一旦换KG，就特别麻烦了，得微调</h5>
<h3 id="这样看下来为了保证方法的通用性最好能做到对query分类的时候不依赖kg所以论文才会考虑仅用query自身来分类加上观察到的经验性结论一个query的hops数其实和它自己的一些特点是有关系的所以不妨直接训练一个模型来捕捉这其中的关系故我们只需要训练一个encoder来编码query得到特征之后训一个二分类模型即可到这里我们就完成了仅用query自身估计query是否复杂从粗粒度的角度看这个也是勉强能用的虽然乍一看特别粗糙真要精确或许可以考虑有没有能快速利用kg的方法以及这个点有些细微未必就会对结果有太大的影响">这样看下来，为了保证方法的通用性，最好能做到，对query分类的时候不依赖KG。所以论文才会考虑仅用query自身来分类。加上观察到的经验性结论，一个query的hops数，其实和它自己的一些特点是有关系的，所以不妨直接训练一个模型来捕捉这其中的关系。故，我们只需要训练一个encoder，来编码query，得到特征，之后训一个二分类模型即可。到这里，我们就完成了：仅用query自身，估计query是否复杂（从粗粒度的角度看，这个也是勉强能用的。虽然乍一看特别粗糙……）（真要精确，或许可以考虑有没有能快速利用KG的方法？以及这个点有些细微，未必就会对结果有太大的影响）</h3>
<h3 id="另外这里有个可选项就是上面提到过的利用llm的输出来优化具体来说我们只需要在给llm的prompt里加一些内容让llm根据它收到的若干条推理路径这个就是从kg里获得的了根据上面我们对query标签的定义来得到一个新的refined后的标签我们得到这个refined-label后可以快速对分类器进行一个微调这样就完成了利用llm来优化的过程这样看这个自优化的过程其实也对上面二分类的粗糙有适当的弥补">另外，这里有个可选项，就是上面提到过的利用llm的输出来优化。具体来说，我们只需要在给llm的prompt里加一些内容，让llm根据它收到的若干条推理路径（这个就是从KG里获得的了），根据上面我们对query标签的定义，来得到一个新的、refined后的标签。我们得到这个refined
label后，可以快速对分类器进行一个微调，这样就完成了利用llm来优化的过程（这样看，这个自优化的过程其实也对上面二分类的粗糙有适当的弥补）</h3>
<h2 id="flexible-retrieval-module">2. Flexible-retrieval Module</h2>
<h3 id="这个模块的最终目标是根据query检索到最精确的推理路径而它具体是分三步走的如上所述preprocessing-retrieval-postprocessing-preprocessing是用于缩减检索范围主要是提取出一些重要的子图可以减少计算量-retrieval就是根据提取出来的子图进行检索-postprocessing则是把上一步检索出来的结果进行一定的筛选以免得到过多的检索结果从而减少引入的噪声提高最终生成的质量-值得注意的是这三步其实是相对独立的所以每一步里都可以采用各自合适的方法">这个模块的最终目标是，根据query，检索到最精确的推理路径。而它具体是分三步走的，如上所述，preprocessing-retrieval-postprocessing<br>preprocessing是用于缩减检索范围，主要是提取出一些重要的子图，可以减少计算量<br>retrieval就是根据提取出来的子图进行检索<br>postprocessing则是把上一步检索出来的结果进行一定的筛选，以免得到过多的检索结果，从而减少引入的噪声，提高最终生成的质量<br>值得注意的是，这三步其实是相对独立的，所以每一步里都可以采用各自合适的方法</h3>
<h3 id="preprocessing">2.1. preprocessing</h3>
<h4 id="这里有两个操作一个是从kg里筛出子图另一个是对筛出的子图再进行化简">这里有两个操作，一个是从KG里筛出子图，另一个是对筛出的子图再进行化简</h4>
<h4 id="筛出子图首先要根据query获取它涉及的实体以这些实体为中心节点获取各个k阶子图然后把这些子图给合并起来这个是相对合理的因为query里涉及的实体经验上看肯定是比较重要的从这些节点出发的子图是比较有可能得到正确答案的至于阶数k它是一个超参数可以根据query的复杂程度进行调节">筛出子图，首先要根据query，获取它涉及的实体，以这些实体为中心节点，获取各个k阶子图，然后把这些子图给合并起来（这个是相对合理的，因为query里涉及的实体，经验上看，肯定是比较重要的。从这些节点出发的子图，是比较有可能得到正确答案的）（至于阶数k，它是一个超参数，可以根据query的复杂程度进行调节）</h4>
<h4 id="对筛出的子图进行化简则是有两个角度即删边或删点但大体的思路都是类似的用某个模型方法对各个边点进行打分再选出得分top-m高的边点就完成了对子图的剪枝了可选用的模型方法还是比较多的">对筛出的子图进行化简，则是有两个角度，即删边或删点。但大体的思路都是类似的，用某个模型/方法，对各个边/点进行打分，再选出得分top-m高的边/点，就完成了对子图的剪枝了。可选用的模型/方法还是比较多的</h4>
<h3 id="retrieval">2.2. retrieval</h3>
<h4 id="此处我们要基于上一步得到的子图进行检索如果是传统的kgqa任务也就是基于知识图谱的问答那么一般答案都是在推理路径里的这种的问题一般也比较呆比如xx电影的xx导演的配偶是逻辑非常线性但是rag它本质上是给llm提供更多辅助性信息比如提供更多事实让llm能更好地输出从这个角度上看我们反而会期望检索到的推理路径在合理的范围内长一点因为这样会包含更多信息当然是仅限合理的范围内如果推理路径过长信息过多prompt就更长计算成本会增加且llm也未必就能捕捉到其中的关键信息了有可能淹没于其中了">此处，我们要基于上一步得到的子图，进行检索。如果是传统的KGQA任务（也就是基于知识图谱的问答），那么一般答案都是在推理路径里的（这种的问题一般也比较“呆”，比如，”xx电影的xx导演的配偶是？“，逻辑非常线性）；但是RAG，它本质上是给llm提供更多辅助性信息（比如提供更多事实），让llm能更好地输出。从这个角度上看，我们反而会期望检索到的推理路径在合理的范围内长一点（因为这样会包含更多信息。当然是仅限合理的范围内。如果推理路径过长，信息过多，prompt就更长，计算成本会增加，且llm也未必就能捕捉到其中的关键信息了，有可能淹没于其中了）</h4>
<h4 id="因此论文才提出对两种不同类型的querysimplecomplex采用不同的检索策略对简单的query它的对应路径一般比较短那么此时采用bfs能相对捕获更多信息同时答案也不容易漏-对复杂的query则不再考虑用bfs因为这样很可能计算量过大这里我们就考虑找尽可能短的路径了所以用dijkstra算法去搜索出query实体到各个节点的最短路径">因此，论文才提出对两种不同类型的query（simple/complex），采用不同的检索策略。对简单的query，它的对应路径一般比较短，那么，此时采用BFS，能相对捕获更多信息，同时答案也不容易漏；<br>对复杂的query，则不再考虑用BFS，因为这样很可能计算量过大。这里我们就考虑找尽可能短的路径了，所以用Dijkstra算法去搜索出query实体到各个节点的最短路径</h4>
<h3 id="postprocessing">2.3. postprocessing</h3>
<h4 id="上面的检索会检索出很多冗余的路径比如对简单query我们的bfs就会检索出特别多的路径类似的对于复杂querydijkstra算法是会检索出query里的实体到各个节点的最短路径的这显然会有特别多路径肯定不能全都拿给llm坏处太多了占用上下文计算成本llm未必能理解-因此需要筛选而显然筛选的标准就是这些路径是否和query尽可能相关-这里主要是用一个path-ranking-modelprm也就是对搜索到的路径进行打分这个分数的含义是这些路径和query的相似性我们认为这表征了相关性再从中选出得分top-u高的路径这些才是最终要给llm的">上面的检索，会检索出很多冗余的路径（比如对简单query，我们的BFS就会检索出特别多的路径；类似的，对于复杂query，Dijkstra算法是会检索出query里的实体到各个节点的最短路径的，这显然会有特别多路径），肯定不能全都拿给llm，坏处太多了（占用上下文，计算成本，llm未必能理解……）<br>因此需要筛选。而显然筛选的标准就是这些路径是否和query尽可能相关<br>这里主要是用一个path
ranking
model（PRM），也就是对搜索到的路径进行打分（这个分数的含义，是这些路径和query的相似性，我们认为这表征了相关性）；再从中选出得分top-u高的路径，这些才是最终要给llm的</h4>
<h2 id="reasoning-module">3. Reasoning Module</h2>
<h3 id="这里就没什么了我们前面已经做了绝大部分的工作了此处会设计一套prompt的模板把原来的问题和我们筛选得到的reasoning-paths结合起来拿给llm让它回答">这里就没什么了，我们前面已经做了绝大部分的工作了。此处会设计一套prompt的模板，把原来的问题和我们筛选得到的reasoning
paths结合起来，拿给llm，让它回答</h3>
<h1 id="发展脉络">发展脉络：</h1>
<h2 id="大体上是因为使用llm的时候遇到了幻觉以及知识过期的问题所以提出rag技术来缓解这些问题早期朴素的rag效果有但是不够好后面的一些改进则主要聚焦于提高检索精确度但这类rag它们都是基于document的经常检索得到的结果里有很多无关的内容也即noise这其实是不利于模型的生成的有可能它迷失在无关的内容里了-因此研究转向考虑更改外部知识的格式来解决上面的检索到噪声的问题具体来说尝试的是把它们组织成graph的形式的且通常是knowledge-graph因此引出了graphrag的研究-基于knowledge-graph的rag技术被称为kg-rag它是graphrag里的一个分支主要做的是检索出query相关的top-k的reasoning-paths把这些加入到llm的输入中帮助llm进行推理">大体上，是因为使用llm的时候遇到了幻觉以及知识过期的问题，所以提出RAG技术来缓解这些问题。早期朴素的RAG，效果有，但是不够好；后面的一些改进则主要聚焦于提高检索精确度。但这类RAG，它们都是基于document的，经常检索得到的结果里有很多无关的内容（也即noise），这其实是不利于模型的生成的（有可能它“迷失”在无关的内容里了）<br>因此，研究转向，考虑更改外部知识的格式，来解决上面的检索到噪声的问题。具体来说，尝试的是把它们组织成graph的形式的（且通常是knowledge
graph），因此引出了GraphRAG的研究<br>基于knowledge
graph的RAG技术，被称为KG-RAG，它是GraphRAG里的一个分支（？），主要做的是，检索出query相关的top-k的reasoning
paths，把这些加入到llm的输入中，帮助llm进行推理</h2>
<h1 id="preliminary">preliminary：</h1>
<h2 id="简要介绍一下kg-rag的工作流程其实和传统的rag差别主要在于检索的过程kg-rag它大体上有两步第一步是检索第二步是生成检索就是根据给定的query在knowledg-graph中检索出若干条reasoning-paths也就是根据query确定一个起始的entity节点然后开始往外跳最终会得到多条路径这就是reasoning-paths再用一个排序的模型对这些paths进行打分最后选出top-k的reasoning-paths-至于生成阶段则是把上面选出的top-k的paths加入到llm的输入中进行增强让llm生成更加合理的输出">简要介绍一下KG-RAG的工作流程。其实和传统的RAG差别主要在于检索的过程。KG-RAG，它大体上有两步，第一步是检索，第二步是生成。检索，就是根据给定的query，在knowledg
graph中检索出若干条reasoning
paths（也就是，根据query，确定一个起始的entity节点，然后开始往外跳，最终会得到多条路径，这就是reasoning
paths）；再用一个排序的模型，对这些paths进行打分，最后选出top-k的reasoning
paths<br>至于生成阶段，则是把上面选出的top-k的paths，加入到llm的输入中进行增强，让llm生成更加合理的输出</h2>
<h2 id="另外对于一个kg-rag-reasoning-task我们可以判断它是简单的还是复杂的主要采用的基准是正确推理路径的跳数hops一般给定一个阈值delta如果一个query的最短的正确推理路径的hops小于等于这个阈值则我们认为这个query是属于简单类型的反之则认为这个query是复杂类型的一个query的正确推理路径是可以有多条的因为我们推理的目标是找到正确的答案实体query则指定了起点那么起点终点一定中间的路程未必仅有一条但多条路径中一定会有一条的hops是最小的我们就考虑用它来判断query的难易程度">另外，对于一个KG-RAG
reasoning
task，我们可以判断它是“简单”的还是“复杂”的。主要采用的基准是正确推理路径的跳数（hops）。一般，给定一个阈值<span class="math inline">\(\delta\)</span>，如果一个query的最短的正确推理路径的hops小于等于这个阈值，则我们认为这个query是属于简单类型的；反之则认为这个query是复杂类型的（一个query的正确推理路径是可以有多条的。因为我们推理的目标是找到正确的答案实体，query则指定了起点；那么起点终点一定，中间的路程未必仅有一条。但多条路径中一定会有一条的hops是最小的，我们就考虑用它来判断query的难易程度）</h2>
<p>FRAG（Flexible Modular Framework）通过 “推理感知模块” 和
“灵活检索模块” 协同工作，平衡灵活性与检索质量，具体如下：</p>
<ol type="1">
<li><p><strong>推理感知模块</strong>：</p>
<ul>
<li>仅基于查询文本（不依赖 KG）预测推理路径的
“跳数范围”（通过粗粒度分类将查询分为 “简单” 或
“复杂”）。例如，查询越复杂，推理路径跳数越多（如≥3 跳为复杂）。</li>
<li>训练跨域二分类器（基于公共 KGQA 数据集），并可通过 LLMs
反馈优化分类器性能，确保对不同领域 KG 的通用性。</li>
</ul></li>
<li><p><strong>灵活检索模块</strong>：</p>
<ul>
<li>针对 “简单” 和 “复杂” 查询设计差异化检索管道（预处理 - 检索 -
后处理）：
<ul>
<li><strong>简单查询</strong>（短推理路径，如≤2
跳）：采用广度优先搜索（BFS）遍历所有路径，确保信息全面，减少遗漏；</li>
<li><strong>复杂查询</strong>（长推理路径，如≥3
跳）：采用最短路径算法（如
Dijkstra），减少冗余和计算开销，提升效率。</li>
</ul></li>
<li>后处理阶段通过路径排序模型（如 BGE）筛选与查询最相关的路径，优化输入
LLMs 的上下文。</li>
</ul></li>
<li><p><strong>推理模块</strong>：将筛选后的推理路径与查询结合，生成增强提示输入
LLMs，无需额外微调或调用，高效生成答案。</p></li>
</ol>
<h3 id="四方法的创新性">四、方法的创新性</h3>
<ol type="1">
<li><p><strong>基于查询推断结构信息，解耦 KG 依赖</strong>：<br>
首次提出仅通过查询文本（而非
KG）预测推理路径的跳数范围（结构信息），既避免了耦合方法对 KG
的强依赖（保持灵活性），又为检索策略提供了针对性指导（提升质量）。</p></li>
<li><p><strong>动态适配的检索策略</strong>：<br>
根据查询复杂度动态切换检索算法（BFS
用于简单任务，最短路径用于复杂任务），解决了模块化方法 “一刀切”
策略的缺陷，兼顾了信息完整性与效率。</p></li>
<li><p><strong>高效低耗的设计</strong>：<br>
无需对 LLMs 进行 KG 相关微调，也无需额外 LLM 调用（检索阶段 0
次调用），大幅降低计算成本（训练仅需 306 秒，远低于 RoG 的 38
小时），同时保持通用性（可适配不同 KG 和 LLMs）。</p></li>
<li><p><strong>融合模块化与耦合方法的优势</strong>：<br>
既保留了模块化方法的灵活性（ plug-and-play 集成外部
KG），又通过结构化信息预测实现了耦合方法的检索质量提升，打破了两者的固有矛盾。</p></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/RAG/" rel="tag"># RAG</a>
              <a href="/tags/GraphRAG/" rel="tag"># GraphRAG</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/07/21/algo/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E7%BB%8F%E5%85%B8%E7%BA%BF%E6%80%A7DP/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97%E7%B1%BB/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/" rel="prev" title="最长递增子序列">
      <i class="fa fa-chevron-left"></i> 最长递增子序列
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/08/04/algo/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E7%BD%91%E6%A0%BC%E5%9B%BEDP/%E5%9F%BA%E7%A1%80%E7%B1%BB/%E4%B8%8D%E5%90%8C%E8%B7%AF%E5%BE%84/" rel="next" title="不同路径">
      不同路径 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#venuearxiv"><span class="nav-number">1.</span> <span class="nav-text">Venue：arxiv</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#date2025-01-22"><span class="nav-number">2.</span> <span class="nav-text">date：2025-01-22</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA"><span class="nav-number">3.</span> <span class="nav-text">动机：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84rag%E6%98%AF%E4%B8%80%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E7%BC%93%E8%A7%A3llm%E5%B9%BB%E8%A7%89%E5%92%8C%E7%9F%A5%E8%AF%86%E5%8C%AE%E4%B9%8F%E7%9A%84%E9%97%AE%E9%A2%98%E7%9A%84%E6%89%8B%E6%AE%B5%E4%BD%86%E6%98%AF%E7%8E%B0%E6%9C%89%E7%9A%84kg-rag%E7%9A%84%E6%96%B9%E6%B3%95%E6%9C%89%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%E5%B0%B1%E6%98%AF%E7%81%B5%E6%B4%BB%E6%80%A7%E5%92%8C%E6%A3%80%E7%B4%A2%E8%B4%A8%E9%87%8F%E4%B9%8B%E9%97%B4%E9%80%9A%E5%B8%B8%E9%9C%80%E8%A6%81%E8%BF%9B%E8%A1%8C%E6%9D%83%E8%A1%A1%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E7%9A%84frag%E5%B0%B1%E6%98%AF%E6%83%B3%E5%9C%A8%E8%BF%99%E4%B8%A4%E4%B8%AA%E6%96%B9%E9%9D%A2%E9%83%BD%E5%81%9A%E5%BE%97%E6%AF%94%E8%BE%83%E5%A5%BD"><span class="nav-number">3.1.</span> <span class="nav-text">基于知识图谱的RAG是一种常用的，缓解llm幻觉和知识匮乏的问题的手段。但是，现有的KG-RAG的方法有一个问题，就是灵活性和检索质量之间通常需要进行权衡。论文提出的FRAG就是想在这两个方面都做得比较好</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E6%9C%89%E7%9A%84kg-rag%E6%96%B9%E6%B3%95%E4%B8%BB%E8%A6%81%E6%9C%89%E4%B8%A4%E7%B1%BB%E4%B8%80%E7%B1%BB%E6%98%AFmodular-methods%E5%AE%83%E7%9A%84%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9%E6%98%AF%E7%AE%80%E5%8D%95%E4%B8%8D%E4%BC%9A%E7%94%A8kg%E5%AF%B9llm%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%9B%A0%E6%AD%A4%E4%BD%BF%E7%94%A8%E6%9B%B4%E5%8A%A0%E6%96%B9%E4%BE%BF%E4%BD%86%E9%80%9A%E5%B8%B8%E6%98%AF%E7%94%A8%E5%9B%BA%E5%AE%9A%E7%9A%84%E7%AD%96%E7%95%A5%E6%9D%A5%E6%A3%80%E7%B4%A2%E6%89%80%E4%BB%A5%E6%A3%80%E7%B4%A2%E7%9A%84%E8%B4%A8%E9%87%8F%E4%B8%80%E8%88%AC%E4%B8%8D%E9%AB%98"><span class="nav-number">3.1.0.1.</span> <span class="nav-text">现有的KG-RAG方法主要有两类，一类是modular
methods，它的主要特点是简单，不会用KG对llm进行微调，因此使用更加方便。但通常是用固定的策略来检索，所以检索的质量一般不高</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%A6%E4%B8%80%E7%B1%BB%E6%98%AFcoupled-methods%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%80%A6%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95%E5%AE%83%E9%9C%80%E8%A6%81%E7%94%A8kg%E5%AF%B9llm%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%E4%BB%8E%E8%80%8C%E8%AE%A9llm%E5%AD%A6%E5%88%B0kg%E9%87%8C%E7%9A%84%E5%85%B3%E7%B3%BB%E4%BD%86%E5%9B%A0%E4%B8%BA%E9%9C%80%E8%A6%81%E5%BE%AE%E8%B0%83%E6%89%80%E4%BB%A5%E5%BE%88%E9%BA%BB%E7%83%A6%E5%8A%A0%E4%B8%8A%E7%94%A8%E4%B8%80%E4%B8%AAkg%E5%BE%AE%E8%B0%83%E5%AE%8C%E4%B9%8B%E5%90%8E%E5%A6%82%E6%9E%9C%E6%8D%A2%E4%BA%86%E4%B8%80%E4%B8%AAkg%E5%8F%88%E9%9C%80%E8%A6%81%E9%87%8D%E6%96%B0%E5%BE%AE%E8%B0%83%E5%B0%B1%E6%9B%B4%E5%8A%A0%E9%BA%BB%E7%83%A6%E4%BA%86"><span class="nav-number">3.1.0.2.</span> <span class="nav-text">另一类是coupled
methods，也就是耦合的方法，它需要用KG对llm进行微调，从而让llm学到KG里的关系。但因为需要微调，所以很麻烦；加上用一个KG微调完之后，如果换了一个KG，又需要重新微调，就更加麻烦了</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#insight"><span class="nav-number">4.</span> <span class="nav-text">insight：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#contribution"><span class="nav-number">5.</span> <span class="nav-text">contribution：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#method"><span class="nav-number">6.</span> <span class="nav-text">method：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E8%A7%88frag%E7%9A%84%E6%96%B9%E6%B3%95%E5%AE%83%E5%A4%A7%E6%A6%82%E6%98%AF%E7%94%B1%E4%B8%89%E4%B8%AA%E6%A8%A1%E5%9D%97%E7%BB%84%E6%88%90%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97%E6%98%AFreasoning-aware-module%E5%AE%83%E7%94%A8%E4%BA%8E%E5%88%A4%E6%96%ADquery%E6%98%AF%E7%AE%80%E5%8D%95%E7%9A%84%E8%BF%98%E6%98%AF%E5%A4%8D%E6%9D%82%E7%9A%84%E5%90%8E%E7%BB%AD%E6%A0%B9%E6%8D%AEquery%E7%9A%84%E9%9A%BE%E5%BA%A6%E6%A3%80%E7%B4%A2%E7%AD%96%E7%95%A5%E4%BC%9A%E6%9C%89%E6%89%80%E4%B8%8D%E5%90%8C%E6%89%80%E4%BB%A5%E6%9C%AC%E8%B4%A8%E4%B8%8A%E8%BF%99%E9%87%8C%E5%B0%B1%E6%98%AF%E4%B8%80%E4%B8%AA%E4%BA%8C%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-%E7%AC%AC%E4%BA%8C%E4%B8%AA%E6%A8%A1%E5%9D%97%E6%98%AFflexible-retrieval-module%E5%AE%83%E5%81%9A%E7%9A%84%E4%B8%BB%E8%A6%81%E6%98%AF%E6%8A%8A%E6%95%B4%E4%B8%AA%E7%9A%84%E6%A3%80%E7%B4%A2%E6%B5%81%E7%A8%8B%E5%88%92%E5%88%86%E6%88%90%E4%B8%89%E6%AD%A5%E5%B9%B6%E4%B8%94%E5%A6%82%E4%B8%8A%E6%89%80%E8%BF%B0%E5%AF%B9%E4%B8%8D%E5%90%8C%E9%9A%BE%E5%BA%A6%E7%9A%84query%E6%9C%89%E4%B8%8D%E5%90%8C%E7%9A%84%E6%A3%80%E7%B4%A2%E7%AD%96%E7%95%A5%E4%B8%BA%E7%9A%84%E5%B0%B1%E6%98%AF%E6%8F%90%E9%AB%98%E6%A3%80%E7%B4%A2%E7%9A%84%E7%B2%BE%E7%A1%AE%E5%BA%A6-%E7%AC%AC%E4%B8%89%E4%B8%AA%E6%A8%A1%E5%9D%97%E5%88%99%E6%98%AFreasoning-module%E8%BF%99%E9%87%8C%E5%81%9A%E7%9A%84%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E6%8A%8A%E4%B8%8A%E9%9D%A2%E6%A3%80%E7%B4%A2%E5%BE%97%E5%88%B0%E7%9A%84%E8%B7%AF%E5%BE%84%E4%BB%A5%E5%8F%8A%E9%97%AE%E9%A2%98%E6%9C%AC%E8%BA%AB%E9%83%BD%E6%8B%BF%E7%BB%99llm%E8%BE%85%E5%8A%A9%E7%94%9F%E6%88%90-%E4%B8%80%E4%B8%AA%E5%8F%AF%E9%80%89%E7%9A%84%E6%93%8D%E4%BD%9C%E6%98%AF%E6%A0%B9%E6%8D%AEllm%E7%9A%84%E7%94%9F%E6%88%90%E7%BB%93%E6%9E%9C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%AF%B9%E5%89%8D%E9%9D%A2%E7%9A%84%E6%A8%A1%E5%9D%97%E8%BF%9B%E8%A1%8C%E8%B0%83%E6%95%B4"><span class="nav-number">6.1.</span> <span class="nav-text">总览FRAG的方法，它大概是由三个模块组成。第一个模块是reasoning-aware
module，它用于判断query是简单的还是复杂的，后续根据query的难度，检索策略会有所不同（所以本质上这里就是一个二分类模型）；第二个模块是flexible-retrieval
module，它做的主要是把整个的检索流程划分成三步，并且如上所述，对不同难度的query有不同的检索策略，为的就是提高检索的精确度第三个模块则是reasoning
module，这里做的其实就是把上面检索得到的路径以及问题本身都拿给llm，辅助生成一个可选的操作是，根据llm的生成结果，我们可以对前面的模块进行调整</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E7%9A%84%E4%BA%AE%E7%82%B9%E6%88%96%E8%80%85%E8%AF%B4%E6%AF%94%E7%8E%B0%E6%9C%89%E6%96%B9%E6%B3%95%E7%9A%84%E4%BC%98%E7%82%B9%E5%9C%A8%E4%BA%8E%E4%B8%A4%E9%83%A8%E5%88%86%E7%AC%AC%E4%B8%80%E7%82%B9%E6%98%AF%E5%9B%A0%E4%B8%BA%E6%9C%89%E5%89%8D%E9%9D%A2%E7%9A%84reasoning-aware-module%E6%88%91%E4%BB%AC%E5%AF%B9%E4%B8%80%E4%B8%AAquery%E4%BC%9A%E4%BA%8B%E5%85%88%E6%9C%89%E4%B8%80%E4%B8%AA%E5%A4%A7%E8%87%B4%E7%9A%84%E9%9A%BE%E5%BA%A6%E5%88%A4%E6%96%AD%E8%99%BD%E7%84%B6%E6%9C%AA%E5%BF%85%E7%99%BE%E5%88%86%E7%99%BE%E5%87%86%E7%A1%AE%E6%A0%B9%E6%8D%AE%E9%9A%BE%E5%BA%A6%E6%88%91%E4%BB%AC%E5%90%8E%E9%9D%A2%E4%B9%9F%E4%BC%9A%E9%87%87%E7%94%A8%E7%9B%B8%E5%BA%94%E7%9A%84%E6%A3%80%E7%B4%A2%E7%AD%96%E7%95%A5%E8%BF%99%E5%B0%B1%E5%87%8F%E5%B0%91%E4%BA%86%E5%B0%8F%E9%A2%98%E5%A4%A7%E5%81%9A%E5%A4%A7%E9%A2%98%E5%B0%8F%E5%81%9A%E7%9A%84%E9%A3%8E%E9%99%A9%E5%B0%8F%E9%A2%98%E5%A4%A7%E5%81%9A%E5%B0%B1%E6%98%AF%E8%AF%B4%E4%B8%80%E4%B8%AA%E5%BE%88%E7%AE%80%E5%8D%95%E7%9A%84query%E4%B8%80%E5%85%B1%E4%B9%9F%E6%B2%A1%E5%87%A0%E4%B8%AAhops%E4%BD%86%E6%98%AF%E6%A3%80%E7%B4%A2%E6%97%B6%E5%8D%B4%E8%B7%B3%E4%BA%86%E5%BE%88%E5%A4%9A%E6%AD%A5%E5%AF%BC%E8%87%B4%E6%A3%80%E7%B4%A2%E5%BE%97%E5%88%B0%E4%BA%86%E5%BE%88%E5%A4%9A%E6%97%A0%E7%94%A8%E7%9A%84%E4%BF%A1%E6%81%AF%E5%A4%A7%E9%A2%98%E5%B0%8F%E5%81%9A%E5%88%99%E6%98%AF%E6%8C%87%E4%B8%80%E4%B8%AA%E5%BE%88%E5%A4%8D%E6%9D%82%E7%9A%84query%E5%AE%83%E7%9A%84%E6%AD%A3%E7%A1%AE%E8%B7%AF%E5%BE%84%E6%98%AF%E5%BE%88%E5%A4%9Ahops%E7%9A%84%E4%BD%86%E6%98%AF%E6%A3%80%E7%B4%A2%E7%9A%84%E6%97%B6%E5%80%99%E5%8D%B4%E5%8F%AA%E6%A3%80%E7%B4%A2%E5%BE%88%E5%B0%91%E7%9A%84hops%E9%82%A3%E6%97%A0%E7%96%91%E6%98%AF%E6%89%BE%E4%B8%8D%E5%88%B0%E6%AD%A3%E7%A1%AE%E7%AD%94%E6%A1%88%E7%9A%84-%E7%AC%AC%E4%BA%8C%E7%82%B9%E5%88%99%E6%98%AF%E6%88%91%E4%BB%AC%E8%AE%AD%E7%BB%83reasoning-aware-module%E7%9A%84%E6%97%B6%E5%80%99%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8D%E6%98%AF%E7%89%B9%E5%AE%9A%E7%9A%84%E6%9F%90%E4%B8%80%E4%B8%AAkg%E8%80%8C%E6%98%AF%E5%85%AC%E5%85%B1%E7%9A%84kg%E8%BF%99%E6%A0%B7%E6%98%AF%E6%9C%89%E5%88%A9%E4%BA%8E%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%9B%E7%94%A8%E6%80%A7%E7%9A%84%E4%B8%8D%E8%87%B3%E4%BA%8E%E8%AF%B4%E6%8D%A2%E4%BA%86%E4%B8%80%E4%B8%AAkg%E5%B0%B1%E5%BF%85%E9%A1%BB%E5%BE%97%E5%86%8D%E5%BE%AE%E8%B0%83%E4%BA%86"><span class="nav-number">6.2.</span> <span class="nav-text">方法的亮点，或者说比现有方法的优点，在于两部分。第一点是，因为有前面的reasoning-aware
module，我们对一个query会事先有一个大致的难度判断（虽然未必百分百准确），根据难度，我们后面也会采用相应的检索策略，这就减少了小题大做、大题小做的风险（小题大做，就是说，一个很简单的query，一共也没几个hops，但是检索时却跳了很多步，导致检索得到了很多无用的信息；大题小做，则是指，一个很复杂的query，它的正确路径是很多hops的，但是检索的时候却只检索很少的hops，那无疑是找不到正确答案的）第二点则是，我们训练reasoning-aware
module的时候，用的数据集不是特定的某一个KG，而是公共的KG，这样是有利于提高模型的泛用性的，不至于说换了一个KG就必须得再微调了</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E9%9D%A2%E8%AF%A6%E7%BB%86%E8%AE%B2%E8%AE%B2%E4%B8%89%E4%B8%AA%E6%A8%A1%E5%9D%97"><span class="nav-number">6.3.</span> <span class="nav-text">下面详细讲讲三个模块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reasoning-aware-module"><span class="nav-number">6.4.</span> <span class="nav-text">1. Reasoning-aware Module</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%A6%82%E4%B8%8A%E6%89%80%E8%BF%B0%E8%BF%99%E4%B8%AA%E6%A8%A1%E5%9D%97%E5%B0%B1%E6%98%AF%E4%B8%BA%E4%BA%86%E5%B0%BD%E5%8F%AF%E8%83%BD%E8%AF%86%E5%88%AB%E5%87%BAquery%E7%9A%84%E5%A4%8D%E6%9D%82%E7%A8%8B%E5%BA%A6%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%A6%81%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%E6%8C%89%E7%85%A7%E4%B8%8A%E9%9D%A2%E7%9A%84%E5%AE%9A%E4%B9%89%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E8%83%BD%E7%9B%B4%E6%8E%A5%E6%A0%B9%E6%8D%AEkg%E6%90%9C%E7%B4%A2%E4%B8%80%E4%B8%8B%E5%BD%93%E5%89%8Dquery%E7%9A%84%E8%B7%AF%E5%BE%84%E9%82%A3%E5%B0%B1%E5%8F%AF%E4%BB%A5%E8%8E%B7%E5%BE%97%E8%AF%A5query%E7%B2%BE%E7%A1%AE%E7%9A%84hops%E6%95%B0%E4%BA%86%E5%8F%AF%E4%BB%A5%E7%B2%BE%E7%A1%AE%E5%9C%B0%E5%AF%B9query%E5%88%86%E7%B1%BB%E4%BD%86%E6%98%AF%E8%BF%99%E6%A0%B7%E5%B0%B1%E4%BC%9A%E5%AF%B9%E5%BD%93%E5%89%8D%E7%9A%84kg%E6%9C%89%E4%BE%9D%E8%B5%96%E6%97%A0%E6%B3%95%E9%80%82%E9%85%8D%E5%85%B6%E5%AE%83%E7%9A%84kg%E4%BA%86"><span class="nav-number">6.4.1.</span> <span class="nav-text">正如上所述，这个模块就是为了尽可能识别出query的复杂程度，也就是要进行分类。按照上面的定义，如果我们能直接根据KG，搜索一下当前query‘的路径，那就可以获得该query精确的hops数了，可以精确地对query分类。但是，这样就会对当前的KG有依赖，无法适配其它的KG了</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%E5%91%A2%E4%B8%BE%E4%B8%AA%E4%BE%8B%E5%AD%90%E5%A6%82%E6%9E%9C%E6%98%AF%E4%B8%80%E4%B8%AAmodular-method%E9%82%A3%E4%B9%88%E5%AE%83%E5%9C%A8%E4%B8%80%E4%B8%AAkg%E4%B8%8A%E6%A3%80%E7%B4%A2%E7%9A%84%E6%97%B6%E5%80%99%E6%98%AF%E9%87%87%E7%94%A8%E5%9B%BA%E5%AE%9A%E7%9A%84%E7%AD%96%E7%95%A5%E4%B8%8D%E4%BC%9A%E7%94%A8kg%E5%AF%B9llm%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%E4%BD%86%E6%98%AF%E8%BF%99%E4%B8%AA%E7%AD%96%E7%95%A5%E6%98%BE%E7%84%B6%E6%98%AF%E5%9C%A8%E5%BD%93%E5%89%8Dkg%E4%B8%8A%E6%95%88%E6%9E%9C%E5%A5%BD%E7%9A%84%E4%BD%86%E8%BF%81%E7%A7%BB%E5%88%B0%E5%85%B6%E5%AE%83kg%E4%B8%8A%E5%88%99%E6%9C%AA%E5%BF%85%E5%A6%82%E6%9E%9C%E6%8D%A2%E4%BA%86kg%E5%8F%AF%E8%83%BD%E5%B0%B1%E8%A6%81%E9%87%8D%E6%96%B0%E6%90%9C%E7%B4%A2%E4%B8%80%E4%B8%8B%E6%8D%A2%E4%B8%80%E4%B8%8B%E7%AD%96%E7%95%A5%E4%BA%86%E8%80%8C%E5%A6%82%E6%9E%9C%E6%98%AFcoupled-method%E9%82%A3%E5%AE%83%E4%BC%9A%E7%94%A8kg%E5%AF%B9llm%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%E9%82%A3%E6%AD%A4%E6%97%B6%E4%B8%80%E6%97%A6%E6%8D%A2kg%E5%B0%B1%E7%89%B9%E5%88%AB%E9%BA%BB%E7%83%A6%E4%BA%86%E5%BE%97%E5%BE%AE%E8%B0%83"><span class="nav-number">6.4.1.0.1.</span> <span class="nav-text">什么意思呢？举个例子，如果是一个modular
method，那么它在一个KG上检索的时候是采用固定的策略，不会用KG对llm进行微调。但是这个策略显然是在当前KG上效果好的，但迁移到其它KG上则未必，如果换了KG，可能就要重新搜索一下，换一下策略了；而如果是coupled
method，那它会用KG对llm进行微调，那此时一旦换KG，就特别麻烦了，得微调</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%99%E6%A0%B7%E7%9C%8B%E4%B8%8B%E6%9D%A5%E4%B8%BA%E4%BA%86%E4%BF%9D%E8%AF%81%E6%96%B9%E6%B3%95%E7%9A%84%E9%80%9A%E7%94%A8%E6%80%A7%E6%9C%80%E5%A5%BD%E8%83%BD%E5%81%9A%E5%88%B0%E5%AF%B9query%E5%88%86%E7%B1%BB%E7%9A%84%E6%97%B6%E5%80%99%E4%B8%8D%E4%BE%9D%E8%B5%96kg%E6%89%80%E4%BB%A5%E8%AE%BA%E6%96%87%E6%89%8D%E4%BC%9A%E8%80%83%E8%99%91%E4%BB%85%E7%94%A8query%E8%87%AA%E8%BA%AB%E6%9D%A5%E5%88%86%E7%B1%BB%E5%8A%A0%E4%B8%8A%E8%A7%82%E5%AF%9F%E5%88%B0%E7%9A%84%E7%BB%8F%E9%AA%8C%E6%80%A7%E7%BB%93%E8%AE%BA%E4%B8%80%E4%B8%AAquery%E7%9A%84hops%E6%95%B0%E5%85%B6%E5%AE%9E%E5%92%8C%E5%AE%83%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%80%E4%BA%9B%E7%89%B9%E7%82%B9%E6%98%AF%E6%9C%89%E5%85%B3%E7%B3%BB%E7%9A%84%E6%89%80%E4%BB%A5%E4%B8%8D%E5%A6%A8%E7%9B%B4%E6%8E%A5%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E6%9D%A5%E6%8D%95%E6%8D%89%E8%BF%99%E5%85%B6%E4%B8%AD%E7%9A%84%E5%85%B3%E7%B3%BB%E6%95%85%E6%88%91%E4%BB%AC%E5%8F%AA%E9%9C%80%E8%A6%81%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAencoder%E6%9D%A5%E7%BC%96%E7%A0%81query%E5%BE%97%E5%88%B0%E7%89%B9%E5%BE%81%E4%B9%8B%E5%90%8E%E8%AE%AD%E4%B8%80%E4%B8%AA%E4%BA%8C%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%8D%B3%E5%8F%AF%E5%88%B0%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E5%AE%8C%E6%88%90%E4%BA%86%E4%BB%85%E7%94%A8query%E8%87%AA%E8%BA%AB%E4%BC%B0%E8%AE%A1query%E6%98%AF%E5%90%A6%E5%A4%8D%E6%9D%82%E4%BB%8E%E7%B2%97%E7%B2%92%E5%BA%A6%E7%9A%84%E8%A7%92%E5%BA%A6%E7%9C%8B%E8%BF%99%E4%B8%AA%E4%B9%9F%E6%98%AF%E5%8B%89%E5%BC%BA%E8%83%BD%E7%94%A8%E7%9A%84%E8%99%BD%E7%84%B6%E4%B9%8D%E4%B8%80%E7%9C%8B%E7%89%B9%E5%88%AB%E7%B2%97%E7%B3%99%E7%9C%9F%E8%A6%81%E7%B2%BE%E7%A1%AE%E6%88%96%E8%AE%B8%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E6%9C%89%E6%B2%A1%E6%9C%89%E8%83%BD%E5%BF%AB%E9%80%9F%E5%88%A9%E7%94%A8kg%E7%9A%84%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E8%BF%99%E4%B8%AA%E7%82%B9%E6%9C%89%E4%BA%9B%E7%BB%86%E5%BE%AE%E6%9C%AA%E5%BF%85%E5%B0%B1%E4%BC%9A%E5%AF%B9%E7%BB%93%E6%9E%9C%E6%9C%89%E5%A4%AA%E5%A4%A7%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">6.4.2.</span> <span class="nav-text">这样看下来，为了保证方法的通用性，最好能做到，对query分类的时候不依赖KG。所以论文才会考虑仅用query自身来分类。加上观察到的经验性结论，一个query的hops数，其实和它自己的一些特点是有关系的，所以不妨直接训练一个模型来捕捉这其中的关系。故，我们只需要训练一个encoder，来编码query，得到特征，之后训一个二分类模型即可。到这里，我们就完成了：仅用query自身，估计query是否复杂（从粗粒度的角度看，这个也是勉强能用的。虽然乍一看特别粗糙……）（真要精确，或许可以考虑有没有能快速利用KG的方法？以及这个点有些细微，未必就会对结果有太大的影响）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%A6%E5%A4%96%E8%BF%99%E9%87%8C%E6%9C%89%E4%B8%AA%E5%8F%AF%E9%80%89%E9%A1%B9%E5%B0%B1%E6%98%AF%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E8%BF%87%E7%9A%84%E5%88%A9%E7%94%A8llm%E7%9A%84%E8%BE%93%E5%87%BA%E6%9D%A5%E4%BC%98%E5%8C%96%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%E6%88%91%E4%BB%AC%E5%8F%AA%E9%9C%80%E8%A6%81%E5%9C%A8%E7%BB%99llm%E7%9A%84prompt%E9%87%8C%E5%8A%A0%E4%B8%80%E4%BA%9B%E5%86%85%E5%AE%B9%E8%AE%A9llm%E6%A0%B9%E6%8D%AE%E5%AE%83%E6%94%B6%E5%88%B0%E7%9A%84%E8%8B%A5%E5%B9%B2%E6%9D%A1%E6%8E%A8%E7%90%86%E8%B7%AF%E5%BE%84%E8%BF%99%E4%B8%AA%E5%B0%B1%E6%98%AF%E4%BB%8Ekg%E9%87%8C%E8%8E%B7%E5%BE%97%E7%9A%84%E4%BA%86%E6%A0%B9%E6%8D%AE%E4%B8%8A%E9%9D%A2%E6%88%91%E4%BB%AC%E5%AF%B9query%E6%A0%87%E7%AD%BE%E7%9A%84%E5%AE%9A%E4%B9%89%E6%9D%A5%E5%BE%97%E5%88%B0%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84refined%E5%90%8E%E7%9A%84%E6%A0%87%E7%AD%BE%E6%88%91%E4%BB%AC%E5%BE%97%E5%88%B0%E8%BF%99%E4%B8%AArefined-label%E5%90%8E%E5%8F%AF%E4%BB%A5%E5%BF%AB%E9%80%9F%E5%AF%B9%E5%88%86%E7%B1%BB%E5%99%A8%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E5%BE%AE%E8%B0%83%E8%BF%99%E6%A0%B7%E5%B0%B1%E5%AE%8C%E6%88%90%E4%BA%86%E5%88%A9%E7%94%A8llm%E6%9D%A5%E4%BC%98%E5%8C%96%E7%9A%84%E8%BF%87%E7%A8%8B%E8%BF%99%E6%A0%B7%E7%9C%8B%E8%BF%99%E4%B8%AA%E8%87%AA%E4%BC%98%E5%8C%96%E7%9A%84%E8%BF%87%E7%A8%8B%E5%85%B6%E5%AE%9E%E4%B9%9F%E5%AF%B9%E4%B8%8A%E9%9D%A2%E4%BA%8C%E5%88%86%E7%B1%BB%E7%9A%84%E7%B2%97%E7%B3%99%E6%9C%89%E9%80%82%E5%BD%93%E7%9A%84%E5%BC%A5%E8%A1%A5"><span class="nav-number">6.4.3.</span> <span class="nav-text">另外，这里有个可选项，就是上面提到过的利用llm的输出来优化。具体来说，我们只需要在给llm的prompt里加一些内容，让llm根据它收到的若干条推理路径（这个就是从KG里获得的了），根据上面我们对query标签的定义，来得到一个新的、refined后的标签。我们得到这个refined
label后，可以快速对分类器进行一个微调，这样就完成了利用llm来优化的过程（这样看，这个自优化的过程其实也对上面二分类的粗糙有适当的弥补）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#flexible-retrieval-module"><span class="nav-number">6.5.</span> <span class="nav-text">2. Flexible-retrieval Module</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%99%E4%B8%AA%E6%A8%A1%E5%9D%97%E7%9A%84%E6%9C%80%E7%BB%88%E7%9B%AE%E6%A0%87%E6%98%AF%E6%A0%B9%E6%8D%AEquery%E6%A3%80%E7%B4%A2%E5%88%B0%E6%9C%80%E7%B2%BE%E7%A1%AE%E7%9A%84%E6%8E%A8%E7%90%86%E8%B7%AF%E5%BE%84%E8%80%8C%E5%AE%83%E5%85%B7%E4%BD%93%E6%98%AF%E5%88%86%E4%B8%89%E6%AD%A5%E8%B5%B0%E7%9A%84%E5%A6%82%E4%B8%8A%E6%89%80%E8%BF%B0preprocessing-retrieval-postprocessing-preprocessing%E6%98%AF%E7%94%A8%E4%BA%8E%E7%BC%A9%E5%87%8F%E6%A3%80%E7%B4%A2%E8%8C%83%E5%9B%B4%E4%B8%BB%E8%A6%81%E6%98%AF%E6%8F%90%E5%8F%96%E5%87%BA%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E7%9A%84%E5%AD%90%E5%9B%BE%E5%8F%AF%E4%BB%A5%E5%87%8F%E5%B0%91%E8%AE%A1%E7%AE%97%E9%87%8F-retrieval%E5%B0%B1%E6%98%AF%E6%A0%B9%E6%8D%AE%E6%8F%90%E5%8F%96%E5%87%BA%E6%9D%A5%E7%9A%84%E5%AD%90%E5%9B%BE%E8%BF%9B%E8%A1%8C%E6%A3%80%E7%B4%A2-postprocessing%E5%88%99%E6%98%AF%E6%8A%8A%E4%B8%8A%E4%B8%80%E6%AD%A5%E6%A3%80%E7%B4%A2%E5%87%BA%E6%9D%A5%E7%9A%84%E7%BB%93%E6%9E%9C%E8%BF%9B%E8%A1%8C%E4%B8%80%E5%AE%9A%E7%9A%84%E7%AD%9B%E9%80%89%E4%BB%A5%E5%85%8D%E5%BE%97%E5%88%B0%E8%BF%87%E5%A4%9A%E7%9A%84%E6%A3%80%E7%B4%A2%E7%BB%93%E6%9E%9C%E4%BB%8E%E8%80%8C%E5%87%8F%E5%B0%91%E5%BC%95%E5%85%A5%E7%9A%84%E5%99%AA%E5%A3%B0%E6%8F%90%E9%AB%98%E6%9C%80%E7%BB%88%E7%94%9F%E6%88%90%E7%9A%84%E8%B4%A8%E9%87%8F-%E5%80%BC%E5%BE%97%E6%B3%A8%E6%84%8F%E7%9A%84%E6%98%AF%E8%BF%99%E4%B8%89%E6%AD%A5%E5%85%B6%E5%AE%9E%E6%98%AF%E7%9B%B8%E5%AF%B9%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%89%80%E4%BB%A5%E6%AF%8F%E4%B8%80%E6%AD%A5%E9%87%8C%E9%83%BD%E5%8F%AF%E4%BB%A5%E9%87%87%E7%94%A8%E5%90%84%E8%87%AA%E5%90%88%E9%80%82%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">6.5.1.</span> <span class="nav-text">这个模块的最终目标是，根据query，检索到最精确的推理路径。而它具体是分三步走的，如上所述，preprocessing-retrieval-postprocessingpreprocessing是用于缩减检索范围，主要是提取出一些重要的子图，可以减少计算量retrieval就是根据提取出来的子图进行检索postprocessing则是把上一步检索出来的结果进行一定的筛选，以免得到过多的检索结果，从而减少引入的噪声，提高最终生成的质量值得注意的是，这三步其实是相对独立的，所以每一步里都可以采用各自合适的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#preprocessing"><span class="nav-number">6.5.2.</span> <span class="nav-text">2.1. preprocessing</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E9%87%8C%E6%9C%89%E4%B8%A4%E4%B8%AA%E6%93%8D%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%98%AF%E4%BB%8Ekg%E9%87%8C%E7%AD%9B%E5%87%BA%E5%AD%90%E5%9B%BE%E5%8F%A6%E4%B8%80%E4%B8%AA%E6%98%AF%E5%AF%B9%E7%AD%9B%E5%87%BA%E7%9A%84%E5%AD%90%E5%9B%BE%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%8C%96%E7%AE%80"><span class="nav-number">6.5.2.1.</span> <span class="nav-text">这里有两个操作，一个是从KG里筛出子图，另一个是对筛出的子图再进行化简</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%9B%E5%87%BA%E5%AD%90%E5%9B%BE%E9%A6%96%E5%85%88%E8%A6%81%E6%A0%B9%E6%8D%AEquery%E8%8E%B7%E5%8F%96%E5%AE%83%E6%B6%89%E5%8F%8A%E7%9A%84%E5%AE%9E%E4%BD%93%E4%BB%A5%E8%BF%99%E4%BA%9B%E5%AE%9E%E4%BD%93%E4%B8%BA%E4%B8%AD%E5%BF%83%E8%8A%82%E7%82%B9%E8%8E%B7%E5%8F%96%E5%90%84%E4%B8%AAk%E9%98%B6%E5%AD%90%E5%9B%BE%E7%84%B6%E5%90%8E%E6%8A%8A%E8%BF%99%E4%BA%9B%E5%AD%90%E5%9B%BE%E7%BB%99%E5%90%88%E5%B9%B6%E8%B5%B7%E6%9D%A5%E8%BF%99%E4%B8%AA%E6%98%AF%E7%9B%B8%E5%AF%B9%E5%90%88%E7%90%86%E7%9A%84%E5%9B%A0%E4%B8%BAquery%E9%87%8C%E6%B6%89%E5%8F%8A%E7%9A%84%E5%AE%9E%E4%BD%93%E7%BB%8F%E9%AA%8C%E4%B8%8A%E7%9C%8B%E8%82%AF%E5%AE%9A%E6%98%AF%E6%AF%94%E8%BE%83%E9%87%8D%E8%A6%81%E7%9A%84%E4%BB%8E%E8%BF%99%E4%BA%9B%E8%8A%82%E7%82%B9%E5%87%BA%E5%8F%91%E7%9A%84%E5%AD%90%E5%9B%BE%E6%98%AF%E6%AF%94%E8%BE%83%E6%9C%89%E5%8F%AF%E8%83%BD%E5%BE%97%E5%88%B0%E6%AD%A3%E7%A1%AE%E7%AD%94%E6%A1%88%E7%9A%84%E8%87%B3%E4%BA%8E%E9%98%B6%E6%95%B0k%E5%AE%83%E6%98%AF%E4%B8%80%E4%B8%AA%E8%B6%85%E5%8F%82%E6%95%B0%E5%8F%AF%E4%BB%A5%E6%A0%B9%E6%8D%AEquery%E7%9A%84%E5%A4%8D%E6%9D%82%E7%A8%8B%E5%BA%A6%E8%BF%9B%E8%A1%8C%E8%B0%83%E8%8A%82"><span class="nav-number">6.5.2.2.</span> <span class="nav-text">筛出子图，首先要根据query，获取它涉及的实体，以这些实体为中心节点，获取各个k阶子图，然后把这些子图给合并起来（这个是相对合理的，因为query里涉及的实体，经验上看，肯定是比较重要的。从这些节点出发的子图，是比较有可能得到正确答案的）（至于阶数k，它是一个超参数，可以根据query的复杂程度进行调节）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E7%AD%9B%E5%87%BA%E7%9A%84%E5%AD%90%E5%9B%BE%E8%BF%9B%E8%A1%8C%E5%8C%96%E7%AE%80%E5%88%99%E6%98%AF%E6%9C%89%E4%B8%A4%E4%B8%AA%E8%A7%92%E5%BA%A6%E5%8D%B3%E5%88%A0%E8%BE%B9%E6%88%96%E5%88%A0%E7%82%B9%E4%BD%86%E5%A4%A7%E4%BD%93%E7%9A%84%E6%80%9D%E8%B7%AF%E9%83%BD%E6%98%AF%E7%B1%BB%E4%BC%BC%E7%9A%84%E7%94%A8%E6%9F%90%E4%B8%AA%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%95%E5%AF%B9%E5%90%84%E4%B8%AA%E8%BE%B9%E7%82%B9%E8%BF%9B%E8%A1%8C%E6%89%93%E5%88%86%E5%86%8D%E9%80%89%E5%87%BA%E5%BE%97%E5%88%86top-m%E9%AB%98%E7%9A%84%E8%BE%B9%E7%82%B9%E5%B0%B1%E5%AE%8C%E6%88%90%E4%BA%86%E5%AF%B9%E5%AD%90%E5%9B%BE%E7%9A%84%E5%89%AA%E6%9E%9D%E4%BA%86%E5%8F%AF%E9%80%89%E7%94%A8%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%95%E8%BF%98%E6%98%AF%E6%AF%94%E8%BE%83%E5%A4%9A%E7%9A%84"><span class="nav-number">6.5.2.3.</span> <span class="nav-text">对筛出的子图进行化简，则是有两个角度，即删边或删点。但大体的思路都是类似的，用某个模型&#x2F;方法，对各个边&#x2F;点进行打分，再选出得分top-m高的边&#x2F;点，就完成了对子图的剪枝了。可选用的模型&#x2F;方法还是比较多的</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#retrieval"><span class="nav-number">6.5.3.</span> <span class="nav-text">2.2. retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A4%E5%A4%84%E6%88%91%E4%BB%AC%E8%A6%81%E5%9F%BA%E4%BA%8E%E4%B8%8A%E4%B8%80%E6%AD%A5%E5%BE%97%E5%88%B0%E7%9A%84%E5%AD%90%E5%9B%BE%E8%BF%9B%E8%A1%8C%E6%A3%80%E7%B4%A2%E5%A6%82%E6%9E%9C%E6%98%AF%E4%BC%A0%E7%BB%9F%E7%9A%84kgqa%E4%BB%BB%E5%8A%A1%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E9%97%AE%E7%AD%94%E9%82%A3%E4%B9%88%E4%B8%80%E8%88%AC%E7%AD%94%E6%A1%88%E9%83%BD%E6%98%AF%E5%9C%A8%E6%8E%A8%E7%90%86%E8%B7%AF%E5%BE%84%E9%87%8C%E7%9A%84%E8%BF%99%E7%A7%8D%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%80%E8%88%AC%E4%B9%9F%E6%AF%94%E8%BE%83%E5%91%86%E6%AF%94%E5%A6%82xx%E7%94%B5%E5%BD%B1%E7%9A%84xx%E5%AF%BC%E6%BC%94%E7%9A%84%E9%85%8D%E5%81%B6%E6%98%AF%E9%80%BB%E8%BE%91%E9%9D%9E%E5%B8%B8%E7%BA%BF%E6%80%A7%E4%BD%86%E6%98%AFrag%E5%AE%83%E6%9C%AC%E8%B4%A8%E4%B8%8A%E6%98%AF%E7%BB%99llm%E6%8F%90%E4%BE%9B%E6%9B%B4%E5%A4%9A%E8%BE%85%E5%8A%A9%E6%80%A7%E4%BF%A1%E6%81%AF%E6%AF%94%E5%A6%82%E6%8F%90%E4%BE%9B%E6%9B%B4%E5%A4%9A%E4%BA%8B%E5%AE%9E%E8%AE%A9llm%E8%83%BD%E6%9B%B4%E5%A5%BD%E5%9C%B0%E8%BE%93%E5%87%BA%E4%BB%8E%E8%BF%99%E4%B8%AA%E8%A7%92%E5%BA%A6%E4%B8%8A%E7%9C%8B%E6%88%91%E4%BB%AC%E5%8F%8D%E8%80%8C%E4%BC%9A%E6%9C%9F%E6%9C%9B%E6%A3%80%E7%B4%A2%E5%88%B0%E7%9A%84%E6%8E%A8%E7%90%86%E8%B7%AF%E5%BE%84%E5%9C%A8%E5%90%88%E7%90%86%E7%9A%84%E8%8C%83%E5%9B%B4%E5%86%85%E9%95%BF%E4%B8%80%E7%82%B9%E5%9B%A0%E4%B8%BA%E8%BF%99%E6%A0%B7%E4%BC%9A%E5%8C%85%E5%90%AB%E6%9B%B4%E5%A4%9A%E4%BF%A1%E6%81%AF%E5%BD%93%E7%84%B6%E6%98%AF%E4%BB%85%E9%99%90%E5%90%88%E7%90%86%E7%9A%84%E8%8C%83%E5%9B%B4%E5%86%85%E5%A6%82%E6%9E%9C%E6%8E%A8%E7%90%86%E8%B7%AF%E5%BE%84%E8%BF%87%E9%95%BF%E4%BF%A1%E6%81%AF%E8%BF%87%E5%A4%9Aprompt%E5%B0%B1%E6%9B%B4%E9%95%BF%E8%AE%A1%E7%AE%97%E6%88%90%E6%9C%AC%E4%BC%9A%E5%A2%9E%E5%8A%A0%E4%B8%94llm%E4%B9%9F%E6%9C%AA%E5%BF%85%E5%B0%B1%E8%83%BD%E6%8D%95%E6%8D%89%E5%88%B0%E5%85%B6%E4%B8%AD%E7%9A%84%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E4%BA%86%E6%9C%89%E5%8F%AF%E8%83%BD%E6%B7%B9%E6%B2%A1%E4%BA%8E%E5%85%B6%E4%B8%AD%E4%BA%86"><span class="nav-number">6.5.3.1.</span> <span class="nav-text">此处，我们要基于上一步得到的子图，进行检索。如果是传统的KGQA任务（也就是基于知识图谱的问答），那么一般答案都是在推理路径里的（这种的问题一般也比较“呆”，比如，”xx电影的xx导演的配偶是？“，逻辑非常线性）；但是RAG，它本质上是给llm提供更多辅助性信息（比如提供更多事实），让llm能更好地输出。从这个角度上看，我们反而会期望检索到的推理路径在合理的范围内长一点（因为这样会包含更多信息。当然是仅限合理的范围内。如果推理路径过长，信息过多，prompt就更长，计算成本会增加，且llm也未必就能捕捉到其中的关键信息了，有可能淹没于其中了）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%A0%E6%AD%A4%E8%AE%BA%E6%96%87%E6%89%8D%E6%8F%90%E5%87%BA%E5%AF%B9%E4%B8%A4%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84querysimplecomplex%E9%87%87%E7%94%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E6%A3%80%E7%B4%A2%E7%AD%96%E7%95%A5%E5%AF%B9%E7%AE%80%E5%8D%95%E7%9A%84query%E5%AE%83%E7%9A%84%E5%AF%B9%E5%BA%94%E8%B7%AF%E5%BE%84%E4%B8%80%E8%88%AC%E6%AF%94%E8%BE%83%E7%9F%AD%E9%82%A3%E4%B9%88%E6%AD%A4%E6%97%B6%E9%87%87%E7%94%A8bfs%E8%83%BD%E7%9B%B8%E5%AF%B9%E6%8D%95%E8%8E%B7%E6%9B%B4%E5%A4%9A%E4%BF%A1%E6%81%AF%E5%90%8C%E6%97%B6%E7%AD%94%E6%A1%88%E4%B9%9F%E4%B8%8D%E5%AE%B9%E6%98%93%E6%BC%8F-%E5%AF%B9%E5%A4%8D%E6%9D%82%E7%9A%84query%E5%88%99%E4%B8%8D%E5%86%8D%E8%80%83%E8%99%91%E7%94%A8bfs%E5%9B%A0%E4%B8%BA%E8%BF%99%E6%A0%B7%E5%BE%88%E5%8F%AF%E8%83%BD%E8%AE%A1%E7%AE%97%E9%87%8F%E8%BF%87%E5%A4%A7%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E8%80%83%E8%99%91%E6%89%BE%E5%B0%BD%E5%8F%AF%E8%83%BD%E7%9F%AD%E7%9A%84%E8%B7%AF%E5%BE%84%E4%BA%86%E6%89%80%E4%BB%A5%E7%94%A8dijkstra%E7%AE%97%E6%B3%95%E5%8E%BB%E6%90%9C%E7%B4%A2%E5%87%BAquery%E5%AE%9E%E4%BD%93%E5%88%B0%E5%90%84%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84"><span class="nav-number">6.5.3.2.</span> <span class="nav-text">因此，论文才提出对两种不同类型的query（simple&#x2F;complex），采用不同的检索策略。对简单的query，它的对应路径一般比较短，那么，此时采用BFS，能相对捕获更多信息，同时答案也不容易漏；对复杂的query，则不再考虑用BFS，因为这样很可能计算量过大。这里我们就考虑找尽可能短的路径了，所以用Dijkstra算法去搜索出query实体到各个节点的最短路径</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#postprocessing"><span class="nav-number">6.5.4.</span> <span class="nav-text">2.3. postprocessing</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8A%E9%9D%A2%E7%9A%84%E6%A3%80%E7%B4%A2%E4%BC%9A%E6%A3%80%E7%B4%A2%E5%87%BA%E5%BE%88%E5%A4%9A%E5%86%97%E4%BD%99%E7%9A%84%E8%B7%AF%E5%BE%84%E6%AF%94%E5%A6%82%E5%AF%B9%E7%AE%80%E5%8D%95query%E6%88%91%E4%BB%AC%E7%9A%84bfs%E5%B0%B1%E4%BC%9A%E6%A3%80%E7%B4%A2%E5%87%BA%E7%89%B9%E5%88%AB%E5%A4%9A%E7%9A%84%E8%B7%AF%E5%BE%84%E7%B1%BB%E4%BC%BC%E7%9A%84%E5%AF%B9%E4%BA%8E%E5%A4%8D%E6%9D%82querydijkstra%E7%AE%97%E6%B3%95%E6%98%AF%E4%BC%9A%E6%A3%80%E7%B4%A2%E5%87%BAquery%E9%87%8C%E7%9A%84%E5%AE%9E%E4%BD%93%E5%88%B0%E5%90%84%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E7%9A%84%E8%BF%99%E6%98%BE%E7%84%B6%E4%BC%9A%E6%9C%89%E7%89%B9%E5%88%AB%E5%A4%9A%E8%B7%AF%E5%BE%84%E8%82%AF%E5%AE%9A%E4%B8%8D%E8%83%BD%E5%85%A8%E9%83%BD%E6%8B%BF%E7%BB%99llm%E5%9D%8F%E5%A4%84%E5%A4%AA%E5%A4%9A%E4%BA%86%E5%8D%A0%E7%94%A8%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AE%A1%E7%AE%97%E6%88%90%E6%9C%ACllm%E6%9C%AA%E5%BF%85%E8%83%BD%E7%90%86%E8%A7%A3-%E5%9B%A0%E6%AD%A4%E9%9C%80%E8%A6%81%E7%AD%9B%E9%80%89%E8%80%8C%E6%98%BE%E7%84%B6%E7%AD%9B%E9%80%89%E7%9A%84%E6%A0%87%E5%87%86%E5%B0%B1%E6%98%AF%E8%BF%99%E4%BA%9B%E8%B7%AF%E5%BE%84%E6%98%AF%E5%90%A6%E5%92%8Cquery%E5%B0%BD%E5%8F%AF%E8%83%BD%E7%9B%B8%E5%85%B3-%E8%BF%99%E9%87%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E7%94%A8%E4%B8%80%E4%B8%AApath-ranking-modelprm%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%AF%B9%E6%90%9C%E7%B4%A2%E5%88%B0%E7%9A%84%E8%B7%AF%E5%BE%84%E8%BF%9B%E8%A1%8C%E6%89%93%E5%88%86%E8%BF%99%E4%B8%AA%E5%88%86%E6%95%B0%E7%9A%84%E5%90%AB%E4%B9%89%E6%98%AF%E8%BF%99%E4%BA%9B%E8%B7%AF%E5%BE%84%E5%92%8Cquery%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%88%91%E4%BB%AC%E8%AE%A4%E4%B8%BA%E8%BF%99%E8%A1%A8%E5%BE%81%E4%BA%86%E7%9B%B8%E5%85%B3%E6%80%A7%E5%86%8D%E4%BB%8E%E4%B8%AD%E9%80%89%E5%87%BA%E5%BE%97%E5%88%86top-u%E9%AB%98%E7%9A%84%E8%B7%AF%E5%BE%84%E8%BF%99%E4%BA%9B%E6%89%8D%E6%98%AF%E6%9C%80%E7%BB%88%E8%A6%81%E7%BB%99llm%E7%9A%84"><span class="nav-number">6.5.4.1.</span> <span class="nav-text">上面的检索，会检索出很多冗余的路径（比如对简单query，我们的BFS就会检索出特别多的路径；类似的，对于复杂query，Dijkstra算法是会检索出query里的实体到各个节点的最短路径的，这显然会有特别多路径），肯定不能全都拿给llm，坏处太多了（占用上下文，计算成本，llm未必能理解……）因此需要筛选。而显然筛选的标准就是这些路径是否和query尽可能相关这里主要是用一个path
ranking
model（PRM），也就是对搜索到的路径进行打分（这个分数的含义，是这些路径和query的相似性，我们认为这表征了相关性）；再从中选出得分top-u高的路径，这些才是最终要给llm的</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reasoning-module"><span class="nav-number">6.6.</span> <span class="nav-text">3. Reasoning Module</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%99%E9%87%8C%E5%B0%B1%E6%B2%A1%E4%BB%80%E4%B9%88%E4%BA%86%E6%88%91%E4%BB%AC%E5%89%8D%E9%9D%A2%E5%B7%B2%E7%BB%8F%E5%81%9A%E4%BA%86%E7%BB%9D%E5%A4%A7%E9%83%A8%E5%88%86%E7%9A%84%E5%B7%A5%E4%BD%9C%E4%BA%86%E6%AD%A4%E5%A4%84%E4%BC%9A%E8%AE%BE%E8%AE%A1%E4%B8%80%E5%A5%97prompt%E7%9A%84%E6%A8%A1%E6%9D%BF%E6%8A%8A%E5%8E%9F%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E6%88%91%E4%BB%AC%E7%AD%9B%E9%80%89%E5%BE%97%E5%88%B0%E7%9A%84reasoning-paths%E7%BB%93%E5%90%88%E8%B5%B7%E6%9D%A5%E6%8B%BF%E7%BB%99llm%E8%AE%A9%E5%AE%83%E5%9B%9E%E7%AD%94"><span class="nav-number">6.6.1.</span> <span class="nav-text">这里就没什么了，我们前面已经做了绝大部分的工作了。此处会设计一套prompt的模板，把原来的问题和我们筛选得到的reasoning
paths结合起来，拿给llm，让它回答</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C"><span class="nav-number">7.</span> <span class="nav-text">发展脉络：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E4%BD%93%E4%B8%8A%E6%98%AF%E5%9B%A0%E4%B8%BA%E4%BD%BF%E7%94%A8llm%E7%9A%84%E6%97%B6%E5%80%99%E9%81%87%E5%88%B0%E4%BA%86%E5%B9%BB%E8%A7%89%E4%BB%A5%E5%8F%8A%E7%9F%A5%E8%AF%86%E8%BF%87%E6%9C%9F%E7%9A%84%E9%97%AE%E9%A2%98%E6%89%80%E4%BB%A5%E6%8F%90%E5%87%BArag%E6%8A%80%E6%9C%AF%E6%9D%A5%E7%BC%93%E8%A7%A3%E8%BF%99%E4%BA%9B%E9%97%AE%E9%A2%98%E6%97%A9%E6%9C%9F%E6%9C%B4%E7%B4%A0%E7%9A%84rag%E6%95%88%E6%9E%9C%E6%9C%89%E4%BD%86%E6%98%AF%E4%B8%8D%E5%A4%9F%E5%A5%BD%E5%90%8E%E9%9D%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E6%94%B9%E8%BF%9B%E5%88%99%E4%B8%BB%E8%A6%81%E8%81%9A%E7%84%A6%E4%BA%8E%E6%8F%90%E9%AB%98%E6%A3%80%E7%B4%A2%E7%B2%BE%E7%A1%AE%E5%BA%A6%E4%BD%86%E8%BF%99%E7%B1%BBrag%E5%AE%83%E4%BB%AC%E9%83%BD%E6%98%AF%E5%9F%BA%E4%BA%8Edocument%E7%9A%84%E7%BB%8F%E5%B8%B8%E6%A3%80%E7%B4%A2%E5%BE%97%E5%88%B0%E7%9A%84%E7%BB%93%E6%9E%9C%E9%87%8C%E6%9C%89%E5%BE%88%E5%A4%9A%E6%97%A0%E5%85%B3%E7%9A%84%E5%86%85%E5%AE%B9%E4%B9%9F%E5%8D%B3noise%E8%BF%99%E5%85%B6%E5%AE%9E%E6%98%AF%E4%B8%8D%E5%88%A9%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%94%9F%E6%88%90%E7%9A%84%E6%9C%89%E5%8F%AF%E8%83%BD%E5%AE%83%E8%BF%B7%E5%A4%B1%E5%9C%A8%E6%97%A0%E5%85%B3%E7%9A%84%E5%86%85%E5%AE%B9%E9%87%8C%E4%BA%86-%E5%9B%A0%E6%AD%A4%E7%A0%94%E7%A9%B6%E8%BD%AC%E5%90%91%E8%80%83%E8%99%91%E6%9B%B4%E6%94%B9%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AF%86%E7%9A%84%E6%A0%BC%E5%BC%8F%E6%9D%A5%E8%A7%A3%E5%86%B3%E4%B8%8A%E9%9D%A2%E7%9A%84%E6%A3%80%E7%B4%A2%E5%88%B0%E5%99%AA%E5%A3%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%E5%B0%9D%E8%AF%95%E7%9A%84%E6%98%AF%E6%8A%8A%E5%AE%83%E4%BB%AC%E7%BB%84%E7%BB%87%E6%88%90graph%E7%9A%84%E5%BD%A2%E5%BC%8F%E7%9A%84%E4%B8%94%E9%80%9A%E5%B8%B8%E6%98%AFknowledge-graph%E5%9B%A0%E6%AD%A4%E5%BC%95%E5%87%BA%E4%BA%86graphrag%E7%9A%84%E7%A0%94%E7%A9%B6-%E5%9F%BA%E4%BA%8Eknowledge-graph%E7%9A%84rag%E6%8A%80%E6%9C%AF%E8%A2%AB%E7%A7%B0%E4%B8%BAkg-rag%E5%AE%83%E6%98%AFgraphrag%E9%87%8C%E7%9A%84%E4%B8%80%E4%B8%AA%E5%88%86%E6%94%AF%E4%B8%BB%E8%A6%81%E5%81%9A%E7%9A%84%E6%98%AF%E6%A3%80%E7%B4%A2%E5%87%BAquery%E7%9B%B8%E5%85%B3%E7%9A%84top-k%E7%9A%84reasoning-paths%E6%8A%8A%E8%BF%99%E4%BA%9B%E5%8A%A0%E5%85%A5%E5%88%B0llm%E7%9A%84%E8%BE%93%E5%85%A5%E4%B8%AD%E5%B8%AE%E5%8A%A9llm%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86"><span class="nav-number">7.1.</span> <span class="nav-text">大体上，是因为使用llm的时候遇到了幻觉以及知识过期的问题，所以提出RAG技术来缓解这些问题。早期朴素的RAG，效果有，但是不够好；后面的一些改进则主要聚焦于提高检索精确度。但这类RAG，它们都是基于document的，经常检索得到的结果里有很多无关的内容（也即noise），这其实是不利于模型的生成的（有可能它“迷失”在无关的内容里了）因此，研究转向，考虑更改外部知识的格式，来解决上面的检索到噪声的问题。具体来说，尝试的是把它们组织成graph的形式的（且通常是knowledge
graph），因此引出了GraphRAG的研究基于knowledge
graph的RAG技术，被称为KG-RAG，它是GraphRAG里的一个分支（？），主要做的是，检索出query相关的top-k的reasoning
paths，把这些加入到llm的输入中，帮助llm进行推理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#preliminary"><span class="nav-number">8.</span> <span class="nav-text">preliminary：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8Bkg-rag%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%85%B6%E5%AE%9E%E5%92%8C%E4%BC%A0%E7%BB%9F%E7%9A%84rag%E5%B7%AE%E5%88%AB%E4%B8%BB%E8%A6%81%E5%9C%A8%E4%BA%8E%E6%A3%80%E7%B4%A2%E7%9A%84%E8%BF%87%E7%A8%8Bkg-rag%E5%AE%83%E5%A4%A7%E4%BD%93%E4%B8%8A%E6%9C%89%E4%B8%A4%E6%AD%A5%E7%AC%AC%E4%B8%80%E6%AD%A5%E6%98%AF%E6%A3%80%E7%B4%A2%E7%AC%AC%E4%BA%8C%E6%AD%A5%E6%98%AF%E7%94%9F%E6%88%90%E6%A3%80%E7%B4%A2%E5%B0%B1%E6%98%AF%E6%A0%B9%E6%8D%AE%E7%BB%99%E5%AE%9A%E7%9A%84query%E5%9C%A8knowledg-graph%E4%B8%AD%E6%A3%80%E7%B4%A2%E5%87%BA%E8%8B%A5%E5%B9%B2%E6%9D%A1reasoning-paths%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%A0%B9%E6%8D%AEquery%E7%A1%AE%E5%AE%9A%E4%B8%80%E4%B8%AA%E8%B5%B7%E5%A7%8B%E7%9A%84entity%E8%8A%82%E7%82%B9%E7%84%B6%E5%90%8E%E5%BC%80%E5%A7%8B%E5%BE%80%E5%A4%96%E8%B7%B3%E6%9C%80%E7%BB%88%E4%BC%9A%E5%BE%97%E5%88%B0%E5%A4%9A%E6%9D%A1%E8%B7%AF%E5%BE%84%E8%BF%99%E5%B0%B1%E6%98%AFreasoning-paths%E5%86%8D%E7%94%A8%E4%B8%80%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%BF%99%E4%BA%9Bpaths%E8%BF%9B%E8%A1%8C%E6%89%93%E5%88%86%E6%9C%80%E5%90%8E%E9%80%89%E5%87%BAtop-k%E7%9A%84reasoning-paths-%E8%87%B3%E4%BA%8E%E7%94%9F%E6%88%90%E9%98%B6%E6%AE%B5%E5%88%99%E6%98%AF%E6%8A%8A%E4%B8%8A%E9%9D%A2%E9%80%89%E5%87%BA%E7%9A%84top-k%E7%9A%84paths%E5%8A%A0%E5%85%A5%E5%88%B0llm%E7%9A%84%E8%BE%93%E5%85%A5%E4%B8%AD%E8%BF%9B%E8%A1%8C%E5%A2%9E%E5%BC%BA%E8%AE%A9llm%E7%94%9F%E6%88%90%E6%9B%B4%E5%8A%A0%E5%90%88%E7%90%86%E7%9A%84%E8%BE%93%E5%87%BA"><span class="nav-number">8.1.</span> <span class="nav-text">简要介绍一下KG-RAG的工作流程。其实和传统的RAG差别主要在于检索的过程。KG-RAG，它大体上有两步，第一步是检索，第二步是生成。检索，就是根据给定的query，在knowledg
graph中检索出若干条reasoning
paths（也就是，根据query，确定一个起始的entity节点，然后开始往外跳，最终会得到多条路径，这就是reasoning
paths）；再用一个排序的模型，对这些paths进行打分，最后选出top-k的reasoning
paths至于生成阶段，则是把上面选出的top-k的paths，加入到llm的输入中进行增强，让llm生成更加合理的输出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%A6%E5%A4%96%E5%AF%B9%E4%BA%8E%E4%B8%80%E4%B8%AAkg-rag-reasoning-task%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%88%A4%E6%96%AD%E5%AE%83%E6%98%AF%E7%AE%80%E5%8D%95%E7%9A%84%E8%BF%98%E6%98%AF%E5%A4%8D%E6%9D%82%E7%9A%84%E4%B8%BB%E8%A6%81%E9%87%87%E7%94%A8%E7%9A%84%E5%9F%BA%E5%87%86%E6%98%AF%E6%AD%A3%E7%A1%AE%E6%8E%A8%E7%90%86%E8%B7%AF%E5%BE%84%E7%9A%84%E8%B7%B3%E6%95%B0hops%E4%B8%80%E8%88%AC%E7%BB%99%E5%AE%9A%E4%B8%80%E4%B8%AA%E9%98%88%E5%80%BCdelta%E5%A6%82%E6%9E%9C%E4%B8%80%E4%B8%AAquery%E7%9A%84%E6%9C%80%E7%9F%AD%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%8E%A8%E7%90%86%E8%B7%AF%E5%BE%84%E7%9A%84hops%E5%B0%8F%E4%BA%8E%E7%AD%89%E4%BA%8E%E8%BF%99%E4%B8%AA%E9%98%88%E5%80%BC%E5%88%99%E6%88%91%E4%BB%AC%E8%AE%A4%E4%B8%BA%E8%BF%99%E4%B8%AAquery%E6%98%AF%E5%B1%9E%E4%BA%8E%E7%AE%80%E5%8D%95%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%8F%8D%E4%B9%8B%E5%88%99%E8%AE%A4%E4%B8%BA%E8%BF%99%E4%B8%AAquery%E6%98%AF%E5%A4%8D%E6%9D%82%E7%B1%BB%E5%9E%8B%E7%9A%84%E4%B8%80%E4%B8%AAquery%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%8E%A8%E7%90%86%E8%B7%AF%E5%BE%84%E6%98%AF%E5%8F%AF%E4%BB%A5%E6%9C%89%E5%A4%9A%E6%9D%A1%E7%9A%84%E5%9B%A0%E4%B8%BA%E6%88%91%E4%BB%AC%E6%8E%A8%E7%90%86%E7%9A%84%E7%9B%AE%E6%A0%87%E6%98%AF%E6%89%BE%E5%88%B0%E6%AD%A3%E7%A1%AE%E7%9A%84%E7%AD%94%E6%A1%88%E5%AE%9E%E4%BD%93query%E5%88%99%E6%8C%87%E5%AE%9A%E4%BA%86%E8%B5%B7%E7%82%B9%E9%82%A3%E4%B9%88%E8%B5%B7%E7%82%B9%E7%BB%88%E7%82%B9%E4%B8%80%E5%AE%9A%E4%B8%AD%E9%97%B4%E7%9A%84%E8%B7%AF%E7%A8%8B%E6%9C%AA%E5%BF%85%E4%BB%85%E6%9C%89%E4%B8%80%E6%9D%A1%E4%BD%86%E5%A4%9A%E6%9D%A1%E8%B7%AF%E5%BE%84%E4%B8%AD%E4%B8%80%E5%AE%9A%E4%BC%9A%E6%9C%89%E4%B8%80%E6%9D%A1%E7%9A%84hops%E6%98%AF%E6%9C%80%E5%B0%8F%E7%9A%84%E6%88%91%E4%BB%AC%E5%B0%B1%E8%80%83%E8%99%91%E7%94%A8%E5%AE%83%E6%9D%A5%E5%88%A4%E6%96%ADquery%E7%9A%84%E9%9A%BE%E6%98%93%E7%A8%8B%E5%BA%A6"><span class="nav-number">8.2.</span> <span class="nav-text">另外，对于一个KG-RAG
reasoning
task，我们可以判断它是“简单”的还是“复杂”的。主要采用的基准是正确推理路径的跳数（hops）。一般，给定一个阈值\(\delta\)，如果一个query的最短的正确推理路径的hops小于等于这个阈值，则我们认为这个query是属于简单类型的；反之则认为这个query是复杂类型的（一个query的正确推理路径是可以有多条的。因为我们推理的目标是找到正确的答案实体，query则指定了起点；那么起点终点一定，中间的路程未必仅有一条。但多条路径中一定会有一条的hops是最小的，我们就考虑用它来判断query的难易程度）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E6%96%B9%E6%B3%95%E7%9A%84%E5%88%9B%E6%96%B0%E6%80%A7"><span class="nav-number">8.2.1.</span> <span class="nav-text">四、方法的创新性</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">190</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">102</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
