<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本章大纲： 先讲了基本的pytorch概念（权当复习了），然后讲了衡量efficiency的基本指标">
<meta property="og:type" content="article">
<meta property="og:title" content="lec02-basics">
<meta property="og:url" content="https://blueeemouse.github.io/2025/08/12/course_learning/efficientml_mit/lec02-basics/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="本章大纲： 先讲了基本的pytorch概念（权当复习了），然后讲了衡量efficiency的基本指标">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/efficiency_metrics_framework.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/latency_estimate.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/energy_consumption.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/number_of_params.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/notation.png#pic_center">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/peak-activations-ex1.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/peak-activations-ex2.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/peak-activations-ex3.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/peak-activations-ex4.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/MAC-ex1.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/number_of_MACs.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/efficientml/lec02/notation.png">
<meta property="article:published_time" content="2025-08-12T10:13:00.000Z">
<meta property="article:modified_time" content="2025-09-03T09:09:31.068Z">
<meta property="article:author" content="bluemouse">
<meta property="article:tag" content="efficiency metrics">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blueeemouse.github.io/images/efficientml/lec02/efficiency_metrics_framework.png">

<link rel="canonical" href="https://blueeemouse.github.io/2025/08/12/course_learning/efficientml_mit/lec02-basics/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>lec02-basics | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/08/12/course_learning/efficientml_mit/lec02-basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          lec02-basics
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-08-12 18:13:00" itemprop="dateCreated datePublished" datetime="2025-08-12T18:13:00+08:00">2025-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-09-03 17:09:31" itemprop="dateModified" datetime="2025-09-03T17:09:31+08:00">2025-09-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/course-learning/" itemprop="url" rel="index"><span itemprop="name">course_learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/course-learning/efficientml/" itemprop="url" rel="index"><span itemprop="name">efficientml</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="本章大纲">本章大纲：</h1>
<h2 id="先讲了基本的pytorch概念权当复习了然后讲了衡量efficiency的基本指标">先讲了基本的pytorch概念（权当复习了），然后讲了衡量efficiency的基本指标</h2>
<span id="more"></span>
<h1 id="pytorch基础">1. pytorch基础</h1>
<h2 id="都比较简单了一个点在于slides里关于各种norm的描述值得看一下也就是batch-normlayer-norminstance-normgroup-norm最核心的公式其实就是hatx_ifrac1sigmax_i-mu_i其中的sigma和mu_i则是某个像素集合delta里的标准差和均值而各种不同的norm其实本质就是这个集合delta的选取方式不同这个感觉一下就说到精髓了">都比较简单了。一个点在于，slides里关于各种norm的描述值得看一下（也就是batch
norm、layer norm、instance norm、group norm）。最核心的公式其实就是<span class="math inline">\(\hat{x_{i}}=\frac{1}{\sigma}(x_{i}-\mu_{i})\)</span>，其中的<span class="math inline">\(\sigma\)</span>和<span class="math inline">\(\mu_{i}\)</span>则是某个像素集合<span class="math inline">\(\delta\)</span>里的标准差和均值。而各种不同的norm，其实本质就是这个集合<span class="math inline">\(\delta\)</span>的选取方式不同（这个感觉一下就说到精髓了）</h2>
<h1 id="efficiency-metrics">2. efficiency metrics</h1>
<h2 id="我们可以结合这幅图来简单理解一下最外层的三角形的三个顶点其实就是我们应用ml模型时候的需求或者说期望我们希望模型能尽可能小这样下载使用的时候会更轻便我们也希望模型的推理能尽可能快这样在实时应用的场景里才能提高使用效果和体验我们当然也希望模型的能耗可以尽可能小虽然这是离使用者最远的一项需求但ml对能耗的需求其实已经不容小觑-进一步的中间的三角形其实就是外层的需求直接影响到的方面模型的大小直接影响了模型权重的存储模型推理的快慢直接反映在latency指标上而模型是否足够节能则表现在它的能耗上-最终根本性的影响因素其实就是两个即最里面的computation和memory围绕这两方面则有若干的具体指标正如图上右侧所示关于memory我们有paramsmodel-sizetotalpeakactivations而关于computation则有macflopflopsopops-下面会一一进行介绍但此前还需要简单梳理一下latency与throughput之间的关系以及介绍一些基本的意识或者说思想常识"><img src="/images/efficientml/lec02/efficiency_metrics_framework.png">我们可以结合这幅图来简单理解一下。最外层的三角形的三个顶点，其实就是我们应用ml模型时候的需求，或者说期望：我们希望模型能尽可能小，这样下载、使用的时候会更轻便；我们也希望模型的推理能尽可能快，这样在实时应用的场景里才能提高使用效果和体验；我们当然也希望模型的能耗可以尽可能小，虽然这是离使用者最远的一项需求，但ml对能耗的需求其实已经不容小觑<br>进一步的，中间的三角形其实就是外层的需求直接影响到的方面。模型的大小，直接影响了模型权重的存储；模型推理的快慢，直接反映在latency指标上；而模型是否足够节能，则表现在它的能耗上<br>最终，根本性的影响因素其实就是两个，即最里面的computation和memory。围绕这两方面，则有若干的具体指标，正如图上右侧所示：关于memory，我们有#params，model
size，total/peak，#activations；而关于computation，则有MAC，FLOP/FLOPS，OP/OPS<br>下面会一一进行介绍。但此前还需要简单梳理一下latency与throughput之间的关系，以及介绍一些基本的意识（或者说思想/常识？）</h2>
<h2 id="latency-throughput-common-sense">2.0. latency &amp; throughput
&amp; common sense</h2>
<h3 id="latency延时衡量了在一个特定任务上的延迟显然latency越小说明模型推理得越快">latency，延时，衡量了在一个特定任务上的延迟。显然，latency越小，说明模型推理得越快</h3>
<h3 id="throughput吞吐量它衡量的是模型同时处理数据的速率或者说单位时间内处理的数据量也可以此时可以发现如果throughput越大那么单位时间内模型能处理的数据量也就越多这样看似乎也是表明了模型推理的快速">throughput，吞吐量，它衡量的是模型同时处理数据的速率（或者说单位时间内处理的数据量也可以）。此时可以发现，如果throughput越大，那么单位时间内模型能处理的数据量也就越多。这样看，似乎也是表明了模型推理的快速</h3>
<h3 id="因此是否low-latency就一定代表了high-throughput呢反过来是不是high-throughput就一定代表了low-latency呢答案是否定的核心在于latency衡量的延时其实是模型处理一个batch-data的延迟但这个batch-data有多少呢这是不确定的所以据此我们可以举出反例模型a其latency为50ms每次处理一张image那么可以算出其throughput为20-imagess模型b其latency为100ms但每次处理4张images因此算出其throughput为40-imagess那么对比两个模型不难发现a的延时更低但吞吐量其实也更低b的吞吐量更大可是延时也更大">因此，是否low
latency就一定代表了high throughput呢？反过来，是不是high
throughput就一定代表了low
latency呢？答案是否定的。核心在于，latency衡量的延时，其实是模型处理一个batch
data的延迟。但这个batch
data有多少呢？这是不确定的。所以据此，我们可以举出反例：模型a，其latency为50ms，每次处理一张image，那么可以算出其throughput为20
images/s；模型b，其latency为100ms，但每次处理4张images，因此算出其throughput为40
images/s。那么对比两个模型，不难发现，a的延时更低，但吞吐量其实也更低；b的吞吐量更大，可是延时也更大</h3>
<h3 id="再讲讲latency的大致的估计方法可以参考下图的公式那其实就是说计算上的时间和访存花的时间哪个更大一般它就相应决定了latency而对于计算的时间开销大致上就是模型的操作数除以处理器每秒可以处理的操作数当然之所以说是估算就是因为这里我们做了一些近似把各种操作的耗时都视为时间开销一样的了所以就变成了单纯从操作数量来估计时间开销了-至于访存的时间开销大体上是中间值也就是激活值的搬移和模型权重的搬移模型权重的搬移我们近似成模型大小除以处理器带宽而中间值的移动则近似为输入与输出的激活值的大小除以处理器的宽带这里我们把其它地方的激活值忽略了">再讲讲latency的大致的估计方法。可以参考下图的公式：<img src="/images/efficientml/lec02/latency_estimate.png">那其实就是说，计算上的时间和访存花的时间，哪个更大，一般它就相应决定了latency。而对于计算的时间开销，大致上就是模型的操作数除以处理器每秒可以处理的操作数（当然，之所以说是估算，就是因为这里我们做了一些近似，把各种操作的耗时都视为时间开销一样的了，所以就变成了单纯从操作数量来估计时间开销了）<br>至于访存的时间开销，大体上是中间值（也就是激活值）的搬移和模型权重的搬移。模型权重的搬移，我们近似成模型大小除以处理器带宽；而中间值的移动则近似为输入与输出的激活值的大小除以处理器的宽带（这里我们把其它地方的激活值忽略了）</h3>
<h3 id="一般情况下是数据的搬移决定了latency尤其是现在模型大起来了gpu装不下因此常常需要把模型分割放到不同的gpu上之后又需要频繁的交流通信且从能耗的角度上看也是数据的搬移开销更大可参考下图因此我们实验的时候会希望尽量减少数据搬运这也是大显存的gpu的好处除了说可能小显存的卡放不下模型或数据大的则能放下以外因为一张卡放的东西更多了那么需要的卡就少一些搬移和通信就会少一些因此这方面的开销会降低">一般情况下，是数据的搬移决定了latency（尤其是现在模型大起来了，gpu装不下，因此常常需要把模型分割，放到不同的gpu上，之后又需要频繁的交流通信）。且从能耗的角度上看，也是数据的搬移开销更大。可参考下图：<img src="/images/efficientml/lec02/energy_consumption.png">因此，我们实验的时候会希望尽量减少数据搬运。这也是大显存的gpu的好处。除了说可能小显存的卡放不下模型或数据，大的则能放下以外，因为一张卡放的东西更多了，那么需要的卡就少一些，搬移和通信就会少一些，因此这方面的开销会降低</h3>
<h2 id="memory-related-metrics">2.1. memory-related metrics</h2>
<h3 id="parameters">2.1.1. #parameters</h3>
<h4 id="这个指标很直观就是算一下模型有多少参数这里以经典的卷积网络为例简单看看如何计算参数量其中的各个记号也都是常见的用法表示含义如下400挨个来仔细分析一下当练手了">这个指标很直观，就是算一下模型有多少参数。这里以经典的卷积网络为例，简单看看如何计算参数量：<img src="/images/efficientml/lec02/number_of_params.png">其中的各个记号也都是常见的用法，表示含义如下：<img src="/images/efficientml/lec02/notation.png#pic_center" alt="400">挨个来仔细分析一下，当练手了</h4>
<ul>
<li><h4 id="线性层感觉无需多言它就是对特征维度进行一个变换输入的维度是c_i输出维度是c_o参数形状就是一个矩形">线性层，感觉无需多言，它就是对特征维度进行一个变换，输入的维度是<span class="math inline">\(c_{i}\)</span>，输出维度是<span class="math inline">\(c_{o}\)</span>，参数形状就是一个矩形</h4></li>
<li><h4 id="一般卷积层有c_o个每个kernel的通道数和输入通道相同以便进行卷积宽高也是已知的所以就是全部乘起来">一般卷积层，有<span class="math inline">\(c_{o}\)</span>个，每个kernel的通道数和输入通道相同（以便进行卷积），宽高也是已知的，所以就是全部乘起来</h4></li>
<li><h4 id="分组卷积这个本质上就是若干小的卷积核负责处理图像的一部分通道最后把所有结果拼起来假设分了g组那么每一组小kernel负责的输入通道就是fracc_ig这组kernel的个数也即这组卷积结果的输出通道数就是fracc_og每个kernel的宽高还是不变的最后这g组每一组的结果的通道数都是fracc_og把g组的结果在通道维度上拼起来就得到最终结果此时通道数就是c_o所以参数量是g组数乘上fracc_ogkernel个数乘上fracc_ig每个kernel的通道数乘上k_hcdot-k_w每个kernel的宽高">分组卷积，这个本质上就是若干小的卷积核负责处理图像的一部分通道，最后把所有结果拼起来。假设分了g组，那么每一组小kernel负责的输入通道就是<span class="math inline">\(\frac{c_{i}}{g}\)</span>，这组kernel的个数（也即这组卷积结果的输出通道数）就是<span class="math inline">\(\frac{c_{o}}{g}\)</span>，每个kernel的宽高还是不变的。最后这g组，每一组的结果的通道数都是<span class="math inline">\(\frac{c_{o}}{g}\)</span>，把g组的结果在通道维度上拼起来，就得到最终结果（此时通道数就是<span class="math inline">\(c_{o}\)</span>）。所以参数量是g（组数）乘上<span class="math inline">\(\frac{c_{o}}{g}\)</span>（kernel个数）乘上<span class="math inline">\(\frac{c_{i}}{g}\)</span>（每个kernel的通道数）乘上<span class="math inline">\(k_{h}\cdot
k_{w}\)</span>（每个kernel的宽高）</h4></li>
<li><h4 id="深度卷积这种卷积主要特点是一个kernel只负责一个通道所以它一定输出通道和输入通道数相同且因为每个kernel只负责一个通道那么就相当于每个kernel的通道数是1所以参数量是c_okernel个数也等于c_i乘上1kernel通道数乘上k_hcdot-k_wkernel宽高">深度卷积，这种卷积主要特点是一个kernel只负责一个通道，所以它一定输出通道和输入通道数相同。且因为每个kernel只负责一个通道，那么就相当于每个kernel的通道数是1，所以参数量是<span class="math inline">\(c_{o}\)</span>（kernel个数，也等于<span class="math inline">\(c_{i}\)</span>）乘上1（kernel通道数）乘上<span class="math inline">\(k_{h}\cdot k_{w}\)</span>（kernel宽高）</h4>
<h3 id="model-size">2.1.2. model size</h3>
<h4 id="这个指标和上面的params息息相关如果说上面的指标它衡量的是模型的参数有多少那么这个指标衡量的就是存储这些模型的参数需要多大的空间此时的单位就是kbmb乃至更大的存储单位了">这个指标和上面的#params息息相关。如果说上面的指标它衡量的是模型的参数有多少，那么这个指标衡量的就是，存储这些模型的参数，需要多大的空间（此时的单位就是KB，MB，乃至更大的存储单位了）</h4>
<h4 id="一种最简单的情况是模型的所有参数都是用同一种格式来存储的那么此时的model-size就等于-textparameters-cdot-textbit-width-比如alexnet有61m的参数假如都用int-8来存储那么model-size为61m-times-1byte8位-61mb-假如都用float-32来存储则model-size为61m-times-4byte32位-244mb-可见采用什么数据类型存储会极大地影响model-size也有很多研究提出新的数据类型之类的力图减少model-size的同时尽量保证效果">一种最简单的情况是，模型的所有参数都是用同一种格式来存储的，那么此时的model
size就等于<span class="math inline">\(\#\ \text{Parameters} \cdot
\text{Bit Width}\)</span><br>比如，AlexNet有61M的参数，假如都用int
8来存储，那么model size为61M <span class="math inline">\(\times\)</span>
1Byte（8位）= 61MB<br>假如都用float 32来存储，则model size为61M <span class="math inline">\(\times\)</span> 4Byte（32位）=
244MB<br>可见采用什么数据类型存储，会极大地影响model
size。也有很多研究提出新的数据类型之类的，力图减少model
size的同时尽量保证效果</h4>
<h3 id="totalpeak-activations">2.1.3. total/peak #activations</h3>
<h4 id="感觉这一节主要是说明了model-size即使降下去但totalpeak-activations未必会降下去二者不是完全线性相关的并且上面也讲到了数据搬移部分我们要搬移的既有中间的激活值也有模型的参数而事实上很多时候占大头的其实是中间的activations也就是说有可能memory-bottleneck是activations">感觉这一节主要是说明了，model
size即使降下去，但total/peak
#activations未必会降下去，二者不是完全线性相关的。并且，上面也讲到了，数据搬移部分，我们要搬移的，既有中间的激活值，也有模型的参数。而事实上，很多时候，占大头的其实是中间的activations，也就是说，有可能memory
bottleneck是#activations</h4>
<h4 id="看看这幅图可以发现所谓的轻量化的网络它确实大幅降低了参数量但是peak-activations却并未降低多少反而还有上升可见想降低这个peak-activations是有难度的"><img src="/images/efficientml/lec02/peak-activations-ex1.png">看看这幅图，可以发现，所谓的轻量化的网络，它确实大幅降低了参数量，但是peak
activations却并未降低多少，反而还有上升。可见，想降低这个peak
activations是有难度的</h4>
<h4 id="并且可以看到peak-activations有可能很大在推理时占据主导地位决定了显存的最大开销是多少"><img src="/images/efficientml/lec02/peak-activations-ex2.png">并且可以看到，peak
activations有可能很大，在推理时占据主导地位，决定了显存的最大开销是多少</h4>
<h4 id="一个有趣的现象是浅层的时候一般activation-memory开销会比较高以cnn为例因为在一开始处理图像视频的时候分辨率还很高没降下去但后面因为会经过降采样等操作所以activation-memory再后面会低很多-而层数深的时候weight-memory开销又会更大仍然以cnn为例虽然在层数深了之后数据的分辨率小了但是通道数一般会增多而这就意味着卷积核的数量的增多且卷积核的大小宽高一般不会变即使变也不会变太多所以后面weight-memory会增大-综合下来看中间的时候反而两种开销都不太大数据经过了一定的降采样且通道数还没有那么多">一个有趣的现象是，<img src="/images/efficientml/lec02/peak-activations-ex3.png">浅层的时候，一般activation
memory开销会比较高（以cnn为例，因为在一开始处理图像/视频的时候，分辨率还很高，没降下去；但后面因为会经过降采样等操作，所以activation
memory再后面会低很多）<br>而层数深的时候，weight
memory开销又会更大（仍然以cnn为例。虽然在层数深了之后，数据的分辨率小了，但是通道数一般会增多。而这就意味着，卷积核的数量的增多。且卷积核的大小（宽高）一般不会变，即使变也不会变太多。所以后面weight
memory会增大）<br>综合下来看，中间的时候反而两种开销都不太大：数据经过了一定的降采样，且通道数还没有那么多</h4>
<h4 id="以alexnet为例分析一下两种activation怎么算total的很简单就是算出所有的加起来即可而peak的理论上讲当然是算出所有层的activation然后取最大值但是这显然工作量很大根据上面观察出的经验结论我们可以考虑用最前面的层也即第一层的输入与输出activation进行一个近似从这里的结果上看估计出来的值应该也挺准的毕竟这里估出来的"><img src="/images/efficientml/lec02/peak-activations-ex4.png">以AlexNet为例，分析一下两种#activation怎么算。total的很简单，就是算出所有的加起来即可。而peak的，理论上讲，当然是算出所有层的#activation，然后取最大值；但是这显然工作量很大。根据上面观察出的经验结论，我们可以考虑用最前面的层（也即第一层）的输入与输出#activation进行一个近似（从这里的结果上看，估计出来的值应该也挺准的。毕竟这里估出来的</h4>
<h2 id="computation-related-metrics">2.2. computation-related
metrics</h2>
<h3 id="macmultiply-accumulate-operations">2.2.1.
MAC（Multiply-Accumulate Operations）</h3>
<h4 id="正如字面意思这个操作代表了乘一次和加一次以下图为例很容易了解">正如字面意思，这个操作代表了乘一次和加一次。以下图为例，很容易了解：<img src="/images/efficientml/lec02/MAC-ex1.png"></h4>
<h4 id="进一步的不妨看看各种常用的网络它们的mac如何计算各符号的含义为线性层的很好分析略过一般卷积呢可以这样思考输出的每个像素都是经过一次卷积得到的所以我们可以得到总共的卷积次数就是输出的总像素数下面只需要算出每一次卷积的mac是多少了而一次卷积一个卷积核的形状是c_ik_hk_w所以乘加的次数就是c_itimes-k_htimes-k_w综上把次数和单次的mac乘起来就得到结果">进一步的，不妨看看各种常用的网络它们的MAC如何计算：<img src="/images/efficientml/lec02/number_of_MACs.png">各符号的含义为：<img src="/images/efficientml/lec02/notation.png">线性层的很好分析，略过。一般卷积呢，可以这样思考：输出的每个像素，都是经过一次卷积得到的。所以我们可以得到总共的卷积次数，就是输出的总像素数。下面只需要算出每一次卷积的MAC是多少了。而一次卷积，一个卷积核的形状是<span class="math inline">\((c_{i},k_{h},k_{w})\)</span>，所以乘加的次数就是<span class="math inline">\(c_{i}\times k_{h}\times
k_{w}\)</span>，综上，把次数和单次的MAC乘起来就得到结果</h4>
<h4 id="类似的对分组卷积也可以采用类似的分析方法虽然说它最后的结果是有一个concat的但这不影响每一个输出像素依然对应了一次卷积而每次卷积的mac是多少呢这里需要注意分组的情况下每个kernel的形状就是fracc_igk_hk_w只有这一点不同而已最后依然是把所有的乘起来也可以看到分组的卷积在mac上也比一般的卷积要小一些">类似的，对分组卷积，也可以采用类似的分析方法。虽然说它最后的结果是有一个concat的，但这不影响每一个输出像素依然对应了一次卷积。而每次卷积的MAC是多少呢？这里需要注意，分组的情况下，每个kernel的形状就是<span class="math inline">\((\frac{c_{i}}{g},k_{h},k_{w})\)</span>，只有这一点不同而已。最后依然是把所有的乘起来（也可以看到，分组的卷积，在MAC上也比一般的卷积要小一些）</h4>
<h4 id="至于最后的深度卷积它其实就是分组卷积的一种特殊情况即组数g恰好等于输入通道数c_i所以套用上一个的结论进行化简即可">至于最后的深度卷积，它其实就是分组卷积的一种特殊情况，即组数g恰好等于输入通道数<span class="math inline">\(c_{i}\)</span>，所以套用上一个的结论进行化简即可</h4>
<h3 id="flop-flpos">2.2.2. FLOP, FLPOS</h3>
<h4 id="flop全称是floating-point-operations举几个例子一次乘法是一个浮点数操作一个加法也是一个浮点数操作因此一个乘加操作就是两个浮点数操作标准的alexnet它有724m的macs因此它的flop为724m-2-1.4g-flops注意这里的结尾s是小写的只是表示复数和下面的flops的大写结尾s含义不同">FLOP，全称是Floating
Point
Operations。举几个例子，一次乘法，是一个浮点数操作；一个加法，也是一个浮点数操作。因此，一个乘加操作就是两个浮点数操作。标准的AlexNet它有724M的MACs，因此它的FLOP为724M
× 2 = 1.4G
FLOPs（注意，这里的结尾s是小写的，只是表示复数，和下面的FLOPS的大写结尾S含义不同）</h4>
<h4 id="flops全称是floating-point-operation-per-second它是一个衡量计算速度的指标所以它的计算公式为flopsfracflopssecond">FLOPS，全称是Floating
Point Operation Per
Second，它是一个衡量计算速度的指标。所以它的计算公式为<span class="math inline">\(FLOPS=\frac{FLOPS}{second}\)</span></h4>
<h3 id="op-ops">2.2.3. OP, OPS</h3>
<h4 id="这里的两个概念完全类似上面的只不过上面是专注于浮点型操作而这里的范围更加广泛并不局限于浮点操作比如一个int操作也可以算作一个op">这里的两个概念完全类似上面的。只不过上面是专注于浮点型操作，而这里的范围更加广泛，并不局限于浮点操作（比如一个int操作也可以算作一个OP）</h4></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/efficiency-metrics/" rel="tag"># efficiency metrics</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/08/11/algo/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/%E8%83%8C%E5%8C%85/0-1%E8%83%8C%E5%8C%85/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/" rel="prev" title="分割等和子集">
      <i class="fa fa-chevron-left"></i> 分割等和子集
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/08/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/naive_rag/Enhancing%20Retrieval-Augmented%20Large%20Language%20Models%20with%20Iterative%20Retrieval-Generation%20Synergy/" rel="next" title="Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy">
      Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%AC%E7%AB%A0%E5%A4%A7%E7%BA%B2"><span class="nav-number">1.</span> <span class="nav-text">本章大纲：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%88%E8%AE%B2%E4%BA%86%E5%9F%BA%E6%9C%AC%E7%9A%84pytorch%E6%A6%82%E5%BF%B5%E6%9D%83%E5%BD%93%E5%A4%8D%E4%B9%A0%E4%BA%86%E7%84%B6%E5%90%8E%E8%AE%B2%E4%BA%86%E8%A1%A1%E9%87%8Fefficiency%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%8C%87%E6%A0%87"><span class="nav-number">1.1.</span> <span class="nav-text">先讲了基本的pytorch概念（权当复习了），然后讲了衡量efficiency的基本指标</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch%E5%9F%BA%E7%A1%80"><span class="nav-number">2.</span> <span class="nav-text">1. pytorch基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%BD%E6%AF%94%E8%BE%83%E7%AE%80%E5%8D%95%E4%BA%86%E4%B8%80%E4%B8%AA%E7%82%B9%E5%9C%A8%E4%BA%8Eslides%E9%87%8C%E5%85%B3%E4%BA%8E%E5%90%84%E7%A7%8Dnorm%E7%9A%84%E6%8F%8F%E8%BF%B0%E5%80%BC%E5%BE%97%E7%9C%8B%E4%B8%80%E4%B8%8B%E4%B9%9F%E5%B0%B1%E6%98%AFbatch-normlayer-norminstance-normgroup-norm%E6%9C%80%E6%A0%B8%E5%BF%83%E7%9A%84%E5%85%AC%E5%BC%8F%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AFhatx_ifrac1sigmax_i-mu_i%E5%85%B6%E4%B8%AD%E7%9A%84sigma%E5%92%8Cmu_i%E5%88%99%E6%98%AF%E6%9F%90%E4%B8%AA%E5%83%8F%E7%B4%A0%E9%9B%86%E5%90%88delta%E9%87%8C%E7%9A%84%E6%A0%87%E5%87%86%E5%B7%AE%E5%92%8C%E5%9D%87%E5%80%BC%E8%80%8C%E5%90%84%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84norm%E5%85%B6%E5%AE%9E%E6%9C%AC%E8%B4%A8%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B8%AA%E9%9B%86%E5%90%88delta%E7%9A%84%E9%80%89%E5%8F%96%E6%96%B9%E5%BC%8F%E4%B8%8D%E5%90%8C%E8%BF%99%E4%B8%AA%E6%84%9F%E8%A7%89%E4%B8%80%E4%B8%8B%E5%B0%B1%E8%AF%B4%E5%88%B0%E7%B2%BE%E9%AB%93%E4%BA%86"><span class="nav-number">2.1.</span> <span class="nav-text">都比较简单了。一个点在于，slides里关于各种norm的描述值得看一下（也就是batch
norm、layer norm、instance norm、group norm）。最核心的公式其实就是\(\hat{x_{i}}&#x3D;\frac{1}{\sigma}(x_{i}-\mu_{i})\)，其中的\(\sigma\)和\(\mu_{i}\)则是某个像素集合\(\delta\)里的标准差和均值。而各种不同的norm，其实本质就是这个集合\(\delta\)的选取方式不同（这个感觉一下就说到精髓了）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#efficiency-metrics"><span class="nav-number">3.</span> <span class="nav-text">2. efficiency metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%BB%93%E5%90%88%E8%BF%99%E5%B9%85%E5%9B%BE%E6%9D%A5%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3%E4%B8%80%E4%B8%8B%E6%9C%80%E5%A4%96%E5%B1%82%E7%9A%84%E4%B8%89%E8%A7%92%E5%BD%A2%E7%9A%84%E4%B8%89%E4%B8%AA%E9%A1%B6%E7%82%B9%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E6%88%91%E4%BB%AC%E5%BA%94%E7%94%A8ml%E6%A8%A1%E5%9E%8B%E6%97%B6%E5%80%99%E7%9A%84%E9%9C%80%E6%B1%82%E6%88%96%E8%80%85%E8%AF%B4%E6%9C%9F%E6%9C%9B%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%B0%8F%E8%BF%99%E6%A0%B7%E4%B8%8B%E8%BD%BD%E4%BD%BF%E7%94%A8%E7%9A%84%E6%97%B6%E5%80%99%E4%BC%9A%E6%9B%B4%E8%BD%BB%E4%BE%BF%E6%88%91%E4%BB%AC%E4%B9%9F%E5%B8%8C%E6%9C%9B%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86%E8%83%BD%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%BF%AB%E8%BF%99%E6%A0%B7%E5%9C%A8%E5%AE%9E%E6%97%B6%E5%BA%94%E7%94%A8%E7%9A%84%E5%9C%BA%E6%99%AF%E9%87%8C%E6%89%8D%E8%83%BD%E6%8F%90%E9%AB%98%E4%BD%BF%E7%94%A8%E6%95%88%E6%9E%9C%E5%92%8C%E4%BD%93%E9%AA%8C%E6%88%91%E4%BB%AC%E5%BD%93%E7%84%B6%E4%B9%9F%E5%B8%8C%E6%9C%9B%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%83%BD%E8%80%97%E5%8F%AF%E4%BB%A5%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%B0%8F%E8%99%BD%E7%84%B6%E8%BF%99%E6%98%AF%E7%A6%BB%E4%BD%BF%E7%94%A8%E8%80%85%E6%9C%80%E8%BF%9C%E7%9A%84%E4%B8%80%E9%A1%B9%E9%9C%80%E6%B1%82%E4%BD%86ml%E5%AF%B9%E8%83%BD%E8%80%97%E7%9A%84%E9%9C%80%E6%B1%82%E5%85%B6%E5%AE%9E%E5%B7%B2%E7%BB%8F%E4%B8%8D%E5%AE%B9%E5%B0%8F%E8%A7%91-%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E4%B8%AD%E9%97%B4%E7%9A%84%E4%B8%89%E8%A7%92%E5%BD%A2%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E5%A4%96%E5%B1%82%E7%9A%84%E9%9C%80%E6%B1%82%E7%9B%B4%E6%8E%A5%E5%BD%B1%E5%93%8D%E5%88%B0%E7%9A%84%E6%96%B9%E9%9D%A2%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%A7%E5%B0%8F%E7%9B%B4%E6%8E%A5%E5%BD%B1%E5%93%8D%E4%BA%86%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E7%9A%84%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E7%9A%84%E5%BF%AB%E6%85%A2%E7%9B%B4%E6%8E%A5%E5%8F%8D%E6%98%A0%E5%9C%A8latency%E6%8C%87%E6%A0%87%E4%B8%8A%E8%80%8C%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E8%B6%B3%E5%A4%9F%E8%8A%82%E8%83%BD%E5%88%99%E8%A1%A8%E7%8E%B0%E5%9C%A8%E5%AE%83%E7%9A%84%E8%83%BD%E8%80%97%E4%B8%8A-%E6%9C%80%E7%BB%88%E6%A0%B9%E6%9C%AC%E6%80%A7%E7%9A%84%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E4%B8%A4%E4%B8%AA%E5%8D%B3%E6%9C%80%E9%87%8C%E9%9D%A2%E7%9A%84computation%E5%92%8Cmemory%E5%9B%B4%E7%BB%95%E8%BF%99%E4%B8%A4%E6%96%B9%E9%9D%A2%E5%88%99%E6%9C%89%E8%8B%A5%E5%B9%B2%E7%9A%84%E5%85%B7%E4%BD%93%E6%8C%87%E6%A0%87%E6%AD%A3%E5%A6%82%E5%9B%BE%E4%B8%8A%E5%8F%B3%E4%BE%A7%E6%89%80%E7%A4%BA%E5%85%B3%E4%BA%8Ememory%E6%88%91%E4%BB%AC%E6%9C%89paramsmodel-sizetotalpeakactivations%E8%80%8C%E5%85%B3%E4%BA%8Ecomputation%E5%88%99%E6%9C%89macflopflopsopops-%E4%B8%8B%E9%9D%A2%E4%BC%9A%E4%B8%80%E4%B8%80%E8%BF%9B%E8%A1%8C%E4%BB%8B%E7%BB%8D%E4%BD%86%E6%AD%A4%E5%89%8D%E8%BF%98%E9%9C%80%E8%A6%81%E7%AE%80%E5%8D%95%E6%A2%B3%E7%90%86%E4%B8%80%E4%B8%8Blatency%E4%B8%8Ethroughput%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%E4%BB%A5%E5%8F%8A%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E7%9A%84%E6%84%8F%E8%AF%86%E6%88%96%E8%80%85%E8%AF%B4%E6%80%9D%E6%83%B3%E5%B8%B8%E8%AF%86"><span class="nav-number">3.1.</span> <span class="nav-text">我们可以结合这幅图来简单理解一下。最外层的三角形的三个顶点，其实就是我们应用ml模型时候的需求，或者说期望：我们希望模型能尽可能小，这样下载、使用的时候会更轻便；我们也希望模型的推理能尽可能快，这样在实时应用的场景里才能提高使用效果和体验；我们当然也希望模型的能耗可以尽可能小，虽然这是离使用者最远的一项需求，但ml对能耗的需求其实已经不容小觑进一步的，中间的三角形其实就是外层的需求直接影响到的方面。模型的大小，直接影响了模型权重的存储；模型推理的快慢，直接反映在latency指标上；而模型是否足够节能，则表现在它的能耗上最终，根本性的影响因素其实就是两个，即最里面的computation和memory。围绕这两方面，则有若干的具体指标，正如图上右侧所示：关于memory，我们有#params，model
size，total&#x2F;peak，#activations；而关于computation，则有MAC，FLOP&#x2F;FLOPS，OP&#x2F;OPS下面会一一进行介绍。但此前还需要简单梳理一下latency与throughput之间的关系，以及介绍一些基本的意识（或者说思想&#x2F;常识？）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#latency-throughput-common-sense"><span class="nav-number">3.2.</span> <span class="nav-text">2.0. latency &amp; throughput
&amp; common sense</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#latency%E5%BB%B6%E6%97%B6%E8%A1%A1%E9%87%8F%E4%BA%86%E5%9C%A8%E4%B8%80%E4%B8%AA%E7%89%B9%E5%AE%9A%E4%BB%BB%E5%8A%A1%E4%B8%8A%E7%9A%84%E5%BB%B6%E8%BF%9F%E6%98%BE%E7%84%B6latency%E8%B6%8A%E5%B0%8F%E8%AF%B4%E6%98%8E%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%BE%97%E8%B6%8A%E5%BF%AB"><span class="nav-number">3.2.1.</span> <span class="nav-text">latency，延时，衡量了在一个特定任务上的延迟。显然，latency越小，说明模型推理得越快</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#throughput%E5%90%9E%E5%90%90%E9%87%8F%E5%AE%83%E8%A1%A1%E9%87%8F%E7%9A%84%E6%98%AF%E6%A8%A1%E5%9E%8B%E5%90%8C%E6%97%B6%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E7%9A%84%E9%80%9F%E7%8E%87%E6%88%96%E8%80%85%E8%AF%B4%E5%8D%95%E4%BD%8D%E6%97%B6%E9%97%B4%E5%86%85%E5%A4%84%E7%90%86%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%E4%B9%9F%E5%8F%AF%E4%BB%A5%E6%AD%A4%E6%97%B6%E5%8F%AF%E4%BB%A5%E5%8F%91%E7%8E%B0%E5%A6%82%E6%9E%9Cthroughput%E8%B6%8A%E5%A4%A7%E9%82%A3%E4%B9%88%E5%8D%95%E4%BD%8D%E6%97%B6%E9%97%B4%E5%86%85%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%A4%84%E7%90%86%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%E4%B9%9F%E5%B0%B1%E8%B6%8A%E5%A4%9A%E8%BF%99%E6%A0%B7%E7%9C%8B%E4%BC%BC%E4%B9%8E%E4%B9%9F%E6%98%AF%E8%A1%A8%E6%98%8E%E4%BA%86%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E7%9A%84%E5%BF%AB%E9%80%9F"><span class="nav-number">3.2.2.</span> <span class="nav-text">throughput，吞吐量，它衡量的是模型同时处理数据的速率（或者说单位时间内处理的数据量也可以）。此时可以发现，如果throughput越大，那么单位时间内模型能处理的数据量也就越多。这样看，似乎也是表明了模型推理的快速</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%A0%E6%AD%A4%E6%98%AF%E5%90%A6low-latency%E5%B0%B1%E4%B8%80%E5%AE%9A%E4%BB%A3%E8%A1%A8%E4%BA%86high-throughput%E5%91%A2%E5%8F%8D%E8%BF%87%E6%9D%A5%E6%98%AF%E4%B8%8D%E6%98%AFhigh-throughput%E5%B0%B1%E4%B8%80%E5%AE%9A%E4%BB%A3%E8%A1%A8%E4%BA%86low-latency%E5%91%A2%E7%AD%94%E6%A1%88%E6%98%AF%E5%90%A6%E5%AE%9A%E7%9A%84%E6%A0%B8%E5%BF%83%E5%9C%A8%E4%BA%8Elatency%E8%A1%A1%E9%87%8F%E7%9A%84%E5%BB%B6%E6%97%B6%E5%85%B6%E5%AE%9E%E6%98%AF%E6%A8%A1%E5%9E%8B%E5%A4%84%E7%90%86%E4%B8%80%E4%B8%AAbatch-data%E7%9A%84%E5%BB%B6%E8%BF%9F%E4%BD%86%E8%BF%99%E4%B8%AAbatch-data%E6%9C%89%E5%A4%9A%E5%B0%91%E5%91%A2%E8%BF%99%E6%98%AF%E4%B8%8D%E7%A1%AE%E5%AE%9A%E7%9A%84%E6%89%80%E4%BB%A5%E6%8D%AE%E6%AD%A4%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E4%B8%BE%E5%87%BA%E5%8F%8D%E4%BE%8B%E6%A8%A1%E5%9E%8Ba%E5%85%B6latency%E4%B8%BA50ms%E6%AF%8F%E6%AC%A1%E5%A4%84%E7%90%86%E4%B8%80%E5%BC%A0image%E9%82%A3%E4%B9%88%E5%8F%AF%E4%BB%A5%E7%AE%97%E5%87%BA%E5%85%B6throughput%E4%B8%BA20-imagess%E6%A8%A1%E5%9E%8Bb%E5%85%B6latency%E4%B8%BA100ms%E4%BD%86%E6%AF%8F%E6%AC%A1%E5%A4%84%E7%90%864%E5%BC%A0images%E5%9B%A0%E6%AD%A4%E7%AE%97%E5%87%BA%E5%85%B6throughput%E4%B8%BA40-imagess%E9%82%A3%E4%B9%88%E5%AF%B9%E6%AF%94%E4%B8%A4%E4%B8%AA%E6%A8%A1%E5%9E%8B%E4%B8%8D%E9%9A%BE%E5%8F%91%E7%8E%B0a%E7%9A%84%E5%BB%B6%E6%97%B6%E6%9B%B4%E4%BD%8E%E4%BD%86%E5%90%9E%E5%90%90%E9%87%8F%E5%85%B6%E5%AE%9E%E4%B9%9F%E6%9B%B4%E4%BD%8Eb%E7%9A%84%E5%90%9E%E5%90%90%E9%87%8F%E6%9B%B4%E5%A4%A7%E5%8F%AF%E6%98%AF%E5%BB%B6%E6%97%B6%E4%B9%9F%E6%9B%B4%E5%A4%A7"><span class="nav-number">3.2.3.</span> <span class="nav-text">因此，是否low
latency就一定代表了high throughput呢？反过来，是不是high
throughput就一定代表了low
latency呢？答案是否定的。核心在于，latency衡量的延时，其实是模型处理一个batch
data的延迟。但这个batch
data有多少呢？这是不确定的。所以据此，我们可以举出反例：模型a，其latency为50ms，每次处理一张image，那么可以算出其throughput为20
images&#x2F;s；模型b，其latency为100ms，但每次处理4张images，因此算出其throughput为40
images&#x2F;s。那么对比两个模型，不难发现，a的延时更低，但吞吐量其实也更低；b的吞吐量更大，可是延时也更大</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%8D%E8%AE%B2%E8%AE%B2latency%E7%9A%84%E5%A4%A7%E8%87%B4%E7%9A%84%E4%BC%B0%E8%AE%A1%E6%96%B9%E6%B3%95%E5%8F%AF%E4%BB%A5%E5%8F%82%E8%80%83%E4%B8%8B%E5%9B%BE%E7%9A%84%E5%85%AC%E5%BC%8F%E9%82%A3%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E8%AF%B4%E8%AE%A1%E7%AE%97%E4%B8%8A%E7%9A%84%E6%97%B6%E9%97%B4%E5%92%8C%E8%AE%BF%E5%AD%98%E8%8A%B1%E7%9A%84%E6%97%B6%E9%97%B4%E5%93%AA%E4%B8%AA%E6%9B%B4%E5%A4%A7%E4%B8%80%E8%88%AC%E5%AE%83%E5%B0%B1%E7%9B%B8%E5%BA%94%E5%86%B3%E5%AE%9A%E4%BA%86latency%E8%80%8C%E5%AF%B9%E4%BA%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%E5%A4%A7%E8%87%B4%E4%B8%8A%E5%B0%B1%E6%98%AF%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%93%8D%E4%BD%9C%E6%95%B0%E9%99%A4%E4%BB%A5%E5%A4%84%E7%90%86%E5%99%A8%E6%AF%8F%E7%A7%92%E5%8F%AF%E4%BB%A5%E5%A4%84%E7%90%86%E7%9A%84%E6%93%8D%E4%BD%9C%E6%95%B0%E5%BD%93%E7%84%B6%E4%B9%8B%E6%89%80%E4%BB%A5%E8%AF%B4%E6%98%AF%E4%BC%B0%E7%AE%97%E5%B0%B1%E6%98%AF%E5%9B%A0%E4%B8%BA%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E5%81%9A%E4%BA%86%E4%B8%80%E4%BA%9B%E8%BF%91%E4%BC%BC%E6%8A%8A%E5%90%84%E7%A7%8D%E6%93%8D%E4%BD%9C%E7%9A%84%E8%80%97%E6%97%B6%E9%83%BD%E8%A7%86%E4%B8%BA%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%E4%B8%80%E6%A0%B7%E7%9A%84%E4%BA%86%E6%89%80%E4%BB%A5%E5%B0%B1%E5%8F%98%E6%88%90%E4%BA%86%E5%8D%95%E7%BA%AF%E4%BB%8E%E6%93%8D%E4%BD%9C%E6%95%B0%E9%87%8F%E6%9D%A5%E4%BC%B0%E8%AE%A1%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%E4%BA%86-%E8%87%B3%E4%BA%8E%E8%AE%BF%E5%AD%98%E7%9A%84%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%E5%A4%A7%E4%BD%93%E4%B8%8A%E6%98%AF%E4%B8%AD%E9%97%B4%E5%80%BC%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%BF%80%E6%B4%BB%E5%80%BC%E7%9A%84%E6%90%AC%E7%A7%BB%E5%92%8C%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E7%9A%84%E6%90%AC%E7%A7%BB%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E7%9A%84%E6%90%AC%E7%A7%BB%E6%88%91%E4%BB%AC%E8%BF%91%E4%BC%BC%E6%88%90%E6%A8%A1%E5%9E%8B%E5%A4%A7%E5%B0%8F%E9%99%A4%E4%BB%A5%E5%A4%84%E7%90%86%E5%99%A8%E5%B8%A6%E5%AE%BD%E8%80%8C%E4%B8%AD%E9%97%B4%E5%80%BC%E7%9A%84%E7%A7%BB%E5%8A%A8%E5%88%99%E8%BF%91%E4%BC%BC%E4%B8%BA%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA%E7%9A%84%E6%BF%80%E6%B4%BB%E5%80%BC%E7%9A%84%E5%A4%A7%E5%B0%8F%E9%99%A4%E4%BB%A5%E5%A4%84%E7%90%86%E5%99%A8%E7%9A%84%E5%AE%BD%E5%B8%A6%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E6%8A%8A%E5%85%B6%E5%AE%83%E5%9C%B0%E6%96%B9%E7%9A%84%E6%BF%80%E6%B4%BB%E5%80%BC%E5%BF%BD%E7%95%A5%E4%BA%86"><span class="nav-number">3.2.4.</span> <span class="nav-text">再讲讲latency的大致的估计方法。可以参考下图的公式：那其实就是说，计算上的时间和访存花的时间，哪个更大，一般它就相应决定了latency。而对于计算的时间开销，大致上就是模型的操作数除以处理器每秒可以处理的操作数（当然，之所以说是估算，就是因为这里我们做了一些近似，把各种操作的耗时都视为时间开销一样的了，所以就变成了单纯从操作数量来估计时间开销了）至于访存的时间开销，大体上是中间值（也就是激活值）的搬移和模型权重的搬移。模型权重的搬移，我们近似成模型大小除以处理器带宽；而中间值的移动则近似为输入与输出的激活值的大小除以处理器的宽带（这里我们把其它地方的激活值忽略了）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E6%83%85%E5%86%B5%E4%B8%8B%E6%98%AF%E6%95%B0%E6%8D%AE%E7%9A%84%E6%90%AC%E7%A7%BB%E5%86%B3%E5%AE%9A%E4%BA%86latency%E5%B0%A4%E5%85%B6%E6%98%AF%E7%8E%B0%E5%9C%A8%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%B5%B7%E6%9D%A5%E4%BA%86gpu%E8%A3%85%E4%B8%8D%E4%B8%8B%E5%9B%A0%E6%AD%A4%E5%B8%B8%E5%B8%B8%E9%9C%80%E8%A6%81%E6%8A%8A%E6%A8%A1%E5%9E%8B%E5%88%86%E5%89%B2%E6%94%BE%E5%88%B0%E4%B8%8D%E5%90%8C%E7%9A%84gpu%E4%B8%8A%E4%B9%8B%E5%90%8E%E5%8F%88%E9%9C%80%E8%A6%81%E9%A2%91%E7%B9%81%E7%9A%84%E4%BA%A4%E6%B5%81%E9%80%9A%E4%BF%A1%E4%B8%94%E4%BB%8E%E8%83%BD%E8%80%97%E7%9A%84%E8%A7%92%E5%BA%A6%E4%B8%8A%E7%9C%8B%E4%B9%9F%E6%98%AF%E6%95%B0%E6%8D%AE%E7%9A%84%E6%90%AC%E7%A7%BB%E5%BC%80%E9%94%80%E6%9B%B4%E5%A4%A7%E5%8F%AF%E5%8F%82%E8%80%83%E4%B8%8B%E5%9B%BE%E5%9B%A0%E6%AD%A4%E6%88%91%E4%BB%AC%E5%AE%9E%E9%AA%8C%E7%9A%84%E6%97%B6%E5%80%99%E4%BC%9A%E5%B8%8C%E6%9C%9B%E5%B0%BD%E9%87%8F%E5%87%8F%E5%B0%91%E6%95%B0%E6%8D%AE%E6%90%AC%E8%BF%90%E8%BF%99%E4%B9%9F%E6%98%AF%E5%A4%A7%E6%98%BE%E5%AD%98%E7%9A%84gpu%E7%9A%84%E5%A5%BD%E5%A4%84%E9%99%A4%E4%BA%86%E8%AF%B4%E5%8F%AF%E8%83%BD%E5%B0%8F%E6%98%BE%E5%AD%98%E7%9A%84%E5%8D%A1%E6%94%BE%E4%B8%8D%E4%B8%8B%E6%A8%A1%E5%9E%8B%E6%88%96%E6%95%B0%E6%8D%AE%E5%A4%A7%E7%9A%84%E5%88%99%E8%83%BD%E6%94%BE%E4%B8%8B%E4%BB%A5%E5%A4%96%E5%9B%A0%E4%B8%BA%E4%B8%80%E5%BC%A0%E5%8D%A1%E6%94%BE%E7%9A%84%E4%B8%9C%E8%A5%BF%E6%9B%B4%E5%A4%9A%E4%BA%86%E9%82%A3%E4%B9%88%E9%9C%80%E8%A6%81%E7%9A%84%E5%8D%A1%E5%B0%B1%E5%B0%91%E4%B8%80%E4%BA%9B%E6%90%AC%E7%A7%BB%E5%92%8C%E9%80%9A%E4%BF%A1%E5%B0%B1%E4%BC%9A%E5%B0%91%E4%B8%80%E4%BA%9B%E5%9B%A0%E6%AD%A4%E8%BF%99%E6%96%B9%E9%9D%A2%E7%9A%84%E5%BC%80%E9%94%80%E4%BC%9A%E9%99%8D%E4%BD%8E"><span class="nav-number">3.2.5.</span> <span class="nav-text">一般情况下，是数据的搬移决定了latency（尤其是现在模型大起来了，gpu装不下，因此常常需要把模型分割，放到不同的gpu上，之后又需要频繁的交流通信）。且从能耗的角度上看，也是数据的搬移开销更大。可参考下图：因此，我们实验的时候会希望尽量减少数据搬运。这也是大显存的gpu的好处。除了说可能小显存的卡放不下模型或数据，大的则能放下以外，因为一张卡放的东西更多了，那么需要的卡就少一些，搬移和通信就会少一些，因此这方面的开销会降低</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#memory-related-metrics"><span class="nav-number">3.3.</span> <span class="nav-text">2.1. memory-related metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#parameters"><span class="nav-number">3.3.1.</span> <span class="nav-text">2.1.1. #parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E4%B8%AA%E6%8C%87%E6%A0%87%E5%BE%88%E7%9B%B4%E8%A7%82%E5%B0%B1%E6%98%AF%E7%AE%97%E4%B8%80%E4%B8%8B%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%A4%9A%E5%B0%91%E5%8F%82%E6%95%B0%E8%BF%99%E9%87%8C%E4%BB%A5%E7%BB%8F%E5%85%B8%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BE%8B%E7%AE%80%E5%8D%95%E7%9C%8B%E7%9C%8B%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E5%8F%82%E6%95%B0%E9%87%8F%E5%85%B6%E4%B8%AD%E7%9A%84%E5%90%84%E4%B8%AA%E8%AE%B0%E5%8F%B7%E4%B9%9F%E9%83%BD%E6%98%AF%E5%B8%B8%E8%A7%81%E7%9A%84%E7%94%A8%E6%B3%95%E8%A1%A8%E7%A4%BA%E5%90%AB%E4%B9%89%E5%A6%82%E4%B8%8B400%E6%8C%A8%E4%B8%AA%E6%9D%A5%E4%BB%94%E7%BB%86%E5%88%86%E6%9E%90%E4%B8%80%E4%B8%8B%E5%BD%93%E7%BB%83%E6%89%8B%E4%BA%86"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">这个指标很直观，就是算一下模型有多少参数。这里以经典的卷积网络为例，简单看看如何计算参数量：其中的各个记号也都是常见的用法，表示含义如下：挨个来仔细分析一下，当练手了</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%B1%82%E6%84%9F%E8%A7%89%E6%97%A0%E9%9C%80%E5%A4%9A%E8%A8%80%E5%AE%83%E5%B0%B1%E6%98%AF%E5%AF%B9%E7%89%B9%E5%BE%81%E7%BB%B4%E5%BA%A6%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E5%8F%98%E6%8D%A2%E8%BE%93%E5%85%A5%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%98%AFc_i%E8%BE%93%E5%87%BA%E7%BB%B4%E5%BA%A6%E6%98%AFc_o%E5%8F%82%E6%95%B0%E5%BD%A2%E7%8A%B6%E5%B0%B1%E6%98%AF%E4%B8%80%E4%B8%AA%E7%9F%A9%E5%BD%A2"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">线性层，感觉无需多言，它就是对特征维度进行一个变换，输入的维度是\(c_{i}\)，输出维度是\(c_{o}\)，参数形状就是一个矩形</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E5%8D%B7%E7%A7%AF%E5%B1%82%E6%9C%89c_o%E4%B8%AA%E6%AF%8F%E4%B8%AAkernel%E7%9A%84%E9%80%9A%E9%81%93%E6%95%B0%E5%92%8C%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E7%9B%B8%E5%90%8C%E4%BB%A5%E4%BE%BF%E8%BF%9B%E8%A1%8C%E5%8D%B7%E7%A7%AF%E5%AE%BD%E9%AB%98%E4%B9%9F%E6%98%AF%E5%B7%B2%E7%9F%A5%E7%9A%84%E6%89%80%E4%BB%A5%E5%B0%B1%E6%98%AF%E5%85%A8%E9%83%A8%E4%B9%98%E8%B5%B7%E6%9D%A5"><span class="nav-number">3.3.1.3.</span> <span class="nav-text">一般卷积层，有\(c_{o}\)个，每个kernel的通道数和输入通道相同（以便进行卷积），宽高也是已知的，所以就是全部乘起来</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF%E8%BF%99%E4%B8%AA%E6%9C%AC%E8%B4%A8%E4%B8%8A%E5%B0%B1%E6%98%AF%E8%8B%A5%E5%B9%B2%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E8%B4%9F%E8%B4%A3%E5%A4%84%E7%90%86%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%80%E9%83%A8%E5%88%86%E9%80%9A%E9%81%93%E6%9C%80%E5%90%8E%E6%8A%8A%E6%89%80%E6%9C%89%E7%BB%93%E6%9E%9C%E6%8B%BC%E8%B5%B7%E6%9D%A5%E5%81%87%E8%AE%BE%E5%88%86%E4%BA%86g%E7%BB%84%E9%82%A3%E4%B9%88%E6%AF%8F%E4%B8%80%E7%BB%84%E5%B0%8Fkernel%E8%B4%9F%E8%B4%A3%E7%9A%84%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E5%B0%B1%E6%98%AFfracc_ig%E8%BF%99%E7%BB%84kernel%E7%9A%84%E4%B8%AA%E6%95%B0%E4%B9%9F%E5%8D%B3%E8%BF%99%E7%BB%84%E5%8D%B7%E7%A7%AF%E7%BB%93%E6%9E%9C%E7%9A%84%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93%E6%95%B0%E5%B0%B1%E6%98%AFfracc_og%E6%AF%8F%E4%B8%AAkernel%E7%9A%84%E5%AE%BD%E9%AB%98%E8%BF%98%E6%98%AF%E4%B8%8D%E5%8F%98%E7%9A%84%E6%9C%80%E5%90%8E%E8%BF%99g%E7%BB%84%E6%AF%8F%E4%B8%80%E7%BB%84%E7%9A%84%E7%BB%93%E6%9E%9C%E7%9A%84%E9%80%9A%E9%81%93%E6%95%B0%E9%83%BD%E6%98%AFfracc_og%E6%8A%8Ag%E7%BB%84%E7%9A%84%E7%BB%93%E6%9E%9C%E5%9C%A8%E9%80%9A%E9%81%93%E7%BB%B4%E5%BA%A6%E4%B8%8A%E6%8B%BC%E8%B5%B7%E6%9D%A5%E5%B0%B1%E5%BE%97%E5%88%B0%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C%E6%AD%A4%E6%97%B6%E9%80%9A%E9%81%93%E6%95%B0%E5%B0%B1%E6%98%AFc_o%E6%89%80%E4%BB%A5%E5%8F%82%E6%95%B0%E9%87%8F%E6%98%AFg%E7%BB%84%E6%95%B0%E4%B9%98%E4%B8%8Afracc_ogkernel%E4%B8%AA%E6%95%B0%E4%B9%98%E4%B8%8Afracc_ig%E6%AF%8F%E4%B8%AAkernel%E7%9A%84%E9%80%9A%E9%81%93%E6%95%B0%E4%B9%98%E4%B8%8Ak_hcdot-k_w%E6%AF%8F%E4%B8%AAkernel%E7%9A%84%E5%AE%BD%E9%AB%98"><span class="nav-number">3.3.1.4.</span> <span class="nav-text">分组卷积，这个本质上就是若干小的卷积核负责处理图像的一部分通道，最后把所有结果拼起来。假设分了g组，那么每一组小kernel负责的输入通道就是\(\frac{c_{i}}{g}\)，这组kernel的个数（也即这组卷积结果的输出通道数）就是\(\frac{c_{o}}{g}\)，每个kernel的宽高还是不变的。最后这g组，每一组的结果的通道数都是\(\frac{c_{o}}{g}\)，把g组的结果在通道维度上拼起来，就得到最终结果（此时通道数就是\(c_{o}\)）。所以参数量是g（组数）乘上\(\frac{c_{o}}{g}\)（kernel个数）乘上\(\frac{c_{i}}{g}\)（每个kernel的通道数）乘上\(k_{h}\cdot
k_{w}\)（每个kernel的宽高）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E8%BF%99%E7%A7%8D%E5%8D%B7%E7%A7%AF%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9%E6%98%AF%E4%B8%80%E4%B8%AAkernel%E5%8F%AA%E8%B4%9F%E8%B4%A3%E4%B8%80%E4%B8%AA%E9%80%9A%E9%81%93%E6%89%80%E4%BB%A5%E5%AE%83%E4%B8%80%E5%AE%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93%E5%92%8C%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E6%95%B0%E7%9B%B8%E5%90%8C%E4%B8%94%E5%9B%A0%E4%B8%BA%E6%AF%8F%E4%B8%AAkernel%E5%8F%AA%E8%B4%9F%E8%B4%A3%E4%B8%80%E4%B8%AA%E9%80%9A%E9%81%93%E9%82%A3%E4%B9%88%E5%B0%B1%E7%9B%B8%E5%BD%93%E4%BA%8E%E6%AF%8F%E4%B8%AAkernel%E7%9A%84%E9%80%9A%E9%81%93%E6%95%B0%E6%98%AF1%E6%89%80%E4%BB%A5%E5%8F%82%E6%95%B0%E9%87%8F%E6%98%AFc_okernel%E4%B8%AA%E6%95%B0%E4%B9%9F%E7%AD%89%E4%BA%8Ec_i%E4%B9%98%E4%B8%8A1kernel%E9%80%9A%E9%81%93%E6%95%B0%E4%B9%98%E4%B8%8Ak_hcdot-k_wkernel%E5%AE%BD%E9%AB%98"><span class="nav-number">3.3.1.5.</span> <span class="nav-text">深度卷积，这种卷积主要特点是一个kernel只负责一个通道，所以它一定输出通道和输入通道数相同。且因为每个kernel只负责一个通道，那么就相当于每个kernel的通道数是1，所以参数量是\(c_{o}\)（kernel个数，也等于\(c_{i}\)）乘上1（kernel通道数）乘上\(k_{h}\cdot k_{w}\)（kernel宽高）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-size"><span class="nav-number">3.3.2.</span> <span class="nav-text">2.1.2. model size</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E4%B8%AA%E6%8C%87%E6%A0%87%E5%92%8C%E4%B8%8A%E9%9D%A2%E7%9A%84params%E6%81%AF%E6%81%AF%E7%9B%B8%E5%85%B3%E5%A6%82%E6%9E%9C%E8%AF%B4%E4%B8%8A%E9%9D%A2%E7%9A%84%E6%8C%87%E6%A0%87%E5%AE%83%E8%A1%A1%E9%87%8F%E7%9A%84%E6%98%AF%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0%E6%9C%89%E5%A4%9A%E5%B0%91%E9%82%A3%E4%B9%88%E8%BF%99%E4%B8%AA%E6%8C%87%E6%A0%87%E8%A1%A1%E9%87%8F%E7%9A%84%E5%B0%B1%E6%98%AF%E5%AD%98%E5%82%A8%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0%E9%9C%80%E8%A6%81%E5%A4%9A%E5%A4%A7%E7%9A%84%E7%A9%BA%E9%97%B4%E6%AD%A4%E6%97%B6%E7%9A%84%E5%8D%95%E4%BD%8D%E5%B0%B1%E6%98%AFkbmb%E4%B9%83%E8%87%B3%E6%9B%B4%E5%A4%A7%E7%9A%84%E5%AD%98%E5%82%A8%E5%8D%95%E4%BD%8D%E4%BA%86"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">这个指标和上面的#params息息相关。如果说上面的指标它衡量的是模型的参数有多少，那么这个指标衡量的就是，存储这些模型的参数，需要多大的空间（此时的单位就是KB，MB，乃至更大的存储单位了）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E7%A7%8D%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E6%83%85%E5%86%B5%E6%98%AF%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%80%E6%9C%89%E5%8F%82%E6%95%B0%E9%83%BD%E6%98%AF%E7%94%A8%E5%90%8C%E4%B8%80%E7%A7%8D%E6%A0%BC%E5%BC%8F%E6%9D%A5%E5%AD%98%E5%82%A8%E7%9A%84%E9%82%A3%E4%B9%88%E6%AD%A4%E6%97%B6%E7%9A%84model-size%E5%B0%B1%E7%AD%89%E4%BA%8E-textparameters-cdot-textbit-width-%E6%AF%94%E5%A6%82alexnet%E6%9C%8961m%E7%9A%84%E5%8F%82%E6%95%B0%E5%81%87%E5%A6%82%E9%83%BD%E7%94%A8int-8%E6%9D%A5%E5%AD%98%E5%82%A8%E9%82%A3%E4%B9%88model-size%E4%B8%BA61m-times-1byte8%E4%BD%8D-61mb-%E5%81%87%E5%A6%82%E9%83%BD%E7%94%A8float-32%E6%9D%A5%E5%AD%98%E5%82%A8%E5%88%99model-size%E4%B8%BA61m-times-4byte32%E4%BD%8D-244mb-%E5%8F%AF%E8%A7%81%E9%87%87%E7%94%A8%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%AD%98%E5%82%A8%E4%BC%9A%E6%9E%81%E5%A4%A7%E5%9C%B0%E5%BD%B1%E5%93%8Dmodel-size%E4%B9%9F%E6%9C%89%E5%BE%88%E5%A4%9A%E7%A0%94%E7%A9%B6%E6%8F%90%E5%87%BA%E6%96%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8B%E7%B1%BB%E7%9A%84%E5%8A%9B%E5%9B%BE%E5%87%8F%E5%B0%91model-size%E7%9A%84%E5%90%8C%E6%97%B6%E5%B0%BD%E9%87%8F%E4%BF%9D%E8%AF%81%E6%95%88%E6%9E%9C"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">一种最简单的情况是，模型的所有参数都是用同一种格式来存储的，那么此时的model
size就等于\(\#\ \text{Parameters} \cdot
\text{Bit Width}\)比如，AlexNet有61M的参数，假如都用int
8来存储，那么model size为61M \(\times\)
1Byte（8位）&#x3D; 61MB假如都用float 32来存储，则model size为61M \(\times\) 4Byte（32位）&#x3D;
244MB可见采用什么数据类型存储，会极大地影响model
size。也有很多研究提出新的数据类型之类的，力图减少model
size的同时尽量保证效果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#totalpeak-activations"><span class="nav-number">3.3.3.</span> <span class="nav-text">2.1.3. total&#x2F;peak #activations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%84%9F%E8%A7%89%E8%BF%99%E4%B8%80%E8%8A%82%E4%B8%BB%E8%A6%81%E6%98%AF%E8%AF%B4%E6%98%8E%E4%BA%86model-size%E5%8D%B3%E4%BD%BF%E9%99%8D%E4%B8%8B%E5%8E%BB%E4%BD%86totalpeak-activations%E6%9C%AA%E5%BF%85%E4%BC%9A%E9%99%8D%E4%B8%8B%E5%8E%BB%E4%BA%8C%E8%80%85%E4%B8%8D%E6%98%AF%E5%AE%8C%E5%85%A8%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E7%9A%84%E5%B9%B6%E4%B8%94%E4%B8%8A%E9%9D%A2%E4%B9%9F%E8%AE%B2%E5%88%B0%E4%BA%86%E6%95%B0%E6%8D%AE%E6%90%AC%E7%A7%BB%E9%83%A8%E5%88%86%E6%88%91%E4%BB%AC%E8%A6%81%E6%90%AC%E7%A7%BB%E7%9A%84%E6%97%A2%E6%9C%89%E4%B8%AD%E9%97%B4%E7%9A%84%E6%BF%80%E6%B4%BB%E5%80%BC%E4%B9%9F%E6%9C%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0%E8%80%8C%E4%BA%8B%E5%AE%9E%E4%B8%8A%E5%BE%88%E5%A4%9A%E6%97%B6%E5%80%99%E5%8D%A0%E5%A4%A7%E5%A4%B4%E7%9A%84%E5%85%B6%E5%AE%9E%E6%98%AF%E4%B8%AD%E9%97%B4%E7%9A%84activations%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%E6%9C%89%E5%8F%AF%E8%83%BDmemory-bottleneck%E6%98%AFactivations"><span class="nav-number">3.3.3.1.</span> <span class="nav-text">感觉这一节主要是说明了，model
size即使降下去，但total&#x2F;peak
#activations未必会降下去，二者不是完全线性相关的。并且，上面也讲到了，数据搬移部分，我们要搬移的，既有中间的激活值，也有模型的参数。而事实上，很多时候，占大头的其实是中间的activations，也就是说，有可能memory
bottleneck是#activations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9C%8B%E7%9C%8B%E8%BF%99%E5%B9%85%E5%9B%BE%E5%8F%AF%E4%BB%A5%E5%8F%91%E7%8E%B0%E6%89%80%E8%B0%93%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%9A%84%E7%BD%91%E7%BB%9C%E5%AE%83%E7%A1%AE%E5%AE%9E%E5%A4%A7%E5%B9%85%E9%99%8D%E4%BD%8E%E4%BA%86%E5%8F%82%E6%95%B0%E9%87%8F%E4%BD%86%E6%98%AFpeak-activations%E5%8D%B4%E5%B9%B6%E6%9C%AA%E9%99%8D%E4%BD%8E%E5%A4%9A%E5%B0%91%E5%8F%8D%E8%80%8C%E8%BF%98%E6%9C%89%E4%B8%8A%E5%8D%87%E5%8F%AF%E8%A7%81%E6%83%B3%E9%99%8D%E4%BD%8E%E8%BF%99%E4%B8%AApeak-activations%E6%98%AF%E6%9C%89%E9%9A%BE%E5%BA%A6%E7%9A%84"><span class="nav-number">3.3.3.2.</span> <span class="nav-text">看看这幅图，可以发现，所谓的轻量化的网络，它确实大幅降低了参数量，但是peak
activations却并未降低多少，反而还有上升。可见，想降低这个peak
activations是有难度的</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B6%E4%B8%94%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0peak-activations%E6%9C%89%E5%8F%AF%E8%83%BD%E5%BE%88%E5%A4%A7%E5%9C%A8%E6%8E%A8%E7%90%86%E6%97%B6%E5%8D%A0%E6%8D%AE%E4%B8%BB%E5%AF%BC%E5%9C%B0%E4%BD%8D%E5%86%B3%E5%AE%9A%E4%BA%86%E6%98%BE%E5%AD%98%E7%9A%84%E6%9C%80%E5%A4%A7%E5%BC%80%E9%94%80%E6%98%AF%E5%A4%9A%E5%B0%91"><span class="nav-number">3.3.3.3.</span> <span class="nav-text">并且可以看到，peak
activations有可能很大，在推理时占据主导地位，决定了显存的最大开销是多少</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E6%9C%89%E8%B6%A3%E7%9A%84%E7%8E%B0%E8%B1%A1%E6%98%AF%E6%B5%85%E5%B1%82%E7%9A%84%E6%97%B6%E5%80%99%E4%B8%80%E8%88%ACactivation-memory%E5%BC%80%E9%94%80%E4%BC%9A%E6%AF%94%E8%BE%83%E9%AB%98%E4%BB%A5cnn%E4%B8%BA%E4%BE%8B%E5%9B%A0%E4%B8%BA%E5%9C%A8%E4%B8%80%E5%BC%80%E5%A7%8B%E5%A4%84%E7%90%86%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E7%9A%84%E6%97%B6%E5%80%99%E5%88%86%E8%BE%A8%E7%8E%87%E8%BF%98%E5%BE%88%E9%AB%98%E6%B2%A1%E9%99%8D%E4%B8%8B%E5%8E%BB%E4%BD%86%E5%90%8E%E9%9D%A2%E5%9B%A0%E4%B8%BA%E4%BC%9A%E7%BB%8F%E8%BF%87%E9%99%8D%E9%87%87%E6%A0%B7%E7%AD%89%E6%93%8D%E4%BD%9C%E6%89%80%E4%BB%A5activation-memory%E5%86%8D%E5%90%8E%E9%9D%A2%E4%BC%9A%E4%BD%8E%E5%BE%88%E5%A4%9A-%E8%80%8C%E5%B1%82%E6%95%B0%E6%B7%B1%E7%9A%84%E6%97%B6%E5%80%99weight-memory%E5%BC%80%E9%94%80%E5%8F%88%E4%BC%9A%E6%9B%B4%E5%A4%A7%E4%BB%8D%E7%84%B6%E4%BB%A5cnn%E4%B8%BA%E4%BE%8B%E8%99%BD%E7%84%B6%E5%9C%A8%E5%B1%82%E6%95%B0%E6%B7%B1%E4%BA%86%E4%B9%8B%E5%90%8E%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%86%E8%BE%A8%E7%8E%87%E5%B0%8F%E4%BA%86%E4%BD%86%E6%98%AF%E9%80%9A%E9%81%93%E6%95%B0%E4%B8%80%E8%88%AC%E4%BC%9A%E5%A2%9E%E5%A4%9A%E8%80%8C%E8%BF%99%E5%B0%B1%E6%84%8F%E5%91%B3%E7%9D%80%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E6%95%B0%E9%87%8F%E7%9A%84%E5%A2%9E%E5%A4%9A%E4%B8%94%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E5%A4%A7%E5%B0%8F%E5%AE%BD%E9%AB%98%E4%B8%80%E8%88%AC%E4%B8%8D%E4%BC%9A%E5%8F%98%E5%8D%B3%E4%BD%BF%E5%8F%98%E4%B9%9F%E4%B8%8D%E4%BC%9A%E5%8F%98%E5%A4%AA%E5%A4%9A%E6%89%80%E4%BB%A5%E5%90%8E%E9%9D%A2weight-memory%E4%BC%9A%E5%A2%9E%E5%A4%A7-%E7%BB%BC%E5%90%88%E4%B8%8B%E6%9D%A5%E7%9C%8B%E4%B8%AD%E9%97%B4%E7%9A%84%E6%97%B6%E5%80%99%E5%8F%8D%E8%80%8C%E4%B8%A4%E7%A7%8D%E5%BC%80%E9%94%80%E9%83%BD%E4%B8%8D%E5%A4%AA%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%8F%E8%BF%87%E4%BA%86%E4%B8%80%E5%AE%9A%E7%9A%84%E9%99%8D%E9%87%87%E6%A0%B7%E4%B8%94%E9%80%9A%E9%81%93%E6%95%B0%E8%BF%98%E6%B2%A1%E6%9C%89%E9%82%A3%E4%B9%88%E5%A4%9A"><span class="nav-number">3.3.3.4.</span> <span class="nav-text">一个有趣的现象是，浅层的时候，一般activation
memory开销会比较高（以cnn为例，因为在一开始处理图像&#x2F;视频的时候，分辨率还很高，没降下去；但后面因为会经过降采样等操作，所以activation
memory再后面会低很多）而层数深的时候，weight
memory开销又会更大（仍然以cnn为例。虽然在层数深了之后，数据的分辨率小了，但是通道数一般会增多。而这就意味着，卷积核的数量的增多。且卷积核的大小（宽高）一般不会变，即使变也不会变太多。所以后面weight
memory会增大）综合下来看，中间的时候反而两种开销都不太大：数据经过了一定的降采样，且通道数还没有那么多</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A5alexnet%E4%B8%BA%E4%BE%8B%E5%88%86%E6%9E%90%E4%B8%80%E4%B8%8B%E4%B8%A4%E7%A7%8Dactivation%E6%80%8E%E4%B9%88%E7%AE%97total%E7%9A%84%E5%BE%88%E7%AE%80%E5%8D%95%E5%B0%B1%E6%98%AF%E7%AE%97%E5%87%BA%E6%89%80%E6%9C%89%E7%9A%84%E5%8A%A0%E8%B5%B7%E6%9D%A5%E5%8D%B3%E5%8F%AF%E8%80%8Cpeak%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8A%E8%AE%B2%E5%BD%93%E7%84%B6%E6%98%AF%E7%AE%97%E5%87%BA%E6%89%80%E6%9C%89%E5%B1%82%E7%9A%84activation%E7%84%B6%E5%90%8E%E5%8F%96%E6%9C%80%E5%A4%A7%E5%80%BC%E4%BD%86%E6%98%AF%E8%BF%99%E6%98%BE%E7%84%B6%E5%B7%A5%E4%BD%9C%E9%87%8F%E5%BE%88%E5%A4%A7%E6%A0%B9%E6%8D%AE%E4%B8%8A%E9%9D%A2%E8%A7%82%E5%AF%9F%E5%87%BA%E7%9A%84%E7%BB%8F%E9%AA%8C%E7%BB%93%E8%AE%BA%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E7%94%A8%E6%9C%80%E5%89%8D%E9%9D%A2%E7%9A%84%E5%B1%82%E4%B9%9F%E5%8D%B3%E7%AC%AC%E4%B8%80%E5%B1%82%E7%9A%84%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BAactivation%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E8%BF%91%E4%BC%BC%E4%BB%8E%E8%BF%99%E9%87%8C%E7%9A%84%E7%BB%93%E6%9E%9C%E4%B8%8A%E7%9C%8B%E4%BC%B0%E8%AE%A1%E5%87%BA%E6%9D%A5%E7%9A%84%E5%80%BC%E5%BA%94%E8%AF%A5%E4%B9%9F%E6%8C%BA%E5%87%86%E7%9A%84%E6%AF%95%E7%AB%9F%E8%BF%99%E9%87%8C%E4%BC%B0%E5%87%BA%E6%9D%A5%E7%9A%84"><span class="nav-number">3.3.3.5.</span> <span class="nav-text">以AlexNet为例，分析一下两种#activation怎么算。total的很简单，就是算出所有的加起来即可。而peak的，理论上讲，当然是算出所有层的#activation，然后取最大值；但是这显然工作量很大。根据上面观察出的经验结论，我们可以考虑用最前面的层（也即第一层）的输入与输出#activation进行一个近似（从这里的结果上看，估计出来的值应该也挺准的。毕竟这里估出来的</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#computation-related-metrics"><span class="nav-number">3.4.</span> <span class="nav-text">2.2. computation-related
metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#macmultiply-accumulate-operations"><span class="nav-number">3.4.1.</span> <span class="nav-text">2.2.1.
MAC（Multiply-Accumulate Operations）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E5%A6%82%E5%AD%97%E9%9D%A2%E6%84%8F%E6%80%9D%E8%BF%99%E4%B8%AA%E6%93%8D%E4%BD%9C%E4%BB%A3%E8%A1%A8%E4%BA%86%E4%B9%98%E4%B8%80%E6%AC%A1%E5%92%8C%E5%8A%A0%E4%B8%80%E6%AC%A1%E4%BB%A5%E4%B8%8B%E5%9B%BE%E4%B8%BA%E4%BE%8B%E5%BE%88%E5%AE%B9%E6%98%93%E4%BA%86%E8%A7%A3"><span class="nav-number">3.4.1.1.</span> <span class="nav-text">正如字面意思，这个操作代表了乘一次和加一次。以下图为例，很容易了解：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E4%B8%8D%E5%A6%A8%E7%9C%8B%E7%9C%8B%E5%90%84%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E5%AE%83%E4%BB%AC%E7%9A%84mac%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E5%90%84%E7%AC%A6%E5%8F%B7%E7%9A%84%E5%90%AB%E4%B9%89%E4%B8%BA%E7%BA%BF%E6%80%A7%E5%B1%82%E7%9A%84%E5%BE%88%E5%A5%BD%E5%88%86%E6%9E%90%E7%95%A5%E8%BF%87%E4%B8%80%E8%88%AC%E5%8D%B7%E7%A7%AF%E5%91%A2%E5%8F%AF%E4%BB%A5%E8%BF%99%E6%A0%B7%E6%80%9D%E8%80%83%E8%BE%93%E5%87%BA%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%83%8F%E7%B4%A0%E9%83%BD%E6%98%AF%E7%BB%8F%E8%BF%87%E4%B8%80%E6%AC%A1%E5%8D%B7%E7%A7%AF%E5%BE%97%E5%88%B0%E7%9A%84%E6%89%80%E4%BB%A5%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%BE%97%E5%88%B0%E6%80%BB%E5%85%B1%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%AC%A1%E6%95%B0%E5%B0%B1%E6%98%AF%E8%BE%93%E5%87%BA%E7%9A%84%E6%80%BB%E5%83%8F%E7%B4%A0%E6%95%B0%E4%B8%8B%E9%9D%A2%E5%8F%AA%E9%9C%80%E8%A6%81%E7%AE%97%E5%87%BA%E6%AF%8F%E4%B8%80%E6%AC%A1%E5%8D%B7%E7%A7%AF%E7%9A%84mac%E6%98%AF%E5%A4%9A%E5%B0%91%E4%BA%86%E8%80%8C%E4%B8%80%E6%AC%A1%E5%8D%B7%E7%A7%AF%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E5%BD%A2%E7%8A%B6%E6%98%AFc_ik_hk_w%E6%89%80%E4%BB%A5%E4%B9%98%E5%8A%A0%E7%9A%84%E6%AC%A1%E6%95%B0%E5%B0%B1%E6%98%AFc_itimes-k_htimes-k_w%E7%BB%BC%E4%B8%8A%E6%8A%8A%E6%AC%A1%E6%95%B0%E5%92%8C%E5%8D%95%E6%AC%A1%E7%9A%84mac%E4%B9%98%E8%B5%B7%E6%9D%A5%E5%B0%B1%E5%BE%97%E5%88%B0%E7%BB%93%E6%9E%9C"><span class="nav-number">3.4.1.2.</span> <span class="nav-text">进一步的，不妨看看各种常用的网络它们的MAC如何计算：各符号的含义为：线性层的很好分析，略过。一般卷积呢，可以这样思考：输出的每个像素，都是经过一次卷积得到的。所以我们可以得到总共的卷积次数，就是输出的总像素数。下面只需要算出每一次卷积的MAC是多少了。而一次卷积，一个卷积核的形状是\((c_{i},k_{h},k_{w})\)，所以乘加的次数就是\(c_{i}\times k_{h}\times
k_{w}\)，综上，把次数和单次的MAC乘起来就得到结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B1%BB%E4%BC%BC%E7%9A%84%E5%AF%B9%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF%E4%B9%9F%E5%8F%AF%E4%BB%A5%E9%87%87%E7%94%A8%E7%B1%BB%E4%BC%BC%E7%9A%84%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%E8%99%BD%E7%84%B6%E8%AF%B4%E5%AE%83%E6%9C%80%E5%90%8E%E7%9A%84%E7%BB%93%E6%9E%9C%E6%98%AF%E6%9C%89%E4%B8%80%E4%B8%AAconcat%E7%9A%84%E4%BD%86%E8%BF%99%E4%B8%8D%E5%BD%B1%E5%93%8D%E6%AF%8F%E4%B8%80%E4%B8%AA%E8%BE%93%E5%87%BA%E5%83%8F%E7%B4%A0%E4%BE%9D%E7%84%B6%E5%AF%B9%E5%BA%94%E4%BA%86%E4%B8%80%E6%AC%A1%E5%8D%B7%E7%A7%AF%E8%80%8C%E6%AF%8F%E6%AC%A1%E5%8D%B7%E7%A7%AF%E7%9A%84mac%E6%98%AF%E5%A4%9A%E5%B0%91%E5%91%A2%E8%BF%99%E9%87%8C%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E5%88%86%E7%BB%84%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E6%AF%8F%E4%B8%AAkernel%E7%9A%84%E5%BD%A2%E7%8A%B6%E5%B0%B1%E6%98%AFfracc_igk_hk_w%E5%8F%AA%E6%9C%89%E8%BF%99%E4%B8%80%E7%82%B9%E4%B8%8D%E5%90%8C%E8%80%8C%E5%B7%B2%E6%9C%80%E5%90%8E%E4%BE%9D%E7%84%B6%E6%98%AF%E6%8A%8A%E6%89%80%E6%9C%89%E7%9A%84%E4%B9%98%E8%B5%B7%E6%9D%A5%E4%B9%9F%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E5%88%86%E7%BB%84%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%9C%A8mac%E4%B8%8A%E4%B9%9F%E6%AF%94%E4%B8%80%E8%88%AC%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%A6%81%E5%B0%8F%E4%B8%80%E4%BA%9B"><span class="nav-number">3.4.1.3.</span> <span class="nav-text">类似的，对分组卷积，也可以采用类似的分析方法。虽然说它最后的结果是有一个concat的，但这不影响每一个输出像素依然对应了一次卷积。而每次卷积的MAC是多少呢？这里需要注意，分组的情况下，每个kernel的形状就是\((\frac{c_{i}}{g},k_{h},k_{w})\)，只有这一点不同而已。最后依然是把所有的乘起来（也可以看到，分组的卷积，在MAC上也比一般的卷积要小一些）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%B3%E4%BA%8E%E6%9C%80%E5%90%8E%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E5%AE%83%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%B8%80%E7%A7%8D%E7%89%B9%E6%AE%8A%E6%83%85%E5%86%B5%E5%8D%B3%E7%BB%84%E6%95%B0g%E6%81%B0%E5%A5%BD%E7%AD%89%E4%BA%8E%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E6%95%B0c_i%E6%89%80%E4%BB%A5%E5%A5%97%E7%94%A8%E4%B8%8A%E4%B8%80%E4%B8%AA%E7%9A%84%E7%BB%93%E8%AE%BA%E8%BF%9B%E8%A1%8C%E5%8C%96%E7%AE%80%E5%8D%B3%E5%8F%AF"><span class="nav-number">3.4.1.4.</span> <span class="nav-text">至于最后的深度卷积，它其实就是分组卷积的一种特殊情况，即组数g恰好等于输入通道数\(c_{i}\)，所以套用上一个的结论进行化简即可</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flop-flpos"><span class="nav-number">3.4.2.</span> <span class="nav-text">2.2.2. FLOP, FLPOS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#flop%E5%85%A8%E7%A7%B0%E6%98%AFfloating-point-operations%E4%B8%BE%E5%87%A0%E4%B8%AA%E4%BE%8B%E5%AD%90%E4%B8%80%E6%AC%A1%E4%B9%98%E6%B3%95%E6%98%AF%E4%B8%80%E4%B8%AA%E6%B5%AE%E7%82%B9%E6%95%B0%E6%93%8D%E4%BD%9C%E4%B8%80%E4%B8%AA%E5%8A%A0%E6%B3%95%E4%B9%9F%E6%98%AF%E4%B8%80%E4%B8%AA%E6%B5%AE%E7%82%B9%E6%95%B0%E6%93%8D%E4%BD%9C%E5%9B%A0%E6%AD%A4%E4%B8%80%E4%B8%AA%E4%B9%98%E5%8A%A0%E6%93%8D%E4%BD%9C%E5%B0%B1%E6%98%AF%E4%B8%A4%E4%B8%AA%E6%B5%AE%E7%82%B9%E6%95%B0%E6%93%8D%E4%BD%9C%E6%A0%87%E5%87%86%E7%9A%84alexnet%E5%AE%83%E6%9C%89724m%E7%9A%84macs%E5%9B%A0%E6%AD%A4%E5%AE%83%E7%9A%84flop%E4%B8%BA724m-2-1.4g-flops%E6%B3%A8%E6%84%8F%E8%BF%99%E9%87%8C%E7%9A%84%E7%BB%93%E5%B0%BEs%E6%98%AF%E5%B0%8F%E5%86%99%E7%9A%84%E5%8F%AA%E6%98%AF%E8%A1%A8%E7%A4%BA%E5%A4%8D%E6%95%B0%E5%92%8C%E4%B8%8B%E9%9D%A2%E7%9A%84flops%E7%9A%84%E5%A4%A7%E5%86%99%E7%BB%93%E5%B0%BEs%E5%90%AB%E4%B9%89%E4%B8%8D%E5%90%8C"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">FLOP，全称是Floating
Point
Operations。举几个例子，一次乘法，是一个浮点数操作；一个加法，也是一个浮点数操作。因此，一个乘加操作就是两个浮点数操作。标准的AlexNet它有724M的MACs，因此它的FLOP为724M
× 2 &#x3D; 1.4G
FLOPs（注意，这里的结尾s是小写的，只是表示复数，和下面的FLOPS的大写结尾S含义不同）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#flops%E5%85%A8%E7%A7%B0%E6%98%AFfloating-point-operation-per-second%E5%AE%83%E6%98%AF%E4%B8%80%E4%B8%AA%E8%A1%A1%E9%87%8F%E8%AE%A1%E7%AE%97%E9%80%9F%E5%BA%A6%E7%9A%84%E6%8C%87%E6%A0%87%E6%89%80%E4%BB%A5%E5%AE%83%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F%E4%B8%BAflopsfracflopssecond"><span class="nav-number">3.4.2.2.</span> <span class="nav-text">FLOPS，全称是Floating
Point Operation Per
Second，它是一个衡量计算速度的指标。所以它的计算公式为\(FLOPS&#x3D;\frac{FLOPS}{second}\)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#op-ops"><span class="nav-number">3.4.3.</span> <span class="nav-text">2.2.3. OP, OPS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E9%87%8C%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%AE%8C%E5%85%A8%E7%B1%BB%E4%BC%BC%E4%B8%8A%E9%9D%A2%E7%9A%84%E5%8F%AA%E4%B8%8D%E8%BF%87%E4%B8%8A%E9%9D%A2%E6%98%AF%E4%B8%93%E6%B3%A8%E4%BA%8E%E6%B5%AE%E7%82%B9%E5%9E%8B%E6%93%8D%E4%BD%9C%E8%80%8C%E8%BF%99%E9%87%8C%E7%9A%84%E8%8C%83%E5%9B%B4%E6%9B%B4%E5%8A%A0%E5%B9%BF%E6%B3%9B%E5%B9%B6%E4%B8%8D%E5%B1%80%E9%99%90%E4%BA%8E%E6%B5%AE%E7%82%B9%E6%93%8D%E4%BD%9C%E6%AF%94%E5%A6%82%E4%B8%80%E4%B8%AAint%E6%93%8D%E4%BD%9C%E4%B9%9F%E5%8F%AF%E4%BB%A5%E7%AE%97%E4%BD%9C%E4%B8%80%E4%B8%AAop"><span class="nav-number">3.4.3.1.</span> <span class="nav-text">这里的两个概念完全类似上面的。只不过上面是专注于浮点型操作，而这里的范围更加广泛，并不局限于浮点操作（比如一个int操作也可以算作一个OP）</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">230</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">73</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">104</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
