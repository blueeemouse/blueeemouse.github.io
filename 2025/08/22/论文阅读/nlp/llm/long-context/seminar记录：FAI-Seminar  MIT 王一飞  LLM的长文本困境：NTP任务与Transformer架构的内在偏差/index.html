<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="如今的llm，大体上就是两个构件：nest token prediction训练范式（NTP）和transformer架构（虽然有很多变体）。目前大概还是没有逃出这个范式，以及主流架构依然是transformer类型的。这个seminar主要探讨的就是这两个点：NTP的范式还能做得更好吗？（虽然一套训练范式也许是有它自己的上限的，现在的结果也有点表明了，因为scaling law不再像最开始那样卓">
<meta property="og:type" content="article">
<meta property="og:title" content="seminar记录：FAI-Seminar MIT 王一飞 LLM的长文本困境：NTP任务与Transformer架构的内在偏差">
<meta property="og:url" content="https://blueeemouse.github.io/2025/08/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/long-context/seminar%E8%AE%B0%E5%BD%95%EF%BC%9AFAI-Seminar%20%20MIT%20%E7%8E%8B%E4%B8%80%E9%A3%9E%20%20LLM%E7%9A%84%E9%95%BF%E6%96%87%E6%9C%AC%E5%9B%B0%E5%A2%83%EF%BC%9ANTP%E4%BB%BB%E5%8A%A1%E4%B8%8ETransformer%E6%9E%B6%E6%9E%84%E7%9A%84%E5%86%85%E5%9C%A8%E5%81%8F%E5%B7%AE/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="如今的llm，大体上就是两个构件：nest token prediction训练范式（NTP）和transformer架构（虽然有很多变体）。目前大概还是没有逃出这个范式，以及主流架构依然是transformer类型的。这个seminar主要探讨的就是这两个点：NTP的范式还能做得更好吗？（虽然一套训练范式也许是有它自己的上限的，现在的结果也有点表明了，因为scaling law不再像最开始那样卓">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blueeemouse.github.io/images/FAI-Seminar-MIT-%E7%8E%8B%E4%B8%80%E9%A3%9E-%E9%95%BF%E6%96%87%E6%9C%AC%E7%9A%84%E5%9B%B0%E5%A2%83/position-bias-from-arch.png">
<meta property="og:image" content="https://blueeemouse.github.io/images/FAI-Seminar-MIT-%E7%8E%8B%E4%B8%80%E9%A3%9E-%E9%95%BF%E6%96%87%E6%9C%AC%E7%9A%84%E5%9B%B0%E5%A2%83/attn-induces-graph.png">
<meta property="article:published_time" content="2025-08-22T09:36:00.000Z">
<meta property="article:modified_time" content="2025-09-24T07:59:46.986Z">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blueeemouse.github.io/images/FAI-Seminar-MIT-%E7%8E%8B%E4%B8%80%E9%A3%9E-%E9%95%BF%E6%96%87%E6%9C%AC%E7%9A%84%E5%9B%B0%E5%A2%83/position-bias-from-arch.png">

<link rel="canonical" href="https://blueeemouse.github.io/2025/08/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/long-context/seminar%E8%AE%B0%E5%BD%95%EF%BC%9AFAI-Seminar%20%20MIT%20%E7%8E%8B%E4%B8%80%E9%A3%9E%20%20LLM%E7%9A%84%E9%95%BF%E6%96%87%E6%9C%AC%E5%9B%B0%E5%A2%83%EF%BC%9ANTP%E4%BB%BB%E5%8A%A1%E4%B8%8ETransformer%E6%9E%B6%E6%9E%84%E7%9A%84%E5%86%85%E5%9C%A8%E5%81%8F%E5%B7%AE/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>seminar记录：FAI-Seminar MIT 王一飞 LLM的长文本困境：NTP任务与Transformer架构的内在偏差 | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/08/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/long-context/seminar%E8%AE%B0%E5%BD%95%EF%BC%9AFAI-Seminar%20%20MIT%20%E7%8E%8B%E4%B8%80%E9%A3%9E%20%20LLM%E7%9A%84%E9%95%BF%E6%96%87%E6%9C%AC%E5%9B%B0%E5%A2%83%EF%BC%9ANTP%E4%BB%BB%E5%8A%A1%E4%B8%8ETransformer%E6%9E%B6%E6%9E%84%E7%9A%84%E5%86%85%E5%9C%A8%E5%81%8F%E5%B7%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          seminar记录：FAI-Seminar MIT 王一飞 LLM的长文本困境：NTP任务与Transformer架构的内在偏差
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-08-22 17:36:00" itemprop="dateCreated datePublished" datetime="2025-08-22T17:36:00+08:00">2025-08-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-09-24 15:59:46" itemprop="dateModified" datetime="2025-09-24T15:59:46+08:00">2025-09-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="如今的llm，大体上就是两个构件：nest-token-prediction训练范式（NTP）和transformer架构（虽然有很多变体）。目前大概还是没有逃出这个范式，以及主流架构依然是transformer类型的。这个seminar主要探讨的就是这两个点：NTP的范式还能做得更好吗？（虽然一套训练范式也许是有它自己的上限的，现在的结果也有点表明了，因为scaling-law不再像最开始那样卓有成效了。但现在NTP范式就已经达到上限了吗？还能不能做得更好呢？）Transformer架构呢？它有什么自己的问题吗？"><a href="#如今的llm，大体上就是两个构件：nest-token-prediction训练范式（NTP）和transformer架构（虽然有很多变体）。目前大概还是没有逃出这个范式，以及主流架构依然是transformer类型的。这个seminar主要探讨的就是这两个点：NTP的范式还能做得更好吗？（虽然一套训练范式也许是有它自己的上限的，现在的结果也有点表明了，因为scaling-law不再像最开始那样卓有成效了。但现在NTP范式就已经达到上限了吗？还能不能做得更好呢？）Transformer架构呢？它有什么自己的问题吗？" class="headerlink" title="如今的llm，大体上就是两个构件：nest token prediction训练范式（NTP）和transformer架构（虽然有很多变体）。目前大概还是没有逃出这个范式，以及主流架构依然是transformer类型的。这个seminar主要探讨的就是这两个点：NTP的范式还能做得更好吗？（虽然一套训练范式也许是有它自己的上限的，现在的结果也有点表明了，因为scaling law不再像最开始那样卓有成效了。但现在NTP范式就已经达到上限了吗？还能不能做得更好呢？）Transformer架构呢？它有什么自己的问题吗？"></a>如今的llm，大体上就是两个构件：nest token prediction训练范式（NTP）和transformer架构（虽然有很多变体）。目前大概还是没有逃出这个范式，以及主流架构依然是transformer类型的。这个seminar主要探讨的就是这两个点：NTP的范式还能做得更好吗？（虽然一套训练范式也许是有它自己的上限的，现在的结果也有点表明了，因为scaling law不再像最开始那样卓有成效了。但现在NTP范式就已经达到上限了吗？还能不能做得更好呢？）<br>Transformer架构呢？它有什么自己的问题吗？</h1><span id="more"></span>
<h1 id="首先，实验表明，llm的PPL指标得分和它在人为构造的long-context的benchmark上的得分，似乎二者之间的相关性很低（有图表），所以这在某种程度上表明PPL指标似乎并不能比较好地反映模型的长文本能力？NTP这个训练范式似乎对于long-context能力而言，不是那么高效的范式？针对这两点，其实现有工作有在尝试解决。对于PPL指标可能不够好的问题，有一些工作设计了各种人工的、针对long-context的benchmark（比如RULER，LongEval，LongBench）对于第二点，一个很基本的思路是继续加大数据量，同时人工进行适当的筛选处理。毕竟，虽然PPL指标和在long-context-benchmark上的分数相关性不大，但多少是有一点相关性的。只不过这样很低效就是了但总的来说，现在的工作里还是融入了太多人的先验了。所以，有没有办法不要融入这么多人的先验知识？有没有可能PPL指标其实是没问题的，而之所以没能揭示llm的long-context能力，是有一些问题而已？能否把这些问题给解决了？"><a href="#首先，实验表明，llm的PPL指标得分和它在人为构造的long-context的benchmark上的得分，似乎二者之间的相关性很低（有图表），所以这在某种程度上表明PPL指标似乎并不能比较好地反映模型的长文本能力？NTP这个训练范式似乎对于long-context能力而言，不是那么高效的范式？针对这两点，其实现有工作有在尝试解决。对于PPL指标可能不够好的问题，有一些工作设计了各种人工的、针对long-context的benchmark（比如RULER，LongEval，LongBench）对于第二点，一个很基本的思路是继续加大数据量，同时人工进行适当的筛选处理。毕竟，虽然PPL指标和在long-context-benchmark上的分数相关性不大，但多少是有一点相关性的。只不过这样很低效就是了但总的来说，现在的工作里还是融入了太多人的先验了。所以，有没有办法不要融入这么多人的先验知识？有没有可能PPL指标其实是没问题的，而之所以没能揭示llm的long-context能力，是有一些问题而已？能否把这些问题给解决了？" class="headerlink" title="首先，实验表明，llm的PPL指标得分和它在人为构造的long-context的benchmark上的得分，似乎二者之间的相关性很低（有图表），所以这在某种程度上表明PPL指标似乎并不能比较好地反映模型的长文本能力？NTP这个训练范式似乎对于long context能力而言，不是那么高效的范式？针对这两点，其实现有工作有在尝试解决。对于PPL指标可能不够好的问题，有一些工作设计了各种人工的、针对long-context的benchmark（比如RULER，LongEval，LongBench）对于第二点，一个很基本的思路是继续加大数据量，同时人工进行适当的筛选处理。毕竟，虽然PPL指标和在long-context benchmark上的分数相关性不大，但多少是有一点相关性的。只不过这样很低效就是了但总的来说，现在的工作里还是融入了太多人的先验了。所以，有没有办法不要融入这么多人的先验知识？有没有可能PPL指标其实是没问题的，而之所以没能揭示llm的long-context能力，是有一些问题而已？能否把这些问题给解决了？"></a>首先，实验表明，llm的PPL指标得分和它在人为构造的long-context的benchmark上的得分，似乎二者之间的相关性很低（有图表），所以这在某种程度上表明PPL指标似乎并不能比较好地反映模型的长文本能力？NTP这个训练范式似乎对于long context能力而言，不是那么高效的范式？<br>针对这两点，其实现有工作有在尝试解决。对于PPL指标可能不够好的问题，有一些工作设计了各种人工的、针对long-context的benchmark（比如RULER，LongEval，LongBench）<br>对于第二点，一个很基本的思路是继续加大数据量，同时人工进行适当的筛选处理。毕竟，虽然PPL指标和在long-context benchmark上的分数相关性不大，但多少是有一点相关性的。只不过这样很低效就是了<br>但总的来说，现在的工作里还是融入了太多人的先验了。所以，有没有办法不要融入这么多人的先验知识？有没有可能PPL指标其实是没问题的，而之所以没能揭示llm的long-context能力，是有一些问题而已？能否把这些问题给解决了？</h1><h1 id="通过分析那些人工构造的benchmark的数据，作者发现，它们构造的格式其实是相似的，就是在prompt（也就是给llm的问题）中，混入很多的key-query对，最后问llm的问题，就是问其中的某个key对应的query是谁。那么，如果llm能正确回答，确实表明了它有一定的long-context能力（毕竟能排除其它无关的key-query对的影响）但一个核心的点在于，llm它是会回答一句完整的话的，可能类似于：The-xxx-in-xxx-is-这时我们实际关心的其实只有最后的Answer那个token，也只有它才真正测试了llm的long-context能力。但是在训练的时候，算损失是会把llm的所有回答的token都拿去算损失的（PPL就是如此），因此这样算出来的东西其实混入了对很多不那么重要的token的预测结果的评价，而非专注于对体现long-context的关键token的评价因此，如果我们把这个Answer-token视为key-token，那么我们能否这样：找出key-token，然后只在这些key-tokens上算PPL指标。这样的PPL是否能更加好地、更加准确地体现llm的long-context能力？结果表明，确实是可以的因此，这说明PPL指标本身没有太多问题，它是能反映long-context能力的。只不过需要在key-tokens上算而已。因此，现在的问题就转化成，如何找到这些key-tokens呢？因为我们希望的是不需要人为构造，仅用自然的数据（也就是那些比较原始的数据）就能进行改进。但自然的数据里是不会有key-tokens的标注的。那其实这就很像一个无监督学习，或者说自监督学习（SSL）的问题了。大体的思路就是经典的，”一边标，一边学“，token是否是key-token，就是要打的标签"><a href="#通过分析那些人工构造的benchmark的数据，作者发现，它们构造的格式其实是相似的，就是在prompt（也就是给llm的问题）中，混入很多的key-query对，最后问llm的问题，就是问其中的某个key对应的query是谁。那么，如果llm能正确回答，确实表明了它有一定的long-context能力（毕竟能排除其它无关的key-query对的影响）但一个核心的点在于，llm它是会回答一句完整的话的，可能类似于：The-xxx-in-xxx-is-这时我们实际关心的其实只有最后的Answer那个token，也只有它才真正测试了llm的long-context能力。但是在训练的时候，算损失是会把llm的所有回答的token都拿去算损失的（PPL就是如此），因此这样算出来的东西其实混入了对很多不那么重要的token的预测结果的评价，而非专注于对体现long-context的关键token的评价因此，如果我们把这个Answer-token视为key-token，那么我们能否这样：找出key-token，然后只在这些key-tokens上算PPL指标。这样的PPL是否能更加好地、更加准确地体现llm的long-context能力？结果表明，确实是可以的因此，这说明PPL指标本身没有太多问题，它是能反映long-context能力的。只不过需要在key-tokens上算而已。因此，现在的问题就转化成，如何找到这些key-tokens呢？因为我们希望的是不需要人为构造，仅用自然的数据（也就是那些比较原始的数据）就能进行改进。但自然的数据里是不会有key-tokens的标注的。那其实这就很像一个无监督学习，或者说自监督学习（SSL）的问题了。大体的思路就是经典的，”一边标，一边学“，token是否是key-token，就是要打的标签" class="headerlink" title="通过分析那些人工构造的benchmark的数据，作者发现，它们构造的格式其实是相似的，就是在prompt（也就是给llm的问题）中，混入很多的key-query对，最后问llm的问题，就是问其中的某个key对应的query是谁。那么，如果llm能正确回答，确实表明了它有一定的long-context能力（毕竟能排除其它无关的key-query对的影响）但一个核心的点在于，llm它是会回答一句完整的话的，可能类似于：The xxx in xxx is &lt;Answer&gt;这时我们实际关心的其实只有最后的Answer那个token，也只有它才真正测试了llm的long-context能力。但是在训练的时候，算损失是会把llm的所有回答的token都拿去算损失的（PPL就是如此），因此这样算出来的东西其实混入了对很多不那么重要的token的预测结果的评价，而非专注于对体现long-context的关键token的评价因此，如果我们把这个Answer token视为key token，那么我们能否这样：找出key token，然后只在这些key tokens上算PPL指标。这样的PPL是否能更加好地、更加准确地体现llm的long-context能力？结果表明，确实是可以的因此，这说明PPL指标本身没有太多问题，它是能反映long-context能力的。只不过需要在key tokens上算而已。因此，现在的问题就转化成，如何找到这些key tokens呢？因为我们希望的是不需要人为构造，仅用自然的数据（也就是那些比较原始的数据）就能进行改进。但自然的数据里是不会有key tokens的标注的。那其实这就很像一个无监督学习，或者说自监督学习（SSL）的问题了。大体的思路就是经典的，”一边标，一边学“，token是否是key token，就是要打的标签"></a>通过分析那些人工构造的benchmark的数据，作者发现，它们构造的格式其实是相似的，就是在prompt（也就是给llm的问题）中，混入很多的key-query对，最后问llm的问题，就是问其中的某个key对应的query是谁。那么，如果llm能正确回答，确实表明了它有一定的long-context能力（毕竟能排除其它无关的key-query对的影响）<br>但一个核心的点在于，llm它是会回答一句完整的话的，可能类似于：<code>The xxx in xxx is &lt;Answer&gt;</code><br>这时我们实际关心的其实只有最后的Answer那个token，也只有它才真正测试了llm的long-context能力。但是在训练的时候，算损失是会把llm的所有回答的token都拿去算损失的（PPL就是如此），因此这样算出来的东西其实混入了对很多不那么重要的token的预测结果的评价，而非专注于对体现long-context的关键token的评价<br>因此，如果我们把这个Answer token视为key token，那么我们能否这样：找出key token，然后只在这些key tokens上算PPL指标。这样的PPL是否能更加好地、更加准确地体现llm的long-context能力？结果表明，确实是可以的<br>因此，这说明PPL指标本身没有太多问题，它是能反映long-context能力的。只不过需要在key tokens上算而已。因此，现在的问题就转化成，如何找到这些key tokens呢？因为我们希望的是不需要人为构造，仅用自然的数据（也就是那些比较原始的数据）就能进行改进。但自然的数据里是不会有key tokens的标注的。那其实这就很像一个无监督学习，或者说自监督学习（SSL）的问题了。大体的思路就是经典的，”一边标，一边学“，token是否是key token，就是要打的标签</h1><h1 id="那怎么打标签？这里是一个特别巧妙的思路，或者说trick？然而的的确确是在其它领域出现过的我们的目标是找到能反映llm的long-context能力的关键tokens，怎样判断一个token是否反映了long-context能力呢？作者给出了一个例子。比如我们现在有一大串long-context，最后有一句话要预测，且最后的一个token是key-token，它只在long-context的一开始出现了，后面都没出现。那如果我们把要预测的那句话所用的long-context给截断，截断成llm正常的上下文窗口的长度（也就是pretrain阶段的长度，例如4K），再来预测一下。截断前和截断后都进行预测。那么，最后的那句话里，按理说，除了key-token以外，其它的非key-tokens，应该预测的概率都差别不大（因为它们不是key-tokens，它们并没有反映llm的long-context能力，也就是说，llm按理说不需要那么多的context也能预测出来）；相比之下，那个key-token是能反映llm的long-context能力的，因为它需要用到一大串context，所以一旦截断context，可以预测的是，在这个token上预测的概率应该有比较大的下降。这无疑就是一个判断是否为key-token的指标（很眼熟啊，mask一下看看前后的变化。它是属于causal-intervention（因果干预）的一种。广义的causal-intervention是变动一个变量（称为干预变量），观察另一个变量的变化情况（称为结果变量），从而揭示二者之间的因果关系。实在是很妙）因此，我们考虑利用这一点，把截断前和阶段后token的预测概率做一个比值，得到一个指标LPG（Log-Probability-Gain），这个比值越大，则这个token越有可能是key-token。实操的时候，我们就设定一个threshold，比值超过了threshold就认为这个token是key-token"><a href="#那怎么打标签？这里是一个特别巧妙的思路，或者说trick？然而的的确确是在其它领域出现过的我们的目标是找到能反映llm的long-context能力的关键tokens，怎样判断一个token是否反映了long-context能力呢？作者给出了一个例子。比如我们现在有一大串long-context，最后有一句话要预测，且最后的一个token是key-token，它只在long-context的一开始出现了，后面都没出现。那如果我们把要预测的那句话所用的long-context给截断，截断成llm正常的上下文窗口的长度（也就是pretrain阶段的长度，例如4K），再来预测一下。截断前和截断后都进行预测。那么，最后的那句话里，按理说，除了key-token以外，其它的非key-tokens，应该预测的概率都差别不大（因为它们不是key-tokens，它们并没有反映llm的long-context能力，也就是说，llm按理说不需要那么多的context也能预测出来）；相比之下，那个key-token是能反映llm的long-context能力的，因为它需要用到一大串context，所以一旦截断context，可以预测的是，在这个token上预测的概率应该有比较大的下降。这无疑就是一个判断是否为key-token的指标（很眼熟啊，mask一下看看前后的变化。它是属于causal-intervention（因果干预）的一种。广义的causal-intervention是变动一个变量（称为干预变量），观察另一个变量的变化情况（称为结果变量），从而揭示二者之间的因果关系。实在是很妙）因此，我们考虑利用这一点，把截断前和阶段后token的预测概率做一个比值，得到一个指标LPG（Log-Probability-Gain），这个比值越大，则这个token越有可能是key-token。实操的时候，我们就设定一个threshold，比值超过了threshold就认为这个token是key-token" class="headerlink" title="那怎么打标签？这里是一个特别巧妙的思路，或者说trick？然而的的确确是在其它领域出现过的我们的目标是找到能反映llm的long-context能力的关键tokens，怎样判断一个token是否反映了long-context能力呢？作者给出了一个例子。比如我们现在有一大串long context，最后有一句话要预测，且最后的一个token是key token，它只在long context的一开始出现了，后面都没出现。那如果我们把要预测的那句话所用的long context给截断，截断成llm正常的上下文窗口的长度（也就是pretrain阶段的长度，例如4K），再来预测一下。截断前和截断后都进行预测。那么，最后的那句话里，按理说，除了key token以外，其它的非key tokens，应该预测的概率都差别不大（因为它们不是key tokens，它们并没有反映llm的long-context能力，也就是说，llm按理说不需要那么多的context也能预测出来）；相比之下，那个key token是能反映llm的long-context能力的，因为它需要用到一大串context，所以一旦截断context，可以预测的是，在这个token上预测的概率应该有比较大的下降。这无疑就是一个判断是否为key token的指标（很眼熟啊，mask一下看看前后的变化。它是属于causal intervention（因果干预）的一种。广义的causal intervention是变动一个变量（称为干预变量），观察另一个变量的变化情况（称为结果变量），从而揭示二者之间的因果关系。实在是很妙）因此，我们考虑利用这一点，把截断前和阶段后token的预测概率做一个比值，得到一个指标LPG（Log Probability Gain），这个比值越大，则这个token越有可能是key token。实操的时候，我们就设定一个threshold，比值超过了threshold就认为这个token是key token"></a>那怎么打标签？这里是一个特别巧妙的思路，或者说trick？然而的的确确是在其它领域出现过的<br>我们的目标是找到能反映llm的long-context能力的关键tokens，怎样判断一个token是否反映了long-context能力呢？作者给出了一个例子。比如我们现在有一大串long context，最后有一句话要预测，且最后的一个token是key token，它只在long context的一开始出现了，后面都没出现。那如果我们把要预测的那句话所用的long context给截断，截断成llm正常的上下文窗口的长度（也就是pretrain阶段的长度，例如4K），再来预测一下。截断前和截断后都进行预测。那么，最后的那句话里，按理说，除了key token以外，其它的非key tokens，应该预测的概率都差别不大（因为它们不是key tokens，它们并没有反映llm的long-context能力，也就是说，llm按理说不需要那么多的context也能预测出来）；相比之下，那个key token是能反映llm的long-context能力的，因为它需要用到一大串context，所以一旦截断context，可以预测的是，在这个token上预测的概率应该有比较大的下降。这无疑就是一个判断是否为key token的指标（很眼熟啊，mask一下看看前后的变化。它是属于causal intervention（因果干预）的一种。广义的causal intervention是变动一个变量（称为干预变量），观察另一个变量的变化情况（称为结果变量），从而揭示二者之间的因果关系。实在是很妙）<br>因此，我们考虑利用这一点，把截断前和阶段后token的预测概率做一个比值，得到一个指标LPG（Log Probability Gain），这个比值越大，则这个token越有可能是key token。实操的时候，我们就设定一个threshold，比值超过了threshold就认为这个token是key token</h1><h2 id="首先，这里关于判断key-token的方法，有一些点需要澄清，或者说阐明。上面我们举的似乎是一个有点极端的例子（事实上讲座上举的就是这个例子），即有用的上文仅出现在context的一开始。这时，我们把long-context截断到short-context，来计算前后的预测概率比值，确实应该会有很大的变化。但如果，这个有用的上文，它并非出现在context的一开始，而是出现在，比方说context的中间呢？这时有可能出现：我们截断context（截断context，个人的理解是，从待预测的token，或者seq开始，往上去计数，计到short-context的范围为止），然后这个有用的上文，它被包含在了阶段后的short-context里了。这样，截断前后预测这个token的概率的比值，应该不大吧？这样我们不就会判断它为非key-token了吗？但其实，本来就是啊……我们不能墨守成规地认为一个token一定就是key-token或者不是key-token。本质上，一个token是key-token，只需要它待预测的位置和关键的提示信息离得足够远就行（虽说这一点吧，也不全面，但显然这种情况是体现long-context的一个场景）。所以，并不是说只有关键信息出现在context的一开始的时候，它对应的token才是key-token。只要一个token的关键信息离它足够远，远到从这个待预测token开始往上数，数到short-context的范围上限了，还是没有出现关键信息，那这个token就可以当作是key-token"><a href="#首先，这里关于判断key-token的方法，有一些点需要澄清，或者说阐明。上面我们举的似乎是一个有点极端的例子（事实上讲座上举的就是这个例子），即有用的上文仅出现在context的一开始。这时，我们把long-context截断到short-context，来计算前后的预测概率比值，确实应该会有很大的变化。但如果，这个有用的上文，它并非出现在context的一开始，而是出现在，比方说context的中间呢？这时有可能出现：我们截断context（截断context，个人的理解是，从待预测的token，或者seq开始，往上去计数，计到short-context的范围为止），然后这个有用的上文，它被包含在了阶段后的short-context里了。这样，截断前后预测这个token的概率的比值，应该不大吧？这样我们不就会判断它为非key-token了吗？但其实，本来就是啊……我们不能墨守成规地认为一个token一定就是key-token或者不是key-token。本质上，一个token是key-token，只需要它待预测的位置和关键的提示信息离得足够远就行（虽说这一点吧，也不全面，但显然这种情况是体现long-context的一个场景）。所以，并不是说只有关键信息出现在context的一开始的时候，它对应的token才是key-token。只要一个token的关键信息离它足够远，远到从这个待预测token开始往上数，数到short-context的范围上限了，还是没有出现关键信息，那这个token就可以当作是key-token" class="headerlink" title="首先，这里关于判断key token的方法，有一些点需要澄清，或者说阐明。上面我们举的似乎是一个有点极端的例子（事实上讲座上举的就是这个例子），即有用的上文仅出现在context的一开始。这时，我们把long context截断到short context，来计算前后的预测概率比值，确实应该会有很大的变化。但如果，这个有用的上文，它并非出现在context的一开始，而是出现在，比方说context的中间呢？这时有可能出现：我们截断context（截断context，个人的理解是，从待预测的token，或者seq开始，往上去计数，计到short context的范围为止），然后这个有用的上文，它被包含在了阶段后的short context里了。这样，截断前后预测这个token的概率的比值，应该不大吧？这样我们不就会判断它为非key token了吗？但其实，本来就是啊……我们不能墨守成规地认为一个token一定就是key token或者不是key token。本质上，一个token是key token，只需要它待预测的位置和关键的提示信息离得足够远就行（虽说这一点吧，也不全面，但显然这种情况是体现long context的一个场景）。所以，并不是说只有关键信息出现在context的一开始的时候，它对应的token才是key token。只要一个token的关键信息离它足够远，远到从这个待预测token开始往上数，数到short context的范围上限了，还是没有出现关键信息，那这个token就可以当作是key token"></a>首先，这里关于判断key token的方法，有一些点需要澄清，或者说阐明。上面我们举的似乎是一个有点极端的例子（事实上讲座上举的就是这个例子），即有用的上文仅出现在context的一开始。这时，我们把long context截断到short context，来计算前后的预测概率比值，确实应该会有很大的变化。但如果，这个有用的上文，它并非出现在context的一开始，而是出现在，比方说context的中间呢？这时有可能出现：我们截断context（截断context，个人的理解是，从待预测的token，或者seq开始，往上去计数，计到short context的范围为止），然后这个有用的上文，它被包含在了阶段后的short context里了。这样，截断前后预测这个token的概率的比值，应该不大吧？这样我们不就会判断它为非key token了吗？但其实，本来就是啊……我们不能墨守成规地认为一个token一定就是key token或者不是key token。本质上，一个token是key token，只需要它待预测的位置和关键的提示信息离得足够远就行（虽说这一点吧，也不全面，但显然这种情况是体现long context的一个场景）。所以，并不是说只有关键信息出现在context的一开始的时候，它对应的token才是key token。只要一个token的关键信息离它足够远，远到从这个待预测token开始往上数，数到short context的范围上限了，还是没有出现关键信息，那这个token就可以当作是key token</h2><h2 id="其次，下面的改进与分析则更加巧妙，感觉也体现了研究的一个常用思路。作者发现，用这个指标，可以找到85-6-的key-token，其实效果已经还不错了，但作者希望能更好。因此他分析了那些被错认为key-token的token（也就是那些并非key-token，但是LPG值却挺大的token）的预测概率，发现它们在截断前，本身被预测的概率就比较小。即，它们本身就是比较难被预测的token（就是说，截断前后其实这些token都比较难以预测。可能是因为预测它们所需要的上下文，在这整一段的long-context里就压根没有，所以相对于这一段long-context而言，这个token并不是key-token；也可以类比一下，人在进行写作等创作输出的活动的时候，的确有时候会毫无逻辑地、或者说逻辑跳跃地太大以至于显得天马行空的想到某个事物。这种内容，喂给llm，它就不太能基于上下文预测到了。解释其实可以有很多，但这并不是重点，因为这里主要是基于统计观察发现了一个事实规律，并且这个规律可以辅助我们改进上面的LPG指标）。基于这个观察，我们可以把上面的指标修正一下：一个token被认为是key-token，不仅要它的LPG指标够大，而且它在截断前出现的概率也不能太小（这里就需要再另行设定一个threshold了，但实验一下就可以得到。而且后面作者也说了，实验发现这个threshold还是比较鲁棒的，在一套数据集上实验得到的threshold可以迁移到其它的数据集上，效果也不差）最终用改进后的指标，作者实验发现可以找到98-2-的key-token了。这个准确率就比较令人满意了"><a href="#其次，下面的改进与分析则更加巧妙，感觉也体现了研究的一个常用思路。作者发现，用这个指标，可以找到85-6-的key-token，其实效果已经还不错了，但作者希望能更好。因此他分析了那些被错认为key-token的token（也就是那些并非key-token，但是LPG值却挺大的token）的预测概率，发现它们在截断前，本身被预测的概率就比较小。即，它们本身就是比较难被预测的token（就是说，截断前后其实这些token都比较难以预测。可能是因为预测它们所需要的上下文，在这整一段的long-context里就压根没有，所以相对于这一段long-context而言，这个token并不是key-token；也可以类比一下，人在进行写作等创作输出的活动的时候，的确有时候会毫无逻辑地、或者说逻辑跳跃地太大以至于显得天马行空的想到某个事物。这种内容，喂给llm，它就不太能基于上下文预测到了。解释其实可以有很多，但这并不是重点，因为这里主要是基于统计观察发现了一个事实规律，并且这个规律可以辅助我们改进上面的LPG指标）。基于这个观察，我们可以把上面的指标修正一下：一个token被认为是key-token，不仅要它的LPG指标够大，而且它在截断前出现的概率也不能太小（这里就需要再另行设定一个threshold了，但实验一下就可以得到。而且后面作者也说了，实验发现这个threshold还是比较鲁棒的，在一套数据集上实验得到的threshold可以迁移到其它的数据集上，效果也不差）最终用改进后的指标，作者实验发现可以找到98-2-的key-token了。这个准确率就比较令人满意了" class="headerlink" title="其次，下面的改进与分析则更加巧妙，感觉也体现了研究的一个常用思路。作者发现，用这个指标，可以找到85.6%的key token，其实效果已经还不错了，但作者希望能更好。因此他分析了那些被错认为key token的token（也就是那些并非key token，但是LPG值却挺大的token）的预测概率，发现它们在截断前，本身被预测的概率就比较小。即，它们本身就是比较难被预测的token（就是说，截断前后其实这些token都比较难以预测。可能是因为预测它们所需要的上下文，在这整一段的long context里就压根没有，所以相对于这一段long context而言，这个token并不是key token；也可以类比一下，人在进行写作等创作输出的活动的时候，的确有时候会毫无逻辑地、或者说逻辑跳跃地太大以至于显得天马行空的想到某个事物。这种内容，喂给llm，它就不太能基于上下文预测到了。解释其实可以有很多，但这并不是重点，因为这里主要是基于统计观察发现了一个事实规律，并且这个规律可以辅助我们改进上面的LPG指标）。基于这个观察，我们可以把上面的指标修正一下：一个token被认为是key token，不仅要它的LPG指标够大，而且它在截断前出现的概率也不能太小（这里就需要再另行设定一个threshold了，但实验一下就可以得到。而且后面作者也说了，实验发现这个threshold还是比较鲁棒的，在一套数据集上实验得到的threshold可以迁移到其它的数据集上，效果也不差）最终用改进后的指标，作者实验发现可以找到98.2%的key token了。这个准确率就比较令人满意了"></a>其次，下面的改进与分析则更加巧妙，感觉也体现了研究的一个常用思路。作者发现，用这个指标，可以找到85.6%的key token，其实效果已经还不错了，但作者希望能更好。因此他分析了那些被错认为key token的token（也就是那些并非key token，但是LPG值却挺大的token）的预测概率，发现它们在截断前，本身被预测的概率就比较小。即，它们本身就是比较难被预测的token（就是说，截断前后其实这些token都比较难以预测。可能是因为预测它们所需要的上下文，在这整一段的long context里就压根没有，所以相对于这一段long context而言，这个token并不是key token；也可以类比一下，人在进行写作等创作输出的活动的时候，的确有时候会毫无逻辑地、或者说逻辑跳跃地太大以至于显得天马行空的想到某个事物。这种内容，喂给llm，它就不太能基于上下文预测到了。解释其实可以有很多，但这并不是重点，因为这里主要是基于统计观察发现了一个事实规律，并且这个规律可以辅助我们改进上面的LPG指标）。基于这个观察，我们可以把上面的指标修正一下：一个token被认为是key token，不仅要它的LPG指标够大，而且它在截断前出现的概率也不能太小（这里就需要再另行设定一个threshold了，但实验一下就可以得到。而且后面作者也说了，实验发现这个threshold还是比较鲁棒的，在一套数据集上实验得到的threshold可以迁移到其它的数据集上，效果也不差）<br>最终用改进后的指标，作者实验发现可以找到98.2%的key token了。这个准确率就比较令人满意了</h2><h1 id="利用改进后的指标进行实验，发现它就和long-context-benchmark上的得分有比较好的相关性了，这表明新的指标（称为LongPPL）的确能更好地反映llm的长文本能力。并且这套指标它可以直接利用自然数据（real-world），也不需要根据不同domain设计不同的benchmark（adaptive）（因为在不同的domain，例如coding，math，其实都可以用这套方法找出key-token），同时更加自动，减少了人为的干预如今这个指标也开始被逐步地使用了"><a href="#利用改进后的指标进行实验，发现它就和long-context-benchmark上的得分有比较好的相关性了，这表明新的指标（称为LongPPL）的确能更好地反映llm的长文本能力。并且这套指标它可以直接利用自然数据（real-world），也不需要根据不同domain设计不同的benchmark（adaptive）（因为在不同的domain，例如coding，math，其实都可以用这套方法找出key-token），同时更加自动，减少了人为的干预如今这个指标也开始被逐步地使用了" class="headerlink" title="利用改进后的指标进行实验，发现它就和long-context benchmark上的得分有比较好的相关性了，这表明新的指标（称为LongPPL）的确能更好地反映llm的长文本能力。并且这套指标它可以直接利用自然数据（real-world），也不需要根据不同domain设计不同的benchmark（adaptive）（因为在不同的domain，例如coding，math，其实都可以用这套方法找出key token），同时更加自动，减少了人为的干预如今这个指标也开始被逐步地使用了"></a>利用改进后的指标进行实验，发现它就和long-context benchmark上的得分有比较好的相关性了，这表明新的指标（称为LongPPL）的确能更好地反映llm的长文本能力。并且这套指标它可以直接利用自然数据（real-world），也不需要根据不同domain设计不同的benchmark（adaptive）（因为在不同的domain，例如coding，math，其实都可以用这套方法找出key token），同时更加自动，减少了人为的干预如今这个指标也开始被逐步地使用了</h1><h1 id="而既然已经改进了训练的目标，对应的可以考虑改进训练的损失函数。我们的目标是提高模型对key-tokens的预测的准确率（因为这体现了llm的long-context能力的提升），因此可以考虑，在计算损失的时候，加大key-tokens的权重。那么一个自然但有些极端的想法是，我们只计算预测key-tokens的loss（比如用Cross-Entropy）。然而这样就相当于把其它tokens的权重置为0了。有可能发生的事情是llm训崩了，因为这样有失偏颇，把基本的目标都给丢了，可能会让llm发生类似遗忘的事情（我们当然是希望llm在保证正常的生成能力的前提下提高长文本能力的）。因此，可以考虑，计算损失的时候提高key-tokens的权重，适当降低其它tokens的权重，但不要直接置为0。由此得到的损失就是作者提出的LongCE。一般的CE计算公式为：-CE-x-theta-frac-1-n-sum-i-1-n-log-P-theta-x-i-mathbf-x"><a href="#而既然已经改进了训练的目标，对应的可以考虑改进训练的损失函数。我们的目标是提高模型对key-tokens的预测的准确率（因为这体现了llm的long-context能力的提升），因此可以考虑，在计算损失的时候，加大key-tokens的权重。那么一个自然但有些极端的想法是，我们只计算预测key-tokens的loss（比如用Cross-Entropy）。然而这样就相当于把其它tokens的权重置为0了。有可能发生的事情是llm训崩了，因为这样有失偏颇，把基本的目标都给丢了，可能会让llm发生类似遗忘的事情（我们当然是希望llm在保证正常的生成能力的前提下提高长文本能力的）。因此，可以考虑，计算损失的时候提高key-tokens的权重，适当降低其它tokens的权重，但不要直接置为0。由此得到的损失就是作者提出的LongCE。一般的CE计算公式为：-CE-x-theta-frac-1-n-sum-i-1-n-log-P-theta-x-i-mathbf-x" class="headerlink" title="而既然已经改进了训练的目标，对应的可以考虑改进训练的损失函数。我们的目标是提高模型对key tokens的预测的准确率（因为这体现了llm的long-context能力的提升），因此可以考虑，在计算损失的时候，加大key tokens的权重。那么一个自然但有些极端的想法是，我们只计算预测key tokens的loss（比如用Cross Entropy）。然而这样就相当于把其它tokens的权重置为0了。有可能发生的事情是llm训崩了，因为这样有失偏颇，把基本的目标都给丢了，可能会让llm发生类似遗忘的事情（我们当然是希望llm在保证正常的生成能力的前提下提高长文本能力的）。因此，可以考虑，计算损失的时候提高key tokens的权重，适当降低其它tokens的权重，但不要直接置为0。由此得到的损失就是作者提出的LongCE。一般的CE计算公式为：$$CE(x;\theta)&#x3D;-\frac{1}{n}\sum_{i&#x3D;1}^{n}log\ P_{\theta}(x_{i}|\mathbf{x}{&lt;i})$$对应的，LongCE的计算公式为：$$LongCE(x;\theta)&#x3D;-\frac{1}{n}\sum{i&#x3D;1}^{n}I_{soft}(x_{i};\theta)log\ P_{\theta}(x_{i}|\mathbf{x}{&lt;i})$$粗略地看，也就是多了一个逐token的权重项。而这个权重项的表达式为：$$I{soft}(x_{i};\theta)&#x3D;\min{(exp(LSD_{\theta}(x_{i})), \gamma)}&#x3D;\min{(\frac{P_{\theta}(x_{i}|l_{i})}{P_{\theta}(x_{i}|s_{i})}, \gamma)}$$"></a>而既然已经改进了训练的目标，对应的可以考虑改进训练的损失函数。我们的目标是提高模型对key tokens的预测的准确率（因为这体现了llm的long-context能力的提升），因此可以考虑，在计算损失的时候，加大key tokens的权重。那么一个自然但有些极端的想法是，我们只计算预测key tokens的loss（比如用Cross Entropy）。然而这样就相当于把其它tokens的权重置为0了。有可能发生的事情是llm训崩了，因为这样有失偏颇，把基本的目标都给丢了，可能会让llm发生类似遗忘的事情（我们当然是希望llm在保证正常的生成能力的前提下提高长文本能力的）。因此，可以考虑，计算损失的时候提高key tokens的权重，适当降低其它tokens的权重，但不要直接置为0。由此得到的损失就是作者提出的LongCE。一般的CE计算公式为：$$CE(x;\theta)&#x3D;-\frac{1}{n}\sum_{i&#x3D;1}^{n}log\ P_{\theta}(x_{i}|\mathbf{x}<em>{&lt;i})$$对应的，LongCE的计算公式为：$$LongCE(x;\theta)&#x3D;-\frac{1}{n}\sum</em>{i&#x3D;1}^{n}I_{soft}(x_{i};\theta)log\ P_{\theta}(x_{i}|\mathbf{x}<em>{&lt;i})$$粗略地看，也就是多了一个逐token的权重项。而这个权重项的表达式为：$$I</em>{soft}(x_{i};\theta)&#x3D;\min{(exp(LSD_{\theta}(x_{i})), \gamma)}&#x3D;\min{(\frac{P_{\theta}(x_{i}|l_{i})}{P_{\theta}(x_{i}|s_{i})}, \gamma)}$$</h1><h2 id="里面的-P-theta-x-i-l-i-是指在token-x-i-对应的long-context下，预测出token-x-i-的概率；而-P-theta-x-i-s-i-则是指在token-x-i-对应的short-context（也就是long-context截断之后的context）下，预测出token-x-i-的概率。因此，当这个比值项很大的时候，就是说这个token用截断后的context预测，概率大大下降（这表明这个token很可能是key-token），因此我们给它较大的权重（当比值项较大，它应该会大于我们的阈值-gamma-，所以取min之后权重就是-gamma-；而如果比值项较小（也就是小于阈值-gamma-），则权重会取为比值项。因此我们可以看到，虽然对于key-token会取较大的权重，但也是有上限的，而不会任由它大下去。这个，个人推测，可能是出于经验考虑的，也可能是经过实验发现还是加上阈值上限后训练效果更好"><a href="#里面的-P-theta-x-i-l-i-是指在token-x-i-对应的long-context下，预测出token-x-i-的概率；而-P-theta-x-i-s-i-则是指在token-x-i-对应的short-context（也就是long-context截断之后的context）下，预测出token-x-i-的概率。因此，当这个比值项很大的时候，就是说这个token用截断后的context预测，概率大大下降（这表明这个token很可能是key-token），因此我们给它较大的权重（当比值项较大，它应该会大于我们的阈值-gamma-，所以取min之后权重就是-gamma-；而如果比值项较小（也就是小于阈值-gamma-），则权重会取为比值项。因此我们可以看到，虽然对于key-token会取较大的权重，但也是有上限的，而不会任由它大下去。这个，个人推测，可能是出于经验考虑的，也可能是经过实验发现还是加上阈值上限后训练效果更好" class="headerlink" title="里面的$P_{\theta}(x_{i}|l_{i})$是指在token $x_{i}$对应的long context下，预测出token $x_{i}$的概率；而$P_{\theta}(x_{i}|s_{i})$则是指在token $x_{i}$对应的short context（也就是long context截断之后的context）下，预测出token $x_{i}$的概率。因此，当这个比值项很大的时候，就是说这个token用截断后的context预测，概率大大下降（这表明这个token很可能是key token），因此我们给它较大的权重（当比值项较大，它应该会大于我们的阈值$\gamma$，所以取min之后权重就是$\gamma$；而如果比值项较小（也就是小于阈值$\gamma$），则权重会取为比值项。因此我们可以看到，虽然对于key token会取较大的权重，但也是有上限的，而不会任由它大下去。这个，个人推测，可能是出于经验考虑的，也可能是经过实验发现还是加上阈值上限后训练效果更好"></a>里面的$P_{\theta}(x_{i}|l_{i})$是指在token $x_{i}$对应的long context下，预测出token $x_{i}$的概率；而$P_{\theta}(x_{i}|s_{i})$则是指在token $x_{i}$对应的short context（也就是long context截断之后的context）下，预测出token $x_{i}$的概率。因此，当这个比值项很大的时候，就是说这个token用截断后的context预测，概率大大下降（这表明这个token很可能是key token），因此我们给它较大的权重（当比值项较大，它应该会大于我们的阈值$\gamma$，所以取min之后权重就是$\gamma$；而如果比值项较小（也就是小于阈值$\gamma$），则权重会取为比值项。因此我们可以看到，虽然对于key token会取较大的权重，但也是有上限的，而不会任由它大下去。这个，个人推测，可能是出于经验考虑的，也可能是经过实验发现还是加上阈值上限后训练效果更好</h2><h1 id="对LongCE的其它理解角度："><a href="#对LongCE的其它理解角度：" class="headerlink" title="对LongCE的其它理解角度："></a>对LongCE的其它理解角度：</h1><h2 id="1-视为EM算法的一个实例"><a href="#1-视为EM算法的一个实例" class="headerlink" title="1. 视为EM算法的一个实例"></a>1. 视为EM算法的一个实例</h2><h3 id="我们先用llm判断各个token是否为key-token（或者说判断各个token的重要性。各token的重要性是未知的，也就是EM算法中的latent-variable）。这就是EM算法中的E步"><a href="#我们先用llm判断各个token是否为key-token（或者说判断各个token的重要性。各token的重要性是未知的，也就是EM算法中的latent-variable）。这就是EM算法中的E步" class="headerlink" title="我们先用llm判断各个token是否为key token（或者说判断各个token的重要性。各token的重要性是未知的，也就是EM算法中的latent variable）。这就是EM算法中的E步"></a>我们先用llm判断各个token是否为key token（或者说判断各个token的重要性。各token的重要性是未知的，也就是EM算法中的latent variable）。这就是EM算法中的E步</h3><h3 id="之后我们基于预测出的token的重要性，训练llm，让它预测token。这就是M步"><a href="#之后我们基于预测出的token的重要性，训练llm，让它预测token。这就是M步" class="headerlink" title="之后我们基于预测出的token的重要性，训练llm，让它预测token。这就是M步"></a>之后我们基于预测出的token的重要性，训练llm，让它预测token。这就是M步</h3><h3 id="反复迭代上面的EM步，就是训练过程。这个过程也有点像bootstrap：先依赖于llm已有的long-context能力，判断token重要性；然后基于判断出的重要性，再训练llm，提高其long-context能力，反复循环"><a href="#反复迭代上面的EM步，就是训练过程。这个过程也有点像bootstrap：先依赖于llm已有的long-context能力，判断token重要性；然后基于判断出的重要性，再训练llm，提高其long-context能力，反复循环" class="headerlink" title="反复迭代上面的EM步，就是训练过程。这个过程也有点像bootstrap：先依赖于llm已有的long-context能力，判断token重要性；然后基于判断出的重要性，再训练llm，提高其long-context能力，反复循环"></a>反复迭代上面的EM步，就是训练过程。这个过程也有点像bootstrap：先依赖于llm已有的long-context能力，判断token重要性；然后基于判断出的重要性，再训练llm，提高其long-context能力，反复循环</h3><h2 id="2-看成是利用了强化学习算法，只不过是self-reward，且更加细粒度（但这个没太理解……）"><a href="#2-看成是利用了强化学习算法，只不过是self-reward，且更加细粒度（但这个没太理解……）" class="headerlink" title="2. 看成是利用了强化学习算法，只不过是self-reward，且更加细粒度（但这个没太理解……）"></a>2. 看成是利用了强化学习算法，只不过是self-reward，且更加细粒度（但这个没太理解……）</h2><h1 id="LongCE也得到了其它工作的引用和使用，在其它的工作中甚至是非常关键的"><a href="#LongCE也得到了其它工作的引用和使用，在其它的工作中甚至是非常关键的" class="headerlink" title="LongCE也得到了其它工作的引用和使用，在其它的工作中甚至是非常关键的"></a>LongCE也得到了其它工作的引用和使用，在其它的工作中甚至是非常关键的</h1><h2 id="一个小细节是，根据作者在讲座中引用的结果，发现，对于hybrid-model（也就是把linear-attn和一般的平方attn混合的模型），LongCE的提升效果尤为显著。作者提出一个猜测，这可能是因为这类efficient-llm，它们本身的容量相比于正常llm，要更小，因此训练它们的时候更加需要”干净“的信号（也即less-noisy-signal）。正好，LongCE这一套呢，它通过对token的reweighting，让llm更加关注key-token，把算力更多地用到key-token上，这本身就可以视为对signal的一种”清洁“，或者”提纯“，因而把真正重要的learning-signal给剖析出来了，所以有助于训练"><a href="#一个小细节是，根据作者在讲座中引用的结果，发现，对于hybrid-model（也就是把linear-attn和一般的平方attn混合的模型），LongCE的提升效果尤为显著。作者提出一个猜测，这可能是因为这类efficient-llm，它们本身的容量相比于正常llm，要更小，因此训练它们的时候更加需要”干净“的信号（也即less-noisy-signal）。正好，LongCE这一套呢，它通过对token的reweighting，让llm更加关注key-token，把算力更多地用到key-token上，这本身就可以视为对signal的一种”清洁“，或者”提纯“，因而把真正重要的learning-signal给剖析出来了，所以有助于训练" class="headerlink" title="一个小细节是，根据作者在讲座中引用的结果，发现，对于hybrid model（也就是把linear attn和一般的平方attn混合的模型），LongCE的提升效果尤为显著。作者提出一个猜测，这可能是因为这类efficient llm，它们本身的容量相比于正常llm，要更小，因此训练它们的时候更加需要”干净“的信号（也即less noisy signal）。正好，LongCE这一套呢，它通过对token的reweighting，让llm更加关注key token，把算力更多地用到key token上，这本身就可以视为对signal的一种”清洁“，或者”提纯“，因而把真正重要的learning signal给剖析出来了，所以有助于训练"></a>一个小细节是，根据作者在讲座中引用的结果，发现，对于hybrid model（也就是把linear attn和一般的平方attn混合的模型），LongCE的提升效果尤为显著。作者提出一个猜测，这可能是因为这类efficient llm，它们本身的容量相比于正常llm，要更小，因此训练它们的时候更加需要”干净“的信号（也即less noisy signal）。正好，LongCE这一套呢，它通过对token的reweighting，让llm更加关注key token，把算力更多地用到key token上，这本身就可以视为对signal的一种”清洁“，或者”提纯“，因而把真正重要的learning signal给剖析出来了，所以有助于训练</h2><h1 id="总结一下至此的部分："><a href="#总结一下至此的部分：" class="headerlink" title="总结一下至此的部分："></a>总结一下至此的部分：</h1><h2 id="Next-token-prediction的范式，天然的就是有bias的，因为每个token它都是equally-generated的，然而实际可能并非如此（以人为例，人在写东西的时候，并非所有内容都是一样顺畅的写下来的。可能在一个关键的地方，我们需要思考很久才能写出来）"><a href="#Next-token-prediction的范式，天然的就是有bias的，因为每个token它都是equally-generated的，然而实际可能并非如此（以人为例，人在写东西的时候，并非所有内容都是一样顺畅的写下来的。可能在一个关键的地方，我们需要思考很久才能写出来）" class="headerlink" title="Next-token prediction的范式，天然的就是有bias的，因为每个token它都是equally generated的，然而实际可能并非如此（以人为例，人在写东西的时候，并非所有内容都是一样顺畅的写下来的。可能在一个关键的地方，我们需要思考很久才能写出来）"></a>Next-token prediction的范式，天然的就是有bias的，因为每个token它都是equally generated的，然而实际可能并非如此（以人为例，人在写东西的时候，并非所有内容都是一样顺畅的写下来的。可能在一个关键的地方，我们需要思考很久才能写出来）</h2><h2 id="除了用RL-CoT进行改善，一种正交的方法就是上面提到的reweighted-focused-next-token-prediction（也因为正交，所以二者应该是可以一起使用的）。它本质是在提高信噪比"><a href="#除了用RL-CoT进行改善，一种正交的方法就是上面提到的reweighted-focused-next-token-prediction（也因为正交，所以二者应该是可以一起使用的）。它本质是在提高信噪比" class="headerlink" title="除了用RL&#x2F;CoT进行改善，一种正交的方法就是上面提到的reweighted&#x2F;focused next-token prediction（也因为正交，所以二者应该是可以一起使用的）。它本质是在提高信噪比"></a>除了用RL&#x2F;CoT进行改善，一种正交的方法就是上面提到的reweighted&#x2F;focused next-token prediction（也因为正交，所以二者应该是可以一起使用的）。它本质是在提高信噪比</h2><h2 id="contrastive-estimate（就是上面的最朴素的计算权重的式子，-r-x-i-frac-P-theta-x-i-l-i-P-theta-x-i-s-i-）是一种比较通用的方式，它可以适用于各种特定任务，用于识别task-specific-tokens"><a href="#contrastive-estimate（就是上面的最朴素的计算权重的式子，-r-x-i-frac-P-theta-x-i-l-i-P-theta-x-i-s-i-）是一种比较通用的方式，它可以适用于各种特定任务，用于识别task-specific-tokens" class="headerlink" title="contrastive estimate（就是上面的最朴素的计算权重的式子，$r(x_{i})&#x3D;\frac{P_{\theta}(x_{i}|l_{i})}{P_{\theta}(x_{i}|s_{i})}$）是一种比较通用的方式，它可以适用于各种特定任务，用于识别task-specific tokens"></a>contrastive estimate（就是上面的最朴素的计算权重的式子，$r(x_{i})&#x3D;\frac{P_{\theta}(x_{i}|l_{i})}{P_{\theta}(x_{i}|s_{i})}$）是一种比较通用的方式，它可以适用于各种特定任务，用于识别task-specific tokens</h2><h3 id="而关于-r-x-i-frac-P-theta-x-i-l-i-P-theta-x-i-s-i-权重这一项，我们还可以从知识蒸馏的角度来理解。分子就类似于teacher-model，它代表了ideal-performance（因为是基于long-context来预测next-token）；分母则类似于student-model，它代表了original-performance（因为是基于short-context来预测next-token）。这个比值越大，表明student在预测这个token的时候比teacher差了越多，因此teacher指导它，要更多地学习这个token的预测，因此会提高这个token的权重。这里的ideal-performance以及original-performance，个人猜测，应该都只是借用了蒸馏里的一些术语而已吧，应该不至于说真的就是这个意思。可能更多只是说明可以从蒸馏的视角去看待"><a href="#而关于-r-x-i-frac-P-theta-x-i-l-i-P-theta-x-i-s-i-权重这一项，我们还可以从知识蒸馏的角度来理解。分子就类似于teacher-model，它代表了ideal-performance（因为是基于long-context来预测next-token）；分母则类似于student-model，它代表了original-performance（因为是基于short-context来预测next-token）。这个比值越大，表明student在预测这个token的时候比teacher差了越多，因此teacher指导它，要更多地学习这个token的预测，因此会提高这个token的权重。这里的ideal-performance以及original-performance，个人猜测，应该都只是借用了蒸馏里的一些术语而已吧，应该不至于说真的就是这个意思。可能更多只是说明可以从蒸馏的视角去看待" class="headerlink" title="而关于$r(x_{i})&#x3D;\frac{P_{\theta}(x_{i}|l_{i})}{P_{\theta}(x_{i}|s_{i})}$权重这一项，我们还可以从知识蒸馏的角度来理解。分子就类似于teacher model，它代表了ideal performance（因为是基于long-context来预测next token）；分母则类似于student model，它代表了original performance（因为是基于short-context来预测next token）。这个比值越大，表明student在预测这个token的时候比teacher差了越多，因此teacher指导它，要更多地学习这个token的预测，因此会提高这个token的权重。这里的ideal performance以及original performance，个人猜测，应该都只是借用了蒸馏里的一些术语而已吧，应该不至于说真的就是这个意思。可能更多只是说明可以从蒸馏的视角去看待"></a>而关于$r(x_{i})&#x3D;\frac{P_{\theta}(x_{i}|l_{i})}{P_{\theta}(x_{i}|s_{i})}$权重这一项，我们还可以从知识蒸馏的角度来理解。分子就类似于teacher model，它代表了ideal performance（因为是基于long-context来预测next token）；分母则类似于student model，它代表了original performance（因为是基于short-context来预测next token）。这个比值越大，表明student在预测这个token的时候比teacher差了越多，因此teacher指导它，要更多地学习这个token的预测，因此会提高这个token的权重。这里的ideal performance以及original performance，个人猜测，应该都只是借用了蒸馏里的一些术语而已吧，应该不至于说真的就是这个意思。可能更多只是说明可以从蒸馏的视角去看待</h3><h1 id="下面来讲讲关于Transformer架构的一些问题。这个讲座里主要关注的是position-bias的一个问题"><a href="#下面来讲讲关于Transformer架构的一些问题。这个讲座里主要关注的是position-bias的一个问题" class="headerlink" title="下面来讲讲关于Transformer架构的一些问题。这个讲座里主要关注的是position bias的一个问题"></a>下面来讲讲关于Transformer架构的一些问题。这个讲座里主要关注的是position bias的一个问题</h1><h2 id="所谓position-bias的问题，它的表现有很多："><a href="#所谓position-bias的问题，它的表现有很多：" class="headerlink" title="所谓position bias的问题，它的表现有很多："></a>所谓position bias的问题，它的表现有很多：</h2><ul>
<li><h3 id="attention-sink现象（大体上说，就是当我们可视化注意力权重的时候，会发现第一个token的权重似乎尤其大，然后中间的token的权重都比较小，但最后的token权重又会大一些（不过没有第一个token的大）"><a href="#attention-sink现象（大体上说，就是当我们可视化注意力权重的时候，会发现第一个token的权重似乎尤其大，然后中间的token的权重都比较小，但最后的token权重又会大一些（不过没有第一个token的大）" class="headerlink" title="attention sink现象（大体上说，就是当我们可视化注意力权重的时候，会发现第一个token的权重似乎尤其大，然后中间的token的权重都比较小，但最后的token权重又会大一些（不过没有第一个token的大）"></a>attention sink现象（大体上说，就是当我们可视化注意力权重的时候，会发现第一个token的权重似乎尤其大，然后中间的token的权重都比较小，但最后的token权重又会大一些（不过没有第一个token的大）</h3></li>
<li><h3 id="在in-context-learning中，理论上讲，我们prompt里给的示例，即使调换了顺序，也不影响模型的学习；但实际上，当我们调换prompt里示例的顺序，模型的输出会有较大的变化。这就是一种bias（它本应没有的）"><a href="#在in-context-learning中，理论上讲，我们prompt里给的示例，即使调换了顺序，也不影响模型的学习；但实际上，当我们调换prompt里示例的顺序，模型的输出会有较大的变化。这就是一种bias（它本应没有的）" class="headerlink" title="在in-context learning中，理论上讲，我们prompt里给的示例，即使调换了顺序，也不影响模型的学习；但实际上，当我们调换prompt里示例的顺序，模型的输出会有较大的变化。这就是一种bias（它本应没有的）"></a>在in-context learning中，理论上讲，我们prompt里给的示例，即使调换了顺序，也不影响模型的学习；但实际上，当我们调换prompt里示例的顺序，模型的输出会有较大的变化。这就是一种bias（它本应没有的）</h3></li>
<li><h3 id="对prompts的格式也很敏感（本质上，如果信息是全的，那么按照什么格式给llm，应该影响也不大。但一个好的格式对效果的提升还是有影响的）"><a href="#对prompts的格式也很敏感（本质上，如果信息是全的，那么按照什么格式给llm，应该影响也不大。但一个好的格式对效果的提升还是有影响的）" class="headerlink" title="对prompts的格式也很敏感（本质上，如果信息是全的，那么按照什么格式给llm，应该影响也不大。但一个好的格式对效果的提升还是有影响的）"></a>对prompts的格式也很敏感（本质上，如果信息是全的，那么按照什么格式给llm，应该影响也不大。但一个好的格式对效果的提升还是有影响的）</h3></li>
<li><h3 id="lost-in-the-middle现象，即进行大海捞针实验的时候，发现，当needle（即关键信息）出现在中部的位置时，回答的准确率会比较差；而如果needle出现在开头，效果一般是最好的；出现在结尾附近，效果也会比中间的好"><a href="#lost-in-the-middle现象，即进行大海捞针实验的时候，发现，当needle（即关键信息）出现在中部的位置时，回答的准确率会比较差；而如果needle出现在开头，效果一般是最好的；出现在结尾附近，效果也会比中间的好" class="headerlink" title="lost in the middle现象，即进行大海捞针实验的时候，发现，当needle（即关键信息）出现在中部的位置时，回答的准确率会比较差；而如果needle出现在开头，效果一般是最好的；出现在结尾附近，效果也会比中间的好"></a>lost in the middle现象，即进行大海捞针实验的时候，发现，当needle（即关键信息）出现在中部的位置时，回答的准确率会比较差；而如果needle出现在开头，效果一般是最好的；出现在结尾附近，效果也会比中间的好</h3></li>
</ul>
<h2 id="这种bias，乍一看是不好的。但其实也未必，也有人声称这只是llm在学习人类的行为而已（因为可能人类的语料里也是，重要信息一般在开头或者结尾）。但更重要的是，我们对这种bias的成因没有太多了解，也不知道是不是因为transformer这个架构导致的。如果能搞懂这种bias的成因，就有可能更好地设计架构，同时也能改进现有的PE（position-embedding）如果我们粗浅地归纳成因，显然离不开：数据、架构以及PE。因此下面讨论的就是这几个因素的影响"><a href="#这种bias，乍一看是不好的。但其实也未必，也有人声称这只是llm在学习人类的行为而已（因为可能人类的语料里也是，重要信息一般在开头或者结尾）。但更重要的是，我们对这种bias的成因没有太多了解，也不知道是不是因为transformer这个架构导致的。如果能搞懂这种bias的成因，就有可能更好地设计架构，同时也能改进现有的PE（position-embedding）如果我们粗浅地归纳成因，显然离不开：数据、架构以及PE。因此下面讨论的就是这几个因素的影响" class="headerlink" title="这种bias，乍一看是不好的。但其实也未必，也有人声称这只是llm在学习人类的行为而已（因为可能人类的语料里也是，重要信息一般在开头或者结尾）。但更重要的是，我们对这种bias的成因没有太多了解，也不知道是不是因为transformer这个架构导致的。如果能搞懂这种bias的成因，就有可能更好地设计架构，同时也能改进现有的PE（position embedding）如果我们粗浅地归纳成因，显然离不开：数据、架构以及PE。因此下面讨论的就是这几个因素的影响"></a>这种bias，乍一看是不好的。但其实也未必，也有人声称这只是llm在学习人类的行为而已（因为可能人类的语料里也是，重要信息一般在开头或者结尾）。但更重要的是，我们对这种bias的成因没有太多了解，也不知道是不是因为transformer这个架构导致的。如果能搞懂这种bias的成因，就有可能更好地设计架构，同时也能改进现有的PE（position embedding）<br>如果我们粗浅地归纳成因，显然离不开：数据、架构以及PE。因此下面讨论的就是这几个因素的影响</h2><h2 id="首先，是一个控制变量的实验。这主要是为了初步看看position-bias的来源。数据是相对最好控制的，只需要我们确保所有数据是独立同分布的（所以不存在哪个数据更重要的情况）。而想看PE的影响，则只变动它，其它不变（可以不用PE，也可以尝试一些其它的常用PE）。实验结果如下：这个图呢，横轴用于指示比较不同位置的token的具体情况（也就是指明，是哪个位置的token和哪个位置的token的预测效果进行对比），纵轴则是说明二者预测效果之差。为正则表明前者优于后者，且越大表明好的越多，负的则类似由此可以看到，不管用不用PE，用哪个PE，即使我们排除掉了数据的bias，依然会有position-bias，且一个大致的趋势是，越深层，bias越严重。而且，因为无论有无PE都会有这个bias，加上已经控制了数据了，所以可以肯定，transformer的架构一定是造成了影响的另外，可以看到，除却对第一个token的极大的关注（所谓attention-sink现象），对最后的token其实也有一定程度的额外关注（这可以被称为recency-bias，也就是对最近的token有一定的倾向）。但根据图表，在first-vs-last的时候，也通常是first-token预测的更好，所以attention-sink是比recency-bias更“dominant”的，或者说是更具影响力的"><a href="#首先，是一个控制变量的实验。这主要是为了初步看看position-bias的来源。数据是相对最好控制的，只需要我们确保所有数据是独立同分布的（所以不存在哪个数据更重要的情况）。而想看PE的影响，则只变动它，其它不变（可以不用PE，也可以尝试一些其它的常用PE）。实验结果如下：这个图呢，横轴用于指示比较不同位置的token的具体情况（也就是指明，是哪个位置的token和哪个位置的token的预测效果进行对比），纵轴则是说明二者预测效果之差。为正则表明前者优于后者，且越大表明好的越多，负的则类似由此可以看到，不管用不用PE，用哪个PE，即使我们排除掉了数据的bias，依然会有position-bias，且一个大致的趋势是，越深层，bias越严重。而且，因为无论有无PE都会有这个bias，加上已经控制了数据了，所以可以肯定，transformer的架构一定是造成了影响的另外，可以看到，除却对第一个token的极大的关注（所谓attention-sink现象），对最后的token其实也有一定程度的额外关注（这可以被称为recency-bias，也就是对最近的token有一定的倾向）。但根据图表，在first-vs-last的时候，也通常是first-token预测的更好，所以attention-sink是比recency-bias更“dominant”的，或者说是更具影响力的" class="headerlink" title="首先，是一个控制变量的实验。这主要是为了初步看看position bias的来源。数据是相对最好控制的，只需要我们确保所有数据是独立同分布的（所以不存在哪个数据更重要的情况）。而想看PE的影响，则只变动它，其它不变（可以不用PE，也可以尝试一些其它的常用PE）。实验结果如下：这个图呢，横轴用于指示比较不同位置的token的具体情况（也就是指明，是哪个位置的token和哪个位置的token的预测效果进行对比），纵轴则是说明二者预测效果之差。为正则表明前者优于后者，且越大表明好的越多，负的则类似由此可以看到，不管用不用PE，用哪个PE，即使我们排除掉了数据的bias，依然会有position bias，且一个大致的趋势是，越深层，bias越严重。而且，因为无论有无PE都会有这个bias，加上已经控制了数据了，所以可以肯定，transformer的架构一定是造成了影响的另外，可以看到，除却对第一个token的极大的关注（所谓attention sink现象），对最后的token其实也有一定程度的额外关注（这可以被称为recency bias，也就是对最近的token有一定的倾向）。但根据图表，在first vs. last的时候，也通常是first token预测的更好，所以attention sink是比recency bias更“dominant”的，或者说是更具影响力的"></a>首先，是一个控制变量的实验。这主要是为了初步看看position bias的来源。数据是相对最好控制的，只需要我们确保所有数据是独立同分布的（所以不存在哪个数据更重要的情况）。而想看PE的影响，则只变动它，其它不变（可以不用PE，也可以尝试一些其它的常用PE）。实验结果如下：<img src="/images/FAI-Seminar-MIT-%E7%8E%8B%E4%B8%80%E9%A3%9E-%E9%95%BF%E6%96%87%E6%9C%AC%E7%9A%84%E5%9B%B0%E5%A2%83/position-bias-from-arch.png">这个图呢，横轴用于指示比较不同位置的token的具体情况（也就是指明，是哪个位置的token和哪个位置的token的预测效果进行对比），纵轴则是说明二者预测效果之差。为正则表明前者优于后者，且越大表明好的越多，负的则类似<br>由此可以看到，不管用不用PE，用哪个PE，即使我们排除掉了数据的bias，依然会有position bias，且一个大致的趋势是，越深层，bias越严重。而且，因为无论有无PE都会有这个bias，加上已经控制了数据了，所以可以肯定，transformer的架构一定是造成了影响的另外，可以看到，除却对第一个token的极大的关注（所谓attention sink现象），对最后的token其实也有一定程度的额外关注（这可以被称为recency bias，也就是对最近的token有一定的倾向）。但根据图表，在first vs. last的时候，也通常是first token预测的更好，所以attention sink是比recency bias更“dominant”的，或者说是更具影响力的</h2><h2 id="下面就需要进行一些理论分析，来看看为什么会出现这些情况，以及理论能否对应上这些情况"><a href="#下面就需要进行一些理论分析，来看看为什么会出现这些情况，以及理论能否对应上这些情况" class="headerlink" title="下面就需要进行一些理论分析，来看看为什么会出现这些情况，以及理论能否对应上这些情况"></a>下面就需要进行一些理论分析，来看看为什么会出现这些情况，以及理论能否对应上这些情况</h2><h2 id="回顾transformer架构"><a href="#回顾transformer架构" class="headerlink" title="回顾transformer架构"></a>回顾transformer架构</h2><h3 id="transformer架构最主要的构件有这几个：attention，mlp，ln等norm但mlp和norm，其实是对每个token的embedding都会做的，也就是说，这两个构件不会带来token之间的交互，那理论上它们也不应该导致position-bias。因此，只可能是attention导致了position-bias（因为这里会让token之间进行交互）"><a href="#transformer架构最主要的构件有这几个：attention，mlp，ln等norm但mlp和norm，其实是对每个token的embedding都会做的，也就是说，这两个构件不会带来token之间的交互，那理论上它们也不应该导致position-bias。因此，只可能是attention导致了position-bias（因为这里会让token之间进行交互）" class="headerlink" title="transformer架构最主要的构件有这几个：attention，mlp，ln等norm但mlp和norm，其实是对每个token的embedding都会做的，也就是说，这两个构件不会带来token之间的交互，那理论上它们也不应该导致position bias。因此，只可能是attention导致了position bias（因为这里会让token之间进行交互）"></a>transformer架构最主要的构件有这几个：attention，mlp，ln等norm<br>但mlp和norm，其实是对每个token的embedding都会做的，也就是说，这两个构件不会带来token之间的交互，那理论上它们也不应该导致position bias。因此，只可能是attention导致了position bias（因为这里会让token之间进行交互）</h3><h3 id="此处作者是用了graph进行分析。且假设不用什么其它的PE，就用最基础的causal-mask（就是预测当前token时，只是用前面的token的那个mask）。如果把它可视化为一个计算图，可以想见，在我们预测后面的所有token的时候，都会考虑到第一个token（虽然它未必总是有用，但我们一定会考虑它）。可参考下图：可以看到，越前面的token，它的出度越大。直观上，我们或许可以认为，“不自觉”地会对第一个token有更多关注（但这里还是太粗浅了，纯粹成了猜测了。还是得看看原论文才行）。这种graph是非常不平衡的，也很有可能是这种不平衡导致了position-bias的出现（相应的，如果我们希望改进，那么就应该确保改进后的架构对应的计算图不应该是这种非常不平衡的情况）而且随着层数加深，这种不平衡会愈发严重（类似于，每一层第一个token都多获取一些注意力，那层数一旦多了，注意力的大头不就有可能几乎全都聚集到第一个token上了吗）然而，需要注意，这个图应该不能完全解释position-bias。因为position-bias除了说有attention-sink的现象，还有recency-bias。但如果仅考虑上图，那应该完全不可能有recency-bias，last-token获得的注意力应该是最少的。因此，这可能是其它因素，也就是PE导致的"><a href="#此处作者是用了graph进行分析。且假设不用什么其它的PE，就用最基础的causal-mask（就是预测当前token时，只是用前面的token的那个mask）。如果把它可视化为一个计算图，可以想见，在我们预测后面的所有token的时候，都会考虑到第一个token（虽然它未必总是有用，但我们一定会考虑它）。可参考下图：可以看到，越前面的token，它的出度越大。直观上，我们或许可以认为，“不自觉”地会对第一个token有更多关注（但这里还是太粗浅了，纯粹成了猜测了。还是得看看原论文才行）。这种graph是非常不平衡的，也很有可能是这种不平衡导致了position-bias的出现（相应的，如果我们希望改进，那么就应该确保改进后的架构对应的计算图不应该是这种非常不平衡的情况）而且随着层数加深，这种不平衡会愈发严重（类似于，每一层第一个token都多获取一些注意力，那层数一旦多了，注意力的大头不就有可能几乎全都聚集到第一个token上了吗）然而，需要注意，这个图应该不能完全解释position-bias。因为position-bias除了说有attention-sink的现象，还有recency-bias。但如果仅考虑上图，那应该完全不可能有recency-bias，last-token获得的注意力应该是最少的。因此，这可能是其它因素，也就是PE导致的" class="headerlink" title="此处作者是用了graph进行分析。且假设不用什么其它的PE，就用最基础的causal mask（就是预测当前token时，只是用前面的token的那个mask）。如果把它可视化为一个计算图，可以想见，在我们预测后面的所有token的时候，都会考虑到第一个token（虽然它未必总是有用，但我们一定会考虑它）。可参考下图：可以看到，越前面的token，它的出度越大。直观上，我们或许可以认为，“不自觉”地会对第一个token有更多关注（但这里还是太粗浅了，纯粹成了猜测了。还是得看看原论文才行）。这种graph是非常不平衡的，也很有可能是这种不平衡导致了position bias的出现（相应的，如果我们希望改进，那么就应该确保改进后的架构对应的计算图不应该是这种非常不平衡的情况）而且随着层数加深，这种不平衡会愈发严重（类似于，每一层第一个token都多获取一些注意力，那层数一旦多了，注意力的大头不就有可能几乎全都聚集到第一个token上了吗）然而，需要注意，这个图应该不能完全解释position bias。因为position bias除了说有attention sink的现象，还有recency bias。但如果仅考虑上图，那应该完全不可能有recency bias，last token获得的注意力应该是最少的。因此，这可能是其它因素，也就是PE导致的"></a>此处作者是用了graph进行分析。且假设不用什么其它的PE，就用最基础的causal mask（就是预测当前token时，只是用前面的token的那个mask）。如果把它可视化为一个计算图，可以想见，在我们预测后面的所有token的时候，都会考虑到第一个token（虽然它未必总是有用，但我们一定会考虑它）。可参考下图：<img src="/images/FAI-Seminar-MIT-%E7%8E%8B%E4%B8%80%E9%A3%9E-%E9%95%BF%E6%96%87%E6%9C%AC%E7%9A%84%E5%9B%B0%E5%A2%83/attn-induces-graph.png">可以看到，越前面的token，它的出度越大。直观上，我们或许可以认为，“不自觉”地会对第一个token有更多关注（但这里还是太粗浅了，纯粹成了猜测了。还是得看看原论文才行）。这种graph是非常不平衡的，也很有可能是这种不平衡导致了position bias的出现（相应的，如果我们希望改进，那么就应该确保改进后的架构对应的计算图不应该是这种非常不平衡的情况）<br>而且随着层数加深，这种不平衡会愈发严重（类似于，每一层第一个token都多获取一些注意力，那层数一旦多了，注意力的大头不就有可能几乎全都聚集到第一个token上了吗）<br>然而，需要注意，这个图应该不能完全解释position bias。因为position bias除了说有attention sink的现象，还有recency bias。但如果仅考虑上图，那应该完全不可能有recency bias，last token获得的注意力应该是最少的。因此，这可能是其它因素，也就是PE导致的</h3><h2 id="之后作者给出了比较理论的定理，定量说明了，层数无穷深的时候，第一个token对其它所有token的贡献会逼近1，也就是说，第一个token的注意力权重会完全趋向于1。而实际虽然我们不会真的构造一个无穷深的llm，但也有定理给出定量的描述，说明，对于除了第一个token以外的token，它们对其它token的贡献，是在以指数速度下降的（每一层而言）"><a href="#之后作者给出了比较理论的定理，定量说明了，层数无穷深的时候，第一个token对其它所有token的贡献会逼近1，也就是说，第一个token的注意力权重会完全趋向于1。而实际虽然我们不会真的构造一个无穷深的llm，但也有定理给出定量的描述，说明，对于除了第一个token以外的token，它们对其它token的贡献，是在以指数速度下降的（每一层而言）" class="headerlink" title="之后作者给出了比较理论的定理，定量说明了，层数无穷深的时候，第一个token对其它所有token的贡献会逼近1，也就是说，第一个token的注意力权重会完全趋向于1。而实际虽然我们不会真的构造一个无穷深的llm，但也有定理给出定量的描述，说明，对于除了第一个token以外的token，它们对其它token的贡献，是在以指数速度下降的（每一层而言）"></a>之后作者给出了比较理论的定理，定量说明了，层数无穷深的时候，第一个token对其它所有token的贡献会逼近1，也就是说，第一个token的注意力权重会完全趋向于1。而实际虽然我们不会真的构造一个无穷深的llm，但也有定理给出定量的描述，说明，对于除了第一个token以外的token，它们对其它token的贡献，是在以指数速度下降的（每一层而言）</h2><h2 id="至于PE的影响，也是直接给出了定理（还是有点复杂的），大概是说明了常用的PE对attention-mask的影响，也说明了RoPE的好用之处（衰减得没那么快。但到底是什么在衰减？）"><a href="#至于PE的影响，也是直接给出了定理（还是有点复杂的），大概是说明了常用的PE对attention-mask的影响，也说明了RoPE的好用之处（衰减得没那么快。但到底是什么在衰减？）" class="headerlink" title="至于PE的影响，也是直接给出了定理（还是有点复杂的），大概是说明了常用的PE对attention mask的影响，也说明了RoPE的好用之处（衰减得没那么快。但到底是什么在衰减？）"></a>至于PE的影响，也是直接给出了定理（还是有点复杂的），大概是说明了常用的PE对attention mask的影响，也说明了RoPE的好用之处（衰减得没那么快。但到底是什么在衰减？）</h2><h1 id="最后的总结："><a href="#最后的总结：" class="headerlink" title="最后的总结："></a>最后的总结：</h1><h2 id="NTP范式和transformer是一个很好的组合（看看现在的llm的进展和成果就知道了）"><a href="#NTP范式和transformer是一个很好的组合（看看现在的llm的进展和成果就知道了）" class="headerlink" title="NTP范式和transformer是一个很好的组合（看看现在的llm的进展和成果就知道了）"></a>NTP范式和transformer是一个很好的组合（看看现在的llm的进展和成果就知道了）</h2><h2 id="但它们的问题在于，它们预训练可以做得很好，可是预训练做好以后，确实有了很强的能力，但并不完全等同于我们想要的能力。换句话说，预训练即使做得很好，能处理一些任务了，但仍有一些我们关心的任务，它无法处理好（比如长文本，比如position-bias带来的问题）所以需要改进（正如上提到的一些探索，用reweighting改进提高长文本能力的训练方法；考虑bias的成因，指导设计debias的架构）"><a href="#但它们的问题在于，它们预训练可以做得很好，可是预训练做好以后，确实有了很强的能力，但并不完全等同于我们想要的能力。换句话说，预训练即使做得很好，能处理一些任务了，但仍有一些我们关心的任务，它无法处理好（比如长文本，比如position-bias带来的问题）所以需要改进（正如上提到的一些探索，用reweighting改进提高长文本能力的训练方法；考虑bias的成因，指导设计debias的架构）" class="headerlink" title="但它们的问题在于，它们预训练可以做得很好，可是预训练做好以后，确实有了很强的能力，但并不完全等同于我们想要的能力。换句话说，预训练即使做得很好，能处理一些任务了，但仍有一些我们关心的任务，它无法处理好（比如长文本，比如position bias带来的问题）所以需要改进（正如上提到的一些探索，用reweighting改进提高长文本能力的训练方法；考虑bias的成因，指导设计debias的架构）"></a>但它们的问题在于，它们预训练可以做得很好，可是预训练做好以后，确实有了很强的能力，但并不完全等同于我们想要的能力。换句话说，预训练即使做得很好，能处理一些任务了，但仍有一些我们关心的任务，它无法处理好（比如长文本，比如position bias带来的问题）<br>所以需要改进（正如上提到的一些探索，用reweighting改进提高长文本能力的训练方法；考虑bias的成因，指导设计debias的架构）</h2>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/08/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/Divide-Then-Align%EF%BC%9AHonest%20Alignment%20based%20on%20the%20Knowledge%20Boundary%20of%20RAG/" rel="prev" title="Divide-Then-Align：Honest Alignment based on the Knowledge Boundary of RAG">
      <i class="fa fa-chevron-left"></i> Divide-Then-Align：Honest Alignment based on the Knowledge Boundary of RAG
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/08/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4RAG%20paper%E8%B0%83%E7%A0%94/" rel="next" title="近年RAG paper调研">
      近年RAG paper调研 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BB%8A%E7%9A%84llm%EF%BC%8C%E5%A4%A7%E4%BD%93%E4%B8%8A%E5%B0%B1%E6%98%AF%E4%B8%A4%E4%B8%AA%E6%9E%84%E4%BB%B6%EF%BC%9Anest-token-prediction%E8%AE%AD%E7%BB%83%E8%8C%83%E5%BC%8F%EF%BC%88NTP%EF%BC%89%E5%92%8Ctransformer%E6%9E%B6%E6%9E%84%EF%BC%88%E8%99%BD%E7%84%B6%E6%9C%89%E5%BE%88%E5%A4%9A%E5%8F%98%E4%BD%93%EF%BC%89%E3%80%82%E7%9B%AE%E5%89%8D%E5%A4%A7%E6%A6%82%E8%BF%98%E6%98%AF%E6%B2%A1%E6%9C%89%E9%80%83%E5%87%BA%E8%BF%99%E4%B8%AA%E8%8C%83%E5%BC%8F%EF%BC%8C%E4%BB%A5%E5%8F%8A%E4%B8%BB%E6%B5%81%E6%9E%B6%E6%9E%84%E4%BE%9D%E7%84%B6%E6%98%AFtransformer%E7%B1%BB%E5%9E%8B%E7%9A%84%E3%80%82%E8%BF%99%E4%B8%AAseminar%E4%B8%BB%E8%A6%81%E6%8E%A2%E8%AE%A8%E7%9A%84%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B8%A4%E4%B8%AA%E7%82%B9%EF%BC%9ANTP%E7%9A%84%E8%8C%83%E5%BC%8F%E8%BF%98%E8%83%BD%E5%81%9A%E5%BE%97%E6%9B%B4%E5%A5%BD%E5%90%97%EF%BC%9F%EF%BC%88%E8%99%BD%E7%84%B6%E4%B8%80%E5%A5%97%E8%AE%AD%E7%BB%83%E8%8C%83%E5%BC%8F%E4%B9%9F%E8%AE%B8%E6%98%AF%E6%9C%89%E5%AE%83%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%8A%E9%99%90%E7%9A%84%EF%BC%8C%E7%8E%B0%E5%9C%A8%E7%9A%84%E7%BB%93%E6%9E%9C%E4%B9%9F%E6%9C%89%E7%82%B9%E8%A1%A8%E6%98%8E%E4%BA%86%EF%BC%8C%E5%9B%A0%E4%B8%BAscaling-law%E4%B8%8D%E5%86%8D%E5%83%8F%E6%9C%80%E5%BC%80%E5%A7%8B%E9%82%A3%E6%A0%B7%E5%8D%93%E6%9C%89%E6%88%90%E6%95%88%E4%BA%86%E3%80%82%E4%BD%86%E7%8E%B0%E5%9C%A8NTP%E8%8C%83%E5%BC%8F%E5%B0%B1%E5%B7%B2%E7%BB%8F%E8%BE%BE%E5%88%B0%E4%B8%8A%E9%99%90%E4%BA%86%E5%90%97%EF%BC%9F%E8%BF%98%E8%83%BD%E4%B8%8D%E8%83%BD%E5%81%9A%E5%BE%97%E6%9B%B4%E5%A5%BD%E5%91%A2%EF%BC%9F%EF%BC%89Transformer%E6%9E%B6%E6%9E%84%E5%91%A2%EF%BC%9F%E5%AE%83%E6%9C%89%E4%BB%80%E4%B9%88%E8%87%AA%E5%B7%B1%E7%9A%84%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F"><span class="nav-number">1.</span> <span class="nav-text">如今的llm，大体上就是两个构件：nest token prediction训练范式（NTP）和transformer架构（虽然有很多变体）。目前大概还是没有逃出这个范式，以及主流架构依然是transformer类型的。这个seminar主要探讨的就是这两个点：NTP的范式还能做得更好吗？（虽然一套训练范式也许是有它自己的上限的，现在的结果也有点表明了，因为scaling law不再像最开始那样卓有成效了。但现在NTP范式就已经达到上限了吗？还能不能做得更好呢？）Transformer架构呢？它有什么自己的问题吗？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A6%96%E5%85%88%EF%BC%8C%E5%AE%9E%E9%AA%8C%E8%A1%A8%E6%98%8E%EF%BC%8Cllm%E7%9A%84PPL%E6%8C%87%E6%A0%87%E5%BE%97%E5%88%86%E5%92%8C%E5%AE%83%E5%9C%A8%E4%BA%BA%E4%B8%BA%E6%9E%84%E9%80%A0%E7%9A%84long-context%E7%9A%84benchmark%E4%B8%8A%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%8C%E4%BC%BC%E4%B9%8E%E4%BA%8C%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E5%BE%88%E4%BD%8E%EF%BC%88%E6%9C%89%E5%9B%BE%E8%A1%A8%EF%BC%89%EF%BC%8C%E6%89%80%E4%BB%A5%E8%BF%99%E5%9C%A8%E6%9F%90%E7%A7%8D%E7%A8%8B%E5%BA%A6%E4%B8%8A%E8%A1%A8%E6%98%8EPPL%E6%8C%87%E6%A0%87%E4%BC%BC%E4%B9%8E%E5%B9%B6%E4%B8%8D%E8%83%BD%E6%AF%94%E8%BE%83%E5%A5%BD%E5%9C%B0%E5%8F%8D%E6%98%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%95%BF%E6%96%87%E6%9C%AC%E8%83%BD%E5%8A%9B%EF%BC%9FNTP%E8%BF%99%E4%B8%AA%E8%AE%AD%E7%BB%83%E8%8C%83%E5%BC%8F%E4%BC%BC%E4%B9%8E%E5%AF%B9%E4%BA%8Elong-context%E8%83%BD%E5%8A%9B%E8%80%8C%E8%A8%80%EF%BC%8C%E4%B8%8D%E6%98%AF%E9%82%A3%E4%B9%88%E9%AB%98%E6%95%88%E7%9A%84%E8%8C%83%E5%BC%8F%EF%BC%9F%E9%92%88%E5%AF%B9%E8%BF%99%E4%B8%A4%E7%82%B9%EF%BC%8C%E5%85%B6%E5%AE%9E%E7%8E%B0%E6%9C%89%E5%B7%A5%E4%BD%9C%E6%9C%89%E5%9C%A8%E5%B0%9D%E8%AF%95%E8%A7%A3%E5%86%B3%E3%80%82%E5%AF%B9%E4%BA%8EPPL%E6%8C%87%E6%A0%87%E5%8F%AF%E8%83%BD%E4%B8%8D%E5%A4%9F%E5%A5%BD%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E6%9C%89%E4%B8%80%E4%BA%9B%E5%B7%A5%E4%BD%9C%E8%AE%BE%E8%AE%A1%E4%BA%86%E5%90%84%E7%A7%8D%E4%BA%BA%E5%B7%A5%E7%9A%84%E3%80%81%E9%92%88%E5%AF%B9long-context%E7%9A%84benchmark%EF%BC%88%E6%AF%94%E5%A6%82RULER%EF%BC%8CLongEval%EF%BC%8CLongBench%EF%BC%89%E5%AF%B9%E4%BA%8E%E7%AC%AC%E4%BA%8C%E7%82%B9%EF%BC%8C%E4%B8%80%E4%B8%AA%E5%BE%88%E5%9F%BA%E6%9C%AC%E7%9A%84%E6%80%9D%E8%B7%AF%E6%98%AF%E7%BB%A7%E7%BB%AD%E5%8A%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%8F%EF%BC%8C%E5%90%8C%E6%97%B6%E4%BA%BA%E5%B7%A5%E8%BF%9B%E8%A1%8C%E9%80%82%E5%BD%93%E7%9A%84%E7%AD%9B%E9%80%89%E5%A4%84%E7%90%86%E3%80%82%E6%AF%95%E7%AB%9F%EF%BC%8C%E8%99%BD%E7%84%B6PPL%E6%8C%87%E6%A0%87%E5%92%8C%E5%9C%A8long-context-benchmark%E4%B8%8A%E7%9A%84%E5%88%86%E6%95%B0%E7%9B%B8%E5%85%B3%E6%80%A7%E4%B8%8D%E5%A4%A7%EF%BC%8C%E4%BD%86%E5%A4%9A%E5%B0%91%E6%98%AF%E6%9C%89%E4%B8%80%E7%82%B9%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E3%80%82%E5%8F%AA%E4%B8%8D%E8%BF%87%E8%BF%99%E6%A0%B7%E5%BE%88%E4%BD%8E%E6%95%88%E5%B0%B1%E6%98%AF%E4%BA%86%E4%BD%86%E6%80%BB%E7%9A%84%E6%9D%A5%E8%AF%B4%EF%BC%8C%E7%8E%B0%E5%9C%A8%E7%9A%84%E5%B7%A5%E4%BD%9C%E9%87%8C%E8%BF%98%E6%98%AF%E8%9E%8D%E5%85%A5%E4%BA%86%E5%A4%AA%E5%A4%9A%E4%BA%BA%E7%9A%84%E5%85%88%E9%AA%8C%E4%BA%86%E3%80%82%E6%89%80%E4%BB%A5%EF%BC%8C%E6%9C%89%E6%B2%A1%E6%9C%89%E5%8A%9E%E6%B3%95%E4%B8%8D%E8%A6%81%E8%9E%8D%E5%85%A5%E8%BF%99%E4%B9%88%E5%A4%9A%E4%BA%BA%E7%9A%84%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86%EF%BC%9F%E6%9C%89%E6%B2%A1%E6%9C%89%E5%8F%AF%E8%83%BDPPL%E6%8C%87%E6%A0%87%E5%85%B6%E5%AE%9E%E6%98%AF%E6%B2%A1%E9%97%AE%E9%A2%98%E7%9A%84%EF%BC%8C%E8%80%8C%E4%B9%8B%E6%89%80%E4%BB%A5%E6%B2%A1%E8%83%BD%E6%8F%AD%E7%A4%BAllm%E7%9A%84long-context%E8%83%BD%E5%8A%9B%EF%BC%8C%E6%98%AF%E6%9C%89%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E8%80%8C%E5%B7%B2%EF%BC%9F%E8%83%BD%E5%90%A6%E6%8A%8A%E8%BF%99%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%99%E8%A7%A3%E5%86%B3%E4%BA%86%EF%BC%9F"><span class="nav-number">2.</span> <span class="nav-text">首先，实验表明，llm的PPL指标得分和它在人为构造的long-context的benchmark上的得分，似乎二者之间的相关性很低（有图表），所以这在某种程度上表明PPL指标似乎并不能比较好地反映模型的长文本能力？NTP这个训练范式似乎对于long context能力而言，不是那么高效的范式？针对这两点，其实现有工作有在尝试解决。对于PPL指标可能不够好的问题，有一些工作设计了各种人工的、针对long-context的benchmark（比如RULER，LongEval，LongBench）对于第二点，一个很基本的思路是继续加大数据量，同时人工进行适当的筛选处理。毕竟，虽然PPL指标和在long-context benchmark上的分数相关性不大，但多少是有一点相关性的。只不过这样很低效就是了但总的来说，现在的工作里还是融入了太多人的先验了。所以，有没有办法不要融入这么多人的先验知识？有没有可能PPL指标其实是没问题的，而之所以没能揭示llm的long-context能力，是有一些问题而已？能否把这些问题给解决了？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E5%88%86%E6%9E%90%E9%82%A3%E4%BA%9B%E4%BA%BA%E5%B7%A5%E6%9E%84%E9%80%A0%E7%9A%84benchmark%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%BD%9C%E8%80%85%E5%8F%91%E7%8E%B0%EF%BC%8C%E5%AE%83%E4%BB%AC%E6%9E%84%E9%80%A0%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%85%B6%E5%AE%9E%E6%98%AF%E7%9B%B8%E4%BC%BC%E7%9A%84%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%9C%A8prompt%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E7%BB%99llm%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%89%E4%B8%AD%EF%BC%8C%E6%B7%B7%E5%85%A5%E5%BE%88%E5%A4%9A%E7%9A%84key-query%E5%AF%B9%EF%BC%8C%E6%9C%80%E5%90%8E%E9%97%AEllm%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E5%B0%B1%E6%98%AF%E9%97%AE%E5%85%B6%E4%B8%AD%E7%9A%84%E6%9F%90%E4%B8%AAkey%E5%AF%B9%E5%BA%94%E7%9A%84query%E6%98%AF%E8%B0%81%E3%80%82%E9%82%A3%E4%B9%88%EF%BC%8C%E5%A6%82%E6%9E%9Cllm%E8%83%BD%E6%AD%A3%E7%A1%AE%E5%9B%9E%E7%AD%94%EF%BC%8C%E7%A1%AE%E5%AE%9E%E8%A1%A8%E6%98%8E%E4%BA%86%E5%AE%83%E6%9C%89%E4%B8%80%E5%AE%9A%E7%9A%84long-context%E8%83%BD%E5%8A%9B%EF%BC%88%E6%AF%95%E7%AB%9F%E8%83%BD%E6%8E%92%E9%99%A4%E5%85%B6%E5%AE%83%E6%97%A0%E5%85%B3%E7%9A%84key-query%E5%AF%B9%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%89%E4%BD%86%E4%B8%80%E4%B8%AA%E6%A0%B8%E5%BF%83%E7%9A%84%E7%82%B9%E5%9C%A8%E4%BA%8E%EF%BC%8Cllm%E5%AE%83%E6%98%AF%E4%BC%9A%E5%9B%9E%E7%AD%94%E4%B8%80%E5%8F%A5%E5%AE%8C%E6%95%B4%E7%9A%84%E8%AF%9D%E7%9A%84%EF%BC%8C%E5%8F%AF%E8%83%BD%E7%B1%BB%E4%BC%BC%E4%BA%8E%EF%BC%9AThe-xxx-in-xxx-is-%E8%BF%99%E6%97%B6%E6%88%91%E4%BB%AC%E5%AE%9E%E9%99%85%E5%85%B3%E5%BF%83%E7%9A%84%E5%85%B6%E5%AE%9E%E5%8F%AA%E6%9C%89%E6%9C%80%E5%90%8E%E7%9A%84Answer%E9%82%A3%E4%B8%AAtoken%EF%BC%8C%E4%B9%9F%E5%8F%AA%E6%9C%89%E5%AE%83%E6%89%8D%E7%9C%9F%E6%AD%A3%E6%B5%8B%E8%AF%95%E4%BA%86llm%E7%9A%84long-context%E8%83%BD%E5%8A%9B%E3%80%82%E4%BD%86%E6%98%AF%E5%9C%A8%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E7%AE%97%E6%8D%9F%E5%A4%B1%E6%98%AF%E4%BC%9A%E6%8A%8Allm%E7%9A%84%E6%89%80%E6%9C%89%E5%9B%9E%E7%AD%94%E7%9A%84token%E9%83%BD%E6%8B%BF%E5%8E%BB%E7%AE%97%E6%8D%9F%E5%A4%B1%E7%9A%84%EF%BC%88PPL%E5%B0%B1%E6%98%AF%E5%A6%82%E6%AD%A4%EF%BC%89%EF%BC%8C%E5%9B%A0%E6%AD%A4%E8%BF%99%E6%A0%B7%E7%AE%97%E5%87%BA%E6%9D%A5%E7%9A%84%E4%B8%9C%E8%A5%BF%E5%85%B6%E5%AE%9E%E6%B7%B7%E5%85%A5%E4%BA%86%E5%AF%B9%E5%BE%88%E5%A4%9A%E4%B8%8D%E9%82%A3%E4%B9%88%E9%87%8D%E8%A6%81%E7%9A%84token%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E7%9A%84%E8%AF%84%E4%BB%B7%EF%BC%8C%E8%80%8C%E9%9D%9E%E4%B8%93%E6%B3%A8%E4%BA%8E%E5%AF%B9%E4%BD%93%E7%8E%B0long-context%E7%9A%84%E5%85%B3%E9%94%AEtoken%E7%9A%84%E8%AF%84%E4%BB%B7%E5%9B%A0%E6%AD%A4%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E6%8A%8A%E8%BF%99%E4%B8%AAAnswer-token%E8%A7%86%E4%B8%BAkey-token%EF%BC%8C%E9%82%A3%E4%B9%88%E6%88%91%E4%BB%AC%E8%83%BD%E5%90%A6%E8%BF%99%E6%A0%B7%EF%BC%9A%E6%89%BE%E5%87%BAkey-token%EF%BC%8C%E7%84%B6%E5%90%8E%E5%8F%AA%E5%9C%A8%E8%BF%99%E4%BA%9Bkey-tokens%E4%B8%8A%E7%AE%97PPL%E6%8C%87%E6%A0%87%E3%80%82%E8%BF%99%E6%A0%B7%E7%9A%84PPL%E6%98%AF%E5%90%A6%E8%83%BD%E6%9B%B4%E5%8A%A0%E5%A5%BD%E5%9C%B0%E3%80%81%E6%9B%B4%E5%8A%A0%E5%87%86%E7%A1%AE%E5%9C%B0%E4%BD%93%E7%8E%B0llm%E7%9A%84long-context%E8%83%BD%E5%8A%9B%EF%BC%9F%E7%BB%93%E6%9E%9C%E8%A1%A8%E6%98%8E%EF%BC%8C%E7%A1%AE%E5%AE%9E%E6%98%AF%E5%8F%AF%E4%BB%A5%E7%9A%84%E5%9B%A0%E6%AD%A4%EF%BC%8C%E8%BF%99%E8%AF%B4%E6%98%8EPPL%E6%8C%87%E6%A0%87%E6%9C%AC%E8%BA%AB%E6%B2%A1%E6%9C%89%E5%A4%AA%E5%A4%9A%E9%97%AE%E9%A2%98%EF%BC%8C%E5%AE%83%E6%98%AF%E8%83%BD%E5%8F%8D%E6%98%A0long-context%E8%83%BD%E5%8A%9B%E7%9A%84%E3%80%82%E5%8F%AA%E4%B8%8D%E8%BF%87%E9%9C%80%E8%A6%81%E5%9C%A8key-tokens%E4%B8%8A%E7%AE%97%E8%80%8C%E5%B7%B2%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E7%8E%B0%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98%E5%B0%B1%E8%BD%AC%E5%8C%96%E6%88%90%EF%BC%8C%E5%A6%82%E4%BD%95%E6%89%BE%E5%88%B0%E8%BF%99%E4%BA%9Bkey-tokens%E5%91%A2%EF%BC%9F%E5%9B%A0%E4%B8%BA%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E7%9A%84%E6%98%AF%E4%B8%8D%E9%9C%80%E8%A6%81%E4%BA%BA%E4%B8%BA%E6%9E%84%E9%80%A0%EF%BC%8C%E4%BB%85%E7%94%A8%E8%87%AA%E7%84%B6%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E9%82%A3%E4%BA%9B%E6%AF%94%E8%BE%83%E5%8E%9F%E5%A7%8B%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%89%E5%B0%B1%E8%83%BD%E8%BF%9B%E8%A1%8C%E6%94%B9%E8%BF%9B%E3%80%82%E4%BD%86%E8%87%AA%E7%84%B6%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8C%E6%98%AF%E4%B8%8D%E4%BC%9A%E6%9C%89key-tokens%E7%9A%84%E6%A0%87%E6%B3%A8%E7%9A%84%E3%80%82%E9%82%A3%E5%85%B6%E5%AE%9E%E8%BF%99%E5%B0%B1%E5%BE%88%E5%83%8F%E4%B8%80%E4%B8%AA%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88SSL%EF%BC%89%E7%9A%84%E9%97%AE%E9%A2%98%E4%BA%86%E3%80%82%E5%A4%A7%E4%BD%93%E7%9A%84%E6%80%9D%E8%B7%AF%E5%B0%B1%E6%98%AF%E7%BB%8F%E5%85%B8%E7%9A%84%EF%BC%8C%E2%80%9D%E4%B8%80%E8%BE%B9%E6%A0%87%EF%BC%8C%E4%B8%80%E8%BE%B9%E5%AD%A6%E2%80%9C%EF%BC%8Ctoken%E6%98%AF%E5%90%A6%E6%98%AFkey-token%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%A6%81%E6%89%93%E7%9A%84%E6%A0%87%E7%AD%BE"><span class="nav-number">3.</span> <span class="nav-text">通过分析那些人工构造的benchmark的数据，作者发现，它们构造的格式其实是相似的，就是在prompt（也就是给llm的问题）中，混入很多的key-query对，最后问llm的问题，就是问其中的某个key对应的query是谁。那么，如果llm能正确回答，确实表明了它有一定的long-context能力（毕竟能排除其它无关的key-query对的影响）但一个核心的点在于，llm它是会回答一句完整的话的，可能类似于：The xxx in xxx is &lt;Answer&gt;这时我们实际关心的其实只有最后的Answer那个token，也只有它才真正测试了llm的long-context能力。但是在训练的时候，算损失是会把llm的所有回答的token都拿去算损失的（PPL就是如此），因此这样算出来的东西其实混入了对很多不那么重要的token的预测结果的评价，而非专注于对体现long-context的关键token的评价因此，如果我们把这个Answer token视为key token，那么我们能否这样：找出key token，然后只在这些key tokens上算PPL指标。这样的PPL是否能更加好地、更加准确地体现llm的long-context能力？结果表明，确实是可以的因此，这说明PPL指标本身没有太多问题，它是能反映long-context能力的。只不过需要在key tokens上算而已。因此，现在的问题就转化成，如何找到这些key tokens呢？因为我们希望的是不需要人为构造，仅用自然的数据（也就是那些比较原始的数据）就能进行改进。但自然的数据里是不会有key tokens的标注的。那其实这就很像一个无监督学习，或者说自监督学习（SSL）的问题了。大体的思路就是经典的，”一边标，一边学“，token是否是key token，就是要打的标签</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%82%A3%E6%80%8E%E4%B9%88%E6%89%93%E6%A0%87%E7%AD%BE%EF%BC%9F%E8%BF%99%E9%87%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E7%89%B9%E5%88%AB%E5%B7%A7%E5%A6%99%E7%9A%84%E6%80%9D%E8%B7%AF%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4trick%EF%BC%9F%E7%84%B6%E8%80%8C%E7%9A%84%E7%9A%84%E7%A1%AE%E7%A1%AE%E6%98%AF%E5%9C%A8%E5%85%B6%E5%AE%83%E9%A2%86%E5%9F%9F%E5%87%BA%E7%8E%B0%E8%BF%87%E7%9A%84%E6%88%91%E4%BB%AC%E7%9A%84%E7%9B%AE%E6%A0%87%E6%98%AF%E6%89%BE%E5%88%B0%E8%83%BD%E5%8F%8D%E6%98%A0llm%E7%9A%84long-context%E8%83%BD%E5%8A%9B%E7%9A%84%E5%85%B3%E9%94%AEtokens%EF%BC%8C%E6%80%8E%E6%A0%B7%E5%88%A4%E6%96%AD%E4%B8%80%E4%B8%AAtoken%E6%98%AF%E5%90%A6%E5%8F%8D%E6%98%A0%E4%BA%86long-context%E8%83%BD%E5%8A%9B%E5%91%A2%EF%BC%9F%E4%BD%9C%E8%80%85%E7%BB%99%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E3%80%82%E6%AF%94%E5%A6%82%E6%88%91%E4%BB%AC%E7%8E%B0%E5%9C%A8%E6%9C%89%E4%B8%80%E5%A4%A7%E4%B8%B2long-context%EF%BC%8C%E6%9C%80%E5%90%8E%E6%9C%89%E4%B8%80%E5%8F%A5%E8%AF%9D%E8%A6%81%E9%A2%84%E6%B5%8B%EF%BC%8C%E4%B8%94%E6%9C%80%E5%90%8E%E7%9A%84%E4%B8%80%E4%B8%AAtoken%E6%98%AFkey-token%EF%BC%8C%E5%AE%83%E5%8F%AA%E5%9C%A8long-context%E7%9A%84%E4%B8%80%E5%BC%80%E5%A7%8B%E5%87%BA%E7%8E%B0%E4%BA%86%EF%BC%8C%E5%90%8E%E9%9D%A2%E9%83%BD%E6%B2%A1%E5%87%BA%E7%8E%B0%E3%80%82%E9%82%A3%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E6%8A%8A%E8%A6%81%E9%A2%84%E6%B5%8B%E7%9A%84%E9%82%A3%E5%8F%A5%E8%AF%9D%E6%89%80%E7%94%A8%E7%9A%84long-context%E7%BB%99%E6%88%AA%E6%96%AD%EF%BC%8C%E6%88%AA%E6%96%AD%E6%88%90llm%E6%AD%A3%E5%B8%B8%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AA%97%E5%8F%A3%E7%9A%84%E9%95%BF%E5%BA%A6%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AFpretrain%E9%98%B6%E6%AE%B5%E7%9A%84%E9%95%BF%E5%BA%A6%EF%BC%8C%E4%BE%8B%E5%A6%824K%EF%BC%89%EF%BC%8C%E5%86%8D%E6%9D%A5%E9%A2%84%E6%B5%8B%E4%B8%80%E4%B8%8B%E3%80%82%E6%88%AA%E6%96%AD%E5%89%8D%E5%92%8C%E6%88%AA%E6%96%AD%E5%90%8E%E9%83%BD%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B%E3%80%82%E9%82%A3%E4%B9%88%EF%BC%8C%E6%9C%80%E5%90%8E%E7%9A%84%E9%82%A3%E5%8F%A5%E8%AF%9D%E9%87%8C%EF%BC%8C%E6%8C%89%E7%90%86%E8%AF%B4%EF%BC%8C%E9%99%A4%E4%BA%86key-token%E4%BB%A5%E5%A4%96%EF%BC%8C%E5%85%B6%E5%AE%83%E7%9A%84%E9%9D%9Ekey-tokens%EF%BC%8C%E5%BA%94%E8%AF%A5%E9%A2%84%E6%B5%8B%E7%9A%84%E6%A6%82%E7%8E%87%E9%83%BD%E5%B7%AE%E5%88%AB%E4%B8%8D%E5%A4%A7%EF%BC%88%E5%9B%A0%E4%B8%BA%E5%AE%83%E4%BB%AC%E4%B8%8D%E6%98%AFkey-tokens%EF%BC%8C%E5%AE%83%E4%BB%AC%E5%B9%B6%E6%B2%A1%E6%9C%89%E5%8F%8D%E6%98%A0llm%E7%9A%84long-context%E8%83%BD%E5%8A%9B%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8Cllm%E6%8C%89%E7%90%86%E8%AF%B4%E4%B8%8D%E9%9C%80%E8%A6%81%E9%82%A3%E4%B9%88%E5%A4%9A%E7%9A%84context%E4%B9%9F%E8%83%BD%E9%A2%84%E6%B5%8B%E5%87%BA%E6%9D%A5%EF%BC%89%EF%BC%9B%E7%9B%B8%E6%AF%94%E4%B9%8B%E4%B8%8B%EF%BC%8C%E9%82%A3%E4%B8%AAkey-token%E6%98%AF%E8%83%BD%E5%8F%8D%E6%98%A0llm%E7%9A%84long-context%E8%83%BD%E5%8A%9B%E7%9A%84%EF%BC%8C%E5%9B%A0%E4%B8%BA%E5%AE%83%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0%E4%B8%80%E5%A4%A7%E4%B8%B2context%EF%BC%8C%E6%89%80%E4%BB%A5%E4%B8%80%E6%97%A6%E6%88%AA%E6%96%ADcontext%EF%BC%8C%E5%8F%AF%E4%BB%A5%E9%A2%84%E6%B5%8B%E7%9A%84%E6%98%AF%EF%BC%8C%E5%9C%A8%E8%BF%99%E4%B8%AAtoken%E4%B8%8A%E9%A2%84%E6%B5%8B%E7%9A%84%E6%A6%82%E7%8E%87%E5%BA%94%E8%AF%A5%E6%9C%89%E6%AF%94%E8%BE%83%E5%A4%A7%E7%9A%84%E4%B8%8B%E9%99%8D%E3%80%82%E8%BF%99%E6%97%A0%E7%96%91%E5%B0%B1%E6%98%AF%E4%B8%80%E4%B8%AA%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E4%B8%BAkey-token%E7%9A%84%E6%8C%87%E6%A0%87%EF%BC%88%E5%BE%88%E7%9C%BC%E7%86%9F%E5%95%8A%EF%BC%8Cmask%E4%B8%80%E4%B8%8B%E7%9C%8B%E7%9C%8B%E5%89%8D%E5%90%8E%E7%9A%84%E5%8F%98%E5%8C%96%E3%80%82%E5%AE%83%E6%98%AF%E5%B1%9E%E4%BA%8Ecausal-intervention%EF%BC%88%E5%9B%A0%E6%9E%9C%E5%B9%B2%E9%A2%84%EF%BC%89%E7%9A%84%E4%B8%80%E7%A7%8D%E3%80%82%E5%B9%BF%E4%B9%89%E7%9A%84causal-intervention%E6%98%AF%E5%8F%98%E5%8A%A8%E4%B8%80%E4%B8%AA%E5%8F%98%E9%87%8F%EF%BC%88%E7%A7%B0%E4%B8%BA%E5%B9%B2%E9%A2%84%E5%8F%98%E9%87%8F%EF%BC%89%EF%BC%8C%E8%A7%82%E5%AF%9F%E5%8F%A6%E4%B8%80%E4%B8%AA%E5%8F%98%E9%87%8F%E7%9A%84%E5%8F%98%E5%8C%96%E6%83%85%E5%86%B5%EF%BC%88%E7%A7%B0%E4%B8%BA%E7%BB%93%E6%9E%9C%E5%8F%98%E9%87%8F%EF%BC%89%EF%BC%8C%E4%BB%8E%E8%80%8C%E6%8F%AD%E7%A4%BA%E4%BA%8C%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E3%80%82%E5%AE%9E%E5%9C%A8%E6%98%AF%E5%BE%88%E5%A6%99%EF%BC%89%E5%9B%A0%E6%AD%A4%EF%BC%8C%E6%88%91%E4%BB%AC%E8%80%83%E8%99%91%E5%88%A9%E7%94%A8%E8%BF%99%E4%B8%80%E7%82%B9%EF%BC%8C%E6%8A%8A%E6%88%AA%E6%96%AD%E5%89%8D%E5%92%8C%E9%98%B6%E6%AE%B5%E5%90%8Etoken%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87%E5%81%9A%E4%B8%80%E4%B8%AA%E6%AF%94%E5%80%BC%EF%BC%8C%E5%BE%97%E5%88%B0%E4%B8%80%E4%B8%AA%E6%8C%87%E6%A0%87LPG%EF%BC%88Log-Probability-Gain%EF%BC%89%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%AF%94%E5%80%BC%E8%B6%8A%E5%A4%A7%EF%BC%8C%E5%88%99%E8%BF%99%E4%B8%AAtoken%E8%B6%8A%E6%9C%89%E5%8F%AF%E8%83%BD%E6%98%AFkey-token%E3%80%82%E5%AE%9E%E6%93%8D%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E8%AE%BE%E5%AE%9A%E4%B8%80%E4%B8%AAthreshold%EF%BC%8C%E6%AF%94%E5%80%BC%E8%B6%85%E8%BF%87%E4%BA%86threshold%E5%B0%B1%E8%AE%A4%E4%B8%BA%E8%BF%99%E4%B8%AAtoken%E6%98%AFkey-token"><span class="nav-number">4.</span> <span class="nav-text">那怎么打标签？这里是一个特别巧妙的思路，或者说trick？然而的的确确是在其它领域出现过的我们的目标是找到能反映llm的long-context能力的关键tokens，怎样判断一个token是否反映了long-context能力呢？作者给出了一个例子。比如我们现在有一大串long context，最后有一句话要预测，且最后的一个token是key token，它只在long context的一开始出现了，后面都没出现。那如果我们把要预测的那句话所用的long context给截断，截断成llm正常的上下文窗口的长度（也就是pretrain阶段的长度，例如4K），再来预测一下。截断前和截断后都进行预测。那么，最后的那句话里，按理说，除了key token以外，其它的非key tokens，应该预测的概率都差别不大（因为它们不是key tokens，它们并没有反映llm的long-context能力，也就是说，llm按理说不需要那么多的context也能预测出来）；相比之下，那个key token是能反映llm的long-context能力的，因为它需要用到一大串context，所以一旦截断context，可以预测的是，在这个token上预测的概率应该有比较大的下降。这无疑就是一个判断是否为key token的指标（很眼熟啊，mask一下看看前后的变化。它是属于causal intervention（因果干预）的一种。广义的causal intervention是变动一个变量（称为干预变量），观察另一个变量的变化情况（称为结果变量），从而揭示二者之间的因果关系。实在是很妙）因此，我们考虑利用这一点，把截断前和阶段后token的预测概率做一个比值，得到一个指标LPG（Log Probability Gain），这个比值越大，则这个token越有可能是key token。实操的时候，我们就设定一个threshold，比值超过了threshold就认为这个token是key token</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A6%96%E5%85%88%EF%BC%8C%E8%BF%99%E9%87%8C%E5%85%B3%E4%BA%8E%E5%88%A4%E6%96%ADkey-token%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E6%9C%89%E4%B8%80%E4%BA%9B%E7%82%B9%E9%9C%80%E8%A6%81%E6%BE%84%E6%B8%85%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%E9%98%90%E6%98%8E%E3%80%82%E4%B8%8A%E9%9D%A2%E6%88%91%E4%BB%AC%E4%B8%BE%E7%9A%84%E4%BC%BC%E4%B9%8E%E6%98%AF%E4%B8%80%E4%B8%AA%E6%9C%89%E7%82%B9%E6%9E%81%E7%AB%AF%E7%9A%84%E4%BE%8B%E5%AD%90%EF%BC%88%E4%BA%8B%E5%AE%9E%E4%B8%8A%E8%AE%B2%E5%BA%A7%E4%B8%8A%E4%B8%BE%E7%9A%84%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B8%AA%E4%BE%8B%E5%AD%90%EF%BC%89%EF%BC%8C%E5%8D%B3%E6%9C%89%E7%94%A8%E7%9A%84%E4%B8%8A%E6%96%87%E4%BB%85%E5%87%BA%E7%8E%B0%E5%9C%A8context%E7%9A%84%E4%B8%80%E5%BC%80%E5%A7%8B%E3%80%82%E8%BF%99%E6%97%B6%EF%BC%8C%E6%88%91%E4%BB%AC%E6%8A%8Along-context%E6%88%AA%E6%96%AD%E5%88%B0short-context%EF%BC%8C%E6%9D%A5%E8%AE%A1%E7%AE%97%E5%89%8D%E5%90%8E%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87%E6%AF%94%E5%80%BC%EF%BC%8C%E7%A1%AE%E5%AE%9E%E5%BA%94%E8%AF%A5%E4%BC%9A%E6%9C%89%E5%BE%88%E5%A4%A7%E7%9A%84%E5%8F%98%E5%8C%96%E3%80%82%E4%BD%86%E5%A6%82%E6%9E%9C%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%9C%89%E7%94%A8%E7%9A%84%E4%B8%8A%E6%96%87%EF%BC%8C%E5%AE%83%E5%B9%B6%E9%9D%9E%E5%87%BA%E7%8E%B0%E5%9C%A8context%E7%9A%84%E4%B8%80%E5%BC%80%E5%A7%8B%EF%BC%8C%E8%80%8C%E6%98%AF%E5%87%BA%E7%8E%B0%E5%9C%A8%EF%BC%8C%E6%AF%94%E6%96%B9%E8%AF%B4context%E7%9A%84%E4%B8%AD%E9%97%B4%E5%91%A2%EF%BC%9F%E8%BF%99%E6%97%B6%E6%9C%89%E5%8F%AF%E8%83%BD%E5%87%BA%E7%8E%B0%EF%BC%9A%E6%88%91%E4%BB%AC%E6%88%AA%E6%96%ADcontext%EF%BC%88%E6%88%AA%E6%96%ADcontext%EF%BC%8C%E4%B8%AA%E4%BA%BA%E7%9A%84%E7%90%86%E8%A7%A3%E6%98%AF%EF%BC%8C%E4%BB%8E%E5%BE%85%E9%A2%84%E6%B5%8B%E7%9A%84token%EF%BC%8C%E6%88%96%E8%80%85seq%E5%BC%80%E5%A7%8B%EF%BC%8C%E5%BE%80%E4%B8%8A%E5%8E%BB%E8%AE%A1%E6%95%B0%EF%BC%8C%E8%AE%A1%E5%88%B0short-context%E7%9A%84%E8%8C%83%E5%9B%B4%E4%B8%BA%E6%AD%A2%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E8%BF%99%E4%B8%AA%E6%9C%89%E7%94%A8%E7%9A%84%E4%B8%8A%E6%96%87%EF%BC%8C%E5%AE%83%E8%A2%AB%E5%8C%85%E5%90%AB%E5%9C%A8%E4%BA%86%E9%98%B6%E6%AE%B5%E5%90%8E%E7%9A%84short-context%E9%87%8C%E4%BA%86%E3%80%82%E8%BF%99%E6%A0%B7%EF%BC%8C%E6%88%AA%E6%96%AD%E5%89%8D%E5%90%8E%E9%A2%84%E6%B5%8B%E8%BF%99%E4%B8%AAtoken%E7%9A%84%E6%A6%82%E7%8E%87%E7%9A%84%E6%AF%94%E5%80%BC%EF%BC%8C%E5%BA%94%E8%AF%A5%E4%B8%8D%E5%A4%A7%E5%90%A7%EF%BC%9F%E8%BF%99%E6%A0%B7%E6%88%91%E4%BB%AC%E4%B8%8D%E5%B0%B1%E4%BC%9A%E5%88%A4%E6%96%AD%E5%AE%83%E4%B8%BA%E9%9D%9Ekey-token%E4%BA%86%E5%90%97%EF%BC%9F%E4%BD%86%E5%85%B6%E5%AE%9E%EF%BC%8C%E6%9C%AC%E6%9D%A5%E5%B0%B1%E6%98%AF%E5%95%8A%E2%80%A6%E2%80%A6%E6%88%91%E4%BB%AC%E4%B8%8D%E8%83%BD%E5%A2%A8%E5%AE%88%E6%88%90%E8%A7%84%E5%9C%B0%E8%AE%A4%E4%B8%BA%E4%B8%80%E4%B8%AAtoken%E4%B8%80%E5%AE%9A%E5%B0%B1%E6%98%AFkey-token%E6%88%96%E8%80%85%E4%B8%8D%E6%98%AFkey-token%E3%80%82%E6%9C%AC%E8%B4%A8%E4%B8%8A%EF%BC%8C%E4%B8%80%E4%B8%AAtoken%E6%98%AFkey-token%EF%BC%8C%E5%8F%AA%E9%9C%80%E8%A6%81%E5%AE%83%E5%BE%85%E9%A2%84%E6%B5%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E5%92%8C%E5%85%B3%E9%94%AE%E7%9A%84%E6%8F%90%E7%A4%BA%E4%BF%A1%E6%81%AF%E7%A6%BB%E5%BE%97%E8%B6%B3%E5%A4%9F%E8%BF%9C%E5%B0%B1%E8%A1%8C%EF%BC%88%E8%99%BD%E8%AF%B4%E8%BF%99%E4%B8%80%E7%82%B9%E5%90%A7%EF%BC%8C%E4%B9%9F%E4%B8%8D%E5%85%A8%E9%9D%A2%EF%BC%8C%E4%BD%86%E6%98%BE%E7%84%B6%E8%BF%99%E7%A7%8D%E6%83%85%E5%86%B5%E6%98%AF%E4%BD%93%E7%8E%B0long-context%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9C%BA%E6%99%AF%EF%BC%89%E3%80%82%E6%89%80%E4%BB%A5%EF%BC%8C%E5%B9%B6%E4%B8%8D%E6%98%AF%E8%AF%B4%E5%8F%AA%E6%9C%89%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E5%87%BA%E7%8E%B0%E5%9C%A8context%E7%9A%84%E4%B8%80%E5%BC%80%E5%A7%8B%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%AE%83%E5%AF%B9%E5%BA%94%E7%9A%84token%E6%89%8D%E6%98%AFkey-token%E3%80%82%E5%8F%AA%E8%A6%81%E4%B8%80%E4%B8%AAtoken%E7%9A%84%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%E7%A6%BB%E5%AE%83%E8%B6%B3%E5%A4%9F%E8%BF%9C%EF%BC%8C%E8%BF%9C%E5%88%B0%E4%BB%8E%E8%BF%99%E4%B8%AA%E5%BE%85%E9%A2%84%E6%B5%8Btoken%E5%BC%80%E5%A7%8B%E5%BE%80%E4%B8%8A%E6%95%B0%EF%BC%8C%E6%95%B0%E5%88%B0short-context%E7%9A%84%E8%8C%83%E5%9B%B4%E4%B8%8A%E9%99%90%E4%BA%86%EF%BC%8C%E8%BF%98%E6%98%AF%E6%B2%A1%E6%9C%89%E5%87%BA%E7%8E%B0%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%EF%BC%8C%E9%82%A3%E8%BF%99%E4%B8%AAtoken%E5%B0%B1%E5%8F%AF%E4%BB%A5%E5%BD%93%E4%BD%9C%E6%98%AFkey-token"><span class="nav-number">4.1.</span> <span class="nav-text">首先，这里关于判断key token的方法，有一些点需要澄清，或者说阐明。上面我们举的似乎是一个有点极端的例子（事实上讲座上举的就是这个例子），即有用的上文仅出现在context的一开始。这时，我们把long context截断到short context，来计算前后的预测概率比值，确实应该会有很大的变化。但如果，这个有用的上文，它并非出现在context的一开始，而是出现在，比方说context的中间呢？这时有可能出现：我们截断context（截断context，个人的理解是，从待预测的token，或者seq开始，往上去计数，计到short context的范围为止），然后这个有用的上文，它被包含在了阶段后的short context里了。这样，截断前后预测这个token的概率的比值，应该不大吧？这样我们不就会判断它为非key token了吗？但其实，本来就是啊……我们不能墨守成规地认为一个token一定就是key token或者不是key token。本质上，一个token是key token，只需要它待预测的位置和关键的提示信息离得足够远就行（虽说这一点吧，也不全面，但显然这种情况是体现long context的一个场景）。所以，并不是说只有关键信息出现在context的一开始的时候，它对应的token才是key token。只要一个token的关键信息离它足够远，远到从这个待预测token开始往上数，数到short context的范围上限了，还是没有出现关键信息，那这个token就可以当作是key token</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E6%AC%A1%EF%BC%8C%E4%B8%8B%E9%9D%A2%E7%9A%84%E6%94%B9%E8%BF%9B%E4%B8%8E%E5%88%86%E6%9E%90%E5%88%99%E6%9B%B4%E5%8A%A0%E5%B7%A7%E5%A6%99%EF%BC%8C%E6%84%9F%E8%A7%89%E4%B9%9F%E4%BD%93%E7%8E%B0%E4%BA%86%E7%A0%94%E7%A9%B6%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B8%B8%E7%94%A8%E6%80%9D%E8%B7%AF%E3%80%82%E4%BD%9C%E8%80%85%E5%8F%91%E7%8E%B0%EF%BC%8C%E7%94%A8%E8%BF%99%E4%B8%AA%E6%8C%87%E6%A0%87%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%89%BE%E5%88%B085-6-%E7%9A%84key-token%EF%BC%8C%E5%85%B6%E5%AE%9E%E6%95%88%E6%9E%9C%E5%B7%B2%E7%BB%8F%E8%BF%98%E4%B8%8D%E9%94%99%E4%BA%86%EF%BC%8C%E4%BD%86%E4%BD%9C%E8%80%85%E5%B8%8C%E6%9C%9B%E8%83%BD%E6%9B%B4%E5%A5%BD%E3%80%82%E5%9B%A0%E6%AD%A4%E4%BB%96%E5%88%86%E6%9E%90%E4%BA%86%E9%82%A3%E4%BA%9B%E8%A2%AB%E9%94%99%E8%AE%A4%E4%B8%BAkey-token%E7%9A%84token%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E9%82%A3%E4%BA%9B%E5%B9%B6%E9%9D%9Ekey-token%EF%BC%8C%E4%BD%86%E6%98%AFLPG%E5%80%BC%E5%8D%B4%E6%8C%BA%E5%A4%A7%E7%9A%84token%EF%BC%89%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87%EF%BC%8C%E5%8F%91%E7%8E%B0%E5%AE%83%E4%BB%AC%E5%9C%A8%E6%88%AA%E6%96%AD%E5%89%8D%EF%BC%8C%E6%9C%AC%E8%BA%AB%E8%A2%AB%E9%A2%84%E6%B5%8B%E7%9A%84%E6%A6%82%E7%8E%87%E5%B0%B1%E6%AF%94%E8%BE%83%E5%B0%8F%E3%80%82%E5%8D%B3%EF%BC%8C%E5%AE%83%E4%BB%AC%E6%9C%AC%E8%BA%AB%E5%B0%B1%E6%98%AF%E6%AF%94%E8%BE%83%E9%9A%BE%E8%A2%AB%E9%A2%84%E6%B5%8B%E7%9A%84token%EF%BC%88%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E6%88%AA%E6%96%AD%E5%89%8D%E5%90%8E%E5%85%B6%E5%AE%9E%E8%BF%99%E4%BA%9Btoken%E9%83%BD%E6%AF%94%E8%BE%83%E9%9A%BE%E4%BB%A5%E9%A2%84%E6%B5%8B%E3%80%82%E5%8F%AF%E8%83%BD%E6%98%AF%E5%9B%A0%E4%B8%BA%E9%A2%84%E6%B5%8B%E5%AE%83%E4%BB%AC%E6%89%80%E9%9C%80%E8%A6%81%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%EF%BC%8C%E5%9C%A8%E8%BF%99%E6%95%B4%E4%B8%80%E6%AE%B5%E7%9A%84long-context%E9%87%8C%E5%B0%B1%E5%8E%8B%E6%A0%B9%E6%B2%A1%E6%9C%89%EF%BC%8C%E6%89%80%E4%BB%A5%E7%9B%B8%E5%AF%B9%E4%BA%8E%E8%BF%99%E4%B8%80%E6%AE%B5long-context%E8%80%8C%E8%A8%80%EF%BC%8C%E8%BF%99%E4%B8%AAtoken%E5%B9%B6%E4%B8%8D%E6%98%AFkey-token%EF%BC%9B%E4%B9%9F%E5%8F%AF%E4%BB%A5%E7%B1%BB%E6%AF%94%E4%B8%80%E4%B8%8B%EF%BC%8C%E4%BA%BA%E5%9C%A8%E8%BF%9B%E8%A1%8C%E5%86%99%E4%BD%9C%E7%AD%89%E5%88%9B%E4%BD%9C%E8%BE%93%E5%87%BA%E7%9A%84%E6%B4%BB%E5%8A%A8%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E7%9A%84%E7%A1%AE%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E6%AF%AB%E6%97%A0%E9%80%BB%E8%BE%91%E5%9C%B0%E3%80%81%E6%88%96%E8%80%85%E8%AF%B4%E9%80%BB%E8%BE%91%E8%B7%B3%E8%B7%83%E5%9C%B0%E5%A4%AA%E5%A4%A7%E4%BB%A5%E8%87%B3%E4%BA%8E%E6%98%BE%E5%BE%97%E5%A4%A9%E9%A9%AC%E8%A1%8C%E7%A9%BA%E7%9A%84%E6%83%B3%E5%88%B0%E6%9F%90%E4%B8%AA%E4%BA%8B%E7%89%A9%E3%80%82%E8%BF%99%E7%A7%8D%E5%86%85%E5%AE%B9%EF%BC%8C%E5%96%82%E7%BB%99llm%EF%BC%8C%E5%AE%83%E5%B0%B1%E4%B8%8D%E5%A4%AA%E8%83%BD%E5%9F%BA%E4%BA%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E9%A2%84%E6%B5%8B%E5%88%B0%E4%BA%86%E3%80%82%E8%A7%A3%E9%87%8A%E5%85%B6%E5%AE%9E%E5%8F%AF%E4%BB%A5%E6%9C%89%E5%BE%88%E5%A4%9A%EF%BC%8C%E4%BD%86%E8%BF%99%E5%B9%B6%E4%B8%8D%E6%98%AF%E9%87%8D%E7%82%B9%EF%BC%8C%E5%9B%A0%E4%B8%BA%E8%BF%99%E9%87%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E8%A7%82%E5%AF%9F%E5%8F%91%E7%8E%B0%E4%BA%86%E4%B8%80%E4%B8%AA%E4%BA%8B%E5%AE%9E%E8%A7%84%E5%BE%8B%EF%BC%8C%E5%B9%B6%E4%B8%94%E8%BF%99%E4%B8%AA%E8%A7%84%E5%BE%8B%E5%8F%AF%E4%BB%A5%E8%BE%85%E5%8A%A9%E6%88%91%E4%BB%AC%E6%94%B9%E8%BF%9B%E4%B8%8A%E9%9D%A2%E7%9A%84LPG%E6%8C%87%E6%A0%87%EF%BC%89%E3%80%82%E5%9F%BA%E4%BA%8E%E8%BF%99%E4%B8%AA%E8%A7%82%E5%AF%9F%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E6%8A%8A%E4%B8%8A%E9%9D%A2%E7%9A%84%E6%8C%87%E6%A0%87%E4%BF%AE%E6%AD%A3%E4%B8%80%E4%B8%8B%EF%BC%9A%E4%B8%80%E4%B8%AAtoken%E8%A2%AB%E8%AE%A4%E4%B8%BA%E6%98%AFkey-token%EF%BC%8C%E4%B8%8D%E4%BB%85%E8%A6%81%E5%AE%83%E7%9A%84LPG%E6%8C%87%E6%A0%87%E5%A4%9F%E5%A4%A7%EF%BC%8C%E8%80%8C%E4%B8%94%E5%AE%83%E5%9C%A8%E6%88%AA%E6%96%AD%E5%89%8D%E5%87%BA%E7%8E%B0%E7%9A%84%E6%A6%82%E7%8E%87%E4%B9%9F%E4%B8%8D%E8%83%BD%E5%A4%AA%E5%B0%8F%EF%BC%88%E8%BF%99%E9%87%8C%E5%B0%B1%E9%9C%80%E8%A6%81%E5%86%8D%E5%8F%A6%E8%A1%8C%E8%AE%BE%E5%AE%9A%E4%B8%80%E4%B8%AAthreshold%E4%BA%86%EF%BC%8C%E4%BD%86%E5%AE%9E%E9%AA%8C%E4%B8%80%E4%B8%8B%E5%B0%B1%E5%8F%AF%E4%BB%A5%E5%BE%97%E5%88%B0%E3%80%82%E8%80%8C%E4%B8%94%E5%90%8E%E9%9D%A2%E4%BD%9C%E8%80%85%E4%B9%9F%E8%AF%B4%E4%BA%86%EF%BC%8C%E5%AE%9E%E9%AA%8C%E5%8F%91%E7%8E%B0%E8%BF%99%E4%B8%AAthreshold%E8%BF%98%E6%98%AF%E6%AF%94%E8%BE%83%E9%B2%81%E6%A3%92%E7%9A%84%EF%BC%8C%E5%9C%A8%E4%B8%80%E5%A5%97%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E5%AE%9E%E9%AA%8C%E5%BE%97%E5%88%B0%E7%9A%84threshold%E5%8F%AF%E4%BB%A5%E8%BF%81%E7%A7%BB%E5%88%B0%E5%85%B6%E5%AE%83%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%EF%BC%8C%E6%95%88%E6%9E%9C%E4%B9%9F%E4%B8%8D%E5%B7%AE%EF%BC%89%E6%9C%80%E7%BB%88%E7%94%A8%E6%94%B9%E8%BF%9B%E5%90%8E%E7%9A%84%E6%8C%87%E6%A0%87%EF%BC%8C%E4%BD%9C%E8%80%85%E5%AE%9E%E9%AA%8C%E5%8F%91%E7%8E%B0%E5%8F%AF%E4%BB%A5%E6%89%BE%E5%88%B098-2-%E7%9A%84key-token%E4%BA%86%E3%80%82%E8%BF%99%E4%B8%AA%E5%87%86%E7%A1%AE%E7%8E%87%E5%B0%B1%E6%AF%94%E8%BE%83%E4%BB%A4%E4%BA%BA%E6%BB%A1%E6%84%8F%E4%BA%86"><span class="nav-number">4.2.</span> <span class="nav-text">其次，下面的改进与分析则更加巧妙，感觉也体现了研究的一个常用思路。作者发现，用这个指标，可以找到85.6%的key token，其实效果已经还不错了，但作者希望能更好。因此他分析了那些被错认为key token的token（也就是那些并非key token，但是LPG值却挺大的token）的预测概率，发现它们在截断前，本身被预测的概率就比较小。即，它们本身就是比较难被预测的token（就是说，截断前后其实这些token都比较难以预测。可能是因为预测它们所需要的上下文，在这整一段的long context里就压根没有，所以相对于这一段long context而言，这个token并不是key token；也可以类比一下，人在进行写作等创作输出的活动的时候，的确有时候会毫无逻辑地、或者说逻辑跳跃地太大以至于显得天马行空的想到某个事物。这种内容，喂给llm，它就不太能基于上下文预测到了。解释其实可以有很多，但这并不是重点，因为这里主要是基于统计观察发现了一个事实规律，并且这个规律可以辅助我们改进上面的LPG指标）。基于这个观察，我们可以把上面的指标修正一下：一个token被认为是key token，不仅要它的LPG指标够大，而且它在截断前出现的概率也不能太小（这里就需要再另行设定一个threshold了，但实验一下就可以得到。而且后面作者也说了，实验发现这个threshold还是比较鲁棒的，在一套数据集上实验得到的threshold可以迁移到其它的数据集上，效果也不差）最终用改进后的指标，作者实验发现可以找到98.2%的key token了。这个准确率就比较令人满意了</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%A9%E7%94%A8%E6%94%B9%E8%BF%9B%E5%90%8E%E7%9A%84%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E5%AE%9E%E9%AA%8C%EF%BC%8C%E5%8F%91%E7%8E%B0%E5%AE%83%E5%B0%B1%E5%92%8Clong-context-benchmark%E4%B8%8A%E7%9A%84%E5%BE%97%E5%88%86%E6%9C%89%E6%AF%94%E8%BE%83%E5%A5%BD%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E4%BA%86%EF%BC%8C%E8%BF%99%E8%A1%A8%E6%98%8E%E6%96%B0%E7%9A%84%E6%8C%87%E6%A0%87%EF%BC%88%E7%A7%B0%E4%B8%BALongPPL%EF%BC%89%E7%9A%84%E7%A1%AE%E8%83%BD%E6%9B%B4%E5%A5%BD%E5%9C%B0%E5%8F%8D%E6%98%A0llm%E7%9A%84%E9%95%BF%E6%96%87%E6%9C%AC%E8%83%BD%E5%8A%9B%E3%80%82%E5%B9%B6%E4%B8%94%E8%BF%99%E5%A5%97%E6%8C%87%E6%A0%87%E5%AE%83%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E5%88%A9%E7%94%A8%E8%87%AA%E7%84%B6%E6%95%B0%E6%8D%AE%EF%BC%88real-world%EF%BC%89%EF%BC%8C%E4%B9%9F%E4%B8%8D%E9%9C%80%E8%A6%81%E6%A0%B9%E6%8D%AE%E4%B8%8D%E5%90%8Cdomain%E8%AE%BE%E8%AE%A1%E4%B8%8D%E5%90%8C%E7%9A%84benchmark%EF%BC%88adaptive%EF%BC%89%EF%BC%88%E5%9B%A0%E4%B8%BA%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84domain%EF%BC%8C%E4%BE%8B%E5%A6%82coding%EF%BC%8Cmath%EF%BC%8C%E5%85%B6%E5%AE%9E%E9%83%BD%E5%8F%AF%E4%BB%A5%E7%94%A8%E8%BF%99%E5%A5%97%E6%96%B9%E6%B3%95%E6%89%BE%E5%87%BAkey-token%EF%BC%89%EF%BC%8C%E5%90%8C%E6%97%B6%E6%9B%B4%E5%8A%A0%E8%87%AA%E5%8A%A8%EF%BC%8C%E5%87%8F%E5%B0%91%E4%BA%86%E4%BA%BA%E4%B8%BA%E7%9A%84%E5%B9%B2%E9%A2%84%E5%A6%82%E4%BB%8A%E8%BF%99%E4%B8%AA%E6%8C%87%E6%A0%87%E4%B9%9F%E5%BC%80%E5%A7%8B%E8%A2%AB%E9%80%90%E6%AD%A5%E5%9C%B0%E4%BD%BF%E7%94%A8%E4%BA%86"><span class="nav-number">5.</span> <span class="nav-text">利用改进后的指标进行实验，发现它就和long-context benchmark上的得分有比较好的相关性了，这表明新的指标（称为LongPPL）的确能更好地反映llm的长文本能力。并且这套指标它可以直接利用自然数据（real-world），也不需要根据不同domain设计不同的benchmark（adaptive）（因为在不同的domain，例如coding，math，其实都可以用这套方法找出key token），同时更加自动，减少了人为的干预如今这个指标也开始被逐步地使用了</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%80%8C%E6%97%A2%E7%84%B6%E5%B7%B2%E7%BB%8F%E6%94%B9%E8%BF%9B%E4%BA%86%E8%AE%AD%E7%BB%83%E7%9A%84%E7%9B%AE%E6%A0%87%EF%BC%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E6%94%B9%E8%BF%9B%E8%AE%AD%E7%BB%83%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E7%9B%AE%E6%A0%87%E6%98%AF%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E5%AF%B9key-tokens%E7%9A%84%E9%A2%84%E6%B5%8B%E7%9A%84%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%88%E5%9B%A0%E4%B8%BA%E8%BF%99%E4%BD%93%E7%8E%B0%E4%BA%86llm%E7%9A%84long-context%E8%83%BD%E5%8A%9B%E7%9A%84%E6%8F%90%E5%8D%87%EF%BC%89%EF%BC%8C%E5%9B%A0%E6%AD%A4%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%EF%BC%8C%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%8A%A0%E5%A4%A7key-tokens%E7%9A%84%E6%9D%83%E9%87%8D%E3%80%82%E9%82%A3%E4%B9%88%E4%B8%80%E4%B8%AA%E8%87%AA%E7%84%B6%E4%BD%86%E6%9C%89%E4%BA%9B%E6%9E%81%E7%AB%AF%E7%9A%84%E6%83%B3%E6%B3%95%E6%98%AF%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AA%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8Bkey-tokens%E7%9A%84loss%EF%BC%88%E6%AF%94%E5%A6%82%E7%94%A8Cross-Entropy%EF%BC%89%E3%80%82%E7%84%B6%E8%80%8C%E8%BF%99%E6%A0%B7%E5%B0%B1%E7%9B%B8%E5%BD%93%E4%BA%8E%E6%8A%8A%E5%85%B6%E5%AE%83tokens%E7%9A%84%E6%9D%83%E9%87%8D%E7%BD%AE%E4%B8%BA0%E4%BA%86%E3%80%82%E6%9C%89%E5%8F%AF%E8%83%BD%E5%8F%91%E7%94%9F%E7%9A%84%E4%BA%8B%E6%83%85%E6%98%AFllm%E8%AE%AD%E5%B4%A9%E4%BA%86%EF%BC%8C%E5%9B%A0%E4%B8%BA%E8%BF%99%E6%A0%B7%E6%9C%89%E5%A4%B1%E5%81%8F%E9%A2%87%EF%BC%8C%E6%8A%8A%E5%9F%BA%E6%9C%AC%E7%9A%84%E7%9B%AE%E6%A0%87%E9%83%BD%E7%BB%99%E4%B8%A2%E4%BA%86%EF%BC%8C%E5%8F%AF%E8%83%BD%E4%BC%9A%E8%AE%A9llm%E5%8F%91%E7%94%9F%E7%B1%BB%E4%BC%BC%E9%81%97%E5%BF%98%E7%9A%84%E4%BA%8B%E6%83%85%EF%BC%88%E6%88%91%E4%BB%AC%E5%BD%93%E7%84%B6%E6%98%AF%E5%B8%8C%E6%9C%9Bllm%E5%9C%A8%E4%BF%9D%E8%AF%81%E6%AD%A3%E5%B8%B8%E7%9A%84%E7%94%9F%E6%88%90%E8%83%BD%E5%8A%9B%E7%9A%84%E5%89%8D%E6%8F%90%E4%B8%8B%E6%8F%90%E9%AB%98%E9%95%BF%E6%96%87%E6%9C%AC%E8%83%BD%E5%8A%9B%E7%9A%84%EF%BC%89%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%EF%BC%8C%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1%E7%9A%84%E6%97%B6%E5%80%99%E6%8F%90%E9%AB%98key-tokens%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%8C%E9%80%82%E5%BD%93%E9%99%8D%E4%BD%8E%E5%85%B6%E5%AE%83tokens%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%8C%E4%BD%86%E4%B8%8D%E8%A6%81%E7%9B%B4%E6%8E%A5%E7%BD%AE%E4%B8%BA0%E3%80%82%E7%94%B1%E6%AD%A4%E5%BE%97%E5%88%B0%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%B0%B1%E6%98%AF%E4%BD%9C%E8%80%85%E6%8F%90%E5%87%BA%E7%9A%84LongCE%E3%80%82%E4%B8%80%E8%88%AC%E7%9A%84CE%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F%E4%B8%BA%EF%BC%9A-CE-x-theta-frac-1-n-sum-i-1-n-log-P-theta-x-i-mathbf-x"><span class="nav-number">6.</span> <span class="nav-text">而既然已经改进了训练的目标，对应的可以考虑改进训练的损失函数。我们的目标是提高模型对key tokens的预测的准确率（因为这体现了llm的long-context能力的提升），因此可以考虑，在计算损失的时候，加大key tokens的权重。那么一个自然但有些极端的想法是，我们只计算预测key tokens的loss（比如用Cross Entropy）。然而这样就相当于把其它tokens的权重置为0了。有可能发生的事情是llm训崩了，因为这样有失偏颇，把基本的目标都给丢了，可能会让llm发生类似遗忘的事情（我们当然是希望llm在保证正常的生成能力的前提下提高长文本能力的）。因此，可以考虑，计算损失的时候提高key tokens的权重，适当降低其它tokens的权重，但不要直接置为0。由此得到的损失就是作者提出的LongCE。一般的CE计算公式为：$$CE(x;\theta)&#x3D;-\frac{1}{n}\sum_{i&#x3D;1}^{n}log\ P_{\theta}(x_{i}|\mathbf{x}{&lt;i})$$对应的，LongCE的计算公式为：$$LongCE(x;\theta)&#x3D;-\frac{1}{n}\sum{i&#x3D;1}^{n}I_{soft}(x_{i};\theta)log\ P_{\theta}(x_{i}|\mathbf{x}{&lt;i})$$粗略地看，也就是多了一个逐token的权重项。而这个权重项的表达式为：$$I{soft}(x_{i};\theta)&#x3D;\min{(exp(LSD_{\theta}(x_{i})), \gamma)}&#x3D;\min{(\frac{P_{\theta}(x_{i}|l_{i})}{P_{\theta}(x_{i}|s_{i})}, \gamma)}$$</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%8C%E9%9D%A2%E7%9A%84-P-theta-x-i-l-i-%E6%98%AF%E6%8C%87%E5%9C%A8token-x-i-%E5%AF%B9%E5%BA%94%E7%9A%84long-context%E4%B8%8B%EF%BC%8C%E9%A2%84%E6%B5%8B%E5%87%BAtoken-x-i-%E7%9A%84%E6%A6%82%E7%8E%87%EF%BC%9B%E8%80%8C-P-theta-x-i-s-i-%E5%88%99%E6%98%AF%E6%8C%87%E5%9C%A8token-x-i-%E5%AF%B9%E5%BA%94%E7%9A%84short-context%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AFlong-context%E6%88%AA%E6%96%AD%E4%B9%8B%E5%90%8E%E7%9A%84context%EF%BC%89%E4%B8%8B%EF%BC%8C%E9%A2%84%E6%B5%8B%E5%87%BAtoken-x-i-%E7%9A%84%E6%A6%82%E7%8E%87%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E5%BD%93%E8%BF%99%E4%B8%AA%E6%AF%94%E5%80%BC%E9%A1%B9%E5%BE%88%E5%A4%A7%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%AF%B4%E8%BF%99%E4%B8%AAtoken%E7%94%A8%E6%88%AA%E6%96%AD%E5%90%8E%E7%9A%84context%E9%A2%84%E6%B5%8B%EF%BC%8C%E6%A6%82%E7%8E%87%E5%A4%A7%E5%A4%A7%E4%B8%8B%E9%99%8D%EF%BC%88%E8%BF%99%E8%A1%A8%E6%98%8E%E8%BF%99%E4%B8%AAtoken%E5%BE%88%E5%8F%AF%E8%83%BD%E6%98%AFkey-token%EF%BC%89%EF%BC%8C%E5%9B%A0%E6%AD%A4%E6%88%91%E4%BB%AC%E7%BB%99%E5%AE%83%E8%BE%83%E5%A4%A7%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%88%E5%BD%93%E6%AF%94%E5%80%BC%E9%A1%B9%E8%BE%83%E5%A4%A7%EF%BC%8C%E5%AE%83%E5%BA%94%E8%AF%A5%E4%BC%9A%E5%A4%A7%E4%BA%8E%E6%88%91%E4%BB%AC%E7%9A%84%E9%98%88%E5%80%BC-gamma-%EF%BC%8C%E6%89%80%E4%BB%A5%E5%8F%96min%E4%B9%8B%E5%90%8E%E6%9D%83%E9%87%8D%E5%B0%B1%E6%98%AF-gamma-%EF%BC%9B%E8%80%8C%E5%A6%82%E6%9E%9C%E6%AF%94%E5%80%BC%E9%A1%B9%E8%BE%83%E5%B0%8F%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%B0%8F%E4%BA%8E%E9%98%88%E5%80%BC-gamma-%EF%BC%89%EF%BC%8C%E5%88%99%E6%9D%83%E9%87%8D%E4%BC%9A%E5%8F%96%E4%B8%BA%E6%AF%94%E5%80%BC%E9%A1%B9%E3%80%82%E5%9B%A0%E6%AD%A4%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E8%99%BD%E7%84%B6%E5%AF%B9%E4%BA%8Ekey-token%E4%BC%9A%E5%8F%96%E8%BE%83%E5%A4%A7%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%8C%E4%BD%86%E4%B9%9F%E6%98%AF%E6%9C%89%E4%B8%8A%E9%99%90%E7%9A%84%EF%BC%8C%E8%80%8C%E4%B8%8D%E4%BC%9A%E4%BB%BB%E7%94%B1%E5%AE%83%E5%A4%A7%E4%B8%8B%E5%8E%BB%E3%80%82%E8%BF%99%E4%B8%AA%EF%BC%8C%E4%B8%AA%E4%BA%BA%E6%8E%A8%E6%B5%8B%EF%BC%8C%E5%8F%AF%E8%83%BD%E6%98%AF%E5%87%BA%E4%BA%8E%E7%BB%8F%E9%AA%8C%E8%80%83%E8%99%91%E7%9A%84%EF%BC%8C%E4%B9%9F%E5%8F%AF%E8%83%BD%E6%98%AF%E7%BB%8F%E8%BF%87%E5%AE%9E%E9%AA%8C%E5%8F%91%E7%8E%B0%E8%BF%98%E6%98%AF%E5%8A%A0%E4%B8%8A%E9%98%88%E5%80%BC%E4%B8%8A%E9%99%90%E5%90%8E%E8%AE%AD%E7%BB%83%E6%95%88%E6%9E%9C%E6%9B%B4%E5%A5%BD"><span class="nav-number">6.1.</span> <span class="nav-text">里面的$P_{\theta}(x_{i}|l_{i})$是指在token $x_{i}$对应的long context下，预测出token $x_{i}$的概率；而$P_{\theta}(x_{i}|s_{i})$则是指在token $x_{i}$对应的short context（也就是long context截断之后的context）下，预测出token $x_{i}$的概率。因此，当这个比值项很大的时候，就是说这个token用截断后的context预测，概率大大下降（这表明这个token很可能是key token），因此我们给它较大的权重（当比值项较大，它应该会大于我们的阈值$\gamma$，所以取min之后权重就是$\gamma$；而如果比值项较小（也就是小于阈值$\gamma$），则权重会取为比值项。因此我们可以看到，虽然对于key token会取较大的权重，但也是有上限的，而不会任由它大下去。这个，个人推测，可能是出于经验考虑的，也可能是经过实验发现还是加上阈值上限后训练效果更好</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%B9LongCE%E7%9A%84%E5%85%B6%E5%AE%83%E7%90%86%E8%A7%A3%E8%A7%92%E5%BA%A6%EF%BC%9A"><span class="nav-number">7.</span> <span class="nav-text">对LongCE的其它理解角度：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E8%A7%86%E4%B8%BAEM%E7%AE%97%E6%B3%95%E7%9A%84%E4%B8%80%E4%B8%AA%E5%AE%9E%E4%BE%8B"><span class="nav-number">7.1.</span> <span class="nav-text">1. 视为EM算法的一个实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E5%85%88%E7%94%A8llm%E5%88%A4%E6%96%AD%E5%90%84%E4%B8%AAtoken%E6%98%AF%E5%90%A6%E4%B8%BAkey-token%EF%BC%88%E6%88%96%E8%80%85%E8%AF%B4%E5%88%A4%E6%96%AD%E5%90%84%E4%B8%AAtoken%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E3%80%82%E5%90%84token%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E6%98%AF%E6%9C%AA%E7%9F%A5%E7%9A%84%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AFEM%E7%AE%97%E6%B3%95%E4%B8%AD%E7%9A%84latent-variable%EF%BC%89%E3%80%82%E8%BF%99%E5%B0%B1%E6%98%AFEM%E7%AE%97%E6%B3%95%E4%B8%AD%E7%9A%84E%E6%AD%A5"><span class="nav-number">7.1.1.</span> <span class="nav-text">我们先用llm判断各个token是否为key token（或者说判断各个token的重要性。各token的重要性是未知的，也就是EM算法中的latent variable）。这就是EM算法中的E步</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B9%8B%E5%90%8E%E6%88%91%E4%BB%AC%E5%9F%BA%E4%BA%8E%E9%A2%84%E6%B5%8B%E5%87%BA%E7%9A%84token%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%EF%BC%8C%E8%AE%AD%E7%BB%83llm%EF%BC%8C%E8%AE%A9%E5%AE%83%E9%A2%84%E6%B5%8Btoken%E3%80%82%E8%BF%99%E5%B0%B1%E6%98%AFM%E6%AD%A5"><span class="nav-number">7.1.2.</span> <span class="nav-text">之后我们基于预测出的token的重要性，训练llm，让它预测token。这就是M步</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%A4%8D%E8%BF%AD%E4%BB%A3%E4%B8%8A%E9%9D%A2%E7%9A%84EM%E6%AD%A5%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E3%80%82%E8%BF%99%E4%B8%AA%E8%BF%87%E7%A8%8B%E4%B9%9F%E6%9C%89%E7%82%B9%E5%83%8Fbootstrap%EF%BC%9A%E5%85%88%E4%BE%9D%E8%B5%96%E4%BA%8Ellm%E5%B7%B2%E6%9C%89%E7%9A%84long-context%E8%83%BD%E5%8A%9B%EF%BC%8C%E5%88%A4%E6%96%ADtoken%E9%87%8D%E8%A6%81%E6%80%A7%EF%BC%9B%E7%84%B6%E5%90%8E%E5%9F%BA%E4%BA%8E%E5%88%A4%E6%96%AD%E5%87%BA%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%EF%BC%8C%E5%86%8D%E8%AE%AD%E7%BB%83llm%EF%BC%8C%E6%8F%90%E9%AB%98%E5%85%B6long-context%E8%83%BD%E5%8A%9B%EF%BC%8C%E5%8F%8D%E5%A4%8D%E5%BE%AA%E7%8E%AF"><span class="nav-number">7.1.3.</span> <span class="nav-text">反复迭代上面的EM步，就是训练过程。这个过程也有点像bootstrap：先依赖于llm已有的long-context能力，判断token重要性；然后基于判断出的重要性，再训练llm，提高其long-context能力，反复循环</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%9C%8B%E6%88%90%E6%98%AF%E5%88%A9%E7%94%A8%E4%BA%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%8C%E5%8F%AA%E4%B8%8D%E8%BF%87%E6%98%AFself-reward%EF%BC%8C%E4%B8%94%E6%9B%B4%E5%8A%A0%E7%BB%86%E7%B2%92%E5%BA%A6%EF%BC%88%E4%BD%86%E8%BF%99%E4%B8%AA%E6%B2%A1%E5%A4%AA%E7%90%86%E8%A7%A3%E2%80%A6%E2%80%A6%EF%BC%89"><span class="nav-number">7.2.</span> <span class="nav-text">2. 看成是利用了强化学习算法，只不过是self-reward，且更加细粒度（但这个没太理解……）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LongCE%E4%B9%9F%E5%BE%97%E5%88%B0%E4%BA%86%E5%85%B6%E5%AE%83%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BC%95%E7%94%A8%E5%92%8C%E4%BD%BF%E7%94%A8%EF%BC%8C%E5%9C%A8%E5%85%B6%E5%AE%83%E7%9A%84%E5%B7%A5%E4%BD%9C%E4%B8%AD%E7%94%9A%E8%87%B3%E6%98%AF%E9%9D%9E%E5%B8%B8%E5%85%B3%E9%94%AE%E7%9A%84"><span class="nav-number">8.</span> <span class="nav-text">LongCE也得到了其它工作的引用和使用，在其它的工作中甚至是非常关键的</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%BB%86%E8%8A%82%E6%98%AF%EF%BC%8C%E6%A0%B9%E6%8D%AE%E4%BD%9C%E8%80%85%E5%9C%A8%E8%AE%B2%E5%BA%A7%E4%B8%AD%E5%BC%95%E7%94%A8%E7%9A%84%E7%BB%93%E6%9E%9C%EF%BC%8C%E5%8F%91%E7%8E%B0%EF%BC%8C%E5%AF%B9%E4%BA%8Ehybrid-model%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%8A%8Alinear-attn%E5%92%8C%E4%B8%80%E8%88%AC%E7%9A%84%E5%B9%B3%E6%96%B9attn%E6%B7%B7%E5%90%88%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%89%EF%BC%8CLongCE%E7%9A%84%E6%8F%90%E5%8D%87%E6%95%88%E6%9E%9C%E5%B0%A4%E4%B8%BA%E6%98%BE%E8%91%97%E3%80%82%E4%BD%9C%E8%80%85%E6%8F%90%E5%87%BA%E4%B8%80%E4%B8%AA%E7%8C%9C%E6%B5%8B%EF%BC%8C%E8%BF%99%E5%8F%AF%E8%83%BD%E6%98%AF%E5%9B%A0%E4%B8%BA%E8%BF%99%E7%B1%BBefficient-llm%EF%BC%8C%E5%AE%83%E4%BB%AC%E6%9C%AC%E8%BA%AB%E7%9A%84%E5%AE%B9%E9%87%8F%E7%9B%B8%E6%AF%94%E4%BA%8E%E6%AD%A3%E5%B8%B8llm%EF%BC%8C%E8%A6%81%E6%9B%B4%E5%B0%8F%EF%BC%8C%E5%9B%A0%E6%AD%A4%E8%AE%AD%E7%BB%83%E5%AE%83%E4%BB%AC%E7%9A%84%E6%97%B6%E5%80%99%E6%9B%B4%E5%8A%A0%E9%9C%80%E8%A6%81%E2%80%9D%E5%B9%B2%E5%87%80%E2%80%9C%E7%9A%84%E4%BF%A1%E5%8F%B7%EF%BC%88%E4%B9%9F%E5%8D%B3less-noisy-signal%EF%BC%89%E3%80%82%E6%AD%A3%E5%A5%BD%EF%BC%8CLongCE%E8%BF%99%E4%B8%80%E5%A5%97%E5%91%A2%EF%BC%8C%E5%AE%83%E9%80%9A%E8%BF%87%E5%AF%B9token%E7%9A%84reweighting%EF%BC%8C%E8%AE%A9llm%E6%9B%B4%E5%8A%A0%E5%85%B3%E6%B3%A8key-token%EF%BC%8C%E6%8A%8A%E7%AE%97%E5%8A%9B%E6%9B%B4%E5%A4%9A%E5%9C%B0%E7%94%A8%E5%88%B0key-token%E4%B8%8A%EF%BC%8C%E8%BF%99%E6%9C%AC%E8%BA%AB%E5%B0%B1%E5%8F%AF%E4%BB%A5%E8%A7%86%E4%B8%BA%E5%AF%B9signal%E7%9A%84%E4%B8%80%E7%A7%8D%E2%80%9D%E6%B8%85%E6%B4%81%E2%80%9C%EF%BC%8C%E6%88%96%E8%80%85%E2%80%9D%E6%8F%90%E7%BA%AF%E2%80%9C%EF%BC%8C%E5%9B%A0%E8%80%8C%E6%8A%8A%E7%9C%9F%E6%AD%A3%E9%87%8D%E8%A6%81%E7%9A%84learning-signal%E7%BB%99%E5%89%96%E6%9E%90%E5%87%BA%E6%9D%A5%E4%BA%86%EF%BC%8C%E6%89%80%E4%BB%A5%E6%9C%89%E5%8A%A9%E4%BA%8E%E8%AE%AD%E7%BB%83"><span class="nav-number">8.1.</span> <span class="nav-text">一个小细节是，根据作者在讲座中引用的结果，发现，对于hybrid model（也就是把linear attn和一般的平方attn混合的模型），LongCE的提升效果尤为显著。作者提出一个猜测，这可能是因为这类efficient llm，它们本身的容量相比于正常llm，要更小，因此训练它们的时候更加需要”干净“的信号（也即less noisy signal）。正好，LongCE这一套呢，它通过对token的reweighting，让llm更加关注key token，把算力更多地用到key token上，这本身就可以视为对signal的一种”清洁“，或者”提纯“，因而把真正重要的learning signal给剖析出来了，所以有助于训练</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E4%B8%80%E4%B8%8B%E8%87%B3%E6%AD%A4%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%9A"><span class="nav-number">9.</span> <span class="nav-text">总结一下至此的部分：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Next-token-prediction%E7%9A%84%E8%8C%83%E5%BC%8F%EF%BC%8C%E5%A4%A9%E7%84%B6%E7%9A%84%E5%B0%B1%E6%98%AF%E6%9C%89bias%E7%9A%84%EF%BC%8C%E5%9B%A0%E4%B8%BA%E6%AF%8F%E4%B8%AAtoken%E5%AE%83%E9%83%BD%E6%98%AFequally-generated%E7%9A%84%EF%BC%8C%E7%84%B6%E8%80%8C%E5%AE%9E%E9%99%85%E5%8F%AF%E8%83%BD%E5%B9%B6%E9%9D%9E%E5%A6%82%E6%AD%A4%EF%BC%88%E4%BB%A5%E4%BA%BA%E4%B8%BA%E4%BE%8B%EF%BC%8C%E4%BA%BA%E5%9C%A8%E5%86%99%E4%B8%9C%E8%A5%BF%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%B9%B6%E9%9D%9E%E6%89%80%E6%9C%89%E5%86%85%E5%AE%B9%E9%83%BD%E6%98%AF%E4%B8%80%E6%A0%B7%E9%A1%BA%E7%95%85%E7%9A%84%E5%86%99%E4%B8%8B%E6%9D%A5%E7%9A%84%E3%80%82%E5%8F%AF%E8%83%BD%E5%9C%A8%E4%B8%80%E4%B8%AA%E5%85%B3%E9%94%AE%E7%9A%84%E5%9C%B0%E6%96%B9%EF%BC%8C%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E6%80%9D%E8%80%83%E5%BE%88%E4%B9%85%E6%89%8D%E8%83%BD%E5%86%99%E5%87%BA%E6%9D%A5%EF%BC%89"><span class="nav-number">9.1.</span> <span class="nav-text">Next-token prediction的范式，天然的就是有bias的，因为每个token它都是equally generated的，然而实际可能并非如此（以人为例，人在写东西的时候，并非所有内容都是一样顺畅的写下来的。可能在一个关键的地方，我们需要思考很久才能写出来）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%A4%E4%BA%86%E7%94%A8RL-CoT%E8%BF%9B%E8%A1%8C%E6%94%B9%E5%96%84%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%AD%A3%E4%BA%A4%E7%9A%84%E6%96%B9%E6%B3%95%E5%B0%B1%E6%98%AF%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84reweighted-focused-next-token-prediction%EF%BC%88%E4%B9%9F%E5%9B%A0%E4%B8%BA%E6%AD%A3%E4%BA%A4%EF%BC%8C%E6%89%80%E4%BB%A5%E4%BA%8C%E8%80%85%E5%BA%94%E8%AF%A5%E6%98%AF%E5%8F%AF%E4%BB%A5%E4%B8%80%E8%B5%B7%E4%BD%BF%E7%94%A8%E7%9A%84%EF%BC%89%E3%80%82%E5%AE%83%E6%9C%AC%E8%B4%A8%E6%98%AF%E5%9C%A8%E6%8F%90%E9%AB%98%E4%BF%A1%E5%99%AA%E6%AF%94"><span class="nav-number">9.2.</span> <span class="nav-text">除了用RL&#x2F;CoT进行改善，一种正交的方法就是上面提到的reweighted&#x2F;focused next-token prediction（也因为正交，所以二者应该是可以一起使用的）。它本质是在提高信噪比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#contrastive-estimate%EF%BC%88%E5%B0%B1%E6%98%AF%E4%B8%8A%E9%9D%A2%E7%9A%84%E6%9C%80%E6%9C%B4%E7%B4%A0%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9D%83%E9%87%8D%E7%9A%84%E5%BC%8F%E5%AD%90%EF%BC%8C-r-x-i-frac-P-theta-x-i-l-i-P-theta-x-i-s-i-%EF%BC%89%E6%98%AF%E4%B8%80%E7%A7%8D%E6%AF%94%E8%BE%83%E9%80%9A%E7%94%A8%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%8C%E5%AE%83%E5%8F%AF%E4%BB%A5%E9%80%82%E7%94%A8%E4%BA%8E%E5%90%84%E7%A7%8D%E7%89%B9%E5%AE%9A%E4%BB%BB%E5%8A%A1%EF%BC%8C%E7%94%A8%E4%BA%8E%E8%AF%86%E5%88%ABtask-specific-tokens"><span class="nav-number">9.3.</span> <span class="nav-text">contrastive estimate（就是上面的最朴素的计算权重的式子，$r(x_{i})&#x3D;\frac{P_{\theta}(x_{i}|l_{i})}{P_{\theta}(x_{i}|s_{i})}$）是一种比较通用的方式，它可以适用于各种特定任务，用于识别task-specific tokens</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%80%8C%E5%85%B3%E4%BA%8E-r-x-i-frac-P-theta-x-i-l-i-P-theta-x-i-s-i-%E6%9D%83%E9%87%8D%E8%BF%99%E4%B8%80%E9%A1%B9%EF%BC%8C%E6%88%91%E4%BB%AC%E8%BF%98%E5%8F%AF%E4%BB%A5%E4%BB%8E%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%9A%84%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%90%86%E8%A7%A3%E3%80%82%E5%88%86%E5%AD%90%E5%B0%B1%E7%B1%BB%E4%BC%BC%E4%BA%8Eteacher-model%EF%BC%8C%E5%AE%83%E4%BB%A3%E8%A1%A8%E4%BA%86ideal-performance%EF%BC%88%E5%9B%A0%E4%B8%BA%E6%98%AF%E5%9F%BA%E4%BA%8Elong-context%E6%9D%A5%E9%A2%84%E6%B5%8Bnext-token%EF%BC%89%EF%BC%9B%E5%88%86%E6%AF%8D%E5%88%99%E7%B1%BB%E4%BC%BC%E4%BA%8Estudent-model%EF%BC%8C%E5%AE%83%E4%BB%A3%E8%A1%A8%E4%BA%86original-performance%EF%BC%88%E5%9B%A0%E4%B8%BA%E6%98%AF%E5%9F%BA%E4%BA%8Eshort-context%E6%9D%A5%E9%A2%84%E6%B5%8Bnext-token%EF%BC%89%E3%80%82%E8%BF%99%E4%B8%AA%E6%AF%94%E5%80%BC%E8%B6%8A%E5%A4%A7%EF%BC%8C%E8%A1%A8%E6%98%8Estudent%E5%9C%A8%E9%A2%84%E6%B5%8B%E8%BF%99%E4%B8%AAtoken%E7%9A%84%E6%97%B6%E5%80%99%E6%AF%94teacher%E5%B7%AE%E4%BA%86%E8%B6%8A%E5%A4%9A%EF%BC%8C%E5%9B%A0%E6%AD%A4teacher%E6%8C%87%E5%AF%BC%E5%AE%83%EF%BC%8C%E8%A6%81%E6%9B%B4%E5%A4%9A%E5%9C%B0%E5%AD%A6%E4%B9%A0%E8%BF%99%E4%B8%AAtoken%E7%9A%84%E9%A2%84%E6%B5%8B%EF%BC%8C%E5%9B%A0%E6%AD%A4%E4%BC%9A%E6%8F%90%E9%AB%98%E8%BF%99%E4%B8%AAtoken%E7%9A%84%E6%9D%83%E9%87%8D%E3%80%82%E8%BF%99%E9%87%8C%E7%9A%84ideal-performance%E4%BB%A5%E5%8F%8Aoriginal-performance%EF%BC%8C%E4%B8%AA%E4%BA%BA%E7%8C%9C%E6%B5%8B%EF%BC%8C%E5%BA%94%E8%AF%A5%E9%83%BD%E5%8F%AA%E6%98%AF%E5%80%9F%E7%94%A8%E4%BA%86%E8%92%B8%E9%A6%8F%E9%87%8C%E7%9A%84%E4%B8%80%E4%BA%9B%E6%9C%AF%E8%AF%AD%E8%80%8C%E5%B7%B2%E5%90%A7%EF%BC%8C%E5%BA%94%E8%AF%A5%E4%B8%8D%E8%87%B3%E4%BA%8E%E8%AF%B4%E7%9C%9F%E7%9A%84%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B8%AA%E6%84%8F%E6%80%9D%E3%80%82%E5%8F%AF%E8%83%BD%E6%9B%B4%E5%A4%9A%E5%8F%AA%E6%98%AF%E8%AF%B4%E6%98%8E%E5%8F%AF%E4%BB%A5%E4%BB%8E%E8%92%B8%E9%A6%8F%E7%9A%84%E8%A7%86%E8%A7%92%E5%8E%BB%E7%9C%8B%E5%BE%85"><span class="nav-number">9.3.1.</span> <span class="nav-text">而关于$r(x_{i})&#x3D;\frac{P_{\theta}(x_{i}|l_{i})}{P_{\theta}(x_{i}|s_{i})}$权重这一项，我们还可以从知识蒸馏的角度来理解。分子就类似于teacher model，它代表了ideal performance（因为是基于long-context来预测next token）；分母则类似于student model，它代表了original performance（因为是基于short-context来预测next token）。这个比值越大，表明student在预测这个token的时候比teacher差了越多，因此teacher指导它，要更多地学习这个token的预测，因此会提高这个token的权重。这里的ideal performance以及original performance，个人猜测，应该都只是借用了蒸馏里的一些术语而已吧，应该不至于说真的就是这个意思。可能更多只是说明可以从蒸馏的视角去看待</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%8B%E9%9D%A2%E6%9D%A5%E8%AE%B2%E8%AE%B2%E5%85%B3%E4%BA%8ETransformer%E6%9E%B6%E6%9E%84%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E3%80%82%E8%BF%99%E4%B8%AA%E8%AE%B2%E5%BA%A7%E9%87%8C%E4%B8%BB%E8%A6%81%E5%85%B3%E6%B3%A8%E7%9A%84%E6%98%AFposition-bias%E7%9A%84%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="nav-number">10.</span> <span class="nav-text">下面来讲讲关于Transformer架构的一些问题。这个讲座里主要关注的是position bias的一个问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%80%E8%B0%93position-bias%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E5%AE%83%E7%9A%84%E8%A1%A8%E7%8E%B0%E6%9C%89%E5%BE%88%E5%A4%9A%EF%BC%9A"><span class="nav-number">10.1.</span> <span class="nav-text">所谓position bias的问题，它的表现有很多：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#attention-sink%E7%8E%B0%E8%B1%A1%EF%BC%88%E5%A4%A7%E4%BD%93%E4%B8%8A%E8%AF%B4%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%BD%93%E6%88%91%E4%BB%AC%E5%8F%AF%E8%A7%86%E5%8C%96%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9D%83%E9%87%8D%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E4%BC%9A%E5%8F%91%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%E7%9A%84%E6%9D%83%E9%87%8D%E4%BC%BC%E4%B9%8E%E5%B0%A4%E5%85%B6%E5%A4%A7%EF%BC%8C%E7%84%B6%E5%90%8E%E4%B8%AD%E9%97%B4%E7%9A%84token%E7%9A%84%E6%9D%83%E9%87%8D%E9%83%BD%E6%AF%94%E8%BE%83%E5%B0%8F%EF%BC%8C%E4%BD%86%E6%9C%80%E5%90%8E%E7%9A%84token%E6%9D%83%E9%87%8D%E5%8F%88%E4%BC%9A%E5%A4%A7%E4%B8%80%E4%BA%9B%EF%BC%88%E4%B8%8D%E8%BF%87%E6%B2%A1%E6%9C%89%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%E7%9A%84%E5%A4%A7%EF%BC%89"><span class="nav-number">10.1.1.</span> <span class="nav-text">attention sink现象（大体上说，就是当我们可视化注意力权重的时候，会发现第一个token的权重似乎尤其大，然后中间的token的权重都比较小，但最后的token权重又会大一些（不过没有第一个token的大）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8in-context-learning%E4%B8%AD%EF%BC%8C%E7%90%86%E8%AE%BA%E4%B8%8A%E8%AE%B2%EF%BC%8C%E6%88%91%E4%BB%ACprompt%E9%87%8C%E7%BB%99%E7%9A%84%E7%A4%BA%E4%BE%8B%EF%BC%8C%E5%8D%B3%E4%BD%BF%E8%B0%83%E6%8D%A2%E4%BA%86%E9%A1%BA%E5%BA%8F%EF%BC%8C%E4%B9%9F%E4%B8%8D%E5%BD%B1%E5%93%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%9B%E4%BD%86%E5%AE%9E%E9%99%85%E4%B8%8A%EF%BC%8C%E5%BD%93%E6%88%91%E4%BB%AC%E8%B0%83%E6%8D%A2prompt%E9%87%8C%E7%A4%BA%E4%BE%8B%E7%9A%84%E9%A1%BA%E5%BA%8F%EF%BC%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BE%93%E5%87%BA%E4%BC%9A%E6%9C%89%E8%BE%83%E5%A4%A7%E7%9A%84%E5%8F%98%E5%8C%96%E3%80%82%E8%BF%99%E5%B0%B1%E6%98%AF%E4%B8%80%E7%A7%8Dbias%EF%BC%88%E5%AE%83%E6%9C%AC%E5%BA%94%E6%B2%A1%E6%9C%89%E7%9A%84%EF%BC%89"><span class="nav-number">10.1.2.</span> <span class="nav-text">在in-context learning中，理论上讲，我们prompt里给的示例，即使调换了顺序，也不影响模型的学习；但实际上，当我们调换prompt里示例的顺序，模型的输出会有较大的变化。这就是一种bias（它本应没有的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9prompts%E7%9A%84%E6%A0%BC%E5%BC%8F%E4%B9%9F%E5%BE%88%E6%95%8F%E6%84%9F%EF%BC%88%E6%9C%AC%E8%B4%A8%E4%B8%8A%EF%BC%8C%E5%A6%82%E6%9E%9C%E4%BF%A1%E6%81%AF%E6%98%AF%E5%85%A8%E7%9A%84%EF%BC%8C%E9%82%A3%E4%B9%88%E6%8C%89%E7%85%A7%E4%BB%80%E4%B9%88%E6%A0%BC%E5%BC%8F%E7%BB%99llm%EF%BC%8C%E5%BA%94%E8%AF%A5%E5%BD%B1%E5%93%8D%E4%B9%9F%E4%B8%8D%E5%A4%A7%E3%80%82%E4%BD%86%E4%B8%80%E4%B8%AA%E5%A5%BD%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%AF%B9%E6%95%88%E6%9E%9C%E7%9A%84%E6%8F%90%E5%8D%87%E8%BF%98%E6%98%AF%E6%9C%89%E5%BD%B1%E5%93%8D%E7%9A%84%EF%BC%89"><span class="nav-number">10.1.3.</span> <span class="nav-text">对prompts的格式也很敏感（本质上，如果信息是全的，那么按照什么格式给llm，应该影响也不大。但一个好的格式对效果的提升还是有影响的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lost-in-the-middle%E7%8E%B0%E8%B1%A1%EF%BC%8C%E5%8D%B3%E8%BF%9B%E8%A1%8C%E5%A4%A7%E6%B5%B7%E6%8D%9E%E9%92%88%E5%AE%9E%E9%AA%8C%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%8F%91%E7%8E%B0%EF%BC%8C%E5%BD%93needle%EF%BC%88%E5%8D%B3%E5%85%B3%E9%94%AE%E4%BF%A1%E6%81%AF%EF%BC%89%E5%87%BA%E7%8E%B0%E5%9C%A8%E4%B8%AD%E9%83%A8%E7%9A%84%E4%BD%8D%E7%BD%AE%E6%97%B6%EF%BC%8C%E5%9B%9E%E7%AD%94%E7%9A%84%E5%87%86%E7%A1%AE%E7%8E%87%E4%BC%9A%E6%AF%94%E8%BE%83%E5%B7%AE%EF%BC%9B%E8%80%8C%E5%A6%82%E6%9E%9Cneedle%E5%87%BA%E7%8E%B0%E5%9C%A8%E5%BC%80%E5%A4%B4%EF%BC%8C%E6%95%88%E6%9E%9C%E4%B8%80%E8%88%AC%E6%98%AF%E6%9C%80%E5%A5%BD%E7%9A%84%EF%BC%9B%E5%87%BA%E7%8E%B0%E5%9C%A8%E7%BB%93%E5%B0%BE%E9%99%84%E8%BF%91%EF%BC%8C%E6%95%88%E6%9E%9C%E4%B9%9F%E4%BC%9A%E6%AF%94%E4%B8%AD%E9%97%B4%E7%9A%84%E5%A5%BD"><span class="nav-number">10.1.4.</span> <span class="nav-text">lost in the middle现象，即进行大海捞针实验的时候，发现，当needle（即关键信息）出现在中部的位置时，回答的准确率会比较差；而如果needle出现在开头，效果一般是最好的；出现在结尾附近，效果也会比中间的好</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E7%A7%8Dbias%EF%BC%8C%E4%B9%8D%E4%B8%80%E7%9C%8B%E6%98%AF%E4%B8%8D%E5%A5%BD%E7%9A%84%E3%80%82%E4%BD%86%E5%85%B6%E5%AE%9E%E4%B9%9F%E6%9C%AA%E5%BF%85%EF%BC%8C%E4%B9%9F%E6%9C%89%E4%BA%BA%E5%A3%B0%E7%A7%B0%E8%BF%99%E5%8F%AA%E6%98%AFllm%E5%9C%A8%E5%AD%A6%E4%B9%A0%E4%BA%BA%E7%B1%BB%E7%9A%84%E8%A1%8C%E4%B8%BA%E8%80%8C%E5%B7%B2%EF%BC%88%E5%9B%A0%E4%B8%BA%E5%8F%AF%E8%83%BD%E4%BA%BA%E7%B1%BB%E7%9A%84%E8%AF%AD%E6%96%99%E9%87%8C%E4%B9%9F%E6%98%AF%EF%BC%8C%E9%87%8D%E8%A6%81%E4%BF%A1%E6%81%AF%E4%B8%80%E8%88%AC%E5%9C%A8%E5%BC%80%E5%A4%B4%E6%88%96%E8%80%85%E7%BB%93%E5%B0%BE%EF%BC%89%E3%80%82%E4%BD%86%E6%9B%B4%E9%87%8D%E8%A6%81%E7%9A%84%E6%98%AF%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AF%B9%E8%BF%99%E7%A7%8Dbias%E7%9A%84%E6%88%90%E5%9B%A0%E6%B2%A1%E6%9C%89%E5%A4%AA%E5%A4%9A%E4%BA%86%E8%A7%A3%EF%BC%8C%E4%B9%9F%E4%B8%8D%E7%9F%A5%E9%81%93%E6%98%AF%E4%B8%8D%E6%98%AF%E5%9B%A0%E4%B8%BAtransformer%E8%BF%99%E4%B8%AA%E6%9E%B6%E6%9E%84%E5%AF%BC%E8%87%B4%E7%9A%84%E3%80%82%E5%A6%82%E6%9E%9C%E8%83%BD%E6%90%9E%E6%87%82%E8%BF%99%E7%A7%8Dbias%E7%9A%84%E6%88%90%E5%9B%A0%EF%BC%8C%E5%B0%B1%E6%9C%89%E5%8F%AF%E8%83%BD%E6%9B%B4%E5%A5%BD%E5%9C%B0%E8%AE%BE%E8%AE%A1%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%90%8C%E6%97%B6%E4%B9%9F%E8%83%BD%E6%94%B9%E8%BF%9B%E7%8E%B0%E6%9C%89%E7%9A%84PE%EF%BC%88position-embedding%EF%BC%89%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E7%B2%97%E6%B5%85%E5%9C%B0%E5%BD%92%E7%BA%B3%E6%88%90%E5%9B%A0%EF%BC%8C%E6%98%BE%E7%84%B6%E7%A6%BB%E4%B8%8D%E5%BC%80%EF%BC%9A%E6%95%B0%E6%8D%AE%E3%80%81%E6%9E%B6%E6%9E%84%E4%BB%A5%E5%8F%8APE%E3%80%82%E5%9B%A0%E6%AD%A4%E4%B8%8B%E9%9D%A2%E8%AE%A8%E8%AE%BA%E7%9A%84%E5%B0%B1%E6%98%AF%E8%BF%99%E5%87%A0%E4%B8%AA%E5%9B%A0%E7%B4%A0%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">10.2.</span> <span class="nav-text">这种bias，乍一看是不好的。但其实也未必，也有人声称这只是llm在学习人类的行为而已（因为可能人类的语料里也是，重要信息一般在开头或者结尾）。但更重要的是，我们对这种bias的成因没有太多了解，也不知道是不是因为transformer这个架构导致的。如果能搞懂这种bias的成因，就有可能更好地设计架构，同时也能改进现有的PE（position embedding）如果我们粗浅地归纳成因，显然离不开：数据、架构以及PE。因此下面讨论的就是这几个因素的影响</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A6%96%E5%85%88%EF%BC%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E6%8E%A7%E5%88%B6%E5%8F%98%E9%87%8F%E7%9A%84%E5%AE%9E%E9%AA%8C%E3%80%82%E8%BF%99%E4%B8%BB%E8%A6%81%E6%98%AF%E4%B8%BA%E4%BA%86%E5%88%9D%E6%AD%A5%E7%9C%8B%E7%9C%8Bposition-bias%E7%9A%84%E6%9D%A5%E6%BA%90%E3%80%82%E6%95%B0%E6%8D%AE%E6%98%AF%E7%9B%B8%E5%AF%B9%E6%9C%80%E5%A5%BD%E6%8E%A7%E5%88%B6%E7%9A%84%EF%BC%8C%E5%8F%AA%E9%9C%80%E8%A6%81%E6%88%91%E4%BB%AC%E7%A1%AE%E4%BF%9D%E6%89%80%E6%9C%89%E6%95%B0%E6%8D%AE%E6%98%AF%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83%E7%9A%84%EF%BC%88%E6%89%80%E4%BB%A5%E4%B8%8D%E5%AD%98%E5%9C%A8%E5%93%AA%E4%B8%AA%E6%95%B0%E6%8D%AE%E6%9B%B4%E9%87%8D%E8%A6%81%E7%9A%84%E6%83%85%E5%86%B5%EF%BC%89%E3%80%82%E8%80%8C%E6%83%B3%E7%9C%8BPE%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%8C%E5%88%99%E5%8F%AA%E5%8F%98%E5%8A%A8%E5%AE%83%EF%BC%8C%E5%85%B6%E5%AE%83%E4%B8%8D%E5%8F%98%EF%BC%88%E5%8F%AF%E4%BB%A5%E4%B8%8D%E7%94%A8PE%EF%BC%8C%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%B0%9D%E8%AF%95%E4%B8%80%E4%BA%9B%E5%85%B6%E5%AE%83%E7%9A%84%E5%B8%B8%E7%94%A8PE%EF%BC%89%E3%80%82%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%A6%82%E4%B8%8B%EF%BC%9A%E8%BF%99%E4%B8%AA%E5%9B%BE%E5%91%A2%EF%BC%8C%E6%A8%AA%E8%BD%B4%E7%94%A8%E4%BA%8E%E6%8C%87%E7%A4%BA%E6%AF%94%E8%BE%83%E4%B8%8D%E5%90%8C%E4%BD%8D%E7%BD%AE%E7%9A%84token%E7%9A%84%E5%85%B7%E4%BD%93%E6%83%85%E5%86%B5%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%8C%87%E6%98%8E%EF%BC%8C%E6%98%AF%E5%93%AA%E4%B8%AA%E4%BD%8D%E7%BD%AE%E7%9A%84token%E5%92%8C%E5%93%AA%E4%B8%AA%E4%BD%8D%E7%BD%AE%E7%9A%84token%E7%9A%84%E9%A2%84%E6%B5%8B%E6%95%88%E6%9E%9C%E8%BF%9B%E8%A1%8C%E5%AF%B9%E6%AF%94%EF%BC%89%EF%BC%8C%E7%BA%B5%E8%BD%B4%E5%88%99%E6%98%AF%E8%AF%B4%E6%98%8E%E4%BA%8C%E8%80%85%E9%A2%84%E6%B5%8B%E6%95%88%E6%9E%9C%E4%B9%8B%E5%B7%AE%E3%80%82%E4%B8%BA%E6%AD%A3%E5%88%99%E8%A1%A8%E6%98%8E%E5%89%8D%E8%80%85%E4%BC%98%E4%BA%8E%E5%90%8E%E8%80%85%EF%BC%8C%E4%B8%94%E8%B6%8A%E5%A4%A7%E8%A1%A8%E6%98%8E%E5%A5%BD%E7%9A%84%E8%B6%8A%E5%A4%9A%EF%BC%8C%E8%B4%9F%E7%9A%84%E5%88%99%E7%B1%BB%E4%BC%BC%E7%94%B1%E6%AD%A4%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E4%B8%8D%E7%AE%A1%E7%94%A8%E4%B8%8D%E7%94%A8PE%EF%BC%8C%E7%94%A8%E5%93%AA%E4%B8%AAPE%EF%BC%8C%E5%8D%B3%E4%BD%BF%E6%88%91%E4%BB%AC%E6%8E%92%E9%99%A4%E6%8E%89%E4%BA%86%E6%95%B0%E6%8D%AE%E7%9A%84bias%EF%BC%8C%E4%BE%9D%E7%84%B6%E4%BC%9A%E6%9C%89position-bias%EF%BC%8C%E4%B8%94%E4%B8%80%E4%B8%AA%E5%A4%A7%E8%87%B4%E7%9A%84%E8%B6%8B%E5%8A%BF%E6%98%AF%EF%BC%8C%E8%B6%8A%E6%B7%B1%E5%B1%82%EF%BC%8Cbias%E8%B6%8A%E4%B8%A5%E9%87%8D%E3%80%82%E8%80%8C%E4%B8%94%EF%BC%8C%E5%9B%A0%E4%B8%BA%E6%97%A0%E8%AE%BA%E6%9C%89%E6%97%A0PE%E9%83%BD%E4%BC%9A%E6%9C%89%E8%BF%99%E4%B8%AAbias%EF%BC%8C%E5%8A%A0%E4%B8%8A%E5%B7%B2%E7%BB%8F%E6%8E%A7%E5%88%B6%E4%BA%86%E6%95%B0%E6%8D%AE%E4%BA%86%EF%BC%8C%E6%89%80%E4%BB%A5%E5%8F%AF%E4%BB%A5%E8%82%AF%E5%AE%9A%EF%BC%8Ctransformer%E7%9A%84%E6%9E%B6%E6%9E%84%E4%B8%80%E5%AE%9A%E6%98%AF%E9%80%A0%E6%88%90%E4%BA%86%E5%BD%B1%E5%93%8D%E7%9A%84%E5%8F%A6%E5%A4%96%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E9%99%A4%E5%8D%B4%E5%AF%B9%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%E7%9A%84%E6%9E%81%E5%A4%A7%E7%9A%84%E5%85%B3%E6%B3%A8%EF%BC%88%E6%89%80%E8%B0%93attention-sink%E7%8E%B0%E8%B1%A1%EF%BC%89%EF%BC%8C%E5%AF%B9%E6%9C%80%E5%90%8E%E7%9A%84token%E5%85%B6%E5%AE%9E%E4%B9%9F%E6%9C%89%E4%B8%80%E5%AE%9A%E7%A8%8B%E5%BA%A6%E7%9A%84%E9%A2%9D%E5%A4%96%E5%85%B3%E6%B3%A8%EF%BC%88%E8%BF%99%E5%8F%AF%E4%BB%A5%E8%A2%AB%E7%A7%B0%E4%B8%BArecency-bias%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%AF%B9%E6%9C%80%E8%BF%91%E7%9A%84token%E6%9C%89%E4%B8%80%E5%AE%9A%E7%9A%84%E5%80%BE%E5%90%91%EF%BC%89%E3%80%82%E4%BD%86%E6%A0%B9%E6%8D%AE%E5%9B%BE%E8%A1%A8%EF%BC%8C%E5%9C%A8first-vs-last%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E4%B9%9F%E9%80%9A%E5%B8%B8%E6%98%AFfirst-token%E9%A2%84%E6%B5%8B%E7%9A%84%E6%9B%B4%E5%A5%BD%EF%BC%8C%E6%89%80%E4%BB%A5attention-sink%E6%98%AF%E6%AF%94recency-bias%E6%9B%B4%E2%80%9Cdominant%E2%80%9D%E7%9A%84%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%E6%98%AF%E6%9B%B4%E5%85%B7%E5%BD%B1%E5%93%8D%E5%8A%9B%E7%9A%84"><span class="nav-number">10.3.</span> <span class="nav-text">首先，是一个控制变量的实验。这主要是为了初步看看position bias的来源。数据是相对最好控制的，只需要我们确保所有数据是独立同分布的（所以不存在哪个数据更重要的情况）。而想看PE的影响，则只变动它，其它不变（可以不用PE，也可以尝试一些其它的常用PE）。实验结果如下：这个图呢，横轴用于指示比较不同位置的token的具体情况（也就是指明，是哪个位置的token和哪个位置的token的预测效果进行对比），纵轴则是说明二者预测效果之差。为正则表明前者优于后者，且越大表明好的越多，负的则类似由此可以看到，不管用不用PE，用哪个PE，即使我们排除掉了数据的bias，依然会有position bias，且一个大致的趋势是，越深层，bias越严重。而且，因为无论有无PE都会有这个bias，加上已经控制了数据了，所以可以肯定，transformer的架构一定是造成了影响的另外，可以看到，除却对第一个token的极大的关注（所谓attention sink现象），对最后的token其实也有一定程度的额外关注（这可以被称为recency bias，也就是对最近的token有一定的倾向）。但根据图表，在first vs. last的时候，也通常是first token预测的更好，所以attention sink是比recency bias更“dominant”的，或者说是更具影响力的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E9%9D%A2%E5%B0%B1%E9%9C%80%E8%A6%81%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%BA%9B%E7%90%86%E8%AE%BA%E5%88%86%E6%9E%90%EF%BC%8C%E6%9D%A5%E7%9C%8B%E7%9C%8B%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0%E8%BF%99%E4%BA%9B%E6%83%85%E5%86%B5%EF%BC%8C%E4%BB%A5%E5%8F%8A%E7%90%86%E8%AE%BA%E8%83%BD%E5%90%A6%E5%AF%B9%E5%BA%94%E4%B8%8A%E8%BF%99%E4%BA%9B%E6%83%85%E5%86%B5"><span class="nav-number">10.4.</span> <span class="nav-text">下面就需要进行一些理论分析，来看看为什么会出现这些情况，以及理论能否对应上这些情况</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E9%A1%BEtransformer%E6%9E%B6%E6%9E%84"><span class="nav-number">10.5.</span> <span class="nav-text">回顾transformer架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer%E6%9E%B6%E6%9E%84%E6%9C%80%E4%B8%BB%E8%A6%81%E7%9A%84%E6%9E%84%E4%BB%B6%E6%9C%89%E8%BF%99%E5%87%A0%E4%B8%AA%EF%BC%9Aattention%EF%BC%8Cmlp%EF%BC%8Cln%E7%AD%89norm%E4%BD%86mlp%E5%92%8Cnorm%EF%BC%8C%E5%85%B6%E5%AE%9E%E6%98%AF%E5%AF%B9%E6%AF%8F%E4%B8%AAtoken%E7%9A%84embedding%E9%83%BD%E4%BC%9A%E5%81%9A%E7%9A%84%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E8%BF%99%E4%B8%A4%E4%B8%AA%E6%9E%84%E4%BB%B6%E4%B8%8D%E4%BC%9A%E5%B8%A6%E6%9D%A5token%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BA%A4%E4%BA%92%EF%BC%8C%E9%82%A3%E7%90%86%E8%AE%BA%E4%B8%8A%E5%AE%83%E4%BB%AC%E4%B9%9F%E4%B8%8D%E5%BA%94%E8%AF%A5%E5%AF%BC%E8%87%B4position-bias%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E5%8F%AA%E5%8F%AF%E8%83%BD%E6%98%AFattention%E5%AF%BC%E8%87%B4%E4%BA%86position-bias%EF%BC%88%E5%9B%A0%E4%B8%BA%E8%BF%99%E9%87%8C%E4%BC%9A%E8%AE%A9token%E4%B9%8B%E9%97%B4%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92%EF%BC%89"><span class="nav-number">10.5.1.</span> <span class="nav-text">transformer架构最主要的构件有这几个：attention，mlp，ln等norm但mlp和norm，其实是对每个token的embedding都会做的，也就是说，这两个构件不会带来token之间的交互，那理论上它们也不应该导致position bias。因此，只可能是attention导致了position bias（因为这里会让token之间进行交互）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A4%E5%A4%84%E4%BD%9C%E8%80%85%E6%98%AF%E7%94%A8%E4%BA%86graph%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E3%80%82%E4%B8%94%E5%81%87%E8%AE%BE%E4%B8%8D%E7%94%A8%E4%BB%80%E4%B9%88%E5%85%B6%E5%AE%83%E7%9A%84PE%EF%BC%8C%E5%B0%B1%E7%94%A8%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84causal-mask%EF%BC%88%E5%B0%B1%E6%98%AF%E9%A2%84%E6%B5%8B%E5%BD%93%E5%89%8Dtoken%E6%97%B6%EF%BC%8C%E5%8F%AA%E6%98%AF%E7%94%A8%E5%89%8D%E9%9D%A2%E7%9A%84token%E7%9A%84%E9%82%A3%E4%B8%AAmask%EF%BC%89%E3%80%82%E5%A6%82%E6%9E%9C%E6%8A%8A%E5%AE%83%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%BA%E4%B8%80%E4%B8%AA%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%83%B3%E8%A7%81%EF%BC%8C%E5%9C%A8%E6%88%91%E4%BB%AC%E9%A2%84%E6%B5%8B%E5%90%8E%E9%9D%A2%E7%9A%84%E6%89%80%E6%9C%89token%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E9%83%BD%E4%BC%9A%E8%80%83%E8%99%91%E5%88%B0%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%EF%BC%88%E8%99%BD%E7%84%B6%E5%AE%83%E6%9C%AA%E5%BF%85%E6%80%BB%E6%98%AF%E6%9C%89%E7%94%A8%EF%BC%8C%E4%BD%86%E6%88%91%E4%BB%AC%E4%B8%80%E5%AE%9A%E4%BC%9A%E8%80%83%E8%99%91%E5%AE%83%EF%BC%89%E3%80%82%E5%8F%AF%E5%8F%82%E8%80%83%E4%B8%8B%E5%9B%BE%EF%BC%9A%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E8%B6%8A%E5%89%8D%E9%9D%A2%E7%9A%84token%EF%BC%8C%E5%AE%83%E7%9A%84%E5%87%BA%E5%BA%A6%E8%B6%8A%E5%A4%A7%E3%80%82%E7%9B%B4%E8%A7%82%E4%B8%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E6%88%96%E8%AE%B8%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%EF%BC%8C%E2%80%9C%E4%B8%8D%E8%87%AA%E8%A7%89%E2%80%9D%E5%9C%B0%E4%BC%9A%E5%AF%B9%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%E6%9C%89%E6%9B%B4%E5%A4%9A%E5%85%B3%E6%B3%A8%EF%BC%88%E4%BD%86%E8%BF%99%E9%87%8C%E8%BF%98%E6%98%AF%E5%A4%AA%E7%B2%97%E6%B5%85%E4%BA%86%EF%BC%8C%E7%BA%AF%E7%B2%B9%E6%88%90%E4%BA%86%E7%8C%9C%E6%B5%8B%E4%BA%86%E3%80%82%E8%BF%98%E6%98%AF%E5%BE%97%E7%9C%8B%E7%9C%8B%E5%8E%9F%E8%AE%BA%E6%96%87%E6%89%8D%E8%A1%8C%EF%BC%89%E3%80%82%E8%BF%99%E7%A7%8Dgraph%E6%98%AF%E9%9D%9E%E5%B8%B8%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%EF%BC%8C%E4%B9%9F%E5%BE%88%E6%9C%89%E5%8F%AF%E8%83%BD%E6%98%AF%E8%BF%99%E7%A7%8D%E4%B8%8D%E5%B9%B3%E8%A1%A1%E5%AF%BC%E8%87%B4%E4%BA%86position-bias%E7%9A%84%E5%87%BA%E7%8E%B0%EF%BC%88%E7%9B%B8%E5%BA%94%E7%9A%84%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E6%94%B9%E8%BF%9B%EF%BC%8C%E9%82%A3%E4%B9%88%E5%B0%B1%E5%BA%94%E8%AF%A5%E7%A1%AE%E4%BF%9D%E6%94%B9%E8%BF%9B%E5%90%8E%E7%9A%84%E6%9E%B6%E6%9E%84%E5%AF%B9%E5%BA%94%E7%9A%84%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8D%E5%BA%94%E8%AF%A5%E6%98%AF%E8%BF%99%E7%A7%8D%E9%9D%9E%E5%B8%B8%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E6%83%85%E5%86%B5%EF%BC%89%E8%80%8C%E4%B8%94%E9%9A%8F%E7%9D%80%E5%B1%82%E6%95%B0%E5%8A%A0%E6%B7%B1%EF%BC%8C%E8%BF%99%E7%A7%8D%E4%B8%8D%E5%B9%B3%E8%A1%A1%E4%BC%9A%E6%84%88%E5%8F%91%E4%B8%A5%E9%87%8D%EF%BC%88%E7%B1%BB%E4%BC%BC%E4%BA%8E%EF%BC%8C%E6%AF%8F%E4%B8%80%E5%B1%82%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%E9%83%BD%E5%A4%9A%E8%8E%B7%E5%8F%96%E4%B8%80%E4%BA%9B%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8C%E9%82%A3%E5%B1%82%E6%95%B0%E4%B8%80%E6%97%A6%E5%A4%9A%E4%BA%86%EF%BC%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E5%A4%A7%E5%A4%B4%E4%B8%8D%E5%B0%B1%E6%9C%89%E5%8F%AF%E8%83%BD%E5%87%A0%E4%B9%8E%E5%85%A8%E9%83%BD%E8%81%9A%E9%9B%86%E5%88%B0%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%E4%B8%8A%E4%BA%86%E5%90%97%EF%BC%89%E7%84%B6%E8%80%8C%EF%BC%8C%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%9B%BE%E5%BA%94%E8%AF%A5%E4%B8%8D%E8%83%BD%E5%AE%8C%E5%85%A8%E8%A7%A3%E9%87%8Aposition-bias%E3%80%82%E5%9B%A0%E4%B8%BAposition-bias%E9%99%A4%E4%BA%86%E8%AF%B4%E6%9C%89attention-sink%E7%9A%84%E7%8E%B0%E8%B1%A1%EF%BC%8C%E8%BF%98%E6%9C%89recency-bias%E3%80%82%E4%BD%86%E5%A6%82%E6%9E%9C%E4%BB%85%E8%80%83%E8%99%91%E4%B8%8A%E5%9B%BE%EF%BC%8C%E9%82%A3%E5%BA%94%E8%AF%A5%E5%AE%8C%E5%85%A8%E4%B8%8D%E5%8F%AF%E8%83%BD%E6%9C%89recency-bias%EF%BC%8Clast-token%E8%8E%B7%E5%BE%97%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%BA%94%E8%AF%A5%E6%98%AF%E6%9C%80%E5%B0%91%E7%9A%84%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E8%BF%99%E5%8F%AF%E8%83%BD%E6%98%AF%E5%85%B6%E5%AE%83%E5%9B%A0%E7%B4%A0%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AFPE%E5%AF%BC%E8%87%B4%E7%9A%84"><span class="nav-number">10.5.2.</span> <span class="nav-text">此处作者是用了graph进行分析。且假设不用什么其它的PE，就用最基础的causal mask（就是预测当前token时，只是用前面的token的那个mask）。如果把它可视化为一个计算图，可以想见，在我们预测后面的所有token的时候，都会考虑到第一个token（虽然它未必总是有用，但我们一定会考虑它）。可参考下图：可以看到，越前面的token，它的出度越大。直观上，我们或许可以认为，“不自觉”地会对第一个token有更多关注（但这里还是太粗浅了，纯粹成了猜测了。还是得看看原论文才行）。这种graph是非常不平衡的，也很有可能是这种不平衡导致了position bias的出现（相应的，如果我们希望改进，那么就应该确保改进后的架构对应的计算图不应该是这种非常不平衡的情况）而且随着层数加深，这种不平衡会愈发严重（类似于，每一层第一个token都多获取一些注意力，那层数一旦多了，注意力的大头不就有可能几乎全都聚集到第一个token上了吗）然而，需要注意，这个图应该不能完全解释position bias。因为position bias除了说有attention sink的现象，还有recency bias。但如果仅考虑上图，那应该完全不可能有recency bias，last token获得的注意力应该是最少的。因此，这可能是其它因素，也就是PE导致的</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%8B%E5%90%8E%E4%BD%9C%E8%80%85%E7%BB%99%E5%87%BA%E4%BA%86%E6%AF%94%E8%BE%83%E7%90%86%E8%AE%BA%E7%9A%84%E5%AE%9A%E7%90%86%EF%BC%8C%E5%AE%9A%E9%87%8F%E8%AF%B4%E6%98%8E%E4%BA%86%EF%BC%8C%E5%B1%82%E6%95%B0%E6%97%A0%E7%A9%B7%E6%B7%B1%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%E5%AF%B9%E5%85%B6%E5%AE%83%E6%89%80%E6%9C%89token%E7%9A%84%E8%B4%A1%E7%8C%AE%E4%BC%9A%E9%80%BC%E8%BF%911%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9D%83%E9%87%8D%E4%BC%9A%E5%AE%8C%E5%85%A8%E8%B6%8B%E5%90%91%E4%BA%8E1%E3%80%82%E8%80%8C%E5%AE%9E%E9%99%85%E8%99%BD%E7%84%B6%E6%88%91%E4%BB%AC%E4%B8%8D%E4%BC%9A%E7%9C%9F%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%80%E4%B8%AA%E6%97%A0%E7%A9%B7%E6%B7%B1%E7%9A%84llm%EF%BC%8C%E4%BD%86%E4%B9%9F%E6%9C%89%E5%AE%9A%E7%90%86%E7%BB%99%E5%87%BA%E5%AE%9A%E9%87%8F%E7%9A%84%E6%8F%8F%E8%BF%B0%EF%BC%8C%E8%AF%B4%E6%98%8E%EF%BC%8C%E5%AF%B9%E4%BA%8E%E9%99%A4%E4%BA%86%E7%AC%AC%E4%B8%80%E4%B8%AAtoken%E4%BB%A5%E5%A4%96%E7%9A%84token%EF%BC%8C%E5%AE%83%E4%BB%AC%E5%AF%B9%E5%85%B6%E5%AE%83token%E7%9A%84%E8%B4%A1%E7%8C%AE%EF%BC%8C%E6%98%AF%E5%9C%A8%E4%BB%A5%E6%8C%87%E6%95%B0%E9%80%9F%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%EF%BC%88%E6%AF%8F%E4%B8%80%E5%B1%82%E8%80%8C%E8%A8%80%EF%BC%89"><span class="nav-number">10.6.</span> <span class="nav-text">之后作者给出了比较理论的定理，定量说明了，层数无穷深的时候，第一个token对其它所有token的贡献会逼近1，也就是说，第一个token的注意力权重会完全趋向于1。而实际虽然我们不会真的构造一个无穷深的llm，但也有定理给出定量的描述，说明，对于除了第一个token以外的token，它们对其它token的贡献，是在以指数速度下降的（每一层而言）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%B3%E4%BA%8EPE%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%8C%E4%B9%9F%E6%98%AF%E7%9B%B4%E6%8E%A5%E7%BB%99%E5%87%BA%E4%BA%86%E5%AE%9A%E7%90%86%EF%BC%88%E8%BF%98%E6%98%AF%E6%9C%89%E7%82%B9%E5%A4%8D%E6%9D%82%E7%9A%84%EF%BC%89%EF%BC%8C%E5%A4%A7%E6%A6%82%E6%98%AF%E8%AF%B4%E6%98%8E%E4%BA%86%E5%B8%B8%E7%94%A8%E7%9A%84PE%E5%AF%B9attention-mask%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%8C%E4%B9%9F%E8%AF%B4%E6%98%8E%E4%BA%86RoPE%E7%9A%84%E5%A5%BD%E7%94%A8%E4%B9%8B%E5%A4%84%EF%BC%88%E8%A1%B0%E5%87%8F%E5%BE%97%E6%B2%A1%E9%82%A3%E4%B9%88%E5%BF%AB%E3%80%82%E4%BD%86%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%E5%9C%A8%E8%A1%B0%E5%87%8F%EF%BC%9F%EF%BC%89"><span class="nav-number">10.7.</span> <span class="nav-text">至于PE的影响，也是直接给出了定理（还是有点复杂的），大概是说明了常用的PE对attention mask的影响，也说明了RoPE的好用之处（衰减得没那么快。但到底是什么在衰减？）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%80%E5%90%8E%E7%9A%84%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="nav-number">11.</span> <span class="nav-text">最后的总结：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#NTP%E8%8C%83%E5%BC%8F%E5%92%8Ctransformer%E6%98%AF%E4%B8%80%E4%B8%AA%E5%BE%88%E5%A5%BD%E7%9A%84%E7%BB%84%E5%90%88%EF%BC%88%E7%9C%8B%E7%9C%8B%E7%8E%B0%E5%9C%A8%E7%9A%84llm%E7%9A%84%E8%BF%9B%E5%B1%95%E5%92%8C%E6%88%90%E6%9E%9C%E5%B0%B1%E7%9F%A5%E9%81%93%E4%BA%86%EF%BC%89"><span class="nav-number">11.1.</span> <span class="nav-text">NTP范式和transformer是一个很好的组合（看看现在的llm的进展和成果就知道了）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%86%E5%AE%83%E4%BB%AC%E7%9A%84%E9%97%AE%E9%A2%98%E5%9C%A8%E4%BA%8E%EF%BC%8C%E5%AE%83%E4%BB%AC%E9%A2%84%E8%AE%AD%E7%BB%83%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%BE%97%E5%BE%88%E5%A5%BD%EF%BC%8C%E5%8F%AF%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83%E5%81%9A%E5%A5%BD%E4%BB%A5%E5%90%8E%EF%BC%8C%E7%A1%AE%E5%AE%9E%E6%9C%89%E4%BA%86%E5%BE%88%E5%BC%BA%E7%9A%84%E8%83%BD%E5%8A%9B%EF%BC%8C%E4%BD%86%E5%B9%B6%E4%B8%8D%E5%AE%8C%E5%85%A8%E7%AD%89%E5%90%8C%E4%BA%8E%E6%88%91%E4%BB%AC%E6%83%B3%E8%A6%81%E7%9A%84%E8%83%BD%E5%8A%9B%E3%80%82%E6%8D%A2%E5%8F%A5%E8%AF%9D%E8%AF%B4%EF%BC%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E5%8D%B3%E4%BD%BF%E5%81%9A%E5%BE%97%E5%BE%88%E5%A5%BD%EF%BC%8C%E8%83%BD%E5%A4%84%E7%90%86%E4%B8%80%E4%BA%9B%E4%BB%BB%E5%8A%A1%E4%BA%86%EF%BC%8C%E4%BD%86%E4%BB%8D%E6%9C%89%E4%B8%80%E4%BA%9B%E6%88%91%E4%BB%AC%E5%85%B3%E5%BF%83%E7%9A%84%E4%BB%BB%E5%8A%A1%EF%BC%8C%E5%AE%83%E6%97%A0%E6%B3%95%E5%A4%84%E7%90%86%E5%A5%BD%EF%BC%88%E6%AF%94%E5%A6%82%E9%95%BF%E6%96%87%E6%9C%AC%EF%BC%8C%E6%AF%94%E5%A6%82position-bias%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%89%E6%89%80%E4%BB%A5%E9%9C%80%E8%A6%81%E6%94%B9%E8%BF%9B%EF%BC%88%E6%AD%A3%E5%A6%82%E4%B8%8A%E6%8F%90%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8E%A2%E7%B4%A2%EF%BC%8C%E7%94%A8reweighting%E6%94%B9%E8%BF%9B%E6%8F%90%E9%AB%98%E9%95%BF%E6%96%87%E6%9C%AC%E8%83%BD%E5%8A%9B%E7%9A%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%EF%BC%9B%E8%80%83%E8%99%91bias%E7%9A%84%E6%88%90%E5%9B%A0%EF%BC%8C%E6%8C%87%E5%AF%BC%E8%AE%BE%E8%AE%A1debias%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%89"><span class="nav-number">11.2.</span> <span class="nav-text">但它们的问题在于，它们预训练可以做得很好，可是预训练做好以后，确实有了很强的能力，但并不完全等同于我们想要的能力。换句话说，预训练即使做得很好，能处理一些任务了，但仍有一些我们关心的任务，它无法处理好（比如长文本，比如position bias带来的问题）所以需要改进（正如上提到的一些探索，用reweighting改进提高长文本能力的训练方法；考虑bias的成因，指导设计debias的架构）</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">232</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">76</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">104</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
