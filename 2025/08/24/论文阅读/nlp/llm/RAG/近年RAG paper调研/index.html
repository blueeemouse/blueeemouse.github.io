<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ACL 25 main HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation RAG-Critic: Leveraging Autom">
<meta property="og:type" content="article">
<meta property="og:title" content="近年RAG paper调研">
<meta property="og:url" content="https://blueeemouse.github.io/2025/08/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4RAG%20paper%E8%B0%83%E7%A0%94/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="ACL 25 main HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation RAG-Critic: Leveraging Autom">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-08-24T15:35:00.000Z">
<meta property="article:modified_time" content="2025-09-07T08:41:58.354Z">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/2025/08/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4RAG%20paper%E8%B0%83%E7%A0%94/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>近年RAG paper调研 | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/08/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4RAG%20paper%E8%B0%83%E7%A0%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          近年RAG paper调研
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-08-24 23:35:00" itemprop="dateCreated datePublished" datetime="2025-08-24T23:35:00+08:00">2025-08-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-09-07 16:41:58" itemprop="dateModified" datetime="2025-09-07T16:41:58+08:00">2025-09-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="acl-25">ACL 25</h1>
<h2 id="main">main</h2>
<h3 id="hybgrag-hybrid-retrieval-augmented-generation-on-textual-and-relational-knowledge-bases"><strong>HybGRAG:
Hybrid Retrieval-Augmented Generation on Textual and Relational
Knowledge Bases</strong></h3>
<h3 id="main-rag-multi-agent-filtering-retrieval-augmented-generation"><strong>MAIN-RAG:
Multi-Agent Filtering Retrieval-Augmented Generation</strong></h3>
<h3 id="rag-critic-leveraging-automated-critic-guided-agentic-workflow-for-retrieval-augmented-generation"><strong>RAG-Critic:
Leveraging Automated Critic-Guided Agentic Workflow for Retrieval
Augmented Generation</strong></h3>
<h3 id="saferag-benchmarking-security-in-retrieval-augmented-generation-of-large-language-model"><strong>SafeRAG:
Benchmarking Security in Retrieval-Augmented Generation of Large
Language Model</strong></h3>
<h3 id="on-the-robustness-of-rag-systems-in-educational-question-answering-under-knowledge-discrepancies"><strong>On
the Robustness of RAG Systems in Educational Question Answering under
Knowledge Discrepancies</strong></h3>
<h3 id="pandoras-box-or-aladdins-lamp-a-comprehensive-analysis-revealing-the-role-of-rag-noise-in-large-language-models"><strong>Pandora’s
Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of
RAG Noise in Large Language Models</strong></h3>
<h3 id="neusym-rag-hybrid-neural-symbolic-retrieval-with-multiview-structuring-for-pdf-question-answering"><strong>NeuSym-RAG:
Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF
Question Answering</strong></h3>
<h3 id="drag-distilling-rag-for-slms-from-llms-to-transfer-knowledge-and-mitigate-hallucination-via-evidence-and-graph-based-distillation"><strong>DRAG:
Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate
Hallucination via Evidence and Graph-based Distillation</strong></h3>
<h3 id="rageval-scenario-specific-rag-evaluation-dataset-generation-framework"><strong>RAGEval:
Scenario Specific RAG Evaluation Dataset Generation
Framework</strong></h3>
<h3 id="are-llms-effective-psychological-assessors-leveraging-adaptive-rag-for-interpretable-mental-health-screening-through-psychometric-practice"><strong>Are
LLMs effective psychological assessors? Leveraging adaptive RAG for
interpretable mental health screening through psychometric
practice</strong></h3>
<h3 id="s-rag-a-novel-audit-framework-for-detecting-unauthorized-use-of-personal-data-in-rag-systems"><strong>S-RAG:
A Novel Audit Framework for Detecting Unauthorized Use of Personal Data
in RAG Systems</strong></h3>
<h3 id="gainrag-preference-alignment-in-retrieval-augmented-generation-through-gain-signal-synthesis"><strong>GainRAG:
Preference Alignment in Retrieval-Augmented Generation through Gain
Signal Synthesis</strong></h3>
<span id="more"></span>
<h3 id="tcrag-turingcomplete-rags-case-study-on-medical-llm-systems"><strong>TC–RAG:
Turing–Complete RAG’s Case study on Medical LLM Systems</strong></h3>
<h3 id="divide-then-align-honest-alignment-based-on-the-knowledge-boundary-of-rag"><strong>Divide-Then-Align:
Honest Alignment based on the Knowledge Boundary of RAG</strong></h3>
<h3 id="hykge-a-hypothesis-knowledge-graph-enhanced-rag-framework-for-accurate-and-reliable-medical-llms-responses"><strong>HyKGE:
A Hypothesis Knowledge Graph Enhanced RAG Framework for Accurate and
Reliable Medical LLMs Responses</strong></h3>
<h3 id="wavrag-audio-integrated-retrieval-augmented-generation-for-spoken-dialogue-models"><strong>WavRAG:
Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue
Models</strong></h3>
<h3 id="unirag-unified-query-understanding-method-for-retrieval-augmented-generation"><strong>UniRAG:
Unified Query Understanding Method for Retrieval Augmented
Generation</strong></h3>
<h3 id="towards-omni-rag-comprehensive-retrieval-augmented-generation-for-large-language-models-in-medical-applications"><strong>Towards
Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large
Language Models in Medical Applications</strong></h3>
<h3 id="molrag-unlocking-the-power-of-large-language-models-for-molecular-property-prediction"><strong>MolRAG:
Unlocking the Power of Large Language Models for Molecular Property
Prediction</strong></h3>
<h3 id="removal-of-hallucination-on-hallucination-debate-augmented-rag"><strong>Removal
of Hallucination on Hallucination: Debate-Augmented RAG</strong></h3>
<h3 id="eventrag-enhancing-llm-generation-with-event-knowledge-graphs"><strong>EventRAG:
Enhancing LLM Generation with Event Knowledge Graphs</strong></h3>
<h3 id="the-distracting-effect-understanding-irrelevant-passages-in-rag"><strong>The
Distracting Effect: Understanding Irrelevant Passages in
RAG</strong></h3>
<h3 id="kirag-knowledge-driven-iterative-retriever-for-enhancing-retrieval-augmented-generation"><strong>KiRAG:
Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented
Generation</strong></h3>
<h3 id="faithfulrag-fact-level-conflict-modeling-for-context-faithful-retrieval-augmented-generation"><strong>FaithfulRAG:
Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented
Generation</strong></h3>
<h3 id="memerag-a-multilingual-end-to-end-meta-evaluation-benchmark-for-retrieval-augmented-generation"><strong>MEMERAG:
A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval
Augmented Generation</strong></h3>
<h3 id="dialogue-rag-enhancing-retrieval-for-llms-via-node-linking-utterance-rewriting"><strong>Dialogue-RAG:
Enhancing Retrieval for LLMs via Node-Linking Utterance
Rewriting</strong></h3>
<h3 id="the-efficiency-vs.-accuracy-trade-off-optimizing-rag-enhanced-llm-recommender-systems-using-multi-head-early-exit"><strong>The
Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM
Recommender Systems Using Multi-Head Early Exit</strong></h3>
<h3 id="sgic-a-self-guided-iterative-calibration-framework-for-rag"><strong>SGIC:
A Self-Guided Iterative Calibration Framework for RAG</strong></h3>
<h3 id="medical-graph-rag-evidence-based-medical-large-language-model-via-graph-retrieval-augmented-generation"><strong>Medical
Graph RAG: Evidence-based Medical Large Language Model via Graph
Retrieval-Augmented Generation</strong></h3>
<h3 id="astute-rag-overcoming-imperfect-retrieval-augmentation-and-knowledge-conflicts-for-large-language-models"><strong>Astute
RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts
for Large Language Models</strong></h3>
<h3 id="real-mm-rag-a-real-world-multi-modal-retrieval-benchmark"><strong>REAL-MM-RAG:
A Real-World Multi-Modal Retrieval Benchmark</strong></h3>
<h3 id="dualrag-a-dual-process-approach-to-integrate-reasoning-and-retrieval-for-multi-hop-question-answering"><strong>DualRAG:
A Dual-Process Approach to Integrate Reasoning and Retrieval for
Multi-Hop Question Answering</strong></h3>
<h3 id="core-mmrag-cross-source-knowledge-reconciliation-for-multimodal-rag"><strong>CoRe-MMRAG:
Cross-Source Knowledge Reconciliation for Multimodal RAG</strong></h3>
<h2 id="findings">findings</h2>
<h3 id="treerag-unleashing-the-power-of-hierarchical-storage-for-enhanced-knowledge-retrieval-in-long-documents"><strong>TreeRAG:
Unleashing the Power of Hierarchical Storage for Enhanced Knowledge
Retrieval in Long Documents</strong></h3>
<h3 id="scrag-hybrid-retrieval-augmented-generation-for-llm-based-cross-tissue-single-cell-annotation"><strong>scRAG:
Hybrid Retrieval-Augmented Generation for LLM-based Cross-Tissue
Single-Cell Annotation</strong></h3>
<h3 id="hoprag-multi-hop-reasoning-for-logic-aware-retrieval-augmented-generation"><strong>HopRAG:
Multi-Hop Reasoning for Logic-Aware Retrieval Augmented
Generation</strong></h3>
<h3 id="ltrag-enhancing-autoformalization-and-self-refinement-for-logical-reasoning-with-thought-guided-rag"><strong>LTRAG:
Enhancing autoformalization and self-refinement for logical reasoning
with Thought-Guided RAG</strong></h3>
<h3 id="simgrag-leveraging-similar-subgraphs-for-knowledge-graphs-driven-retrieval-augmented-generation"><strong>SimGRAG:
Leveraging Similar Subgraphs for Knowledge Graphs Driven
Retrieval-Augmented Generation</strong></h3>
<h3 id="remoterag-a-privacy-preserving-llm-cloud-rag-service"><strong>RemoteRAG:
A Privacy-Preserving LLM Cloud RAG Service</strong></h3>
<h3 id="investigating-language-preference-of-multilingual-rag-systems"><strong>Investigating
Language Preference of Multilingual RAG Systems</strong></h3>
<h3 id="frag-a-flexible-modular-framework-for-retrieval-augmented-generation-based-on-knowledge-graphs"><strong>FRAG:
A Flexible Modular Framework for Retrieval-Augmented Generation based on
Knowledge Graphs</strong></h3>
<h3 id="navrag-generating-user-demand-instructions-for-embodied-navigation-through-retrieval-augmented-llm"><strong>NavRAG:
Generating User Demand Instructions for Embodied Navigation through
Retrieval-Augmented LLM</strong></h3>
<h3 id="speecht-rag-reliable-depression-detection-in-llms-with-retrieval-augmented-generation-using-speech-timing-information"><strong>SpeechT-RAG:
Reliable Depression Detection in LLMs with Retrieval-Augmented
Generation Using Speech Timing Information</strong></h3>
<h3 id="roserag-robust-retrieval-augmented-generation-with-small-scale-llms-via-margin-aware-preference-optimization"><strong>RoseRAG:
Robust Retrieval-augmented Generation with Small-scale LLMs via
Margin-aware Preference Optimization</strong></h3>
<h3 id="gnn-rag-graph-neural-retrieval-for-efficient-large-language-model-reasoning-on-knowledge-graphs"><strong>GNN-RAG:
Graph Neural Retrieval for Efficient Large Language Model Reasoning on
Knowledge Graphs</strong></h3>
<h3 id="astrid---an-automated-and-scalable-triad-for-the-evaluation-of-rag-based-clinical-question-answering-systems"><strong>ASTRID
- An Automated and Scalable TRIaD for the Evaluation of RAG-based
Clinical Question Answering Systems</strong></h3>
<h3 id="garage-a-benchmark-with-grounding-annotations-for-rag-evaluation"><strong>GaRAGe:
A Benchmark with Grounding Annotations for RAG Evaluation</strong></h3>
<h3 id="rag-rewardbench-benchmarking-reward-models-in-retrieval-augmented-generation-for-preference-alignment"><strong>RAG-RewardBench:
Benchmarking Reward Models in Retrieval Augmented Generation for
Preference Alignment</strong></h3>
<h3 id="mitigating-bias-in-rag-controlling-the-embedder"><strong>Mitigating
Bias in RAG: Controlling the Embedder</strong></h3>
<h3 id="synapticrag-enhancing-temporal-memory-retrieval-in-large-language-models-through-synaptic-mechanisms"><strong>SynapticRAG:
Enhancing Temporal Memory Retrieval in Large Language Models through
Synaptic Mechanisms</strong></h3>
<h3 id="techniquerag-retrieval-augmented-generation-for-adversarial-technique-annotation-in-cyber-threat-intelligence-text"><strong>TechniqueRAG:
Retrieval Augmented Generation for Adversarial Technique Annotation in
Cyber Threat Intelligence Text</strong></h3>
<h3 id="videorag-retrieval-augmented-generation-over-video-corpus"><strong>VideoRAG:
Retrieval-Augmented Generation over Video Corpus</strong></h3>
<h3 id="query-driven-multimodal-graphrag-dynamic-local-knowledge-graph-construction-for-online-reasoning"><strong>Query-Driven
Multimodal GraphRAG: Dynamic Local Knowledge Graph Construction for
Online Reasoning</strong></h3>
<h3 id="causalrag-integrating-causal-graphs-into-retrieval-augmented-generation"><strong>CausalRAG:
Integrating Causal Graphs into Retrieval-Augmented
Generation</strong></h3>
<h3 id="eventrag-supportive-event-retrieval-on-hypergraph-for-future-forecasting"><strong>EventRAG:
Supportive Event Retrieval on Hypergraph for Future
Forecasting</strong></h3>
<h3 id="safeguarding-rag-pipelines-with-gmtp-a-gradient-based-masked-token-probability-method-for-poisoned-document-detection"><strong>Safeguarding
RAG Pipelines with GMTP: A Gradient-based Masked Token Probability
Method for Poisoned Document Detection</strong></h3>
<h3 id="ecorag-evidentiality-guided-compression-for-long-context-rag"><strong>ECoRAG:
Evidentiality-guided Compression for Long Context RAG</strong></h3>
<h3 id="hash-rag-bridging-deep-hashing-with-retriever-for-efficient-fine-retrieval-and-augmented-generation"><strong>HASH-RAG:
Bridging Deep Hashing with Retriever for Efficient, Fine Retrieval and
Augmented Generation</strong></h3>
<h1 id="naacl-25">NAACL 25</h1>
<h2 id="main-1">main</h2>
<h3 id="rag-star-enhancing-deliberative-reasoning-with-retrieval-augmented-verification-and-refinement"><strong>RAG-Star:
Enhancing Deliberative Reasoning with Retrieval Augmented Verification
and Refinement</strong></h3>
<h3 id="simrag-self-improving-retrieval-augmented-generation-for-adapting-large-language-models-to-specialized-domains"><strong>SimRAG:
Self-Improving Retrieval-Augmented Generation for Adapting Large
Language Models to Specialized Domains</strong></h3>
<h3 id="do-rag-systems-cover-what-matters-evaluating-and-optimizing-responses-with-sub-question-coverage"><strong>Do
RAG Systems Cover What Matters? Evaluating and Optimizing Responses with
Sub-Question Coverage</strong></h3>
<h3 id="rag-llms-are-not-safer-a-safety-analysis-of-retrieval-augmented-generation-for-large-language-models"><strong>RAG
LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation
for Large Language Models</strong></h3>
<h3 id="trag-term-level-retrieval-augmented-generation-for-domain-adaptive-retrieval"><strong>tRAG:
Term-level Retrieval-Augmented Generation for Domain-Adaptive
Retrieval</strong></h3>
<h3 id="pa-rag-rag-alignment-via-multi-perspective-preference-optimization"><strong>PA-RAG:
RAG Alignment via Multi-Perspective Preference
Optimization</strong></h3>
<h3 id="corag-collaborative-retrieval-augmented-generation"><strong>CoRAG:
Collaborative Retrieval-Augmented Generation</strong></h3>
<h3 id="transform-retrieval-for-textual-entailment-in-rag"><strong>Transform
Retrieval for Textual Entailment in RAG</strong></h3>
<h2 id="findings-1">findings</h2>
<h3 id="systematic-knowledge-injection-into-large-language-models-via-diverse-augmentation-for-domain-specific-rag"><strong>Systematic
Knowledge Injection into Large Language Models via Diverse Augmentation
for Domain-Specific RAG</strong></h3>
<h3 id="probing-rag-self-probing-to-guide-language-models-in-selective-document-retrieval"><strong>Probing-RAG:
Self-Probing to Guide Language Models in Selective Document
Retrieval</strong></h3>
<h3 id="mes-rag-bringing-multi-modal-entity-storage-and-secure-enhancements-to-rag"><strong>MES-RAG:
Bringing Multi-modal, Entity-Storage, and Secure Enhancements to
RAG</strong></h3>
<h3 id="grag-graph-retrieval-augmented-generation"><strong>GRAG: Graph
Retrieval-Augmented Generation</strong></h3>
<h3 id="coderag-bench-can-retrieval-augment-code-generation"><strong>CodeRAG-Bench:
Can Retrieval Augment Code Generation?</strong></h3>
<h3 id="unirag-universal-retrieval-augmentation-for-large-vision-language-models"><strong>UniRAG:
Universal Retrieval Augmentation for Large Vision Language
Models</strong></h3>
<h3 id="funnelrag-a-coarse-to-fine-progressive-retrieval-paradigm-for-rag"><strong>FunnelRAG:
A Coarse-to-Fine Progressive Retrieval Paradigm for RAG</strong></h3>
<h3 id="grappi-a-retrieve-divide-solve-graphrag-framework-for-large-scale-protein-protein-interaction-exploration"><strong>GraPPI:
A Retrieve-Divide-Solve GraphRAG Framework for Large-scale
Protein-protein Interaction Exploration</strong></h3>
<h3 id="chain-of-rank-enhancing-large-language-models-for-domain-specific-rag-in-edge-device"><strong>Chain-of-Rank:
Enhancing Large Language Models for Domain-Specific RAG in Edge
Device</strong></h3>
<h3 id="superrag-beyond-rag-with-layout-aware-graph-modeling"><strong>SuperRAG:
Beyond RAG with Layout-Aware Graph Modeling</strong></h3>
<h3 id="hypa-rag-a-hybrid-parameter-adaptive-retrieval-augmented-generation-system-for-ai-legal-and-policy-applications"><strong>HyPA-RAG:
A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI
Legal and Policy Applications</strong></h3>
<h3 id="evaluating-the-performance-of-rag-methods-for-conversational-ai-in-the-airport-domain"><strong>Evaluating
the Performance of RAG Methods for Conversational AI in the Airport
Domain</strong></h3>
<h3 id="dsrag-a-double-stream-retrieval-augmented-generation-framework-for-countless-intent-detection"><strong>DSRAG:
A Double-Stream Retrieval-Augmented Generation Framework for Countless
Intent Detection</strong></h3>
<h3 id="from-generating-answers-to-building-explanations-integrating-multi-round-rag-and-causal-modeling-for-scientific-qa"><strong>From
Generating Answers to Building Explanations: Integrating Multi-Round RAG
and Causal Modeling for Scientific QA</strong></h3>
<h1 id="emnlp-24">EMNLP 24</h1>
<h2 id="main-2">main</h2>
<h3 id="rule-reliable-multimodal-rag-for-factuality-in-medical-vision-language-models"><strong>RULE:
Reliable Multimodal RAG for Factuality in Medical Vision Language
Models</strong></h3>
<h3 id="rag-qa-arena-evaluating-domain-robustness-for-long-form-retrieval-augmented-question-answering"><strong>RAG-QA
Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented
Question Answering</strong></h3>
<h3 id="from-rag-to-riches-retrieval-interlaced-with-sequence-generation"><strong>From
RAG to Riches: Retrieval Interlaced with Sequence
Generation</strong></h3>
<h3 id="summary-of-a-haystack-a-challenge-to-long-context-llms-and-rag-systems"><strong>Summary
of a Haystack: A Challenge to Long-Context LLMs and RAG
Systems</strong></h3>
<h3 id="dynamicer-resolving-emerging-mentions-to-dynamic-entities-for-rag"><strong>DynamicER:
Resolving Emerging Mentions to Dynamic Entities for RAG</strong></h3>
<h3 id="deciphering-the-interplay-of-parametric-and-non-parametric-memory-in-rag-models"><strong>Deciphering
the Interplay of Parametric and Non-Parametric Memory in RAG
Models</strong></h3>
<h3 id="re-rag-improving-open-domain-qa-performance-and-interpretability-with-relevance-estimator-in-retrieval-augmented-generation"><strong>RE-RAG:
Improving Open-Domain QA Performance and Interpretability with Relevance
Estimator in Retrieval-Augmented Generation</strong></h3>
<h3 id="longrag-a-dual-perspective-retrieval-augmented-generation-paradigm-for-long-context-question-answering"><strong>LongRAG:
A Dual-perspective Retrieval-Augmented Generation Paradigm for
Long-Context Question Answering</strong></h3>
<h2 id="findings-2">findings</h2>
<h3 id="rag-studio-towards-in-domain-adaptation-of-retrieval-augmented-generation-through-self-alignment"><strong>RAG-Studio:
Towards In-Domain Adaptation Of Retrieval Augmented Generation Through
Self-Alignment</strong></h3>
<h3 id="rafe-ranking-feedback-improves-query-rewriting-for-rag"><strong>RaFe:
Ranking Feedback Improves Query Rewriting for RAG</strong></h3>
<h3 id="adaptive-selection-for-homogeneous-tools-an-instantiation-in-the-rag-scenario"><strong>Adaptive
Selection for Homogeneous Tools: An Instantiation in the RAG
Scenario</strong></h3>
<h3 id="bsharedrag-backbone-shared-retrieval-augmented-generation-for-the-e-commerce-domain"><strong>BSharedRAG:
Backbone Shared Retrieval-Augmented Generation for the E-commerce
Domain</strong></h3>
<h3 id="open-rag-enhanced-retrieval-augmented-reasoning-with-open-source-large-language-models"><strong>Open-RAG:
Enhanced Retrieval Augmented Reasoning with Open-Source Large Language
Models</strong></h3>
<h3 id="typos-that-broke-the-rags-back-genetic-attack-on-rag-pipeline-by-simulating-documents-in-the-wild-via-low-level-perturbations"><strong>Typos
that Broke the RAG’s Back: Genetic Attack on RAG Pipeline by Simulating
Documents in the Wild via Low-level Perturbations</strong></h3>
<h3 id="autorag-hp-automatic-online-hyper-parameter-tuning-for-retrieval-augmented-generation"><strong>AutoRAG-HP:
Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented
Generation</strong></h3>
<h3 id="long2rag-evaluating-long-context-long-form-retrieval-augmented-generation-with-key-point-recall"><strong><span class="math inline">\(LONG^{2}RAG\)</span>: Evaluating Long-Context
&amp; Long-Form Retrieval-Augmented Generation with Key Point
Recall</strong></h3>
<h1 id="icml-25按retrieval关键词检索的排除了ai4sci">ICML
25（按retrieval关键词检索的，排除了ai4sci）</h1>
<h2 id="poisonedeye-knowledge-poisoning-attack-on-retrieval-augmented-generation-based-large-vision-language-models"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46373">PoisonedEye: Knowledge
Poisoning Attack on Retrieval-Augmented Generation based Large
Vision-Language Models</a></h2>
<h2 id="retrieval-augmented-zero-shot-enzyme-generation-for-specified-substrate"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45546">Retrieval Augmented
Zero-Shot Enzyme Generation for Specified Substrate</a></h2>
<h2 id="she-streaming-media-hashing-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">SHE: Streaming-media
Hashing Retrieval</a></h2>
<h2 id="contradiction-retrieval-via-contrastive-learning-with-sparsity"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45043">Contradiction Retrieval
via Contrastive Learning with Sparsity</a></h2>
<h2 id="llm-alignment-as-retriever-optimization-an-information-retrieval-perspective"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45659">LLM Alignment as
Retriever Optimization: An Information Retrieval Perspective</a></h2>
<h2 id="efficient-length-generalizable-attention-via-causal-retrieval-for-long-context-language-modeling"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46384">Efficient
Length-Generalizable Attention via Causal Retrieval for Long-Context
Language Modeling</a></h2>
<h2 id="phantomwiki-on-demand-datasets-for-reasoning-and-retrieval-evaluation"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46018">PhantomWiki: On-Demand
Datasets for Reasoning and Retrieval Evaluation</a></h2>
<h2 id="understanding-synthetic-context-extension-via-retrieval-heads"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44638">Understanding Synthetic
Context Extension via Retrieval Heads</a></h2>
<h2 id="rapid-long-context-inference-with-retrieval-augmented-speculative-decoding"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46343">RAPID: Long-Context
Inference with Retrieval-Augmented Speculative Decoding</a></h2>
<h2 id="c-3po-compact-plug-and-play-proxy-optimization-to-achieve-human-like-retrieval-augmented-generation"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44398">C-3PO: Compact
Plug-and-Play Proxy Optimization to Achieve Human-like
Retrieval-Augmented Generation</a></h2>
<h2 id="on-the-vulnerability-of-applying-retrieval-augmented-generation-within-knowledge-intensive-application-domains"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45226">On the Vulnerability of
Applying Retrieval-Augmented Generation within Knowledge-Intensive
Application Domains</a></h2>
<h2 id="in-context-learning-as-conditioned-associative-memory-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44826">In-Context Learning as
Conditioned Associative Memory Retrieval</a></h2>
<h2 id="in-context-denoising-with-one-layer-transformers-connections-between-attention-and-associative-memory-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45913">In-Context Denoising
with One-Layer Transformers: Connections between Attention and
Associative Memory Retrieval</a></h2>
<h2 id="poqd-performance-oriented-query-decomposer-for-multi-vector-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44047">POQD:
Performance-Oriented Query Decomposer for Multi-vector
retrieval</a></h2>
<h2 id="scenir-visual-semantic-clarity-through-unsupervised-scene-graph-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43841">SCENIR: Visual Semantic
Clarity through Unsupervised Scene Graph Retrieval</a></h2>
<h2 id="retrieval-augmented-time-series-forecasting"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45826">Retrieval Augmented
Time Series Forecasting</a></h2>
<h2 id="position-retrieval-augmented-systems-can-be-dangerous-medical-communicators"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/40149">Position:
Retrieval-augmented systems can be dangerous medical
communicators</a></h2>
<h2 id="beyond-cropped-regions-new-benchmark-and-corresponding-baseline-for-chinese-scene-text-retrieval-in-diverse-layouts"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45853">Beyond Cropped Regions:
New Benchmark and Corresponding Baseline for Chinese Scene Text
Retrieval in Diverse Layouts</a></h2>
<h2 id="qure-query-relevant-retrieval-through-hard-negative-sampling-in-composed-image-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43476">QuRe: Query-Relevant
Retrieval through Hard Negative Sampling in Composed Image
Retrieval</a></h2>
<h2 id="learning-attribute-aware-hash-codes-for-fine-grained-image-retrieval-via-query-optimization"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43868">Learning
Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query
Optimization</a></h2>
<h2 id="locality-preserving-markovian-transition-for-instance-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45178">Locality Preserving
Markovian Transition for Instance Retrieval</a></h2>
<h2 id="visual-abstraction-a-plug-and-play-approach-for-text-visual-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">Visual Abstraction: A
Plug-and-Play Approach for Text-Visual Retrieval</a></h2>
<h2 id="docks-rag-optimizing-document-level-relation-extraction-through-llm-enhanced-hybrid-prompt-tuning"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45220">DocKS-RAG: Optimizing
Document-Level Relation Extraction through LLM-Enhanced Hybrid Prompt
Tuning</a></h2>
<h2 id="from-rag-to-memory-non-parametric-continual-learning-for-large-language-models"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45585">From RAG to Memory:
Non-Parametric Continual Learning for Large Language Models</a></h2>
<h2 id="hierarchical-planning-for-complex-tasks-with-knowledge-graph-rag-and-symbolic-verification"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43660">Hierarchical Planning
for Complex Tasks with Knowledge Graph-RAG and Symbolic
Verification</a></h2>
<h2 id="retrieval-augmented-perception-high-resolution-image-perception-meets-visual-rag"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44979">Retrieval-Augmented
Perception: High-resolution Image Perception Meets Visual RAG</a></h2>
<h2 id="realrag-retrieval-augmented-realistic-image-generation-via-self-reflective-contrastive-learning"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44615">RealRAG:
Retrieval-augmented Realistic Image Generation via Self-reflective
Contrastive Learning</a></h2>
<h2 id="lara-benchmarking-retrieval-augmented-generation-and-long-context-llms-no-silver-bullet-for-lc-or-rag-routing"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46069">LaRA: Benchmarking
Retrieval-Augmented Generation and Long-Context LLMs – No Silver Bullet
for LC or RAG Routing</a></h2>
<h2 id="ragged-towards-informed-design-of-scalable-and-stable-rag-systems"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46460">RAGGED: Towards
Informed Design of Scalable and Stable RAG Systems</a></h2>
<h1 id="iclr-25按retrieval关键词检索">ICLR
25（按retrieval关键词检索）</h1>
<ul>
<li><p>From Artificial Needles to Real Haystacks: Improving Retrieval
Capabilities in LLMs by Finetuning on Synthetic Data</p></li>
<li><p>ColPali: Efficient Document Retrieval with Vision Language
Models</p></li>
<li><p>Semi-Parametric Retrieval via Binary Bag-of-Tokens Index</p></li>
<li><p>VisRAG: Vision-based Retrieval-augmented Generation on
Multi-modality Documents</p></li>
<li><p>CoRNStack: High-Quality Contrastive Data for Better Code
Retrieval and Reranking</p></li>
<li><p>RA-TTA: Retrieval-Augmented Test-Time Adaptation for
Vision-Language Models</p></li>
<li><p>Streaming Video Question-Answering with In-context Video KV-Cache
Retrieval</p></li>
<li><p>STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy
Learning</p></li>
<li><p>MAI: A Multi-turn Aggregation-Iteration Model for Composed Image
Retrieval</p></li>
<li><p>Bridging Information Asymmetry in Text-video Retrieval: A
Data-centric Approach</p></li>
<li><p>Sufficient Context: A New Lens on Retrieval Augmented Generation
Systems</p></li>
<li><p>SeCom: On Memory Construction and Retrieval for Personalized
Conversational Agents</p></li>
<li><p>Beyond Content Relevance: Evaluating Instruction Following in
Retrieval Models</p></li>
<li><p>Benchmarking Multimodal Retrieval Augmented Generation with
Dynamic VQA Dataset and Self-adaptive Planning Agent</p></li>
<li><p>ToolGen: Unified Tool Retrieval and Calling via
Generation</p></li>
<li><p>Retrieval Augmented Diffusion Model for Structure-informed
Antibody Design and Optimization</p></li>
<li><p>MM-EMBED: Universal Multimodal Retrieval with Multimodal
LLMs</p></li>
<li><p>ReNovo: Retrieval-Based <em>De Novo</em> Mass Spectrometry
Peptide Sequencing</p></li>
<li><p>TIGeR: Unifying Text-to-Image Generation and Retrieval with Large
Multimodal Models</p></li>
<li><p>InstructRAG: Instructing Retrieval-Augmented Generation via
Self-Synthesized Rationales</p></li>
<li><p>Reasoning-Enhanced Healthcare Predictions with Knowledge Graph
Community Retrieval</p></li>
<li><p>DRoC: Elevating Large Language Models for Complex Vehicle Routing
via Decomposed Retrieval of Constraints</p></li>
<li><p>Multi-Field Adaptive Retrieval</p></li>
<li><p>MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented
Multimodal Models</p></li>
<li><p>ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation
via Mechanistic Interpretability</p></li>
<li><p>LaMP: Language-Motion Pretraining for Motion Generation,
Retrieval, and Captioning</p></li>
<li><p>RAG-DDR: Optimizing Retrieval-Augmented Generation Using
Differentiable Data Rewards</p></li>
<li><p>Follow My Instruction and Spill the Beans: Scalable Data
Extraction from Retrieval-Augmented Generation Systems</p></li>
<li><p>DuoAttention: Efficient Long-Context LLM Inference with Retrieval
and Streaming Heads</p></li>
<li><p>Learning Fine-Grained Representations through Textual Token
Disentanglement in Composed Video Retrieval</p></li>
<li><p>On the Convergence of No-Regret Dynamics in Information Retrieval
Games with Proportional Ranking Functions</p></li>
<li><p>NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for
Retrieval</p></li>
<li><p>TempMe: Video Temporal Token Merging for Efficient Text-Video
Retrieval</p></li>
<li><p>Provence: Efficient and Robust Context Pruning for
Retrieval-Augmented Generation</p></li>
<li><p>Retrieval Head Mechanistically Explains Long-Context
Factuality</p></li>
<li><p>Think-on-Graph 2.0: Deep and Faithful Large Language Model
Reasoning with Knowledge-guided Retrieval Augmented Generation</p></li>
<li><p>SePer: Measure Retrieval Utility Through the Lens of Semantic
Perplexity Reduction</p></li>
<li><p>RNNs are not Transformers (Yet): The Key Bottleneck on In-Context
Retrieval</p></li>
<li><p>Test-time Adaptation for Cross-modal Retrieval with Query
Shift</p></li>
<li><p>SRSA: Skill Retrieval and Adaptation for Robotic Assembly
Tasks</p></li>
<li><p>Look Before You Leap: Universal Emergent Mechanism for Retrieval
in Language Models</p></li>
<li><p>Grounding by Trying: LLMs with Reinforcement Learning-Enhanced
Retrieval</p></li>
<li><p>Inference Scaling for Long-Context Retrieval Augmented
Generation</p></li>
<li><p>REGENT: A Retrieval-Augmented Generalist Agent That Can Act
In-Context in New Environments</p></li>
<li><p>MLLM as Retriever: Interactively Learning Multimodal Retrieval
for Embodied Agents</p></li>
<li><p>Generalized Video Moment Retrieval</p></li>
<li><p>Retri3D: 3D Neural Graphics Representation Retrieval</p></li>
<li><p>Rethinking and Improving Autoformalization: Towards a Faithful
Metric and a Dependency Retrieval-based Approach</p></li>
<li><p>A Theory for Token-Level Harmonization in Retrieval-Augmented
Generation</p></li>
<li><p>Training Large Language Models for Retrieval-Augmented Question
Answering through Backtracking Correction</p></li>
<li><p>Exploiting Distribution Constraints for Scalable and Efficient
Image Retrieval</p></li>
<li><p>Simple is Effective: The Roles of Graphs and Large Language
Models in Knowledge-Graph-Based Retrieval-Augmented Generation</p></li>
<li><p>RAG-SR: Retrieval-Augmented Generation for Neural Symbolic
Regression</p></li>
<li><p>Auto-GDA: Automatic Domain Adaptation for Efficient Grounding
Verification in Retrieval-Augmented Generation</p></li>
<li><p>Not All Heads Matter: A Head-Level KV Cache Compression Method
with Integrated Retrieval and Reasoning</p></li>
<li><p>Accelerating Inference of Retrieval-Augmented Generation via
Sparse Context Selection</p></li>
<li><p>Speculative RAG: Enhancing Retrieval Augmented Generation through
Drafting</p></li>
<li><p>RAPID: Retrieval Augmented Training of Differentially Private
Diffusion Models</p></li>
<li><p>BRIGHT: A Realistic and Challenging Benchmark for
Reasoning-Intensive Retrieval</p></li>
<li><p>RazorAttention: Efficient KV Cache Compression Through Retrieval
Heads # NIPS 24（按retrieval关键词检索）</p></li>
</ul>
<h1 id="cvpr-25之所以在cvpr上找是因为现在似乎有一些研究关注rag和多模态的结合那可能在cvpr上会比较多一些吧按retrieval关键词找的">CVPR
25（之所以在cvpr上找，是因为现在似乎有一些研究关注RAG和多模态的结合，那可能在cvpr上会比较多一些吧）（按retrieval关键词找的）</h1>
<h2 id="drvideo-document-retrieval-based-long-video-understanding"><strong>DrVideo:
Document Retrieval Based Long Video Understanding</strong></h2>
<h2 id="cobra-combinatorial-retrieval-augmentation-for-few-shot-adaptation"><strong>COBRA:
COmBinatorial Retrieval Augmentation for Few-Shot
Adaptation</strong></h2>
<h2 id="salova-segment-augmented-long-video-assistant-for-targeted-retrieval-and-routing-in-long-form-video-analysis"><strong><a target="_blank" rel="noopener" href="https://ivy-lvlm.github.io/SALOVA/">SALOVA: Segment-Augmented Long
Video Assistant for Targeted Retrieval and Routing in Long-Form Video
Analysis</a></strong></h2>
<h2 id="fuzzy-multimodal-learning-for-trusted-cross-modal-retrieval"><strong>Fuzzy
Multimodal Learning for Trusted Cross-modal Retrieval</strong></h2>
<h2 id="few-shot-recognition-via-stage-wise-retrieval-augmented-finetuning"><a target="_blank" rel="noopener" href="https://tian1327.github.io/SWAT/">Few-Shot Recognition via
Stage-Wise Retrieval-Augmented Finetuning</a></h2>
<h2 id="vlog-video-language-models-by-generative-retrieval-of-narration-vocabulary"><strong>VLog:
Video-Language Models by Generative Retrieval of Narration
Vocabulary</strong></h2>
<h2 id="collm-a-large-language-model-for-composed-image-retrieval"><a target="_blank" rel="noopener" href="https://collm-cvpr25.github.io/">CoLLM: A Large Language Model for
Composed Image Retrieval</a></h2>
<h2 id="search-and-detect-training-free-long-tail-object-detection-via-web-image-retrieval"><strong>Search
and Detect: Training-Free Long Tail Object Detection via Web-Image
Retrieval</strong></h2>
<h2 id="learning-compatible-multi-prize-subnetworks-for-asymmetric-retrieval"><strong>Learning
Compatible Multi-Prize Subnetworks for Asymmetric
Retrieval</strong></h2>
<h2 id="learning-audio-guided-video-representation-with-gated-attention-for-video-text-retrieval"><strong>Learning
Audio-guided Video Representation with Gated Attention for Video-Text
Retrieval</strong></h2>
<h2 id="lamra-large-multimodal-model-as-your-advanced-retrieval-assistant"><a target="_blank" rel="noopener" href="https://code-kunkun.github.io/LamRA/">LamRA: Large Multimodal
Model as Your Advanced Retrieval Assistant</a></h2>
<h2 id="recurrence-enhanced-vision-and-language-transformers-for-robust-multimodal-document-retrieval"><a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT">Recurrence-Enhanced
Vision-and-Language Transformers for Robust Multimodal Document
Retrieval</a></h2>
<h2 id="missing-target-relevant-information-prediction-with-world-model-for-accurate-zero-shot-composed-image-retrieval"><strong>Missing
Target-Relevant Information Prediction with World Model for Accurate
Zero-Shot Composed Image Retrieval</strong></h2>
<h2 id="ilias-instance-level-image-retrieval-at-scale"><a target="_blank" rel="noopener" href="https://vrg.fel.cvut.cz/ilias/">ILIAS: Instance-Level Image
retrieval At Scale</a></h2>
<h2 id="clip-is-almost-all-you-need-towards-parameter-efficient-scene-text-retrieval-without-ocr"><strong>CLIP
is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval
without OCR</strong></h2>
<h2 id="rethinking-noisy-video-text-retrieval-via-relation-aware-alignment"><strong>Rethinking
Noisy Video-Text Retrieval via Relation-aware Alignment</strong></h2>
<h2 id="bridging-modalities-improving-universal-multimodal-retrieval-by-multimodal-large-language-models"><a target="_blank" rel="noopener" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct">Bridging
Modalities: Improving Universal Multimodal Retrieval by Multimodal Large
Language Models</a></h2>
<h2 id="reason-before-retrieve-one-stage-reflective-chain-of-thoughts-for-training-free-zero-shot-composed-image-retrieval"><a target="_blank" rel="noopener" href="https://github.com/Pter61/osrcir">Reason-before-Retrieve:
One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot
Composed Image Retrieval</a></h2>
<h2 id="mari-material-retrieval-integration-across-domains"><strong>MaRI:
Material Retrieval Integration across Domains</strong></h2>
<h2 id="vdocrag-retrieval-augmented-generation-over-visually-rich-documents这篇明确出现rag了啊"><a target="_blank" rel="noopener" href="https://vdocrag.github.io/">VDocRAG: Retrieval-Augmented
Generation over Visually-Rich Documents</a>（这篇明确出现RAG了啊）</h2>
<h2 id="range-retrieval-augmented-neural-fields-for-multi-resolution-geo-embeddings这个也是有retrieval-augmented"><strong>RANGE:
Retrieval Augmented Neural Fields for Multi-Resolution
Geo-Embeddings</strong>（这个也是有retrieval augmented）</h2>
<h2 id="rap-retrieval-augmented-personalization-for-multimodal-large-language-models我去这么巧和icml-oral那篇感觉做的题材有点像啊"><a target="_blank" rel="noopener" href="https://hoar012.github.io/RAP-Project/">RAP: Retrieval-Augmented
Personalization for Multimodal Large Language
Models</a>（我去，这么巧，和ICML oral那篇感觉做的题材有点像啊）</h2>
<h2 id="video-colbert-contextualized-late-interaction-for-text-to-video-retrieval"><strong>Video-ColBERT:
Contextualized Late Interaction for Text-to-Video
Retrieval</strong></h2>
<h2 id="imagine-and-seek-improving-composed-image-retrieval-with-an-imagined-proxy"><strong>Imagine
and Seek: Improving Composed Image Retrieval with an Imagined
Proxy</strong></h2>
<h2 id="multivent-2.0-a-massive-multilingual-benchmark-for-event-centric-video-retrieval"><strong>MultiVENT
2.0: A Massive Multilingual Benchmark for Event-Centric Video
Retrieval</strong></h2>
<h2 id="the-devil-is-in-the-prompts-retrieval-augmented-prompt-optimization-for-text-to-video-generation不会是prompt-engineering吧"><strong>The
Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for
Text-to-Video Generation</strong>（不会是prompt engineering吧……）</h2>
<h2 id="narrating-the-video-boosting-text-video-retrieval-via-comprehensive-utilization-of-frame-level-captions"><a target="_blank" rel="noopener" href="https://multimodal-understanding-group.github.io/NarVid/">Narrating
the Video: Boosting Text-Video Retrieval via Comprehensive Utilization
of Frame-Level Captions</a></h2>
<h2 id="chat-based-person-retrieval-via-dialogue-refined-cross-modal-alignment"><strong>Chat-based
Person Retrieval via Dialogue-Refined Cross-Modal
Alignment</strong></h2>
<h2 id="prompthash-affinity-prompted-collaborative-cross-modal-learning-for-adaptive-hashing-retrieval"><a target="_blank" rel="noopener" href="https://github.com/ShiShuMo/PromptHash">PromptHash:
Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive
Hashing Retrieval</a></h2>
<h2 id="neighborretr-balancing-hub-centrality-in-cross-modal-retrieval"><a target="_blank" rel="noopener" href="https://zzezze.github.io/NeighborRetr/">NeighborRetr: Balancing
Hub Centrality in Cross-Modal Retrieval</a></h2>
<h2 id="learning-with-noisy-triplet-correspondence-for-composed-image-retrieval"><a target="_blank" rel="noopener" href="https://github.com/CharlesNeilWilliams/TME">Learning with Noisy
Triplet Correspondence for Composed Image Retrieval</a></h2>
<h2 id="garmentpile-point-level-visual-affordance-guided-retrieval-and-adaptation-for-cluttered-garments-manipulation"><strong>GarmentPile:
Point-Level Visual Affordance Guided Retrieval and Adaptation for
Cluttered Garments Manipulation</strong></h2>
<h2 id="towards-natural-language-based-document-image-retrieval-new-dataset-and-benchmark"><strong>Towards
Natural Language-Based Document Image Retrieval: New Dataset and
Benchmark</strong></h2>
<h2 id="semantic-library-adaptation-lora-retrieval-and-fusion-for-open-vocabulary-semantic-segmentation"><strong>Semantic
Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary
Semantic Segmentation</strong></h2>
<h2 id="ccin-compositional-conflict-identification-and-neutralization-for-composed-image-retrieval"><strong>CCIN:
Compositional Conflict Identification and Neutralization for Composed
Image Retrieval</strong></h2>
<h2 id="generative-zero-shot-composed-image-retrieval"><strong>Generative
Zero-Shot Composed Image Retrieval</strong></h2>
<h2 id="context-cir-learning-from-concepts-in-text-for-composed-image-retrieval好多这种composed-image-retrieval啊这个是啥啊"><strong>ConText-CIR:
Learning from Concepts in Text for Composed Image
Retrieval</strong>（好多这种composed image
retrieval啊，这个是啥啊）</h2>
<h2 id="discovla-discrepancy-reduction-in-vision-language-and-alignment-for-parameter-efficient-video-text-retrieval"><a target="_blank" rel="noopener" href="https://github.com/LunarShen/DsicoVLA">DiscoVLA: Discrepancy
Reduction in Vision, Language, and Alignment for Parameter-Efficient
Video-Text Retrieval</a></h2>
<h1 id="iccv-25按retrieval关键词找的">ICCV
25（按retrieval关键词找的）</h1>
<h2 id="g-dexgrasp-generalizable-dexterous-grasping-synthesis-via-part-aware-prior-retrieval-and-prior-assisted-generation"><strong>G-DexGrasp:
Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior
Retrieval and Prior-Assisted Generation</strong></h2>
<h2 id="signs-as-tokens-a-retrieval-enhanced-multilingual-sign-language-generator"><strong>Signs
as Tokens: A Retrieval-Enhanced Multilingual Sign Language
Generator</strong></h2>
<h2 id="hybrid-tower-fine-grained-pseudo-query-interaction-and-generation-for-text-to-video-retrieval"><strong>Hybrid-Tower:
Fine-grained Pseudo-query Interaction and Generation for Text-to-Video
Retrieval</strong></h2>
<h2 id="the-devil-is-in-the-spurious-correlations-boosting-moment-retrieval-with-dynamic-learning"><a target="_blank" rel="noopener" href="https://xyangzhou.github.io/TD-DETR/">The Devil is in the Spurious
Correlations: Boosting Moment Retrieval with Dynamic Learning</a></h2>
<h2 id="taming-the-untamed-graph-based-knowledge-retrieval-and-reasoning-for-mllms-to-conquer-the-unknown"><strong>Taming
the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to
Conquer the Unknown</strong></h2>
<h2 id="learnable-retrieval-enhanced-visual-text-alignment-and-fusion-for-radiology-report-generation"><strong>Learnable
Retrieval Enhanced Visual-Text Alignment and Fusion for Radiology Report
Generation</strong></h2>
<h2 id="zero-shot-composed-image-retrieval-via-dual-stream-instruction-aware-distillation"><strong>Zero-Shot
Composed Image Retrieval via Dual-Stream Instruction-Aware
Distillation</strong></h2>
<h2 id="sims-simulating-stylized-human-scene-interactions-with-retrieval-augmented-script-generation"><strong>SIMS:
Simulating Stylized Human-Scene Interactions with Retrieval-Augmented
Script Generation</strong></h2>
<h2 id="beyond-simple-edits-composed-video-retrieval-with-dense-modifications"><strong>Beyond
Simple Edits: Composed Video Retrieval with Dense
Modifications</strong></h2>
<h2 id="reference-based-super-resolution-via-image-based-retrieval-augmented-generation-diffusion"><strong>Reference-based
Super-Resolution via Image-based Retrieval-Augmented Generation
Diffusion</strong></h2>
<h2 id="ma-cir-a-multimodal-arithmetic-benchmark-for-composed-image-retrieval"><strong>MA-CIR:
A Multimodal Arithmetic Benchmark for Composed Image
Retrieval</strong></h2>
<h2 id="test-time-retrieval-augmented-adaptation-for-vision-language-models"><strong>Test-Time
Retrieval-Augmented Adaptation for Vision-Language Models</strong></h2>
<h2 id="gesturehydra-semantic-co-speech-gesture-synthesis-via-hybrid-modality-diffusion-transformer-and-cascaded-synchronized-retrieval-augmented-generation"><strong>GestureHYDRA:
Semantic Co-speech Gesture Synthesis via Hybrid Modality Diffusion
Transformer and Cascaded-Synchronized Retrieval-Augmented
Generation</strong></h2>
<h2 id="borrowing-eyes-for-the-blind-spot-overcoming-data-scarcity-in-malicious-video-detection-via-cross-domain-retrieval-augmentation"><a target="_blank" rel="noopener" href="https://github.com/ronpay/CRAVE">Borrowing Eyes for the Blind
Spot: Overcoming Data Scarcity in Malicious Video Detection via
Cross-Domain Retrieval Augmentation</a></h2>
<h2 id="enhancing-partially-relevant-video-retrieval-with-hyperbolic-learning"><strong>Enhancing
Partially Relevant Video Retrieval with Hyperbolic
Learning</strong></h2>
<h2 id="chatreid-open-ended-interactive-person-retrieval-via-hierarchical-progressive-tuning-for-vision-language-models"><strong>ChatReID:
Open-ended Interactive Person Retrieval via Hierarchical Progressive
Tuning for Vision Language Models</strong></h2>
<h2 id="remp-ad-retrieval-enhanced-multi-modal-prompt-fusion-for-few-shot-industrial-visual-anomaly-detection"><strong>ReMP-AD:
Retrieval-enhanced Multi-modal Prompt Fusion for Few-Shot Industrial
Visual Anomaly Detection</strong></h2>
<h2 id="beyond-single-images-retrieval-self-augmented-unsupervised-camouflaged-object-detection"><strong>Beyond
Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object
Detection</strong></h2>
<h2 id="describe-adapt-and-combine-empowering-clip-encoders-for-open-set-3d-object-retrieval"><strong>Describe,
Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object
Retrieval</strong></h2>
<h2 id="hierarchy-aware-pseudo-word-learning-with-text-adaptation-for-zero-shot-composed-image-retrieval"><strong>Hierarchy-Aware
Pseudo Word Learning with Text Adaptation for Zero-Shot Composed Image
Retrieval</strong></h2>
<h2 id="carim-caption-based-autonomous-driving-scene-retrieval-via-inclusive-text-matching"><strong>CARIM:
Caption-Based Autonomous Driving Scene Retrieval via Inclusive Text
Matching</strong></h2>
<h2 id="ophclip-hierarchical-retrieval-augmented-learning-for-ophthalmic-surgical-video-language-pretraining"><strong>OphCLIP:
Hierarchical Retrieval-Augmented Learning for Ophthalmic Surgical
Video-Language Pretraining</strong></h2>
<h2 id="learning-visual-hierarchies-in-hyperbolic-space-for-image-retrieval"><strong>Learning
Visual Hierarchies in Hyperbolic Space for Image Retrieval</strong></h2>
<h2 id="monster-a-unified-model-for-motion-scene-text-retrieval"><strong>MonSTeR:
a Unified Model for Motion, Scene, Text Retrieval</strong></h2>
<h2 id="cotmr-chain-of-thought-multi-scale-reasoning-for-training-free-zero-shot-composed-image-retrieval"><strong>CoTMR:
Chain-of-Thought Multi-Scale Reasoning for Training-Free Zero-Shot
Composed Image Retrieval</strong></h2>
<h2 id="ocr-hinders-rag-evaluating-the-cascading-impact-of-ocr-on-retrieval-augmented-generation"><strong>OCR
Hinders RAG: Evaluating the Cascading Impact of OCR on
Retrieval-Augmented Generation</strong></h2>
<h2 id="augmenting-moment-retrieval-zero-dependency-two-stage-learning"><strong>Augmenting
Moment Retrieval: Zero-Dependency Two-Stage Learning</strong></h2>
<h2 id="quantifying-and-narrowing-the-unknown-interactive-text-to-video-retrieval-via-uncertainty-minimization"><strong>Quantifying
and Narrowing the Unknown: Interactive Text-to-Video Retrieval via
Uncertainty Minimization</strong></h2>
<h2 id="training-free-personalization-via-retrieval-and-reasoning-on-fingerprints"><a target="_blank" rel="noopener" href="https://deepayan137.github.io/papers/training-free-personalization.html">Training-Free
Personalization via Retrieval and Reasoning on Fingerprints</a></h2>
<h2 id="prototypes-are-balanced-units-for-efficient-and-effective-partially-relevant-video-retrieval"><strong>Prototypes
are Balanced Units for Efficient and Effective Partially Relevant Video
Retrieval</strong></h2>
<h2 id="bidirectional-likelihood-estimation-with-multi-modal-large-language-models-for-text-video-retrieval"><strong>Bidirectional
Likelihood Estimation with Multi-Modal Large Language Models for
Text-Video Retrieval</strong></h2>
<h2 id="multi-schema-proximity-network-for-composed-image-retrieval"><strong>Multi-Schema
Proximity Network for Composed Image Retrieval</strong></h2>
<h2 id="an-efficient-post-hoc-framework-for-reducing-task-discrepancy-of-text-encoders-for-composed-image-retrieval"><strong>An
Efficient Post-hoc Framework for Reducing Task Discrepancy of Text
Encoders for Composed Image Retrieval</strong></h2>
<h2 id="autocompose-automatic-generation-of-pose-transition-descriptions-for-composed-pose-retrieval-using-multimodal-llms"><strong>AutoComPose:
Automatic Generation of Pose Transition Descriptions for Composed Pose
Retrieval Using Multimodal LLMs</strong></h2>
<h1 id="ragretrieval-augmented-generation方法拓展与应用">1.
<strong>RAG（Retrieval-Augmented
Generation）方法拓展与应用</strong></h1>
<p>这些论文显式或核心思路就是 <strong>把 RAG
技术迁移到视觉/多模态/特定任务场景</strong>：</p>
<ul>
<li><p><strong>VDocRAG: Retrieval-Augmented Generation over
Visually-Rich Documents (CVPR)</strong></p></li>
<li><p><strong>RANGE: Retrieval Augmented Neural Fields for
Multi-Resolution Geo-Embeddings (CVPR)</strong></p></li>
<li><p><strong>RAP: Retrieval-Augmented Personalization for Multimodal
LLMs (CVPR)</strong></p></li>
<li><p><strong>The Devil is in the Prompts: Retrieval-Augmented Prompt
Optimization for Text-to-Video Generation (CVPR)</strong></p></li>
<li><p><strong>SIMS: Retrieval-Augmented Script Generation
(ICCV)</strong></p></li>
<li><p><strong>Reference-based Super-Resolution via Image-based
Retrieval-Augmented Generation Diffusion (ICCV)</strong></p></li>
<li><p><strong>GestureHYDRA: Retrieval-Augmented Generation for Gesture
Synthesis (ICCV)</strong></p></li>
<li><p><strong>OphCLIP: Hierarchical Retrieval-Augmented Learning for
Ophthalmic Surgical Video-Language Pretraining (ICCV)</strong></p></li>
<li><p><strong>ReMP-AD: Retrieval-enhanced Multi-modal Prompt Fusion for
Few-Shot Industrial Visual Anomaly Detection (ICCV)</strong></p></li>
<li><p><strong>OCR Hinders RAG: Evaluating OCR’s Impact on RAG
(ICCV)</strong></p></li>
<li><p><strong>Borrowing Eyes for the Blind Spot: Malicious Video
Detection via Cross-Domain Retrieval Augmentation
(ICCV)</strong>（这篇竟然还没开放paper吗？奇怪了……） # 2.
<strong>Composed Image/Video Retrieval（组合检索，编辑式检索）</strong>
这类特别多，主要是 <strong>给定图像 +
文本修饰/修改条件，找到目标图像</strong>，近年是 retrieval
热点：</p></li>
<li><p>ConText-CIR, CCIN, Generative CIR, Learning w/ Noisy Triplet,
Imagine and Seek, Reason-before-Retrieve, etc. (CVPR)</p></li>
<li><p>Beyond Simple Edits: Composed Video Retrieval with Dense
Modifications (ICCV)</p></li>
<li><p>Zero-Shot CIR via Dual-Stream Instruction-Aware Distillation
(ICCV)</p></li>
<li><p>Hierarchy-Aware Pseudo Word Learning for Zero-Shot CIR
(ICCV)</p></li>
<li><p>Multi-Schema Proximity Network for CIR (ICCV)</p></li>
<li><p>An Efficient Post-hoc Framework for Reducing Task Discrepancy of
Text Encoders for CIR (ICCV)</p></li>
<li><p>CoTMR: Chain-of-Thought Reasoning for Zero-Shot CIR
(ICCV)</p></li>
<li><p>MA-CIR Benchmark (ICCV)</p></li>
</ul>
<h1 id="video-multimodal-retrieval">3. <strong>Video &amp; Multimodal
Retrieval</strong></h1>
<p>长视频、视频-文本、音频-视频、事件检索等：</p>
<ul>
<li><p>DrVideo, SALOVA, VLog, Narrating the Video, Video-ColBERT,
DiscoVLA, MultiVENT 2.0, Rethinking Noisy Video-Text Retrieval,
Chat-based Person Retrieval, Learning Audio-guided Video Representation
(CVPR)</p></li>
<li><p>Hybrid-Tower, Enhancing Partially Relevant Video Retrieval,
Prototypes are Balanced Units, Quantifying and Narrowing the Unknown,
MonSTeR (ICCV) # 类似ICML
25那篇论文的思路的论文（也就是类比了用RAG输入超长输入的思路）</p></li>
<li><p><strong>VDocRAG (CVPR25)</strong></p>
<ul>
<li><p>任务：长文档视觉问答。</p></li>
<li><p>思路：把超长的文档视为“长文本”，而文档里的图片、表格又和文本混杂
→ 类似于多模态的“超长输入”问题。</p></li>
<li><p>方法：通过 retrieval 选取相关片段（文字/视觉），解决了超长输入的
token 负担。<br>
➝ 本质上和你说的 <em>HR-images → 长文本</em>
类似，都是把“大输入问题”转化为 “检索 + 局部 reasoning”。</p></li>
</ul></li>
<li><p><strong>SALOVA (CVPR25)</strong>（长视频摘要 / QA）</p>
<ul>
<li><p>视频是天然的长序列，直接处理会爆炸。</p></li>
<li><p>他们用 retrieval 在 video segments 中挑出关键信息 →
类似把视频理解类比成长文本阅读。</p></li>
</ul></li>
<li><p><strong>SIMS (ICCV25)</strong>（多模态对话检索）</p>
<ul>
<li>把跨模态对话（文本 + 图像）看作“多源长对话”，通过 retrieval 来挑关键
evidence，避免全量建模。</li>
</ul></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/08/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/long-context/seminar%E8%AE%B0%E5%BD%95%EF%BC%9AFAI-Seminar%20%20MIT%20%E7%8E%8B%E4%B8%80%E9%A3%9E%20%20LLM%E7%9A%84%E9%95%BF%E6%96%87%E6%9C%AC%E5%9B%B0%E5%A2%83%EF%BC%9ANTP%E4%BB%BB%E5%8A%A1%E4%B8%8ETransformer%E6%9E%B6%E6%9E%84%E7%9A%84%E5%86%85%E5%9C%A8%E5%81%8F%E5%B7%AE/" rel="prev" title="seminar记录：FAI-Seminar MIT 王一飞 LLM的长文本困境：NTP任务与Transformer架构的内在偏差">
      <i class="fa fa-chevron-left"></i> seminar记录：FAI-Seminar MIT 王一飞 LLM的长文本困境：NTP任务与Transformer架构的内在偏差
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/08/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/UniRAG%EF%BC%9AUnified%20Query%20Understanding%20Method%20for%20%20Retrieval%20Augmented%20Generation/" rel="next" title="UniRAG：Unified Query Understanding Method for Retrieval Augmented Generation">
      UniRAG：Unified Query Understanding Method for Retrieval Augmented Generation <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#acl-25"><span class="nav-number">1.</span> <span class="nav-text">ACL 25</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#main"><span class="nav-number">1.1.</span> <span class="nav-text">main</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#hybgrag-hybrid-retrieval-augmented-generation-on-textual-and-relational-knowledge-bases"><span class="nav-number">1.1.1.</span> <span class="nav-text">HybGRAG:
Hybrid Retrieval-Augmented Generation on Textual and Relational
Knowledge Bases</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#main-rag-multi-agent-filtering-retrieval-augmented-generation"><span class="nav-number">1.1.2.</span> <span class="nav-text">MAIN-RAG:
Multi-Agent Filtering Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-critic-leveraging-automated-critic-guided-agentic-workflow-for-retrieval-augmented-generation"><span class="nav-number">1.1.3.</span> <span class="nav-text">RAG-Critic:
Leveraging Automated Critic-Guided Agentic Workflow for Retrieval
Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#saferag-benchmarking-security-in-retrieval-augmented-generation-of-large-language-model"><span class="nav-number">1.1.4.</span> <span class="nav-text">SafeRAG:
Benchmarking Security in Retrieval-Augmented Generation of Large
Language Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#on-the-robustness-of-rag-systems-in-educational-question-answering-under-knowledge-discrepancies"><span class="nav-number">1.1.5.</span> <span class="nav-text">On
the Robustness of RAG Systems in Educational Question Answering under
Knowledge Discrepancies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pandoras-box-or-aladdins-lamp-a-comprehensive-analysis-revealing-the-role-of-rag-noise-in-large-language-models"><span class="nav-number">1.1.6.</span> <span class="nav-text">Pandora’s
Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of
RAG Noise in Large Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#neusym-rag-hybrid-neural-symbolic-retrieval-with-multiview-structuring-for-pdf-question-answering"><span class="nav-number">1.1.7.</span> <span class="nav-text">NeuSym-RAG:
Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF
Question Answering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#drag-distilling-rag-for-slms-from-llms-to-transfer-knowledge-and-mitigate-hallucination-via-evidence-and-graph-based-distillation"><span class="nav-number">1.1.8.</span> <span class="nav-text">DRAG:
Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate
Hallucination via Evidence and Graph-based Distillation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rageval-scenario-specific-rag-evaluation-dataset-generation-framework"><span class="nav-number">1.1.9.</span> <span class="nav-text">RAGEval:
Scenario Specific RAG Evaluation Dataset Generation
Framework</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#are-llms-effective-psychological-assessors-leveraging-adaptive-rag-for-interpretable-mental-health-screening-through-psychometric-practice"><span class="nav-number">1.1.10.</span> <span class="nav-text">Are
LLMs effective psychological assessors? Leveraging adaptive RAG for
interpretable mental health screening through psychometric
practice</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#s-rag-a-novel-audit-framework-for-detecting-unauthorized-use-of-personal-data-in-rag-systems"><span class="nav-number">1.1.11.</span> <span class="nav-text">S-RAG:
A Novel Audit Framework for Detecting Unauthorized Use of Personal Data
in RAG Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gainrag-preference-alignment-in-retrieval-augmented-generation-through-gain-signal-synthesis"><span class="nav-number">1.1.12.</span> <span class="nav-text">GainRAG:
Preference Alignment in Retrieval-Augmented Generation through Gain
Signal Synthesis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tcrag-turingcomplete-rags-case-study-on-medical-llm-systems"><span class="nav-number">1.1.13.</span> <span class="nav-text">TC–RAG:
Turing–Complete RAG’s Case study on Medical LLM Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#divide-then-align-honest-alignment-based-on-the-knowledge-boundary-of-rag"><span class="nav-number">1.1.14.</span> <span class="nav-text">Divide-Then-Align:
Honest Alignment based on the Knowledge Boundary of RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hykge-a-hypothesis-knowledge-graph-enhanced-rag-framework-for-accurate-and-reliable-medical-llms-responses"><span class="nav-number">1.1.15.</span> <span class="nav-text">HyKGE:
A Hypothesis Knowledge Graph Enhanced RAG Framework for Accurate and
Reliable Medical LLMs Responses</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wavrag-audio-integrated-retrieval-augmented-generation-for-spoken-dialogue-models"><span class="nav-number">1.1.16.</span> <span class="nav-text">WavRAG:
Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue
Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unirag-unified-query-understanding-method-for-retrieval-augmented-generation"><span class="nav-number">1.1.17.</span> <span class="nav-text">UniRAG:
Unified Query Understanding Method for Retrieval Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#towards-omni-rag-comprehensive-retrieval-augmented-generation-for-large-language-models-in-medical-applications"><span class="nav-number">1.1.18.</span> <span class="nav-text">Towards
Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large
Language Models in Medical Applications</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#molrag-unlocking-the-power-of-large-language-models-for-molecular-property-prediction"><span class="nav-number">1.1.19.</span> <span class="nav-text">MolRAG:
Unlocking the Power of Large Language Models for Molecular Property
Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#removal-of-hallucination-on-hallucination-debate-augmented-rag"><span class="nav-number">1.1.20.</span> <span class="nav-text">Removal
of Hallucination on Hallucination: Debate-Augmented RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#eventrag-enhancing-llm-generation-with-event-knowledge-graphs"><span class="nav-number">1.1.21.</span> <span class="nav-text">EventRAG:
Enhancing LLM Generation with Event Knowledge Graphs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-distracting-effect-understanding-irrelevant-passages-in-rag"><span class="nav-number">1.1.22.</span> <span class="nav-text">The
Distracting Effect: Understanding Irrelevant Passages in
RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kirag-knowledge-driven-iterative-retriever-for-enhancing-retrieval-augmented-generation"><span class="nav-number">1.1.23.</span> <span class="nav-text">KiRAG:
Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#faithfulrag-fact-level-conflict-modeling-for-context-faithful-retrieval-augmented-generation"><span class="nav-number">1.1.24.</span> <span class="nav-text">FaithfulRAG:
Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#memerag-a-multilingual-end-to-end-meta-evaluation-benchmark-for-retrieval-augmented-generation"><span class="nav-number">1.1.25.</span> <span class="nav-text">MEMERAG:
A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval
Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dialogue-rag-enhancing-retrieval-for-llms-via-node-linking-utterance-rewriting"><span class="nav-number">1.1.26.</span> <span class="nav-text">Dialogue-RAG:
Enhancing Retrieval for LLMs via Node-Linking Utterance
Rewriting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-efficiency-vs.-accuracy-trade-off-optimizing-rag-enhanced-llm-recommender-systems-using-multi-head-early-exit"><span class="nav-number">1.1.27.</span> <span class="nav-text">The
Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM
Recommender Systems Using Multi-Head Early Exit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sgic-a-self-guided-iterative-calibration-framework-for-rag"><span class="nav-number">1.1.28.</span> <span class="nav-text">SGIC:
A Self-Guided Iterative Calibration Framework for RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#medical-graph-rag-evidence-based-medical-large-language-model-via-graph-retrieval-augmented-generation"><span class="nav-number">1.1.29.</span> <span class="nav-text">Medical
Graph RAG: Evidence-based Medical Large Language Model via Graph
Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#astute-rag-overcoming-imperfect-retrieval-augmentation-and-knowledge-conflicts-for-large-language-models"><span class="nav-number">1.1.30.</span> <span class="nav-text">Astute
RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts
for Large Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#real-mm-rag-a-real-world-multi-modal-retrieval-benchmark"><span class="nav-number">1.1.31.</span> <span class="nav-text">REAL-MM-RAG:
A Real-World Multi-Modal Retrieval Benchmark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dualrag-a-dual-process-approach-to-integrate-reasoning-and-retrieval-for-multi-hop-question-answering"><span class="nav-number">1.1.32.</span> <span class="nav-text">DualRAG:
A Dual-Process Approach to Integrate Reasoning and Retrieval for
Multi-Hop Question Answering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#core-mmrag-cross-source-knowledge-reconciliation-for-multimodal-rag"><span class="nav-number">1.1.33.</span> <span class="nav-text">CoRe-MMRAG:
Cross-Source Knowledge Reconciliation for Multimodal RAG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#findings"><span class="nav-number">1.2.</span> <span class="nav-text">findings</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#treerag-unleashing-the-power-of-hierarchical-storage-for-enhanced-knowledge-retrieval-in-long-documents"><span class="nav-number">1.2.1.</span> <span class="nav-text">TreeRAG:
Unleashing the Power of Hierarchical Storage for Enhanced Knowledge
Retrieval in Long Documents</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scrag-hybrid-retrieval-augmented-generation-for-llm-based-cross-tissue-single-cell-annotation"><span class="nav-number">1.2.2.</span> <span class="nav-text">scRAG:
Hybrid Retrieval-Augmented Generation for LLM-based Cross-Tissue
Single-Cell Annotation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hoprag-multi-hop-reasoning-for-logic-aware-retrieval-augmented-generation"><span class="nav-number">1.2.3.</span> <span class="nav-text">HopRAG:
Multi-Hop Reasoning for Logic-Aware Retrieval Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ltrag-enhancing-autoformalization-and-self-refinement-for-logical-reasoning-with-thought-guided-rag"><span class="nav-number">1.2.4.</span> <span class="nav-text">LTRAG:
Enhancing autoformalization and self-refinement for logical reasoning
with Thought-Guided RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#simgrag-leveraging-similar-subgraphs-for-knowledge-graphs-driven-retrieval-augmented-generation"><span class="nav-number">1.2.5.</span> <span class="nav-text">SimGRAG:
Leveraging Similar Subgraphs for Knowledge Graphs Driven
Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#remoterag-a-privacy-preserving-llm-cloud-rag-service"><span class="nav-number">1.2.6.</span> <span class="nav-text">RemoteRAG:
A Privacy-Preserving LLM Cloud RAG Service</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#investigating-language-preference-of-multilingual-rag-systems"><span class="nav-number">1.2.7.</span> <span class="nav-text">Investigating
Language Preference of Multilingual RAG Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#frag-a-flexible-modular-framework-for-retrieval-augmented-generation-based-on-knowledge-graphs"><span class="nav-number">1.2.8.</span> <span class="nav-text">FRAG:
A Flexible Modular Framework for Retrieval-Augmented Generation based on
Knowledge Graphs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#navrag-generating-user-demand-instructions-for-embodied-navigation-through-retrieval-augmented-llm"><span class="nav-number">1.2.9.</span> <span class="nav-text">NavRAG:
Generating User Demand Instructions for Embodied Navigation through
Retrieval-Augmented LLM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#speecht-rag-reliable-depression-detection-in-llms-with-retrieval-augmented-generation-using-speech-timing-information"><span class="nav-number">1.2.10.</span> <span class="nav-text">SpeechT-RAG:
Reliable Depression Detection in LLMs with Retrieval-Augmented
Generation Using Speech Timing Information</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#roserag-robust-retrieval-augmented-generation-with-small-scale-llms-via-margin-aware-preference-optimization"><span class="nav-number">1.2.11.</span> <span class="nav-text">RoseRAG:
Robust Retrieval-augmented Generation with Small-scale LLMs via
Margin-aware Preference Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gnn-rag-graph-neural-retrieval-for-efficient-large-language-model-reasoning-on-knowledge-graphs"><span class="nav-number">1.2.12.</span> <span class="nav-text">GNN-RAG:
Graph Neural Retrieval for Efficient Large Language Model Reasoning on
Knowledge Graphs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#astrid---an-automated-and-scalable-triad-for-the-evaluation-of-rag-based-clinical-question-answering-systems"><span class="nav-number">1.2.13.</span> <span class="nav-text">ASTRID
- An Automated and Scalable TRIaD for the Evaluation of RAG-based
Clinical Question Answering Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#garage-a-benchmark-with-grounding-annotations-for-rag-evaluation"><span class="nav-number">1.2.14.</span> <span class="nav-text">GaRAGe:
A Benchmark with Grounding Annotations for RAG Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-rewardbench-benchmarking-reward-models-in-retrieval-augmented-generation-for-preference-alignment"><span class="nav-number">1.2.15.</span> <span class="nav-text">RAG-RewardBench:
Benchmarking Reward Models in Retrieval Augmented Generation for
Preference Alignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mitigating-bias-in-rag-controlling-the-embedder"><span class="nav-number">1.2.16.</span> <span class="nav-text">Mitigating
Bias in RAG: Controlling the Embedder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#synapticrag-enhancing-temporal-memory-retrieval-in-large-language-models-through-synaptic-mechanisms"><span class="nav-number">1.2.17.</span> <span class="nav-text">SynapticRAG:
Enhancing Temporal Memory Retrieval in Large Language Models through
Synaptic Mechanisms</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#techniquerag-retrieval-augmented-generation-for-adversarial-technique-annotation-in-cyber-threat-intelligence-text"><span class="nav-number">1.2.18.</span> <span class="nav-text">TechniqueRAG:
Retrieval Augmented Generation for Adversarial Technique Annotation in
Cyber Threat Intelligence Text</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#videorag-retrieval-augmented-generation-over-video-corpus"><span class="nav-number">1.2.19.</span> <span class="nav-text">VideoRAG:
Retrieval-Augmented Generation over Video Corpus</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#query-driven-multimodal-graphrag-dynamic-local-knowledge-graph-construction-for-online-reasoning"><span class="nav-number">1.2.20.</span> <span class="nav-text">Query-Driven
Multimodal GraphRAG: Dynamic Local Knowledge Graph Construction for
Online Reasoning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#causalrag-integrating-causal-graphs-into-retrieval-augmented-generation"><span class="nav-number">1.2.21.</span> <span class="nav-text">CausalRAG:
Integrating Causal Graphs into Retrieval-Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#eventrag-supportive-event-retrieval-on-hypergraph-for-future-forecasting"><span class="nav-number">1.2.22.</span> <span class="nav-text">EventRAG:
Supportive Event Retrieval on Hypergraph for Future
Forecasting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#safeguarding-rag-pipelines-with-gmtp-a-gradient-based-masked-token-probability-method-for-poisoned-document-detection"><span class="nav-number">1.2.23.</span> <span class="nav-text">Safeguarding
RAG Pipelines with GMTP: A Gradient-based Masked Token Probability
Method for Poisoned Document Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ecorag-evidentiality-guided-compression-for-long-context-rag"><span class="nav-number">1.2.24.</span> <span class="nav-text">ECoRAG:
Evidentiality-guided Compression for Long Context RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hash-rag-bridging-deep-hashing-with-retriever-for-efficient-fine-retrieval-and-augmented-generation"><span class="nav-number">1.2.25.</span> <span class="nav-text">HASH-RAG:
Bridging Deep Hashing with Retriever for Efficient, Fine Retrieval and
Augmented Generation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#naacl-25"><span class="nav-number">2.</span> <span class="nav-text">NAACL 25</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#main-1"><span class="nav-number">2.1.</span> <span class="nav-text">main</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-star-enhancing-deliberative-reasoning-with-retrieval-augmented-verification-and-refinement"><span class="nav-number">2.1.1.</span> <span class="nav-text">RAG-Star:
Enhancing Deliberative Reasoning with Retrieval Augmented Verification
and Refinement</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#simrag-self-improving-retrieval-augmented-generation-for-adapting-large-language-models-to-specialized-domains"><span class="nav-number">2.1.2.</span> <span class="nav-text">SimRAG:
Self-Improving Retrieval-Augmented Generation for Adapting Large
Language Models to Specialized Domains</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#do-rag-systems-cover-what-matters-evaluating-and-optimizing-responses-with-sub-question-coverage"><span class="nav-number">2.1.3.</span> <span class="nav-text">Do
RAG Systems Cover What Matters? Evaluating and Optimizing Responses with
Sub-Question Coverage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-llms-are-not-safer-a-safety-analysis-of-retrieval-augmented-generation-for-large-language-models"><span class="nav-number">2.1.4.</span> <span class="nav-text">RAG
LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation
for Large Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#trag-term-level-retrieval-augmented-generation-for-domain-adaptive-retrieval"><span class="nav-number">2.1.5.</span> <span class="nav-text">tRAG:
Term-level Retrieval-Augmented Generation for Domain-Adaptive
Retrieval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pa-rag-rag-alignment-via-multi-perspective-preference-optimization"><span class="nav-number">2.1.6.</span> <span class="nav-text">PA-RAG:
RAG Alignment via Multi-Perspective Preference
Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#corag-collaborative-retrieval-augmented-generation"><span class="nav-number">2.1.7.</span> <span class="nav-text">CoRAG:
Collaborative Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transform-retrieval-for-textual-entailment-in-rag"><span class="nav-number">2.1.8.</span> <span class="nav-text">Transform
Retrieval for Textual Entailment in RAG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#findings-1"><span class="nav-number">2.2.</span> <span class="nav-text">findings</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#systematic-knowledge-injection-into-large-language-models-via-diverse-augmentation-for-domain-specific-rag"><span class="nav-number">2.2.1.</span> <span class="nav-text">Systematic
Knowledge Injection into Large Language Models via Diverse Augmentation
for Domain-Specific RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#probing-rag-self-probing-to-guide-language-models-in-selective-document-retrieval"><span class="nav-number">2.2.2.</span> <span class="nav-text">Probing-RAG:
Self-Probing to Guide Language Models in Selective Document
Retrieval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mes-rag-bringing-multi-modal-entity-storage-and-secure-enhancements-to-rag"><span class="nav-number">2.2.3.</span> <span class="nav-text">MES-RAG:
Bringing Multi-modal, Entity-Storage, and Secure Enhancements to
RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#grag-graph-retrieval-augmented-generation"><span class="nav-number">2.2.4.</span> <span class="nav-text">GRAG: Graph
Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coderag-bench-can-retrieval-augment-code-generation"><span class="nav-number">2.2.5.</span> <span class="nav-text">CodeRAG-Bench:
Can Retrieval Augment Code Generation?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unirag-universal-retrieval-augmentation-for-large-vision-language-models"><span class="nav-number">2.2.6.</span> <span class="nav-text">UniRAG:
Universal Retrieval Augmentation for Large Vision Language
Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#funnelrag-a-coarse-to-fine-progressive-retrieval-paradigm-for-rag"><span class="nav-number">2.2.7.</span> <span class="nav-text">FunnelRAG:
A Coarse-to-Fine Progressive Retrieval Paradigm for RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#grappi-a-retrieve-divide-solve-graphrag-framework-for-large-scale-protein-protein-interaction-exploration"><span class="nav-number">2.2.8.</span> <span class="nav-text">GraPPI:
A Retrieve-Divide-Solve GraphRAG Framework for Large-scale
Protein-protein Interaction Exploration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chain-of-rank-enhancing-large-language-models-for-domain-specific-rag-in-edge-device"><span class="nav-number">2.2.9.</span> <span class="nav-text">Chain-of-Rank:
Enhancing Large Language Models for Domain-Specific RAG in Edge
Device</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#superrag-beyond-rag-with-layout-aware-graph-modeling"><span class="nav-number">2.2.10.</span> <span class="nav-text">SuperRAG:
Beyond RAG with Layout-Aware Graph Modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hypa-rag-a-hybrid-parameter-adaptive-retrieval-augmented-generation-system-for-ai-legal-and-policy-applications"><span class="nav-number">2.2.11.</span> <span class="nav-text">HyPA-RAG:
A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI
Legal and Policy Applications</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#evaluating-the-performance-of-rag-methods-for-conversational-ai-in-the-airport-domain"><span class="nav-number">2.2.12.</span> <span class="nav-text">Evaluating
the Performance of RAG Methods for Conversational AI in the Airport
Domain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dsrag-a-double-stream-retrieval-augmented-generation-framework-for-countless-intent-detection"><span class="nav-number">2.2.13.</span> <span class="nav-text">DSRAG:
A Double-Stream Retrieval-Augmented Generation Framework for Countless
Intent Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#from-generating-answers-to-building-explanations-integrating-multi-round-rag-and-causal-modeling-for-scientific-qa"><span class="nav-number">2.2.14.</span> <span class="nav-text">From
Generating Answers to Building Explanations: Integrating Multi-Round RAG
and Causal Modeling for Scientific QA</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#emnlp-24"><span class="nav-number">3.</span> <span class="nav-text">EMNLP 24</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#main-2"><span class="nav-number">3.1.</span> <span class="nav-text">main</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rule-reliable-multimodal-rag-for-factuality-in-medical-vision-language-models"><span class="nav-number">3.1.1.</span> <span class="nav-text">RULE:
Reliable Multimodal RAG for Factuality in Medical Vision Language
Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-qa-arena-evaluating-domain-robustness-for-long-form-retrieval-augmented-question-answering"><span class="nav-number">3.1.2.</span> <span class="nav-text">RAG-QA
Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented
Question Answering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#from-rag-to-riches-retrieval-interlaced-with-sequence-generation"><span class="nav-number">3.1.3.</span> <span class="nav-text">From
RAG to Riches: Retrieval Interlaced with Sequence
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-of-a-haystack-a-challenge-to-long-context-llms-and-rag-systems"><span class="nav-number">3.1.4.</span> <span class="nav-text">Summary
of a Haystack: A Challenge to Long-Context LLMs and RAG
Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dynamicer-resolving-emerging-mentions-to-dynamic-entities-for-rag"><span class="nav-number">3.1.5.</span> <span class="nav-text">DynamicER:
Resolving Emerging Mentions to Dynamic Entities for RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deciphering-the-interplay-of-parametric-and-non-parametric-memory-in-rag-models"><span class="nav-number">3.1.6.</span> <span class="nav-text">Deciphering
the Interplay of Parametric and Non-Parametric Memory in RAG
Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#re-rag-improving-open-domain-qa-performance-and-interpretability-with-relevance-estimator-in-retrieval-augmented-generation"><span class="nav-number">3.1.7.</span> <span class="nav-text">RE-RAG:
Improving Open-Domain QA Performance and Interpretability with Relevance
Estimator in Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#longrag-a-dual-perspective-retrieval-augmented-generation-paradigm-for-long-context-question-answering"><span class="nav-number">3.1.8.</span> <span class="nav-text">LongRAG:
A Dual-perspective Retrieval-Augmented Generation Paradigm for
Long-Context Question Answering</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#findings-2"><span class="nav-number">3.2.</span> <span class="nav-text">findings</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-studio-towards-in-domain-adaptation-of-retrieval-augmented-generation-through-self-alignment"><span class="nav-number">3.2.1.</span> <span class="nav-text">RAG-Studio:
Towards In-Domain Adaptation Of Retrieval Augmented Generation Through
Self-Alignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rafe-ranking-feedback-improves-query-rewriting-for-rag"><span class="nav-number">3.2.2.</span> <span class="nav-text">RaFe:
Ranking Feedback Improves Query Rewriting for RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaptive-selection-for-homogeneous-tools-an-instantiation-in-the-rag-scenario"><span class="nav-number">3.2.3.</span> <span class="nav-text">Adaptive
Selection for Homogeneous Tools: An Instantiation in the RAG
Scenario</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bsharedrag-backbone-shared-retrieval-augmented-generation-for-the-e-commerce-domain"><span class="nav-number">3.2.4.</span> <span class="nav-text">BSharedRAG:
Backbone Shared Retrieval-Augmented Generation for the E-commerce
Domain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#open-rag-enhanced-retrieval-augmented-reasoning-with-open-source-large-language-models"><span class="nav-number">3.2.5.</span> <span class="nav-text">Open-RAG:
Enhanced Retrieval Augmented Reasoning with Open-Source Large Language
Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#typos-that-broke-the-rags-back-genetic-attack-on-rag-pipeline-by-simulating-documents-in-the-wild-via-low-level-perturbations"><span class="nav-number">3.2.6.</span> <span class="nav-text">Typos
that Broke the RAG’s Back: Genetic Attack on RAG Pipeline by Simulating
Documents in the Wild via Low-level Perturbations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#autorag-hp-automatic-online-hyper-parameter-tuning-for-retrieval-augmented-generation"><span class="nav-number">3.2.7.</span> <span class="nav-text">AutoRAG-HP:
Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#long2rag-evaluating-long-context-long-form-retrieval-augmented-generation-with-key-point-recall"><span class="nav-number">3.2.8.</span> <span class="nav-text">\(LONG^{2}RAG\): Evaluating Long-Context
&amp; Long-Form Retrieval-Augmented Generation with Key Point
Recall</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#icml-25%E6%8C%89retrieval%E5%85%B3%E9%94%AE%E8%AF%8D%E6%A3%80%E7%B4%A2%E7%9A%84%E6%8E%92%E9%99%A4%E4%BA%86ai4sci"><span class="nav-number">4.</span> <span class="nav-text">ICML
25（按retrieval关键词检索的，排除了ai4sci）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#poisonedeye-knowledge-poisoning-attack-on-retrieval-augmented-generation-based-large-vision-language-models"><span class="nav-number">4.1.</span> <span class="nav-text">PoisonedEye: Knowledge
Poisoning Attack on Retrieval-Augmented Generation based Large
Vision-Language Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieval-augmented-zero-shot-enzyme-generation-for-specified-substrate"><span class="nav-number">4.2.</span> <span class="nav-text">Retrieval Augmented
Zero-Shot Enzyme Generation for Specified Substrate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#she-streaming-media-hashing-retrieval"><span class="nav-number">4.3.</span> <span class="nav-text">SHE: Streaming-media
Hashing Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#contradiction-retrieval-via-contrastive-learning-with-sparsity"><span class="nav-number">4.4.</span> <span class="nav-text">Contradiction Retrieval
via Contrastive Learning with Sparsity</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#llm-alignment-as-retriever-optimization-an-information-retrieval-perspective"><span class="nav-number">4.5.</span> <span class="nav-text">LLM Alignment as
Retriever Optimization: An Information Retrieval Perspective</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#efficient-length-generalizable-attention-via-causal-retrieval-for-long-context-language-modeling"><span class="nav-number">4.6.</span> <span class="nav-text">Efficient
Length-Generalizable Attention via Causal Retrieval for Long-Context
Language Modeling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#phantomwiki-on-demand-datasets-for-reasoning-and-retrieval-evaluation"><span class="nav-number">4.7.</span> <span class="nav-text">PhantomWiki: On-Demand
Datasets for Reasoning and Retrieval Evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#understanding-synthetic-context-extension-via-retrieval-heads"><span class="nav-number">4.8.</span> <span class="nav-text">Understanding Synthetic
Context Extension via Retrieval Heads</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rapid-long-context-inference-with-retrieval-augmented-speculative-decoding"><span class="nav-number">4.9.</span> <span class="nav-text">RAPID: Long-Context
Inference with Retrieval-Augmented Speculative Decoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#c-3po-compact-plug-and-play-proxy-optimization-to-achieve-human-like-retrieval-augmented-generation"><span class="nav-number">4.10.</span> <span class="nav-text">C-3PO: Compact
Plug-and-Play Proxy Optimization to Achieve Human-like
Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#on-the-vulnerability-of-applying-retrieval-augmented-generation-within-knowledge-intensive-application-domains"><span class="nav-number">4.11.</span> <span class="nav-text">On the Vulnerability of
Applying Retrieval-Augmented Generation within Knowledge-Intensive
Application Domains</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#in-context-learning-as-conditioned-associative-memory-retrieval"><span class="nav-number">4.12.</span> <span class="nav-text">In-Context Learning as
Conditioned Associative Memory Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#in-context-denoising-with-one-layer-transformers-connections-between-attention-and-associative-memory-retrieval"><span class="nav-number">4.13.</span> <span class="nav-text">In-Context Denoising
with One-Layer Transformers: Connections between Attention and
Associative Memory Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#poqd-performance-oriented-query-decomposer-for-multi-vector-retrieval"><span class="nav-number">4.14.</span> <span class="nav-text">POQD:
Performance-Oriented Query Decomposer for Multi-vector
retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scenir-visual-semantic-clarity-through-unsupervised-scene-graph-retrieval"><span class="nav-number">4.15.</span> <span class="nav-text">SCENIR: Visual Semantic
Clarity through Unsupervised Scene Graph Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieval-augmented-time-series-forecasting"><span class="nav-number">4.16.</span> <span class="nav-text">Retrieval Augmented
Time Series Forecasting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#position-retrieval-augmented-systems-can-be-dangerous-medical-communicators"><span class="nav-number">4.17.</span> <span class="nav-text">Position:
Retrieval-augmented systems can be dangerous medical
communicators</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#beyond-cropped-regions-new-benchmark-and-corresponding-baseline-for-chinese-scene-text-retrieval-in-diverse-layouts"><span class="nav-number">4.18.</span> <span class="nav-text">Beyond Cropped Regions:
New Benchmark and Corresponding Baseline for Chinese Scene Text
Retrieval in Diverse Layouts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qure-query-relevant-retrieval-through-hard-negative-sampling-in-composed-image-retrieval"><span class="nav-number">4.19.</span> <span class="nav-text">QuRe: Query-Relevant
Retrieval through Hard Negative Sampling in Composed Image
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-attribute-aware-hash-codes-for-fine-grained-image-retrieval-via-query-optimization"><span class="nav-number">4.20.</span> <span class="nav-text">Learning
Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query
Optimization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#locality-preserving-markovian-transition-for-instance-retrieval"><span class="nav-number">4.21.</span> <span class="nav-text">Locality Preserving
Markovian Transition for Instance Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#visual-abstraction-a-plug-and-play-approach-for-text-visual-retrieval"><span class="nav-number">4.22.</span> <span class="nav-text">Visual Abstraction: A
Plug-and-Play Approach for Text-Visual Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#docks-rag-optimizing-document-level-relation-extraction-through-llm-enhanced-hybrid-prompt-tuning"><span class="nav-number">4.23.</span> <span class="nav-text">DocKS-RAG: Optimizing
Document-Level Relation Extraction through LLM-Enhanced Hybrid Prompt
Tuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#from-rag-to-memory-non-parametric-continual-learning-for-large-language-models"><span class="nav-number">4.24.</span> <span class="nav-text">From RAG to Memory:
Non-Parametric Continual Learning for Large Language Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hierarchical-planning-for-complex-tasks-with-knowledge-graph-rag-and-symbolic-verification"><span class="nav-number">4.25.</span> <span class="nav-text">Hierarchical Planning
for Complex Tasks with Knowledge Graph-RAG and Symbolic
Verification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieval-augmented-perception-high-resolution-image-perception-meets-visual-rag"><span class="nav-number">4.26.</span> <span class="nav-text">Retrieval-Augmented
Perception: High-resolution Image Perception Meets Visual RAG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#realrag-retrieval-augmented-realistic-image-generation-via-self-reflective-contrastive-learning"><span class="nav-number">4.27.</span> <span class="nav-text">RealRAG:
Retrieval-augmented Realistic Image Generation via Self-reflective
Contrastive Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lara-benchmarking-retrieval-augmented-generation-and-long-context-llms-no-silver-bullet-for-lc-or-rag-routing"><span class="nav-number">4.28.</span> <span class="nav-text">LaRA: Benchmarking
Retrieval-Augmented Generation and Long-Context LLMs – No Silver Bullet
for LC or RAG Routing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ragged-towards-informed-design-of-scalable-and-stable-rag-systems"><span class="nav-number">4.29.</span> <span class="nav-text">RAGGED: Towards
Informed Design of Scalable and Stable RAG Systems</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#iclr-25%E6%8C%89retrieval%E5%85%B3%E9%94%AE%E8%AF%8D%E6%A3%80%E7%B4%A2"><span class="nav-number">5.</span> <span class="nav-text">ICLR
25（按retrieval关键词检索）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cvpr-25%E4%B9%8B%E6%89%80%E4%BB%A5%E5%9C%A8cvpr%E4%B8%8A%E6%89%BE%E6%98%AF%E5%9B%A0%E4%B8%BA%E7%8E%B0%E5%9C%A8%E4%BC%BC%E4%B9%8E%E6%9C%89%E4%B8%80%E4%BA%9B%E7%A0%94%E7%A9%B6%E5%85%B3%E6%B3%A8rag%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E7%BB%93%E5%90%88%E9%82%A3%E5%8F%AF%E8%83%BD%E5%9C%A8cvpr%E4%B8%8A%E4%BC%9A%E6%AF%94%E8%BE%83%E5%A4%9A%E4%B8%80%E4%BA%9B%E5%90%A7%E6%8C%89retrieval%E5%85%B3%E9%94%AE%E8%AF%8D%E6%89%BE%E7%9A%84"><span class="nav-number">6.</span> <span class="nav-text">CVPR
25（之所以在cvpr上找，是因为现在似乎有一些研究关注RAG和多模态的结合，那可能在cvpr上会比较多一些吧）（按retrieval关键词找的）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#drvideo-document-retrieval-based-long-video-understanding"><span class="nav-number">6.1.</span> <span class="nav-text">DrVideo:
Document Retrieval Based Long Video Understanding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cobra-combinatorial-retrieval-augmentation-for-few-shot-adaptation"><span class="nav-number">6.2.</span> <span class="nav-text">COBRA:
COmBinatorial Retrieval Augmentation for Few-Shot
Adaptation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#salova-segment-augmented-long-video-assistant-for-targeted-retrieval-and-routing-in-long-form-video-analysis"><span class="nav-number">6.3.</span> <span class="nav-text">SALOVA: Segment-Augmented Long
Video Assistant for Targeted Retrieval and Routing in Long-Form Video
Analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fuzzy-multimodal-learning-for-trusted-cross-modal-retrieval"><span class="nav-number">6.4.</span> <span class="nav-text">Fuzzy
Multimodal Learning for Trusted Cross-modal Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#few-shot-recognition-via-stage-wise-retrieval-augmented-finetuning"><span class="nav-number">6.5.</span> <span class="nav-text">Few-Shot Recognition via
Stage-Wise Retrieval-Augmented Finetuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vlog-video-language-models-by-generative-retrieval-of-narration-vocabulary"><span class="nav-number">6.6.</span> <span class="nav-text">VLog:
Video-Language Models by Generative Retrieval of Narration
Vocabulary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#collm-a-large-language-model-for-composed-image-retrieval"><span class="nav-number">6.7.</span> <span class="nav-text">CoLLM: A Large Language Model for
Composed Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#search-and-detect-training-free-long-tail-object-detection-via-web-image-retrieval"><span class="nav-number">6.8.</span> <span class="nav-text">Search
and Detect: Training-Free Long Tail Object Detection via Web-Image
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-compatible-multi-prize-subnetworks-for-asymmetric-retrieval"><span class="nav-number">6.9.</span> <span class="nav-text">Learning
Compatible Multi-Prize Subnetworks for Asymmetric
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-audio-guided-video-representation-with-gated-attention-for-video-text-retrieval"><span class="nav-number">6.10.</span> <span class="nav-text">Learning
Audio-guided Video Representation with Gated Attention for Video-Text
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lamra-large-multimodal-model-as-your-advanced-retrieval-assistant"><span class="nav-number">6.11.</span> <span class="nav-text">LamRA: Large Multimodal
Model as Your Advanced Retrieval Assistant</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#recurrence-enhanced-vision-and-language-transformers-for-robust-multimodal-document-retrieval"><span class="nav-number">6.12.</span> <span class="nav-text">Recurrence-Enhanced
Vision-and-Language Transformers for Robust Multimodal Document
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#missing-target-relevant-information-prediction-with-world-model-for-accurate-zero-shot-composed-image-retrieval"><span class="nav-number">6.13.</span> <span class="nav-text">Missing
Target-Relevant Information Prediction with World Model for Accurate
Zero-Shot Composed Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ilias-instance-level-image-retrieval-at-scale"><span class="nav-number">6.14.</span> <span class="nav-text">ILIAS: Instance-Level Image
retrieval At Scale</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#clip-is-almost-all-you-need-towards-parameter-efficient-scene-text-retrieval-without-ocr"><span class="nav-number">6.15.</span> <span class="nav-text">CLIP
is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval
without OCR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rethinking-noisy-video-text-retrieval-via-relation-aware-alignment"><span class="nav-number">6.16.</span> <span class="nav-text">Rethinking
Noisy Video-Text Retrieval via Relation-aware Alignment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bridging-modalities-improving-universal-multimodal-retrieval-by-multimodal-large-language-models"><span class="nav-number">6.17.</span> <span class="nav-text">Bridging
Modalities: Improving Universal Multimodal Retrieval by Multimodal Large
Language Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reason-before-retrieve-one-stage-reflective-chain-of-thoughts-for-training-free-zero-shot-composed-image-retrieval"><span class="nav-number">6.18.</span> <span class="nav-text">Reason-before-Retrieve:
One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot
Composed Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mari-material-retrieval-integration-across-domains"><span class="nav-number">6.19.</span> <span class="nav-text">MaRI:
Material Retrieval Integration across Domains</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vdocrag-retrieval-augmented-generation-over-visually-rich-documents%E8%BF%99%E7%AF%87%E6%98%8E%E7%A1%AE%E5%87%BA%E7%8E%B0rag%E4%BA%86%E5%95%8A"><span class="nav-number">6.20.</span> <span class="nav-text">VDocRAG: Retrieval-Augmented
Generation over Visually-Rich Documents（这篇明确出现RAG了啊）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#range-retrieval-augmented-neural-fields-for-multi-resolution-geo-embeddings%E8%BF%99%E4%B8%AA%E4%B9%9F%E6%98%AF%E6%9C%89retrieval-augmented"><span class="nav-number">6.21.</span> <span class="nav-text">RANGE:
Retrieval Augmented Neural Fields for Multi-Resolution
Geo-Embeddings（这个也是有retrieval augmented）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rap-retrieval-augmented-personalization-for-multimodal-large-language-models%E6%88%91%E5%8E%BB%E8%BF%99%E4%B9%88%E5%B7%A7%E5%92%8Cicml-oral%E9%82%A3%E7%AF%87%E6%84%9F%E8%A7%89%E5%81%9A%E7%9A%84%E9%A2%98%E6%9D%90%E6%9C%89%E7%82%B9%E5%83%8F%E5%95%8A"><span class="nav-number">6.22.</span> <span class="nav-text">RAP: Retrieval-Augmented
Personalization for Multimodal Large Language
Models（我去，这么巧，和ICML oral那篇感觉做的题材有点像啊）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#video-colbert-contextualized-late-interaction-for-text-to-video-retrieval"><span class="nav-number">6.23.</span> <span class="nav-text">Video-ColBERT:
Contextualized Late Interaction for Text-to-Video
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#imagine-and-seek-improving-composed-image-retrieval-with-an-imagined-proxy"><span class="nav-number">6.24.</span> <span class="nav-text">Imagine
and Seek: Improving Composed Image Retrieval with an Imagined
Proxy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multivent-2.0-a-massive-multilingual-benchmark-for-event-centric-video-retrieval"><span class="nav-number">6.25.</span> <span class="nav-text">MultiVENT
2.0: A Massive Multilingual Benchmark for Event-Centric Video
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-devil-is-in-the-prompts-retrieval-augmented-prompt-optimization-for-text-to-video-generation%E4%B8%8D%E4%BC%9A%E6%98%AFprompt-engineering%E5%90%A7"><span class="nav-number">6.26.</span> <span class="nav-text">The
Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for
Text-to-Video Generation（不会是prompt engineering吧……）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#narrating-the-video-boosting-text-video-retrieval-via-comprehensive-utilization-of-frame-level-captions"><span class="nav-number">6.27.</span> <span class="nav-text">Narrating
the Video: Boosting Text-Video Retrieval via Comprehensive Utilization
of Frame-Level Captions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chat-based-person-retrieval-via-dialogue-refined-cross-modal-alignment"><span class="nav-number">6.28.</span> <span class="nav-text">Chat-based
Person Retrieval via Dialogue-Refined Cross-Modal
Alignment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#prompthash-affinity-prompted-collaborative-cross-modal-learning-for-adaptive-hashing-retrieval"><span class="nav-number">6.29.</span> <span class="nav-text">PromptHash:
Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive
Hashing Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#neighborretr-balancing-hub-centrality-in-cross-modal-retrieval"><span class="nav-number">6.30.</span> <span class="nav-text">NeighborRetr: Balancing
Hub Centrality in Cross-Modal Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-with-noisy-triplet-correspondence-for-composed-image-retrieval"><span class="nav-number">6.31.</span> <span class="nav-text">Learning with Noisy
Triplet Correspondence for Composed Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#garmentpile-point-level-visual-affordance-guided-retrieval-and-adaptation-for-cluttered-garments-manipulation"><span class="nav-number">6.32.</span> <span class="nav-text">GarmentPile:
Point-Level Visual Affordance Guided Retrieval and Adaptation for
Cluttered Garments Manipulation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#towards-natural-language-based-document-image-retrieval-new-dataset-and-benchmark"><span class="nav-number">6.33.</span> <span class="nav-text">Towards
Natural Language-Based Document Image Retrieval: New Dataset and
Benchmark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#semantic-library-adaptation-lora-retrieval-and-fusion-for-open-vocabulary-semantic-segmentation"><span class="nav-number">6.34.</span> <span class="nav-text">Semantic
Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary
Semantic Segmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ccin-compositional-conflict-identification-and-neutralization-for-composed-image-retrieval"><span class="nav-number">6.35.</span> <span class="nav-text">CCIN:
Compositional Conflict Identification and Neutralization for Composed
Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generative-zero-shot-composed-image-retrieval"><span class="nav-number">6.36.</span> <span class="nav-text">Generative
Zero-Shot Composed Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#context-cir-learning-from-concepts-in-text-for-composed-image-retrieval%E5%A5%BD%E5%A4%9A%E8%BF%99%E7%A7%8Dcomposed-image-retrieval%E5%95%8A%E8%BF%99%E4%B8%AA%E6%98%AF%E5%95%A5%E5%95%8A"><span class="nav-number">6.37.</span> <span class="nav-text">ConText-CIR:
Learning from Concepts in Text for Composed Image
Retrieval（好多这种composed image
retrieval啊，这个是啥啊）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#discovla-discrepancy-reduction-in-vision-language-and-alignment-for-parameter-efficient-video-text-retrieval"><span class="nav-number">6.38.</span> <span class="nav-text">DiscoVLA: Discrepancy
Reduction in Vision, Language, and Alignment for Parameter-Efficient
Video-Text Retrieval</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#iccv-25%E6%8C%89retrieval%E5%85%B3%E9%94%AE%E8%AF%8D%E6%89%BE%E7%9A%84"><span class="nav-number">7.</span> <span class="nav-text">ICCV
25（按retrieval关键词找的）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#g-dexgrasp-generalizable-dexterous-grasping-synthesis-via-part-aware-prior-retrieval-and-prior-assisted-generation"><span class="nav-number">7.1.</span> <span class="nav-text">G-DexGrasp:
Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior
Retrieval and Prior-Assisted Generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#signs-as-tokens-a-retrieval-enhanced-multilingual-sign-language-generator"><span class="nav-number">7.2.</span> <span class="nav-text">Signs
as Tokens: A Retrieval-Enhanced Multilingual Sign Language
Generator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hybrid-tower-fine-grained-pseudo-query-interaction-and-generation-for-text-to-video-retrieval"><span class="nav-number">7.3.</span> <span class="nav-text">Hybrid-Tower:
Fine-grained Pseudo-query Interaction and Generation for Text-to-Video
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-devil-is-in-the-spurious-correlations-boosting-moment-retrieval-with-dynamic-learning"><span class="nav-number">7.4.</span> <span class="nav-text">The Devil is in the Spurious
Correlations: Boosting Moment Retrieval with Dynamic Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#taming-the-untamed-graph-based-knowledge-retrieval-and-reasoning-for-mllms-to-conquer-the-unknown"><span class="nav-number">7.5.</span> <span class="nav-text">Taming
the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to
Conquer the Unknown</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learnable-retrieval-enhanced-visual-text-alignment-and-fusion-for-radiology-report-generation"><span class="nav-number">7.6.</span> <span class="nav-text">Learnable
Retrieval Enhanced Visual-Text Alignment and Fusion for Radiology Report
Generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zero-shot-composed-image-retrieval-via-dual-stream-instruction-aware-distillation"><span class="nav-number">7.7.</span> <span class="nav-text">Zero-Shot
Composed Image Retrieval via Dual-Stream Instruction-Aware
Distillation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sims-simulating-stylized-human-scene-interactions-with-retrieval-augmented-script-generation"><span class="nav-number">7.8.</span> <span class="nav-text">SIMS:
Simulating Stylized Human-Scene Interactions with Retrieval-Augmented
Script Generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#beyond-simple-edits-composed-video-retrieval-with-dense-modifications"><span class="nav-number">7.9.</span> <span class="nav-text">Beyond
Simple Edits: Composed Video Retrieval with Dense
Modifications</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reference-based-super-resolution-via-image-based-retrieval-augmented-generation-diffusion"><span class="nav-number">7.10.</span> <span class="nav-text">Reference-based
Super-Resolution via Image-based Retrieval-Augmented Generation
Diffusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ma-cir-a-multimodal-arithmetic-benchmark-for-composed-image-retrieval"><span class="nav-number">7.11.</span> <span class="nav-text">MA-CIR:
A Multimodal Arithmetic Benchmark for Composed Image
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#test-time-retrieval-augmented-adaptation-for-vision-language-models"><span class="nav-number">7.12.</span> <span class="nav-text">Test-Time
Retrieval-Augmented Adaptation for Vision-Language Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gesturehydra-semantic-co-speech-gesture-synthesis-via-hybrid-modality-diffusion-transformer-and-cascaded-synchronized-retrieval-augmented-generation"><span class="nav-number">7.13.</span> <span class="nav-text">GestureHYDRA:
Semantic Co-speech Gesture Synthesis via Hybrid Modality Diffusion
Transformer and Cascaded-Synchronized Retrieval-Augmented
Generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#borrowing-eyes-for-the-blind-spot-overcoming-data-scarcity-in-malicious-video-detection-via-cross-domain-retrieval-augmentation"><span class="nav-number">7.14.</span> <span class="nav-text">Borrowing Eyes for the Blind
Spot: Overcoming Data Scarcity in Malicious Video Detection via
Cross-Domain Retrieval Augmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#enhancing-partially-relevant-video-retrieval-with-hyperbolic-learning"><span class="nav-number">7.15.</span> <span class="nav-text">Enhancing
Partially Relevant Video Retrieval with Hyperbolic
Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chatreid-open-ended-interactive-person-retrieval-via-hierarchical-progressive-tuning-for-vision-language-models"><span class="nav-number">7.16.</span> <span class="nav-text">ChatReID:
Open-ended Interactive Person Retrieval via Hierarchical Progressive
Tuning for Vision Language Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#remp-ad-retrieval-enhanced-multi-modal-prompt-fusion-for-few-shot-industrial-visual-anomaly-detection"><span class="nav-number">7.17.</span> <span class="nav-text">ReMP-AD:
Retrieval-enhanced Multi-modal Prompt Fusion for Few-Shot Industrial
Visual Anomaly Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#beyond-single-images-retrieval-self-augmented-unsupervised-camouflaged-object-detection"><span class="nav-number">7.18.</span> <span class="nav-text">Beyond
Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object
Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#describe-adapt-and-combine-empowering-clip-encoders-for-open-set-3d-object-retrieval"><span class="nav-number">7.19.</span> <span class="nav-text">Describe,
Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hierarchy-aware-pseudo-word-learning-with-text-adaptation-for-zero-shot-composed-image-retrieval"><span class="nav-number">7.20.</span> <span class="nav-text">Hierarchy-Aware
Pseudo Word Learning with Text Adaptation for Zero-Shot Composed Image
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#carim-caption-based-autonomous-driving-scene-retrieval-via-inclusive-text-matching"><span class="nav-number">7.21.</span> <span class="nav-text">CARIM:
Caption-Based Autonomous Driving Scene Retrieval via Inclusive Text
Matching</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ophclip-hierarchical-retrieval-augmented-learning-for-ophthalmic-surgical-video-language-pretraining"><span class="nav-number">7.22.</span> <span class="nav-text">OphCLIP:
Hierarchical Retrieval-Augmented Learning for Ophthalmic Surgical
Video-Language Pretraining</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-visual-hierarchies-in-hyperbolic-space-for-image-retrieval"><span class="nav-number">7.23.</span> <span class="nav-text">Learning
Visual Hierarchies in Hyperbolic Space for Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#monster-a-unified-model-for-motion-scene-text-retrieval"><span class="nav-number">7.24.</span> <span class="nav-text">MonSTeR:
a Unified Model for Motion, Scene, Text Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cotmr-chain-of-thought-multi-scale-reasoning-for-training-free-zero-shot-composed-image-retrieval"><span class="nav-number">7.25.</span> <span class="nav-text">CoTMR:
Chain-of-Thought Multi-Scale Reasoning for Training-Free Zero-Shot
Composed Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ocr-hinders-rag-evaluating-the-cascading-impact-of-ocr-on-retrieval-augmented-generation"><span class="nav-number">7.26.</span> <span class="nav-text">OCR
Hinders RAG: Evaluating the Cascading Impact of OCR on
Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#augmenting-moment-retrieval-zero-dependency-two-stage-learning"><span class="nav-number">7.27.</span> <span class="nav-text">Augmenting
Moment Retrieval: Zero-Dependency Two-Stage Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#quantifying-and-narrowing-the-unknown-interactive-text-to-video-retrieval-via-uncertainty-minimization"><span class="nav-number">7.28.</span> <span class="nav-text">Quantifying
and Narrowing the Unknown: Interactive Text-to-Video Retrieval via
Uncertainty Minimization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#training-free-personalization-via-retrieval-and-reasoning-on-fingerprints"><span class="nav-number">7.29.</span> <span class="nav-text">Training-Free
Personalization via Retrieval and Reasoning on Fingerprints</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#prototypes-are-balanced-units-for-efficient-and-effective-partially-relevant-video-retrieval"><span class="nav-number">7.30.</span> <span class="nav-text">Prototypes
are Balanced Units for Efficient and Effective Partially Relevant Video
Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bidirectional-likelihood-estimation-with-multi-modal-large-language-models-for-text-video-retrieval"><span class="nav-number">7.31.</span> <span class="nav-text">Bidirectional
Likelihood Estimation with Multi-Modal Large Language Models for
Text-Video Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multi-schema-proximity-network-for-composed-image-retrieval"><span class="nav-number">7.32.</span> <span class="nav-text">Multi-Schema
Proximity Network for Composed Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#an-efficient-post-hoc-framework-for-reducing-task-discrepancy-of-text-encoders-for-composed-image-retrieval"><span class="nav-number">7.33.</span> <span class="nav-text">An
Efficient Post-hoc Framework for Reducing Task Discrepancy of Text
Encoders for Composed Image Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#autocompose-automatic-generation-of-pose-transition-descriptions-for-composed-pose-retrieval-using-multimodal-llms"><span class="nav-number">7.34.</span> <span class="nav-text">AutoComPose:
Automatic Generation of Pose Transition Descriptions for Composed Pose
Retrieval Using Multimodal LLMs</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ragretrieval-augmented-generation%E6%96%B9%E6%B3%95%E6%8B%93%E5%B1%95%E4%B8%8E%E5%BA%94%E7%94%A8"><span class="nav-number">8.</span> <span class="nav-text">1.
RAG（Retrieval-Augmented
Generation）方法拓展与应用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#video-multimodal-retrieval"><span class="nav-number">9.</span> <span class="nav-text">3. Video &amp; Multimodal
Retrieval</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">213</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">67</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">103</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
