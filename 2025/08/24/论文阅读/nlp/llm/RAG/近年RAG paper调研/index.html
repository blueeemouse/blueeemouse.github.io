<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ACL 25 main HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation RAG-Critic: Leveraging Autom">
<meta property="og:type" content="article">
<meta property="og:title" content="近年RAG paper调研">
<meta property="og:url" content="https://blueeemouse.github.io/2025/08/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4RAG%20paper%E8%B0%83%E7%A0%94/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="ACL 25 main HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation RAG-Critic: Leveraging Autom">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-08-24T15:35:00.000Z">
<meta property="article:modified_time" content="2025-09-19T01:29:42.764Z">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/2025/08/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4RAG%20paper%E8%B0%83%E7%A0%94/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>近年RAG paper调研 | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/08/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4RAG%20paper%E8%B0%83%E7%A0%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          近年RAG paper调研
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-08-24 23:35:00" itemprop="dateCreated datePublished" datetime="2025-08-24T23:35:00+08:00">2025-08-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-09-19 09:29:42" itemprop="dateModified" datetime="2025-09-19T09:29:42+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="acl-25">ACL 25</h1>
<h2 id="main">main</h2>
<h3 id="hybgrag-hybrid-retrieval-augmented-generation-on-textual-and-relational-knowledge-bases"><strong>HybGRAG:
Hybrid Retrieval-Augmented Generation on Textual and Relational
Knowledge Bases</strong></h3>
<h3 id="main-rag-multi-agent-filtering-retrieval-augmented-generation"><strong>MAIN-RAG:
Multi-Agent Filtering Retrieval-Augmented Generation</strong></h3>
<h3 id="rag-critic-leveraging-automated-critic-guided-agentic-workflow-for-retrieval-augmented-generation"><strong>RAG-Critic:
Leveraging Automated Critic-Guided Agentic Workflow for Retrieval
Augmented Generation</strong></h3>
<h3 id="saferag-benchmarking-security-in-retrieval-augmented-generation-of-large-language-model"><strong>SafeRAG:
Benchmarking Security in Retrieval-Augmented Generation of Large
Language Model</strong></h3>
<h3 id="on-the-robustness-of-rag-systems-in-educational-question-answering-under-knowledge-discrepancies"><strong>On
the Robustness of RAG Systems in Educational Question Answering under
Knowledge Discrepancies</strong></h3>
<h3 id="pandoras-box-or-aladdins-lamp-a-comprehensive-analysis-revealing-the-role-of-rag-noise-in-large-language-models"><strong>Pandora’s
Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of
RAG Noise in Large Language Models</strong></h3>
<h3 id="neusym-rag-hybrid-neural-symbolic-retrieval-with-multiview-structuring-for-pdf-question-answering"><strong>NeuSym-RAG:
Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF
Question Answering</strong></h3>
<h3 id="drag-distilling-rag-for-slms-from-llms-to-transfer-knowledge-and-mitigate-hallucination-via-evidence-and-graph-based-distillation"><strong>DRAG:
Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate
Hallucination via Evidence and Graph-based Distillation</strong></h3>
<h3 id="rageval-scenario-specific-rag-evaluation-dataset-generation-framework"><strong>RAGEval:
Scenario Specific RAG Evaluation Dataset Generation
Framework</strong></h3>
<h3 id="are-llms-effective-psychological-assessors-leveraging-adaptive-rag-for-interpretable-mental-health-screening-through-psychometric-practice"><strong>Are
LLMs effective psychological assessors? Leveraging adaptive RAG for
interpretable mental health screening through psychometric
practice</strong></h3>
<h3 id="s-rag-a-novel-audit-framework-for-detecting-unauthorized-use-of-personal-data-in-rag-systems"><strong>S-RAG:
A Novel Audit Framework for Detecting Unauthorized Use of Personal Data
in RAG Systems</strong></h3>
<h3 id="gainrag-preference-alignment-in-retrieval-augmented-generation-through-gain-signal-synthesis"><strong>GainRAG:
Preference Alignment in Retrieval-Augmented Generation through Gain
Signal Synthesis</strong></h3>
<span id="more"></span>
<h3 id="tcrag-turingcomplete-rags-case-study-on-medical-llm-systems"><strong>TC–RAG:
Turing–Complete RAG’s Case study on Medical LLM Systems</strong></h3>
<h3 id="divide-then-align-honest-alignment-based-on-the-knowledge-boundary-of-rag"><strong>Divide-Then-Align:
Honest Alignment based on the Knowledge Boundary of RAG</strong></h3>
<h3 id="hykge-a-hypothesis-knowledge-graph-enhanced-rag-framework-for-accurate-and-reliable-medical-llms-responses"><strong>HyKGE:
A Hypothesis Knowledge Graph Enhanced RAG Framework for Accurate and
Reliable Medical LLMs Responses</strong></h3>
<h3 id="wavrag-audio-integrated-retrieval-augmented-generation-for-spoken-dialogue-models多模态的不过是audio相关的"><strong>WavRAG:
Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue
Models</strong>（多模态的，不过是audio相关的）</h3>
<h3 id="unirag-unified-query-understanding-method-for-retrieval-augmented-generation"><strong>UniRAG:
Unified Query Understanding Method for Retrieval Augmented
Generation</strong></h3>
<h3 id="towards-omni-rag-comprehensive-retrieval-augmented-generation-for-large-language-models-in-medical-applications"><strong>Towards
Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large
Language Models in Medical Applications</strong></h3>
<h3 id="molrag-unlocking-the-power-of-large-language-models-for-molecular-property-prediction"><strong>MolRAG:
Unlocking the Power of Large Language Models for Molecular Property
Prediction</strong></h3>
<h3 id="removal-of-hallucination-on-hallucination-debate-augmented-rag"><strong>Removal
of Hallucination on Hallucination: Debate-Augmented RAG</strong></h3>
<h3 id="eventrag-enhancing-llm-generation-with-event-knowledge-graphs"><strong>EventRAG:
Enhancing LLM Generation with Event Knowledge Graphs</strong></h3>
<h3 id="the-distracting-effect-understanding-irrelevant-passages-in-rag"><strong>The
Distracting Effect: Understanding Irrelevant Passages in
RAG</strong></h3>
<h3 id="kirag-knowledge-driven-iterative-retriever-for-enhancing-retrieval-augmented-generation"><strong>KiRAG:
Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented
Generation</strong></h3>
<h3 id="faithfulrag-fact-level-conflict-modeling-for-context-faithful-retrieval-augmented-generation"><strong>FaithfulRAG:
Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented
Generation</strong></h3>
<h3 id="memerag-a-multilingual-end-to-end-meta-evaluation-benchmark-for-retrieval-augmented-generation"><strong>MEMERAG:
A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval
Augmented Generation</strong></h3>
<h3 id="dialogue-rag-enhancing-retrieval-for-llms-via-node-linking-utterance-rewriting"><strong>Dialogue-RAG:
Enhancing Retrieval for LLMs via Node-Linking Utterance
Rewriting</strong></h3>
<h3 id="the-efficiency-vs.-accuracy-trade-off-optimizing-rag-enhanced-llm-recommender-systems-using-multi-head-early-exit"><strong>The
Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM
Recommender Systems Using Multi-Head Early Exit</strong></h3>
<h3 id="sgic-a-self-guided-iterative-calibration-framework-for-rag"><strong>SGIC:
A Self-Guided Iterative Calibration Framework for RAG</strong></h3>
<h3 id="medical-graph-rag-evidence-based-medical-large-language-model-via-graph-retrieval-augmented-generation"><strong>Medical
Graph RAG: Evidence-based Medical Large Language Model via Graph
Retrieval-Augmented Generation</strong></h3>
<h3 id="astute-rag-overcoming-imperfect-retrieval-augmentation-and-knowledge-conflicts-for-large-language-models"><strong>Astute
RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts
for Large Language Models</strong></h3>
<h3 id="real-mm-rag-a-real-world-multi-modal-retrieval-benchmark多模态的但是benchmark"><strong>REAL-MM-RAG:
A Real-World Multi-Modal Retrieval
Benchmark</strong>（多模态的，但是benchmark）</h3>
<h3 id="dualrag-a-dual-process-approach-to-integrate-reasoning-and-retrieval-for-multi-hop-question-answering"><strong>DualRAG:
A Dual-Process Approach to Integrate Reasoning and Retrieval for
Multi-Hop Question Answering</strong></h3>
<h3 id="core-mmrag-cross-source-knowledge-reconciliation-for-multimodal-rag"><strong>CoRe-MMRAG:
Cross-Source Knowledge Reconciliation for Multimodal RAG</strong></h3>
<h2 id="findings">findings</h2>
<h3 id="treerag-unleashing-the-power-of-hierarchical-storage-for-enhanced-knowledge-retrieval-in-long-documents"><strong>TreeRAG:
Unleashing the Power of Hierarchical Storage for Enhanced Knowledge
Retrieval in Long Documents</strong></h3>
<h3 id="scrag-hybrid-retrieval-augmented-generation-for-llm-based-cross-tissue-single-cell-annotation"><strong>scRAG:
Hybrid Retrieval-Augmented Generation for LLM-based Cross-Tissue
Single-Cell Annotation</strong></h3>
<h3 id="hoprag-multi-hop-reasoning-for-logic-aware-retrieval-augmented-generation"><strong>HopRAG:
Multi-Hop Reasoning for Logic-Aware Retrieval Augmented
Generation</strong></h3>
<h3 id="ltrag-enhancing-autoformalization-and-self-refinement-for-logical-reasoning-with-thought-guided-rag"><strong>LTRAG:
Enhancing autoformalization and self-refinement for logical reasoning
with Thought-Guided RAG</strong></h3>
<h3 id="simgrag-leveraging-similar-subgraphs-for-knowledge-graphs-driven-retrieval-augmented-generation"><strong>SimGRAG:
Leveraging Similar Subgraphs for Knowledge Graphs Driven
Retrieval-Augmented Generation</strong></h3>
<h3 id="remoterag-a-privacy-preserving-llm-cloud-rag-service"><strong>RemoteRAG:
A Privacy-Preserving LLM Cloud RAG Service</strong></h3>
<h3 id="investigating-language-preference-of-multilingual-rag-systems"><strong>Investigating
Language Preference of Multilingual RAG Systems</strong></h3>
<h3 id="frag-a-flexible-modular-framework-for-retrieval-augmented-generation-based-on-knowledge-graphs"><strong>FRAG:
A Flexible Modular Framework for Retrieval-Augmented Generation based on
Knowledge Graphs</strong></h3>
<h3 id="navrag-generating-user-demand-instructions-for-embodied-navigation-through-retrieval-augmented-llm"><strong>NavRAG:
Generating User Demand Instructions for Embodied Navigation through
Retrieval-Augmented LLM</strong></h3>
<h3 id="speecht-rag-reliable-depression-detection-in-llms-with-retrieval-augmented-generation-using-speech-timing-information"><strong>SpeechT-RAG:
Reliable Depression Detection in LLMs with Retrieval-Augmented
Generation Using Speech Timing Information</strong></h3>
<h3 id="roserag-robust-retrieval-augmented-generation-with-small-scale-llms-via-margin-aware-preference-optimization"><strong>RoseRAG:
Robust Retrieval-augmented Generation with Small-scale LLMs via
Margin-aware Preference Optimization</strong></h3>
<h3 id="gnn-rag-graph-neural-retrieval-for-efficient-large-language-model-reasoning-on-knowledge-graphs"><strong>GNN-RAG:
Graph Neural Retrieval for Efficient Large Language Model Reasoning on
Knowledge Graphs</strong></h3>
<h3 id="astrid---an-automated-and-scalable-triad-for-the-evaluation-of-rag-based-clinical-question-answering-systems"><strong>ASTRID
- An Automated and Scalable TRIaD for the Evaluation of RAG-based
Clinical Question Answering Systems</strong></h3>
<h3 id="garage-a-benchmark-with-grounding-annotations-for-rag-evaluation"><strong>GaRAGe:
A Benchmark with Grounding Annotations for RAG Evaluation</strong></h3>
<h3 id="rag-rewardbench-benchmarking-reward-models-in-retrieval-augmented-generation-for-preference-alignment"><strong>RAG-RewardBench:
Benchmarking Reward Models in Retrieval Augmented Generation for
Preference Alignment</strong></h3>
<h3 id="mitigating-bias-in-rag-controlling-the-embedder"><strong>Mitigating
Bias in RAG: Controlling the Embedder</strong></h3>
<h3 id="synapticrag-enhancing-temporal-memory-retrieval-in-large-language-models-through-synaptic-mechanisms"><strong>SynapticRAG:
Enhancing Temporal Memory Retrieval in Large Language Models through
Synaptic Mechanisms</strong></h3>
<h3 id="techniquerag-retrieval-augmented-generation-for-adversarial-technique-annotation-in-cyber-threat-intelligence-text"><strong>TechniqueRAG:
Retrieval Augmented Generation for Adversarial Technique Annotation in
Cyber Threat Intelligence Text</strong></h3>
<h3 id="videorag-retrieval-augmented-generation-over-video-corpus多模态的"><strong>VideoRAG:
Retrieval-Augmented Generation over Video
Corpus</strong>（多模态的）</h3>
<h3 id="query-driven-multimodal-graphrag-dynamic-local-knowledge-graph-construction-for-online-reasoning"><strong>Query-Driven
Multimodal GraphRAG: Dynamic Local Knowledge Graph Construction for
Online Reasoning</strong></h3>
<h3 id="causalrag-integrating-causal-graphs-into-retrieval-augmented-generation"><strong>CausalRAG:
Integrating Causal Graphs into Retrieval-Augmented
Generation</strong></h3>
<h3 id="eventrag-supportive-event-retrieval-on-hypergraph-for-future-forecasting"><strong>EventRAG:
Supportive Event Retrieval on Hypergraph for Future
Forecasting</strong></h3>
<h3 id="safeguarding-rag-pipelines-with-gmtp-a-gradient-based-masked-token-probability-method-for-poisoned-document-detection"><strong>Safeguarding
RAG Pipelines with GMTP: A Gradient-based Masked Token Probability
Method for Poisoned Document Detection</strong></h3>
<h3 id="ecorag-evidentiality-guided-compression-for-long-context-rag"><strong>ECoRAG:
Evidentiality-guided Compression for Long Context RAG</strong></h3>
<h3 id="hash-rag-bridging-deep-hashing-with-retriever-for-efficient-fine-retrieval-and-augmented-generation"><strong>HASH-RAG:
Bridging Deep Hashing with Retriever for Efficient, Fine Retrieval and
Augmented Generation</strong></h3>
<h1 id="naacl-25">NAACL 25</h1>
<h2 id="main-1">main</h2>
<h3 id="rag-star-enhancing-deliberative-reasoning-with-retrieval-augmented-verification-and-refinement"><strong>RAG-Star:
Enhancing Deliberative Reasoning with Retrieval Augmented Verification
and Refinement</strong></h3>
<h3 id="simrag-self-improving-retrieval-augmented-generation-for-adapting-large-language-models-to-specialized-domains"><strong>SimRAG:
Self-Improving Retrieval-Augmented Generation for Adapting Large
Language Models to Specialized Domains</strong></h3>
<h3 id="do-rag-systems-cover-what-matters-evaluating-and-optimizing-responses-with-sub-question-coverage"><strong>Do
RAG Systems Cover What Matters? Evaluating and Optimizing Responses with
Sub-Question Coverage</strong></h3>
<h3 id="rag-llms-are-not-safer-a-safety-analysis-of-retrieval-augmented-generation-for-large-language-models"><strong>RAG
LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation
for Large Language Models</strong></h3>
<h3 id="trag-term-level-retrieval-augmented-generation-for-domain-adaptive-retrieval"><strong>tRAG:
Term-level Retrieval-Augmented Generation for Domain-Adaptive
Retrieval</strong></h3>
<h3 id="pa-rag-rag-alignment-via-multi-perspective-preference-optimization"><strong>PA-RAG:
RAG Alignment via Multi-Perspective Preference
Optimization</strong></h3>
<h3 id="corag-collaborative-retrieval-augmented-generation"><strong>CoRAG:
Collaborative Retrieval-Augmented Generation</strong></h3>
<h3 id="transform-retrieval-for-textual-entailment-in-rag"><strong>Transform
Retrieval for Textual Entailment in RAG</strong></h3>
<h2 id="findings-1">findings</h2>
<h3 id="systematic-knowledge-injection-into-large-language-models-via-diverse-augmentation-for-domain-specific-rag"><strong>Systematic
Knowledge Injection into Large Language Models via Diverse Augmentation
for Domain-Specific RAG</strong></h3>
<h3 id="probing-rag-self-probing-to-guide-language-models-in-selective-document-retrieval"><strong>Probing-RAG:
Self-Probing to Guide Language Models in Selective Document
Retrieval</strong></h3>
<h3 id="mes-rag-bringing-multi-modal-entity-storage-and-secure-enhancements-to-rag多模态的"><strong>MES-RAG:
Bringing Multi-modal, Entity-Storage, and Secure Enhancements to
RAG</strong>（多模态的）</h3>
<h3 id="grag-graph-retrieval-augmented-generation"><strong>GRAG: Graph
Retrieval-Augmented Generation</strong></h3>
<h3 id="coderag-bench-can-retrieval-augment-code-generation"><strong>CodeRAG-Bench:
Can Retrieval Augment Code Generation?</strong></h3>
<h3 id="unirag-universal-retrieval-augmentation-for-large-vision-language-models多模态的"><strong>UniRAG:
Universal Retrieval Augmentation for Large Vision Language
Models</strong>（多模态的）</h3>
<h3 id="funnelrag-a-coarse-to-fine-progressive-retrieval-paradigm-for-rag"><strong>FunnelRAG:
A Coarse-to-Fine Progressive Retrieval Paradigm for RAG</strong></h3>
<h3 id="grappi-a-retrieve-divide-solve-graphrag-framework-for-large-scale-protein-protein-interaction-exploration"><strong>GraPPI:
A Retrieve-Divide-Solve GraphRAG Framework for Large-scale
Protein-protein Interaction Exploration</strong></h3>
<h3 id="chain-of-rank-enhancing-large-language-models-for-domain-specific-rag-in-edge-device"><strong>Chain-of-Rank:
Enhancing Large Language Models for Domain-Specific RAG in Edge
Device</strong></h3>
<h3 id="superrag-beyond-rag-with-layout-aware-graph-modeling"><strong>SuperRAG:
Beyond RAG with Layout-Aware Graph Modeling</strong></h3>
<h3 id="hypa-rag-a-hybrid-parameter-adaptive-retrieval-augmented-generation-system-for-ai-legal-and-policy-applications"><strong>HyPA-RAG:
A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI
Legal and Policy Applications</strong></h3>
<h3 id="evaluating-the-performance-of-rag-methods-for-conversational-ai-in-the-airport-domain"><strong>Evaluating
the Performance of RAG Methods for Conversational AI in the Airport
Domain</strong></h3>
<h3 id="dsrag-a-double-stream-retrieval-augmented-generation-framework-for-countless-intent-detection"><strong>DSRAG:
A Double-Stream Retrieval-Augmented Generation Framework for Countless
Intent Detection</strong></h3>
<h3 id="from-generating-answers-to-building-explanations-integrating-multi-round-rag-and-causal-modeling-for-scientific-qa"><strong>From
Generating Answers to Building Explanations: Integrating Multi-Round RAG
and Causal Modeling for Scientific QA</strong></h3>
<h1 id="emnlp-24">EMNLP 24</h1>
<h2 id="main-2">main</h2>
<h3 id="rule-reliable-multimodal-rag-for-factuality-in-medical-vision-language-models多模态的但是医学相关的"><strong>RULE:
Reliable Multimodal RAG for Factuality in Medical Vision Language
Models</strong>（多模态的，但是医学相关的）</h3>
<h3 id="rag-qa-arena-evaluating-domain-robustness-for-long-form-retrieval-augmented-question-answering"><strong>RAG-QA
Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented
Question Answering</strong></h3>
<h3 id="from-rag-to-riches-retrieval-interlaced-with-sequence-generation"><strong>From
RAG to Riches: Retrieval Interlaced with Sequence
Generation</strong></h3>
<h3 id="summary-of-a-haystack-a-challenge-to-long-context-llms-and-rag-systems"><strong>Summary
of a Haystack: A Challenge to Long-Context LLMs and RAG
Systems</strong></h3>
<h3 id="dynamicer-resolving-emerging-mentions-to-dynamic-entities-for-rag"><strong>DynamicER:
Resolving Emerging Mentions to Dynamic Entities for RAG</strong></h3>
<h3 id="deciphering-the-interplay-of-parametric-and-non-parametric-memory-in-rag-models"><strong>Deciphering
the Interplay of Parametric and Non-Parametric Memory in RAG
Models</strong></h3>
<h3 id="re-rag-improving-open-domain-qa-performance-and-interpretability-with-relevance-estimator-in-retrieval-augmented-generation"><strong>RE-RAG:
Improving Open-Domain QA Performance and Interpretability with Relevance
Estimator in Retrieval-Augmented Generation</strong></h3>
<h3 id="longrag-a-dual-perspective-retrieval-augmented-generation-paradigm-for-long-context-question-answering"><strong>LongRAG:
A Dual-perspective Retrieval-Augmented Generation Paradigm for
Long-Context Question Answering</strong></h3>
<h2 id="findings-2">findings</h2>
<h3 id="rag-studio-towards-in-domain-adaptation-of-retrieval-augmented-generation-through-self-alignment"><strong>RAG-Studio:
Towards In-Domain Adaptation Of Retrieval Augmented Generation Through
Self-Alignment</strong></h3>
<h3 id="rafe-ranking-feedback-improves-query-rewriting-for-rag真小众啊竟然研究query-rewrite"><strong>RaFe:
Ranking Feedback Improves Query Rewriting for
RAG</strong>（真小众啊，竟然研究query rewrite）</h3>
<h3 id="adaptive-selection-for-homogeneous-tools-an-instantiation-in-the-rag-scenario"><strong>Adaptive
Selection for Homogeneous Tools: An Instantiation in the RAG
Scenario</strong></h3>
<h3 id="bsharedrag-backbone-shared-retrieval-augmented-generation-for-the-e-commerce-domain"><strong>BSharedRAG:
Backbone Shared Retrieval-Augmented Generation for the E-commerce
Domain</strong></h3>
<h3 id="open-rag-enhanced-retrieval-augmented-reasoning-with-open-source-large-language-models"><strong>Open-RAG:
Enhanced Retrieval Augmented Reasoning with Open-Source Large Language
Models</strong></h3>
<h3 id="typos-that-broke-the-rags-back-genetic-attack-on-rag-pipeline-by-simulating-documents-in-the-wild-via-low-level-perturbations"><strong>Typos
that Broke the RAG’s Back: Genetic Attack on RAG Pipeline by Simulating
Documents in the Wild via Low-level Perturbations</strong></h3>
<h3 id="autorag-hp-automatic-online-hyper-parameter-tuning-for-retrieval-augmented-generation"><strong>AutoRAG-HP:
Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented
Generation</strong></h3>
<h3 id="long2rag-evaluating-long-context-long-form-retrieval-augmented-generation-with-key-point-recall"><strong><span class="math inline">\(LONG^{2}RAG\)</span>: Evaluating Long-Context
&amp; Long-Form Retrieval-Augmented Generation with Key Point
Recall</strong></h3>
<h1 id="icml-25按retrieval关键词检索的排除了ai4sci">ICML
25（按retrieval关键词检索的，排除了ai4sci）</h1>
<h2 id="poisonedeye-knowledge-poisoning-attack-on-retrieval-augmented-generation-based-large-vision-language-models多模态的"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46373">PoisonedEye: Knowledge
Poisoning Attack on Retrieval-Augmented Generation based Large
Vision-Language Models</a>（多模态的）</h2>
<h2 id="retrieval-augmented-zero-shot-enzyme-generation-for-specified-substrate"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45546">Retrieval Augmented
Zero-Shot Enzyme Generation for Specified Substrate</a></h2>
<h2 id="she-streaming-media-hashing-retrieval疑似多模态的"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">SHE: Streaming-media
Hashing Retrieval</a>（疑似多模态的）</h2>
<h2 id="contradiction-retrieval-via-contrastive-learning-with-sparsity"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45043">Contradiction Retrieval
via Contrastive Learning with Sparsity</a></h2>
<h2 id="llm-alignment-as-retriever-optimization-an-information-retrieval-perspective"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45659">LLM Alignment as
Retriever Optimization: An Information Retrieval Perspective</a></h2>
<h2 id="efficient-length-generalizable-attention-via-causal-retrieval-for-long-context-language-modeling"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46384">Efficient
Length-Generalizable Attention via Causal Retrieval for Long-Context
Language Modeling</a></h2>
<h2 id="phantomwiki-on-demand-datasets-for-reasoning-and-retrieval-evaluation"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46018">PhantomWiki: On-Demand
Datasets for Reasoning and Retrieval Evaluation</a></h2>
<h2 id="understanding-synthetic-context-extension-via-retrieval-heads"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44638">Understanding Synthetic
Context Extension via Retrieval Heads</a></h2>
<h2 id="rapid-long-context-inference-with-retrieval-augmented-speculative-decoding"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46343">RAPID: Long-Context
Inference with Retrieval-Augmented Speculative Decoding</a></h2>
<h2 id="c-3po-compact-plug-and-play-proxy-optimization-to-achieve-human-like-retrieval-augmented-generation"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44398">C-3PO: Compact
Plug-and-Play Proxy Optimization to Achieve Human-like
Retrieval-Augmented Generation</a></h2>
<h2 id="on-the-vulnerability-of-applying-retrieval-augmented-generation-within-knowledge-intensive-application-domains"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45226">On the Vulnerability of
Applying Retrieval-Augmented Generation within Knowledge-Intensive
Application Domains</a></h2>
<h2 id="in-context-learning-as-conditioned-associative-memory-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44826">In-Context Learning as
Conditioned Associative Memory Retrieval</a></h2>
<h2 id="in-context-denoising-with-one-layer-transformers-connections-between-attention-and-associative-memory-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45913">In-Context Denoising
with One-Layer Transformers: Connections between Attention and
Associative Memory Retrieval</a></h2>
<h2 id="poqd-performance-oriented-query-decomposer-for-multi-vector-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44047">POQD:
Performance-Oriented Query Decomposer for Multi-vector
retrieval</a></h2>
<h2 id="scenir-visual-semantic-clarity-through-unsupervised-scene-graph-retrieval多模态的但是是scene-graph相关的"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43841">SCENIR: Visual Semantic
Clarity through Unsupervised Scene Graph
Retrieval</a>（多模态的？但是是scene graph相关的）</h2>
<h2 id="retrieval-augmented-time-series-forecasting"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45826">Retrieval Augmented
Time Series Forecasting</a></h2>
<h2 id="position-retrieval-augmented-systems-can-be-dangerous-medical-communicators"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/40149">Position:
Retrieval-augmented systems can be dangerous medical
communicators</a></h2>
<h2 id="beyond-cropped-regions-new-benchmark-and-corresponding-baseline-for-chinese-scene-text-retrieval-in-diverse-layouts"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45853">Beyond Cropped Regions:
New Benchmark and Corresponding Baseline for Chinese Scene Text
Retrieval in Diverse Layouts</a></h2>
<h2 id="qure-query-relevant-retrieval-through-hard-negative-sampling-in-composed-image-retrieval多模态的"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43476">QuRe: Query-Relevant
Retrieval through Hard Negative Sampling in Composed Image
Retrieval</a>（多模态的）</h2>
<h2 id="learning-attribute-aware-hash-codes-for-fine-grained-image-retrieval-via-query-optimization多模态的"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43868">Learning
Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query
Optimization</a>（多模态的）</h2>
<h2 id="locality-preserving-markovian-transition-for-instance-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45178">Locality Preserving
Markovian Transition for Instance Retrieval</a></h2>
<h2 id="visual-abstraction-a-plug-and-play-approach-for-text-visual-retrieval多模态的"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">Visual Abstraction: A
Plug-and-Play Approach for Text-Visual Retrieval</a>（多模态的）</h2>
<h2 id="docks-rag-optimizing-document-level-relation-extraction-through-llm-enhanced-hybrid-prompt-tuning多模态的文档类"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45220">DocKS-RAG: Optimizing
Document-Level Relation Extraction through LLM-Enhanced Hybrid Prompt
Tuning</a>（多模态的，文档类）</h2>
<h2 id="from-rag-to-memory-non-parametric-continual-learning-for-large-language-models"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45585">From RAG to Memory:
Non-Parametric Continual Learning for Large Language Models</a></h2>
<h2 id="hierarchical-planning-for-complex-tasks-with-knowledge-graph-rag-and-symbolic-verification"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43660">Hierarchical Planning
for Complex Tasks with Knowledge Graph-RAG and Symbolic
Verification</a></h2>
<h2 id="retrieval-augmented-perception-high-resolution-image-perception-meets-visual-rag多模态的王中王神中神"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44979">Retrieval-Augmented
Perception: High-resolution Image Perception Meets Visual
RAG</a>（多模态的，王中王，神中神）</h2>
<h2 id="realrag-retrieval-augmented-realistic-image-generation-via-self-reflective-contrastive-learning也算多模态的吧结合rag与diffusion"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44615">RealRAG:
Retrieval-augmented Realistic Image Generation via Self-reflective
Contrastive Learning</a>（也算多模态的吧，结合RAG与diffusion）</h2>
<h2 id="lara-benchmarking-retrieval-augmented-generation-and-long-context-llms-no-silver-bullet-for-lc-or-rag-routing"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46069">LaRA: Benchmarking
Retrieval-Augmented Generation and Long-Context LLMs – No Silver Bullet
for LC or RAG Routing</a></h2>
<h2 id="ragged-towards-informed-design-of-scalable-and-stable-rag-systems"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46460">RAGGED: Towards
Informed Design of Scalable and Stable RAG Systems</a></h2>
<h1 id="iclr-25按retrieval关键词检索">ICLR
25（按retrieval关键词检索）</h1>
<ul>
<li><p>From Artificial Needles to Real Haystacks: Improving Retrieval
Capabilities in LLMs by Finetuning on Synthetic Data</p></li>
<li><p>ColPali: Efficient Document Retrieval with Vision Language
Models（优先）（多模态的）</p></li>
<li><p>Semi-Parametric Retrieval via Binary Bag-of-Tokens Index</p></li>
<li><p>VisRAG: Vision-based Retrieval-augmented Generation on
Multi-modality Documents（优先）（多模态的）</p></li>
<li><p>CoRNStack: High-Quality Contrastive Data for Better Code
Retrieval and Reranking</p></li>
<li><p>RA-TTA: Retrieval-Augmented Test-Time Adaptation for
Vision-Language Models（多模态的）</p></li>
<li><p>Streaming Video Question-Answering with In-context Video KV-Cache
Retrieval（多模态的）</p></li>
<li><p>STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy
Learning</p></li>
<li><p>MAI: A Multi-turn Aggregation-Iteration Model for Composed Image
Retrieval（多模态的）</p></li>
<li><p>Bridging Information Asymmetry in Text-video Retrieval: A
Data-centric Approach（多模态的）</p></li>
<li><p>Sufficient Context: A New Lens on Retrieval Augmented Generation
Systems</p></li>
<li><p>SeCom: On Memory Construction and Retrieval for Personalized
Conversational Agents</p></li>
<li><p>Beyond Content Relevance: Evaluating Instruction Following in
Retrieval Models</p></li>
<li><p>Benchmarking Multimodal Retrieval Augmented Generation with
Dynamic VQA Dataset and Self-adaptive Planning
Agent（算多模态的？涉及VQA了。但是是benchmark类的论文）</p></li>
<li><p>ToolGen: Unified Tool Retrieval and Calling via
Generation</p></li>
<li><p>Retrieval Augmented Diffusion Model for Structure-informed
Antibody Design and Optimization（算多模态的？涉及diffusion）</p></li>
<li><p>MM-EMBED: Universal Multimodal Retrieval with Multimodal
LLMs（多模态的）</p></li>
<li><p>ReNovo: Retrieval-Based <em>De Novo</em> Mass Spectrometry
Peptide Sequencing</p></li>
<li><p>TIGeR: Unifying Text-to-Image Generation and Retrieval with Large
Multimodal Models（多模态的）</p></li>
<li><p>InstructRAG: Instructing Retrieval-Augmented Generation via
Self-Synthesized Rationales</p></li>
<li><p>Reasoning-Enhanced Healthcare Predictions with Knowledge Graph
Community Retrieval</p></li>
<li><p>DRoC: Elevating Large Language Models for Complex Vehicle Routing
via Decomposed Retrieval of Constraints</p></li>
<li><p>Multi-Field Adaptive Retrieval</p></li>
<li><p>MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented
Multimodal Models（多模态的，只不过是benchmark类的）</p></li>
<li><p>ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation
via Mechanistic Interpretability</p></li>
<li><p>LaMP: Language-Motion Pretraining for Motion Generation,
Retrieval, and Captioning</p></li>
<li><p>RAG-DDR: Optimizing Retrieval-Augmented Generation Using
Differentiable Data Rewards</p></li>
<li><p>Follow My Instruction and Spill the Beans: Scalable Data
Extraction from Retrieval-Augmented Generation Systems</p></li>
<li><p>DuoAttention: Efficient Long-Context LLM Inference with Retrieval
and Streaming Heads</p></li>
<li><p>Learning Fine-Grained Representations through Textual Token
Disentanglement in Composed Video Retrieval（多模态的）</p></li>
<li><p>On the Convergence of No-Regret Dynamics in Information Retrieval
Games with Proportional Ranking Functions</p></li>
<li><p>NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for
Retrieval</p></li>
<li><p>TempMe: Video Temporal Token Merging for Efficient Text-Video
Retrieval（多模态的）</p></li>
<li><p>Provence: Efficient and Robust Context Pruning for
Retrieval-Augmented Generation</p></li>
<li><p>Retrieval Head Mechanistically Explains Long-Context
Factuality</p></li>
<li><p>Think-on-Graph 2.0: Deep and Faithful Large Language Model
Reasoning with Knowledge-guided Retrieval Augmented Generation</p></li>
<li><p>SePer: Measure Retrieval Utility Through the Lens of Semantic
Perplexity Reduction</p></li>
<li><p>RNNs are not Transformers (Yet): The Key Bottleneck on In-Context
Retrieval</p></li>
<li><p>Test-time Adaptation for Cross-modal Retrieval with Query
Shift</p></li>
<li><p>SRSA: Skill Retrieval and Adaptation for Robotic Assembly
Tasks</p></li>
<li><p>Look Before You Leap: Universal Emergent Mechanism for Retrieval
in Language Models</p></li>
<li><p>Grounding by Trying: LLMs with Reinforcement Learning-Enhanced
Retrieval</p></li>
<li><p>Inference Scaling for Long-Context Retrieval Augmented
Generation</p></li>
<li><p>REGENT: A Retrieval-Augmented Generalist Agent That Can Act
In-Context in New Environments</p></li>
<li><p>MLLM as Retriever: Interactively Learning Multimodal Retrieval
for Embodied Agents（多模态的，不过似乎是应用到具身方面？）</p></li>
<li><p>Generalized Video Moment Retrieval（多模态的）</p></li>
<li><p>Retri3D: 3D Neural Graphics Representation Retrieval</p></li>
<li><p>Rethinking and Improving Autoformalization: Towards a Faithful
Metric and a Dependency Retrieval-based Approach</p></li>
<li><p>A Theory for Token-Level Harmonization in Retrieval-Augmented
Generation</p></li>
<li><p>Training Large Language Models for Retrieval-Augmented Question
Answering through Backtracking Correction</p></li>
<li><p>Exploiting Distribution Constraints for Scalable and Efficient
Image Retrieval（多模态的）</p></li>
<li><p>Simple is Effective: The Roles of Graphs and Large Language
Models in Knowledge-Graph-Based Retrieval-Augmented Generation</p></li>
<li><p>RAG-SR: Retrieval-Augmented Generation for Neural Symbolic
Regression</p></li>
<li><p>Auto-GDA: Automatic Domain Adaptation for Efficient Grounding
Verification in Retrieval-Augmented Generation</p></li>
<li><p>Not All Heads Matter: A Head-Level KV Cache Compression Method
with Integrated Retrieval and Reasoning</p></li>
<li><p>Accelerating Inference of Retrieval-Augmented Generation via
Sparse Context Selection</p></li>
<li><p>Speculative RAG: Enhancing Retrieval Augmented Generation through
Drafting</p></li>
<li><p>RAPID: Retrieval Augmented Training of Differentially Private
Diffusion Models（算多模态的？涉及diffusion models）</p></li>
<li><p>BRIGHT: A Realistic and Challenging Benchmark for
Reasoning-Intensive Retrieval</p></li>
<li><p>RazorAttention: Efficient KV Cache Compression Through Retrieval
Heads # NIPS
24（按retrieval关键词检索）（这里就不把所有的都摘录下来了，只把那些关于多模态的摘录下来）
## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95524">G-Retriever:
Retrieval-Augmented Generation for Textual Graph Understanding and
Question Answering</a> ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95267">An End-To-End Graph
Attention Network Hashing for Cross-Modal
Retrieval</a>（这个应该既是多模态的，也是graph相关的） ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95609">Where's Waldo:
Diffusion Features For Personalized Segmentation and Retrieval</a> ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/93151">Aligning Vision Models
with Human Aesthetics in Retrieval: Benchmarks and
Algorithms</a>（benchmark类啊） ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95072">Diffusion-Inspired
Truncated Sampler for Text-Video Retrieval</a> ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97543">INQUIRE: A Natural
World Text-to-Image Retrieval Benchmark</a>（又是benchmark类） ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97657">BiVLC: Extending
Vision-Language Compositionality Evaluation with Text-to-Image
Retrieval</a> ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97632">VERIFIED: A Video
Corpus Moment Retrieval Benchmark for Fine-Grained Video
Understanding</a>（依然benchmark） ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97785">WikiDO: A New Benchmark
Evaluating Cross-Modal Retrieval for Vision-Language
Models</a>（benchmark again） ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/96776">Retrieval &amp;
Fine-Tuning for In-Context Tabular
Models</a>（似乎是研究表格型数据的？算吗？） ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97735">UDA: A Benchmark Suite
for Retrieval Augmented Generation in Real-World Document
Analysis</a>（研究document analysis的） ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/96281">Semi-Open 3D Object
Retrieval via Hierarchical Equilibrium on
Hypergraph</a>（有点杂啊，检索3D物体）（涉及graph） ## <a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/93088">Assembly Fuzzy
Representation on Hypergraph for Open-Set 3D Object
Retrieval</a>（我去，还有一篇吗，研究3D
retrieval的）（多模态的，涉及graph） # CVPR
25（之所以在cvpr上找，是因为现在似乎有一些研究关注RAG和多模态的结合，那可能在cvpr上会比较多一些吧）（按retrieval关键词找的）
## <strong>DrVideo: Document Retrieval Based Long Video
Understanding</strong> ## <strong>COBRA: COmBinatorial Retrieval
Augmentation for Few-Shot Adaptation</strong> ## <strong><a target="_blank" rel="noopener" href="https://ivy-lvlm.github.io/SALOVA/">SALOVA: Segment-Augmented Long
Video Assistant for Targeted Retrieval and Routing in Long-Form Video
Analysis</a></strong> ## <strong>Fuzzy Multimodal Learning for Trusted
Cross-modal Retrieval</strong> ## <a target="_blank" rel="noopener" href="https://tian1327.github.io/SWAT/">Few-Shot Recognition via
Stage-Wise Retrieval-Augmented Finetuning</a> ## <strong>VLog:
Video-Language Models by Generative Retrieval of Narration
Vocabulary</strong> ## <a target="_blank" rel="noopener" href="https://collm-cvpr25.github.io/">CoLLM:
A Large Language Model for Composed Image Retrieval</a> ##
<strong>Search and Detect: Training-Free Long Tail Object Detection via
Web-Image Retrieval</strong> ## <strong>Learning Compatible Multi-Prize
Subnetworks for Asymmetric Retrieval</strong> ## <strong>Learning
Audio-guided Video Representation with Gated Attention for Video-Text
Retrieval</strong> ## <a target="_blank" rel="noopener" href="https://code-kunkun.github.io/LamRA/">LamRA: Large Multimodal
Model as Your Advanced Retrieval Assistant</a> ## <a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT">Recurrence-Enhanced
Vision-and-Language Transformers for Robust Multimodal Document
Retrieval</a> ## <strong>Missing Target-Relevant Information Prediction
with World Model for Accurate Zero-Shot Composed Image
Retrieval</strong> ## <a target="_blank" rel="noopener" href="https://vrg.fel.cvut.cz/ilias/">ILIAS:
Instance-Level Image retrieval At Scale</a> ## <strong>CLIP is Almost
All You Need: Towards Parameter-Efficient Scene Text Retrieval without
OCR</strong> ## <strong>Rethinking Noisy Video-Text Retrieval via
Relation-aware Alignment</strong> ## <a target="_blank" rel="noopener" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct">Bridging
Modalities: Improving Universal Multimodal Retrieval by Multimodal Large
Language Models</a> ## <a target="_blank" rel="noopener" href="https://github.com/Pter61/osrcir">Reason-before-Retrieve:
One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot
Composed Image Retrieval</a> ## <strong>MaRI: Material Retrieval
Integration across Domains</strong> ## <a target="_blank" rel="noopener" href="https://vdocrag.github.io/">VDocRAG: Retrieval-Augmented
Generation over Visually-Rich Documents</a>（这篇明确出现RAG了啊） ##
<strong>RANGE: Retrieval Augmented Neural Fields for Multi-Resolution
Geo-Embeddings</strong>（这个也是有retrieval augmented） ## <a target="_blank" rel="noopener" href="https://hoar012.github.io/RAP-Project/">RAP: Retrieval-Augmented
Personalization for Multimodal Large Language
Models</a>（我去，这么巧，和ICML oral那篇感觉做的题材有点像啊） ##
<strong>Video-ColBERT: Contextualized Late Interaction for Text-to-Video
Retrieval</strong> ## <strong>Imagine and Seek: Improving Composed Image
Retrieval with an Imagined Proxy</strong> ## <strong>MultiVENT 2.0: A
Massive Multilingual Benchmark for Event-Centric Video
Retrieval</strong> ## <strong>The Devil is in the Prompts:
Retrieval-Augmented Prompt Optimization for Text-to-Video
Generation</strong>（不会是prompt engineering吧……） ## <a target="_blank" rel="noopener" href="https://multimodal-understanding-group.github.io/NarVid/">Narrating
the Video: Boosting Text-Video Retrieval via Comprehensive Utilization
of Frame-Level Captions</a> ## <strong>Chat-based Person Retrieval via
Dialogue-Refined Cross-Modal Alignment</strong> ## <a target="_blank" rel="noopener" href="https://github.com/ShiShuMo/PromptHash">PromptHash:
Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive
Hashing Retrieval</a> ## <a target="_blank" rel="noopener" href="https://zzezze.github.io/NeighborRetr/">NeighborRetr: Balancing
Hub Centrality in Cross-Modal Retrieval</a> ## <a target="_blank" rel="noopener" href="https://github.com/CharlesNeilWilliams/TME">Learning with Noisy
Triplet Correspondence for Composed Image Retrieval</a> ##
<strong>GarmentPile: Point-Level Visual Affordance Guided Retrieval and
Adaptation for Cluttered Garments Manipulation</strong> ##
<strong>Towards Natural Language-Based Document Image Retrieval: New
Dataset and Benchmark</strong> ## <strong>Semantic Library Adaptation:
LoRA Retrieval and Fusion for Open-Vocabulary Semantic
Segmentation</strong> ## <strong>CCIN: Compositional Conflict
Identification and Neutralization for Composed Image Retrieval</strong>
## <strong>Generative Zero-Shot Composed Image Retrieval</strong> ##
<strong>ConText-CIR: Learning from Concepts in Text for Composed Image
Retrieval</strong>（好多这种composed image retrieval啊，这个是啥啊） ##
<a target="_blank" rel="noopener" href="https://github.com/LunarShen/DsicoVLA">DiscoVLA: Discrepancy
Reduction in Vision, Language, and Alignment for Parameter-Efficient
Video-Text Retrieval</a> # ICCV 25（按retrieval关键词找的） ##
<strong>G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via
Part-Aware Prior Retrieval and Prior-Assisted Generation</strong> ##
<strong>Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language
Generator</strong> ## <strong>Hybrid-Tower: Fine-grained Pseudo-query
Interaction and Generation for Text-to-Video Retrieval</strong> ## <a target="_blank" rel="noopener" href="https://xyangzhou.github.io/TD-DETR/">The Devil is in the Spurious
Correlations: Boosting Moment Retrieval with Dynamic Learning</a> ##
<strong>Taming the Untamed: Graph-Based Knowledge Retrieval and
Reasoning for MLLMs to Conquer the
Unknown</strong>（多模态的，VLM方面的，以及涉及graph） ##
<strong>Learnable Retrieval Enhanced Visual-Text Alignment and Fusion
for Radiology Report Generation</strong> ## <strong>Zero-Shot Composed
Image Retrieval via Dual-Stream Instruction-Aware Distillation</strong>
## <strong>SIMS: Simulating Stylized Human-Scene Interactions with
Retrieval-Augmented Script Generation</strong> ## <strong>Beyond Simple
Edits: Composed Video Retrieval with Dense Modifications</strong> ##
<strong>Reference-based Super-Resolution via Image-based
Retrieval-Augmented Generation Diffusion</strong> ## <strong>MA-CIR: A
Multimodal Arithmetic Benchmark for Composed Image Retrieval</strong> ##
<strong>Test-Time Retrieval-Augmented Adaptation for Vision-Language
Models</strong> ## <strong>GestureHYDRA: Semantic Co-speech Gesture
Synthesis via Hybrid Modality Diffusion Transformer and
Cascaded-Synchronized Retrieval-Augmented Generation</strong> ## <a target="_blank" rel="noopener" href="https://github.com/ronpay/CRAVE">Borrowing Eyes for the Blind
Spot: Overcoming Data Scarcity in Malicious Video Detection via
Cross-Domain Retrieval Augmentation</a> ## <strong>Enhancing Partially
Relevant Video Retrieval with Hyperbolic Learning</strong> ##
<strong>ChatReID: Open-ended Interactive Person Retrieval via
Hierarchical Progressive Tuning for Vision Language Models</strong> ##
<strong>ReMP-AD: Retrieval-enhanced Multi-modal Prompt Fusion for
Few-Shot Industrial Visual Anomaly Detection</strong> ## <strong>Beyond
Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object
Detection</strong> ## <strong>Describe, Adapt and Combine: Empowering
CLIP Encoders for Open-set 3D Object Retrieval</strong> ##
<strong>Hierarchy-Aware Pseudo Word Learning with Text Adaptation for
Zero-Shot Composed Image Retrieval</strong> ## <strong>CARIM:
Caption-Based Autonomous Driving Scene Retrieval via Inclusive Text
Matching</strong> ## <strong>OphCLIP: Hierarchical Retrieval-Augmented
Learning for Ophthalmic Surgical Video-Language Pretraining</strong> ##
<strong>Learning Visual Hierarchies in Hyperbolic Space for Image
Retrieval</strong> ## <strong>MonSTeR: a Unified Model for Motion,
Scene, Text Retrieval</strong> ## <strong>CoTMR: Chain-of-Thought
Multi-Scale Reasoning for Training-Free Zero-Shot Composed Image
Retrieval</strong> ## <strong>OCR Hinders RAG: Evaluating the Cascading
Impact of OCR on Retrieval-Augmented Generation</strong> ##
<strong>Augmenting Moment Retrieval: Zero-Dependency Two-Stage
Learning</strong> ## <strong>Quantifying and Narrowing the Unknown:
Interactive Text-to-Video Retrieval via Uncertainty
Minimization</strong> ## <a target="_blank" rel="noopener" href="https://deepayan137.github.io/papers/training-free-personalization.html">Training-Free
Personalization via Retrieval and Reasoning on Fingerprints</a> ##
<strong>Prototypes are Balanced Units for Efficient and Effective
Partially Relevant Video Retrieval</strong> ## <strong>Bidirectional
Likelihood Estimation with Multi-Modal Large Language Models for
Text-Video Retrieval</strong> ## <strong>Multi-Schema Proximity Network
for Composed Image Retrieval</strong> ## <strong>An Efficient Post-hoc
Framework for Reducing Task Discrepancy of Text Encoders for Composed
Image Retrieval</strong> ## <strong>AutoComPose: Automatic Generation of
Pose Transition Descriptions for Composed Pose Retrieval Using
Multimodal LLMs</strong> # ECCV 24（按retrieval关键词检索的） ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2083_ECCV_2024_paper.php">Rethinking
Video-Text Understanding: Retrieval from Counterfactually Augmented
Data</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2265_ECCV_2024_paper.php">IRGen:
Generative Modeling for Image Retrieval</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2302_ECCV_2024_paper.php">FastCAD:
Real-Time CAD Retrieval and Alignment from Scans and Videos</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2904_ECCV_2024_paper.php">Spherical
Linear Interpolation and Text-Anchoring for Zero-shot Composed Image
Retrieval</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3186_ECCV_2024_paper.php">RGNet:
A Unified Clip Retrieval and Grounding Network for Long Videos</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3426_ECCV_2024_paper.php">Elevating
All Zero-Shot Sketch-Based Image Retrieval Through Multimodal Prompt
Learning</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3572_ECCV_2024_paper.php">Is
user feedback always informative? Retrieval Latent Defending for
Semi-Supervised Domain Adaptation without Source Data</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3622_ECCV_2024_paper.php">GarmentAligner:
Text-to-Garment Generation via Retrieval-augmented Multi-level
Corrections</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5033_ECCV_2024_paper.php">KDProR:
A Knowledge-Decoupling Probabilistic Framework for Video-Text
Retrieval</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5363_ECCV_2024_paper.php">EgoCVR:
An Egocentric Benchmark for Fine-Grained Composed Video Retrieval</a> ##
<a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5379_ECCV_2024_paper.php">Cross-view
image geo-localization with Panorama-BEV Co-Retrieval Network</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5418_ECCV_2024_paper.php">Where
am I? Scene Retrieval with Language</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6074_ECCV_2024_paper.php">Uncertainty-aware
sign language video retrieval with probability distribution modeling</a>
## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6283_ECCV_2024_paper.php">UniMD:
Towards Unifying Moment Retrieval and Temporal Action Detection</a> ##
<a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6415_ECCV_2024_paper.php">GLARE:
Low Light Image Enhancement via Generative Latent Feature based Codebook
Retrieval</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6751_ECCV_2024_paper.php">Enhancing
Recipe Retrieval with Foundation Models: A Data Augmentation
Perspective</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6845_ECCV_2024_paper.php">EA-VTR:
Event-Aware Video-Text Retrieval</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7423_ECCV_2024_paper.php">Freeview
Sketching: View-Aware Fine-Grained Sketch-Based Image Retrieval</a> ##
<a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7570_ECCV_2024_paper.php">Chronologically
Accurate Retrieval for Temporal Grounding of Motion-Language Models</a>
## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7666_ECCV_2024_paper.php">ReCON:
Training-Free Acceleration for Text-to-Image Synthesis with Retrieval of
Concept Prompt Trajectories</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7670_ECCV_2024_paper.php">AMES:
Asymmetric and Memory-Efficient Similarity Estimation for Instance-level
Retrieval</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7903_ECCV_2024_paper.php">RealGen:
Retrieval Augmented Generation for Controllable Traffic Scenarios</a> ##
<a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8592_ECCV_2024_paper.php">Revisit
Anything: Visual Place Recognition via Image Segment Retrieval</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9950_ECCV_2024_paper.php">RAP:
Retrieval-Augmented Planner for Adaptive Procedure Planning in
Instructional Videos</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10290_ECCV_2024_paper.php">Object-Aware
Query Perturbation for Cross-Modal Image-Text Retrieval</a> ## <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11229_ECCV_2024_paper.php">Retrieval
Robust to Object Motion Blur</a> # 1. <strong>RAG（Retrieval-Augmented
Generation）方法拓展与应用</strong> 这些论文显式或核心思路就是
<strong>把 RAG 技术迁移到视觉/多模态/特定任务场景</strong>：</p></li>
<li><p><strong>VDocRAG: Retrieval-Augmented Generation over
Visually-Rich Documents (CVPR)</strong></p></li>
<li><p><strong>RANGE: Retrieval Augmented Neural Fields for
Multi-Resolution Geo-Embeddings (CVPR)</strong></p></li>
<li><p><strong>RAP: Retrieval-Augmented Personalization for Multimodal
LLMs (CVPR)</strong></p></li>
<li><p><strong>The Devil is in the Prompts: Retrieval-Augmented Prompt
Optimization for Text-to-Video Generation (CVPR)</strong></p></li>
<li><p><strong>SIMS: Retrieval-Augmented Script Generation
(ICCV)</strong></p></li>
<li><p><strong>Reference-based Super-Resolution via Image-based
Retrieval-Augmented Generation Diffusion (ICCV)</strong></p></li>
<li><p><strong>GestureHYDRA: Retrieval-Augmented Generation for Gesture
Synthesis (ICCV)</strong></p></li>
<li><p><strong>OphCLIP: Hierarchical Retrieval-Augmented Learning for
Ophthalmic Surgical Video-Language Pretraining (ICCV)</strong></p></li>
<li><p><strong>ReMP-AD: Retrieval-enhanced Multi-modal Prompt Fusion for
Few-Shot Industrial Visual Anomaly Detection (ICCV)</strong></p></li>
<li><p><strong>OCR Hinders RAG: Evaluating OCR’s Impact on RAG
(ICCV)</strong></p></li>
<li><p><strong>Borrowing Eyes for the Blind Spot: Malicious Video
Detection via Cross-Domain Retrieval Augmentation
(ICCV)</strong>（这篇竟然还没开放paper吗？奇怪了……） # 2.
<strong>Composed Image/Video Retrieval（组合检索，编辑式检索）</strong>
这类特别多，主要是 <strong>给定图像 +
文本修饰/修改条件，找到目标图像</strong>，近年是 retrieval
热点：</p></li>
<li><p>ConText-CIR, CCIN, Generative CIR, Learning w/ Noisy Triplet,
Imagine and Seek, Reason-before-Retrieve, etc. (CVPR)</p></li>
<li><p>Beyond Simple Edits: Composed Video Retrieval with Dense
Modifications (ICCV)</p></li>
<li><p>Zero-Shot CIR via Dual-Stream Instruction-Aware Distillation
(ICCV)</p></li>
<li><p>Hierarchy-Aware Pseudo Word Learning for Zero-Shot CIR
(ICCV)</p></li>
<li><p>Multi-Schema Proximity Network for CIR (ICCV)</p></li>
<li><p>An Efficient Post-hoc Framework for Reducing Task Discrepancy of
Text Encoders for CIR (ICCV)</p></li>
<li><p>CoTMR: Chain-of-Thought Reasoning for Zero-Shot CIR
(ICCV)</p></li>
<li><p>MA-CIR Benchmark (ICCV)</p></li>
</ul>
<h1 id="video-multimodal-retrieval">3. <strong>Video &amp; Multimodal
Retrieval</strong></h1>
<p>长视频、视频-文本、音频-视频、事件检索等：</p>
<ul>
<li><p>DrVideo, SALOVA, VLog, Narrating the Video, Video-ColBERT,
DiscoVLA, MultiVENT 2.0, Rethinking Noisy Video-Text Retrieval,
Chat-based Person Retrieval, Learning Audio-guided Video Representation
(CVPR)</p></li>
<li><p>Hybrid-Tower, Enhancing Partially Relevant Video Retrieval,
Prototypes are Balanced Units, Quantifying and Narrowing the Unknown,
MonSTeR (ICCV) # 类似ICML
25那篇论文的思路的论文（也就是类比了用RAG输入超长输入的思路）</p></li>
<li><p><strong>VDocRAG (CVPR25)</strong></p>
<ul>
<li><p>任务：长文档视觉问答。</p></li>
<li><p>思路：把超长的文档视为“长文本”，而文档里的图片、表格又和文本混杂
→ 类似于多模态的“超长输入”问题。</p></li>
<li><p>方法：通过 retrieval 选取相关片段（文字/视觉），解决了超长输入的
token 负担。<br>
➝ 本质上和你说的 <em>HR-images → 长文本</em>
类似，都是把“大输入问题”转化为 “检索 + 局部 reasoning”。</p></li>
</ul></li>
<li><p><strong>SALOVA (CVPR25)</strong>（长视频摘要 / QA）</p>
<ul>
<li><p>视频是天然的长序列，直接处理会爆炸。</p></li>
<li><p>他们用 retrieval 在 video segments 中挑出关键信息 →
类似把视频理解类比成长文本阅读。</p></li>
</ul></li>
<li><p><strong>SIMS (ICCV25)</strong>（多模态对话检索）</p>
<ul>
<li>把跨模态对话（文本 + 图像）看作“多源长对话”，通过 retrieval 来挑关键
evidence，避免全量建模。</li>
</ul></li>
</ul>
<h4 id="cross-modal-dense-retrieval18-篇">1. Cross-modal Dense
Retrieval（18 篇）</h4>
<table style="width:100%;">
<colgroup>
<col style="width: 53%">
<col style="width: 4%">
<col style="width: 40%">
<col style="width: 1%">
</colgroup>
<thead>
<tr>
<th>完整题目</th>
<th>会议/年份</th>
<th>官方链接</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>ColPali: Efficient Document Retrieval with Vision Language
Models</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.01449">abs/2407.01449</a></td>
<td>Y</td>
</tr>
<tr>
<td>VisRAG: Vision-based Retrieval-augmented Generation on
Multi-modality Documents</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.12345">abs/2405.12345</a></td>
<td>Y</td>
</tr>
<tr>
<td>MM-EMBED: Universal Multimodal Retrieval with Multimodal LLMs</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.08056">abs/2501.08056</a></td>
<td>Y</td>
</tr>
<tr>
<td>BridgeModalities: Improving Universal Multimodal Retrieval by
Multimodal Large Language Models</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.09795">abs/2504.09795</a></td>
<td>N</td>
</tr>
<tr>
<td>CoLLM: A Large Language Model for Composed Image Retrieval</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06789">abs/2505.06789</a></td>
<td>Y</td>
</tr>
<tr>
<td>MLLM as Retriever: Interactively Learning Multimodal Retrieval for
Embodied Agents</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06789">abs/2505.06789</a></td>
<td>Y</td>
</tr>
<tr>
<td>Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for
Training-Free Zero-Shot Composed Image Retrieval</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://github.com/Pter61/osrcir">github.com/Pter61/osrcir</a></td>
<td>Y</td>
</tr>
<tr>
<td>MAI: A Multi-turn Aggregation-Iteration Model for Composed Image
Retrieval</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.22345">abs/2502.22345</a></td>
<td>N</td>
</tr>
<tr>
<td>QuRe: Query-Relevant Retrieval through Hard Negative Sampling in
Composed Image Retrieval</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43476">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>TIGeR: Unifying Text-to-Image Generation and Retrieval with Large
Multimodal Models</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.05678">abs/2504.05678</a></td>
<td>Y</td>
</tr>
<tr>
<td>Bridge Modalities: Improving Universal Multimodal Retrieval by
Multimodal Large Language Models</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct">HuggingFace</a></td>
<td>Y</td>
</tr>
<tr>
<td>Visual Abstraction: A Plug-and-Play Approach for Text-Visual
Retrieval</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>ReMP-AD: Retrieval-enhanced Multi-modal Prompt Fusion for Few-Shot
Industrial Visual Anomaly Detection</td>
<td>ICCV 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td>N</td>
</tr>
<tr>
<td>Object-Aware Query Perturbation for Cross-Modal Image-Text
Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10290_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>NeighborRetr: Balancing Hub Centrality in Cross-Modal Retrieval</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://zzezze.github.io/NeighborRetr/">project</a></td>
<td>Y</td>
</tr>
<tr>
<td>PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for
Adaptive Hashing Retrieval</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://github.com/ShiShuMo/PromptHash">github.com/ShiShuMo/PromptHash</a></td>
<td>Y</td>
</tr>
<tr>
<td>Learning with Noisy Triplet Correspondence for Composed Image
Retrieval</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://github.com/CharlesNeilWilliams/TME">github.com/CharlesNeilWilliams/TME</a></td>
<td>Y</td>
</tr>
<tr>
<td>ColPali: Efficient Document Retrieval with Vision Language Models
(additional bench)</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.01449">abs/2407.01449</a></td>
<td>Y</td>
</tr>
</tbody>
</table>
<h4 id="fine-grained-patchregion-retrieval15-篇">2. Fine-grained
Patch/Region Retrieval（15 篇）</h4>
<table style="width:100%;">
<colgroup>
<col style="width: 34%">
<col style="width: 3%">
<col style="width: 62%">
<col style="width: 0%">
</colgroup>
<thead>
<tr>
<th>完整题目</th>
<th>会议/年份</th>
<th>官方链接</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>VDocRAG: Retrieval-Augmented Generation over Visually-Rich
Documents</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://vdocrag.github.io/">project</a></td>
<td>Y</td>
</tr>
<tr>
<td>REVEAL: Retrieval-based Vision-Language Pretraining with
Region-Level Alignment</td>
<td>CVPR 2023</td>
<td><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_REVEAL_Retrieval-Augmented_Visual-Language_Pre-Training_With_Multi-Source_Multimodal_Knowledge_Memory_CVPR_2023_paper.pdf">PDF</a></td>
<td>Y</td>
</tr>
<tr>
<td>Retrieval-Augmented Perception: High-resolution Image Perception
Meets Visual RAG</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44979">ICML</a></td>
<td>Y</td>
</tr>
<tr>
<td>Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval
via Query Optimization</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43868">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>Visual Abstraction: A Plug-and-Play Approach for Text-Visual
Retrieval</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>ReMP-AD: Retrieval-enhanced Multi-modal Prompt Fusion for Few-Shot
Industrial Visual Anomaly Detection</td>
<td>ICCV 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td>N</td>
</tr>
<tr>
<td>Object-Aware Query Perturbation for Cross-Modal Image-Text
Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10290_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>Revisit Anything: Visual Place Recognition via Image Segment
Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8592_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>GLARE: Low Light Image Enhancement via Generative Latent Feature
based Codebook Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6415_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>KDProR: A Knowledge-Decoupling Probabilistic Framework for
Video-Text Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5033_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video
Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5363_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>Freeview Sketching: View-Aware Fine-Grained Sketch-Based Image
Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7423_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>Chronologically Accurate Retrieval for Temporal Grounding of
Motion-Language Models</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7570_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>FastCAD: Real-Time CAD Retrieval and Alignment from Scans and
Videos</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2302_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>RGNet: A Unified Clip Retrieval and Grounding Network for Long
Videos</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3186_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
</tbody>
</table>
<h4 id="video-segment-retrieval14-篇">3. Video Segment Retrieval（14
篇）</h4>
<table>
<colgroup>
<col style="width: 52%">
<col style="width: 4%">
<col style="width: 41%">
<col style="width: 0%">
</colgroup>
<thead>
<tr>
<th>完整题目</th>
<th>会议/年份</th>
<th>官方链接</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>VideoRAG: Retrieval-Augmented Generation over Video Corpus</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.13093">abs/2411.13093</a></td>
<td>Y</td>
</tr>
<tr>
<td>SALOVA: Segment-Augmented Long Video Assistant for Targeted
Retrieval and Routing in Long-Form Video Analysis</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://ivy-lvlm.github.io/SALOVA/">project</a></td>
<td>Y</td>
</tr>
<tr>
<td>DrVideo: Document Retrieval Based Long Video Understanding</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06789">abs/2505.06789</a></td>
<td>N</td>
</tr>
<tr>
<td>Streaming Video Question-Answering with In-context Video KV-Cache
Retrieval</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>RGNet: A Unified Clip Retrieval and Grounding Network for Long
Videos</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3186_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>Bridging Information Asymmetry in Text-video Retrieval: A
Data-centric Approach</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11234">abs/2506.11234</a></td>
<td>N</td>
</tr>
<tr>
<td>TempMe: Video Temporal Token Merging for Efficient Text-Video
Retrieval</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01156">abs/2509.01156</a></td>
<td>N</td>
</tr>
<tr>
<td>Generalized Video Moment Retrieval</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td>N</td>
</tr>
<tr>
<td>EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video
Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5363_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>KDProR: A Knowledge-Decoupling Probabilistic Framework for
Video-Text Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5033_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>EA-VTR: Event-Aware Video-Text Retrieval</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6845_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>Chronologically Accurate Retrieval for Temporal Grounding of
Motion-Language Models</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7570_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in
Instructional Videos</td>
<td>ECCV 2024</td>
<td><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9950_ECCV_2024_paper.php">PDF</a></td>
<td>N</td>
</tr>
<tr>
<td>Narrating the Video: Boosting Text-Video Retrieval via Comprehensive
Utilization of Frame-Level Captions</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://multimodal-understanding-group.github.io/NarVid/">project</a></td>
<td>N</td>
</tr>
</tbody>
</table>
<h3 id="unified-multimodal-index12-篇">4. Unified Multimodal Index（12
篇）</h3>
<table>
<colgroup>
<col style="width: 56%">
<col style="width: 4%">
<col style="width: 36%">
<col style="width: 1%">
</colgroup>
<thead>
<tr>
<th>完整题目</th>
<th>会议/年份</th>
<th>官方链接</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>MM-EMBED: Universal Multimodal Retrieval with Multimodal LLMs</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.08056">abs/2501.08056</a></td>
<td>Y</td>
</tr>
<tr>
<td>UniRAG: Universal Retrieval Augmentation for Large Vision Language
Models</td>
<td>NAACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.10311">abs/2501.08056</a></td>
<td>Y</td>
</tr>
<tr>
<td>BridgeModalities: Improving Universal Multimodal Retrieval by
Multimodal Large Language Models</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.09795">abs/2504.09795</a></td>
<td>N</td>
</tr>
<tr>
<td>MLLM as Retriever: Interactively Learning Multimodal Retrieval for
Embodied Agents</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06789">abs/2505.06789</a></td>
<td>Y</td>
</tr>
<tr>
<td>TIGeR: Unifying Text-to-Image Generation and Retrieval with Large
Multimodal Models</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.05678">abs/2504.05678</a></td>
<td>Y</td>
</tr>
<tr>
<td>MAI: A Multi-turn Aggregation-Iteration Model for Composed Image
Retrieval</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.22345">abs/2502.22345</a></td>
<td>N</td>
</tr>
<tr>
<td>QuRe: Query-Relevant Retrieval through Hard Negative Sampling in
Composed Image Retrieval</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43476">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>CoLLM: A Large Language Model for Composed Image Retrieval</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://collm-cvpr25.github.io/">project</a></td>
<td>Y</td>
</tr>
<tr>
<td>Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for
Training-Free Zero-Shot Composed Image Retrieval</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://github.com/Pter61/osrcir">github.com/Pter61/osrcir</a></td>
<td>Y</td>
</tr>
<tr>
<td>Visual Abstraction: A Plug-and-Play Approach for Text-Visual
Retrieval</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>Bridge Modalities: Improving Universal Multimodal Retrieval by
Multimodal Large Language Models</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct">HuggingFace</a></td>
<td>Y</td>
</tr>
<tr>
<td>Recurrence-Enhanced Vision-and-Language Transformers for Robust
Multimodal Document Retrieval</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT">github.com/aimagelab/ReT</a></td>
<td>Y</td>
</tr>
</tbody>
</table>
<h3 id="memory-augmented-retrieval11-篇">5. Memory-Augmented
Retrieval（11 篇）</h3>
<table>
<colgroup>
<col style="width: 36%">
<col style="width: 3%">
<col style="width: 58%">
<col style="width: 1%">
</colgroup>
<thead>
<tr>
<th>完整题目</th>
<th>会议/年份</th>
<th>官方链接</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video
Understanding</td>
<td>CVPR 2024</td>
<td><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/papers/He_MA-LMM_Memory-Augmented_Large_Multimodal_Model_for_Long-Term_Video_Understanding_CVPR_2024_paper.pdf">PDF</a></td>
<td>Y</td>
</tr>
<tr>
<td>SynapticRAG: Enhancing Temporal Memory Retrieval in Large Language
Models through Synaptic Mechanisms</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.03347">abs/2505.03347</a></td>
<td>Y</td>
</tr>
<tr>
<td>From RAG to Memory: Non-Parametric Continual Learning for Large
Language Models</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45585">ICML</a></td>
<td>Y</td>
</tr>
<tr>
<td>Streaming Video Question-Answering with In-context Video KV-Cache
Retrieval</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>SeCom: On Memory Construction and Retrieval for Personalized
Conversational Agents</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>DuoAttention: Efficient Long-Context LLM Inference with Retrieval
and Streaming Heads</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>Provence: Efficient and Robust Context Pruning for
Retrieval-Augmented Generation</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>RazorAttention: Efficient KV Cache Compression Through Retrieval
Heads</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46460">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>Not All Heads Matter: A Head-Level KV Cache Compression Method with
Integrated Retrieval and Reasoning</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>SePer: Measure Retrieval Utility Through the Lens of Semantic
Perplexity Reduction</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>Memory Construction and Retrieval for Personalized Conversational
Agents</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45585">ICML</a></td>
<td>N</td>
</tr>
</tbody>
</table>
<h3 id="structured-evidence-kgscene-graph13-篇">6. Structured Evidence
(KG/Scene-Graph)（13 篇）</h3>
<table>
<colgroup>
<col style="width: 64%">
<col style="width: 6%">
<col style="width: 27%">
<col style="width: 1%">
</colgroup>
<thead>
<tr>
<th>完整题目</th>
<th>会议/年份</th>
<th>官方链接</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>EventRAG: Enhancing LLM Generation with Event Knowledge Graphs</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.234">ACL</a></td>
<td>Y</td>
</tr>
<tr>
<td>G-Retriever: Retrieval-Augmented Generation for Textual Graph
Understanding and Question Answering</td>
<td>NeurIPS 2024</td>
<td><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95524">NeurIPS</a></td>
<td>Y</td>
</tr>
<tr>
<td>MMGraphRAG: Bridging Vision and Language with Interpretable
Multimodal Knowledge Graphs</td>
<td>arXiv 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td>N</td>
</tr>
<tr>
<td>Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning
for MLLMs to Conquer the Unknown</td>
<td>ICCV 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td>N</td>
</tr>
<tr>
<td>Medical Graph RAG: Evidence-based Medical Large Language Model via
Graph Retrieval-Augmented Generation</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>Y</td>
</tr>
<tr>
<td>HyKGE: A Hypothesis Knowledge Graph Enhanced RAG Framework for
Accurate and Reliable Medical LLMs Responses</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>Query-Driven Multimodal GraphRAG: Dynamic Local Knowledge Graph
Construction for Online Reasoning</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>CausalRAG: Integrating Causal Graphs into Retrieval-Augmented
Generation</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>GNN-RAG: Graph Neural Retrieval for Efficient Large Language Model
Reasoning on Knowledge Graphs</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>Y</td>
</tr>
<tr>
<td>SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven
Retrieval-Augmented Generation</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>FRAG: A Flexible Modular Framework for Retrieval-Augmented
Generation based on Knowledge Graphs</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and
Symbolic Verification</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43660">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning
with Knowledge-guided Retrieval Augmented Generation</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>Y</td>
</tr>
</tbody>
</table>
<h3 id="multi-hop-iterative10-篇">7. Multi-hop / Iterative（10 篇）</h3>
<table>
<colgroup>
<col style="width: 65%">
<col style="width: 5%">
<col style="width: 27%">
<col style="width: 1%">
</colgroup>
<thead>
<tr>
<th>完整题目</th>
<th>会议/年份</th>
<th>官方链接</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>VideoRAG: Retrieval-Augmented Generation over Video Corpus</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.13093">abs/2411.13093</a></td>
<td>Y</td>
</tr>
<tr>
<td>VDocRAG: Retrieval-Augmented Generation over Visually-Rich
Documents</td>
<td>CVPR 2025</td>
<td><a target="_blank" rel="noopener" href="https://vdocrag.github.io/">project</a></td>
<td>Y</td>
</tr>
<tr>
<td>GeAR: Graph-enhanced Agent for Retrieval-augmented Generation</td>
<td>arXiv 2024</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.18431">abs/2412.18431</a></td>
<td>Y</td>
</tr>
<tr>
<td>HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval Augmented
Generation</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>Y</td>
</tr>
<tr>
<td>KiRAG: Knowledge-Driven Iterative Retriever for Enhancing
Retrieval-Augmented Generation</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>Dialogue-RAG: Enhancing Retrieval for LLMs via Node-Linking
Utterance Rewriting</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and
Symbolic Verification</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43660">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>Query-Driven Multimodal GraphRAG: Dynamic Local Knowledge Graph
Construction for Online Reasoning</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>EventRAG: Enhancing LLM Generation with Event Knowledge Graphs</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.234">ACL</a></td>
<td>Y</td>
</tr>
<tr>
<td>Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning
with Knowledge-guided Retrieval Augmented Generation</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>Y</td>
</tr>
</tbody>
</table>
<h3 id="security-robustness6-篇">8. Security &amp; Robustness（6
篇）</h3>
<table>
<colgroup>
<col style="width: 66%">
<col style="width: 5%">
<col style="width: 26%">
<col style="width: 1%">
</colgroup>
<thead>
<tr>
<th>完整题目</th>
<th>会议/年份</th>
<th>官方链接</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>PoisonedEye: Knowledge Poisoning Attack on Retrieval-Augmented
Generation based Large Vision-Language Models</td>
<td>ICML 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46373">ICML</a></td>
<td>Y</td>
</tr>
<tr>
<td>SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of
Large Language Model</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>Y</td>
</tr>
<tr>
<td>Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token
Probability Method for Poisoned Document Detection</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by
Simulating Documents in the Wild via Low-level Perturbations</td>
<td>EMNLP 2024</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2024.emnlp-main.123">ACL</a></td>
<td>Y</td>
</tr>
<tr>
<td>Mitigating Bias in RAG: Controlling the Embedder</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>N</td>
</tr>
<tr>
<td>OCR Hinders RAG: Evaluating the Cascading Impact of OCR on
Retrieval-Augmented Generation</td>
<td>ICCV 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td>N</td>
</tr>
</tbody>
</table>
<h3 id="benchmark-evaluation7-篇">9. Benchmark / Evaluation（7 篇）</h3>
<table>
<colgroup>
<col style="width: 62%">
<col style="width: 6%">
<col style="width: 29%">
<col style="width: 1%">
</colgroup>
<thead>
<tr>
<th>完整题目</th>
<th>会议/年份</th>
<th>官方链接</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented
Multimodal Models</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>Y</td>
</tr>
<tr>
<td>REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark</td>
<td>ACL 2025</td>
<td><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td>Y</td>
</tr>
<tr>
<td>MM-EMBED: Universal Multimodal Retrieval with Multimodal LLMs</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.08056">abs/2501.08056</a></td>
<td>Y</td>
</tr>
<tr>
<td>Benchmarking Multimodal Retrieval Augmented Generation with Dynamic
VQA Dataset and Self-adaptive Planning Agent</td>
<td>ICLR 2025</td>
<td><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td>N</td>
</tr>
<tr>
<td>INQUIRE: A Natural World Text-to-Image Retrieval Benchmark</td>
<td>NeurIPS 2024</td>
<td><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97543">NeurIPS</a></td>
<td>N</td>
</tr>
<tr>
<td>VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained
Video Understanding</td>
<td>NeurIPS 2024</td>
<td><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97632">NeurIPS</a></td>
<td>N</td>
</tr>
<tr>
<td>WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for
Vision-Language Models</td>
<td>NeurIPS 2024</td>
<td><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97785">NeurIPS</a></td>
<td>N</td>
</tr>
</tbody>
</table>
<h1 id="去重之后的论文列表">去重之后的论文列表</h1>
<h3 id="cross-modal-dense-retrieval18-篇已去重">1. Cross-modal Dense
Retrieval（18 篇，已去重）</h3>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">完整题目</th>
<th style="text-align: left;">会议/年份</th>
<th style="text-align: left;">官方链接</th>
<th style="text-align: left;">代码</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ColPali: Efficient Document Retrieval with
Vision Language Models</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.01449">abs/2407.01449</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">VisRAG: Vision-based Retrieval-augmented
Generation on Multi-modality Documents</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.12345">abs/2405.12345</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">MM-EMBED: Universal Multimodal Retrieval
with Multimodal LLMs</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.08056">abs/2501.08056</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">BridgeModalities: Improving Universal
Multimodal Retrieval by Multimodal Large Language Models</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.09795">abs/2504.09795</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">CoLLM: A Large Language Model for Composed
Image Retrieval</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://collm-cvpr25.github.io/">project</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">MLLM as Retriever: Interactively Learning
Multimodal Retrieval for Embodied Agents</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06789">abs/2505.06789</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Reason-before-Retrieve: One-Stage
Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image
Retrieval</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://github.com/Pter61/osrcir">github.com/Pter61/osrcir</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">MAI: A Multi-turn Aggregation-Iteration
Model for Composed Image Retrieval</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.22345">abs/2502.22345</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">QuRe: Query-Relevant Retrieval through
Hard Negative Sampling in Composed Image Retrieval</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43476">ICML</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">TIGeR: Unifying Text-to-Image Generation
and Retrieval with Large Multimodal Models</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.05678">abs/2504.05678</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Bridge Modalities: Improving Universal
Multimodal Retrieval by Multimodal Large Language Models</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct">HuggingFace</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Visual Abstraction: A Plug-and-Play
Approach for Text-Visual Retrieval</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">ICML</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">ReMP-AD: Retrieval-enhanced Multi-modal
Prompt Fusion for Few-Shot Industrial Visual Anomaly Detection</td>
<td style="text-align: left;">ICCV 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Object-Aware Query Perturbation for
Cross-Modal Image-Text Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10290_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">NeighborRetr: Balancing Hub Centrality in
Cross-Modal Retrieval</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://zzezze.github.io/NeighborRetr/">project</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">PromptHash: Affinity-Prompted
Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://github.com/ShiShuMo/PromptHash">github.com/ShiShuMo/PromptHash</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Learning with Noisy Triplet Correspondence
for Composed Image Retrieval</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://github.com/CharlesNeilWilliams/TME">github.com/CharlesNeilWilliams/TME</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">ColPali: Efficient Document Retrieval with
Vision Language Models (additional bench)</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.01449">abs/2407.01449</a></td>
<td style="text-align: left;">Y</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="fine-grained-patchregion-retrieval15-篇已去重">2. Fine-grained
Patch/Region Retrieval（15 篇，已去重）</h3>
<table>
<colgroup>
<col style="width: 36%">
<col style="width: 2%">
<col style="width: 59%">
<col style="width: 0%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">完整题目</th>
<th style="text-align: left;">会议/年份</th>
<th style="text-align: left;">官方链接</th>
<th style="text-align: left;">代码</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VDocRAG: Retrieval-Augmented Generation
over Visually-Rich
Documents（这篇放这个类别其实不合适……因为仔细读了之后发现它并没有细粒度patch/region的检索</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://vdocrag.github.io/">project</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">REVEAL: Retrieval-Augmented
Visual-Language Pre-Training with<br>Multi-Source Multimodal Knowledge
Memory</td>
<td style="text-align: left;">CVPR 2023</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_REVEAL_Retrieval-Augmented_Visual-Language_Pre-Training_With_Multi-Source_Multimodal_Knowledge_Memory_CVPR_2023_paper.pdf">PDF</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Retrieval-Augmented Perception:
High-resolution Image Perception Meets Visual RAG</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44979">ICML</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Learning Attribute-Aware Hash Codes for
Fine-Grained Image Retrieval via Query Optimization</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43868">ICML</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Visual Abstraction: A Plug-and-Play
Approach for Text-Visual Retrieval</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">ICML</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">ReMP-AD: Retrieval-enhanced Multi-modal
Prompt Fusion for Few-Shot Industrial Visual Anomaly Detection</td>
<td style="text-align: left;">ICCV 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Object-Aware Query Perturbation for
Cross-Modal Image-Text Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10290_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Revisit Anything: Visual Place Recognition
via Image Segment Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8592_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">GLARE: Low Light Image Enhancement via
Generative Latent Feature based Codebook Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6415_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">KDProR: A Knowledge-Decoupling
Probabilistic Framework for Video-Text Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5033_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">EgoCVR: An Egocentric Benchmark for
Fine-Grained Composed Video Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5363_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Freeview Sketching: View-Aware
Fine-Grained Sketch-Based Image Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7423_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Chronologically Accurate Retrieval for
Temporal Grounding of Motion-Language Models</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7570_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">FastCAD: Real-Time CAD Retrieval and
Alignment from Scans and Videos</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2302_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">RGNet: A Unified Clip Retrieval and
Grounding Network for Long Videos</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3186_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
</tbody>
</table>
<h2 id="嗯fine-grained-regionpatch-retrieval这类方法的大概思路应该是用ocr技术detector如faster-r-cnn先在输入上例如图像进行检测此时会检测出很多的框然后这些框会被加入到一个建好的库之后会用用户的query进行检索和我们关注的rag解决多模态输入token过多的方法相比它们的区别似乎主要在于如何划分输入是否会建库前者是动态划分的且候选框很多因此需要利用库检索的时候需要用到faiss等近似的技术而后者因为是固定的策略进行划分候选框较少因此不需要用到什么近似技术也不需要建库只需要划分完后正常用query进行检索-容易想到一个小的改进就是把两种划分方式结合起来但是已经有一些论文这样做了比如vdocrag好像就已经结合了它先用固定划分策略然后检索到若干个粗的框之后在这些小框里面用ocrdetector技术进行细致的检索这样感觉上效果会好一些">####
嗯，fine-grained region/patch
retrieval，这类方法的大概思路应该是，用OCR技术/detector（如faster
R-CNN），先在输入上（例如图像）进行检测。此时会检测出很多的框。然后这些框会被加入到一个建好的库，之后会用用户的query进行检索。和我们关注的，RAG解决多模态输入token过多的方法相比，它们的区别似乎主要在于如何划分输入，是否会建库。前者是动态划分的，且候选框很多，因此需要利用库，检索的时候需要用到faiss等近似的技术；而后者因为是固定的策略进行划分，候选框较少，因此不需要用到什么近似技术，也不需要建库，只需要划分完后正常用query进行检索<br>容易想到一个小的改进，就是把两种划分方式结合起来。但是已经有一些论文这样做了。比如VDocRAG，好像就已经结合了：它先用固定划分策略，然后检索到若干个粗的框之后，在这些小框里面用OCR/detector技术进行细致的检索。这样感觉上效果会好一些</h2>
<h3 id="video-segment-retrieval14-篇已去重">3. Video Segment
Retrieval（14 篇，已去重）</h3>
<table style="width:100%;">
<colgroup>
<col style="width: 52%">
<col style="width: 4%">
<col style="width: 41%">
<col style="width: 1%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">完整题目</th>
<th style="text-align: left;">会议/年份</th>
<th style="text-align: left;">官方链接</th>
<th style="text-align: left;">代码</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VideoRAG: Retrieval-Augmented Generation
over Video Corpus</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.13093">abs/2411.13093</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">SALOVA: Segment-Augmented Long Video
Assistant for Targeted Retrieval and Routing in Long-Form Video
Analysis</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://ivy-lvlm.github.io/SALOVA/">project</a></td>
<td style="text-align: left;">Y</td>
</tr>
<tr>
<td style="text-align: left;">DrVideo: Document Retrieval Based Long
Video Understanding</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06789">abs/2505.06789</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Streaming Video Question-Answering with
In-context Video KV-Cache Retrieval</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">RGNet: A Unified Clip Retrieval and
Grounding Network for Long Videos</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3186_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Bridging Information Asymmetry in
Text-video Retrieval: A Data-centric Approach</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11234">abs/2506.11234</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">TempMe: Video Temporal Token Merging for
Efficient Text-Video Retrieval</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01156">abs/2509.01156</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Generalized Video Moment Retrieval</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">EgoCVR: An Egocentric Benchmark for
Fine-Grained Composed Video Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5363_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">KDProR: A Knowledge-Decoupling
Probabilistic Framework for Video-Text Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5033_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">EA-VTR: Event-Aware Video-Text
Retrieval</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6845_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Chronologically Accurate Retrieval for
Temporal Grounding of Motion-Language Models</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7570_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">RAP: Retrieval-Augmented Planner for
Adaptive Procedure Planning in Instructional Videos</td>
<td style="text-align: left;">ECCV 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9950_ECCV_2024_paper.php">PDF</a></td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Narrating the Video: Boosting Text-Video
Retrieval via Comprehensive Utilization of Frame-Level Captions</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h3 id="unified-multimodal-index12-篇已去重">4. Unified Multimodal
Index（12 篇，已去重）</h3>
<p>表格</p>
<p>复制</p>
<table>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">完整题目</th>
<th style="text-align: left;">会议/年份</th>
<th style="text-align: left;">官方链接</th>
<th style="text-align: left;">代码</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MM-EMBED: Universal Multimodal Retrieval
with Multimodal LLMs</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.08056">abs/2501.08056</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">UniRAG: Universal Retrieval Augmentation
for Large Vision Language Models</td>
<td style="text-align: left;">NAACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.08056">abs/2501.08056</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BridgeModalities: Improving Universal
Multimodal Retrieval by Multimodal Large Language Models</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.09795">abs/2504.09795</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MLLM as Retriever: Interactively Learning
Multimodal Retrieval for Embodied Agents</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.06789">abs/2505.06789</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">TIGeR: Unifying Text-to-Image Generation
and Retrieval with Large Multimodal Models</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.05678">abs/2504.05678</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MAI: A Multi-turn Aggregation-Iteration
Model for Composed Image Retrieval</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.22345">abs/2502.22345</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">QuRe: Query-Relevant Retrieval through
Hard Negative Sampling in Composed Image Retrieval</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43476">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">CoLLM: A Large Language Model for Composed
Image Retrieval</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://collm-cvpr25.github.io/">project</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Reason-before-Retrieve: One-Stage
Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image
Retrieval</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://github.com/Pter61/osrcir">github.com/Pter61/osrcir</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Visual Abstraction: A Plug-and-Play
Approach for Text-Visual Retrieval</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Bridge Modalities: Improving Universal
Multimodal Retrieval by Multimodal Large Language Models</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct">HuggingFace</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Recurrence-Enhanced Vision-and-Language
Transformers for Robust Multimodal Document Retrieval</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT">github.com/aimagelab/ReT</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr>
<h3 id="memory-augmented-retrieval11-篇已去重">5. Memory-Augmented
Retrieval（11 篇，已去重）</h3>
<table>
<colgroup>
<col style="width: 36%">
<col style="width: 3%">
<col style="width: 58%">
<col style="width: 1%">
<col style="width: 1%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">完整题目</th>
<th style="text-align: left;">会议/年份</th>
<th style="text-align: left;">官方链接</th>
<th style="text-align: left;">代码</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MA-LMM: Memory-Augmented Large Multimodal
Model for Long-Term Video Understanding</td>
<td style="text-align: left;">CVPR 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/papers/He_MA-LMM_Memory-Augmented_Large_Multimodal_Model_for_Long-Term_Video_Understanding_CVPR_2024_paper.pdf">PDF</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SynapticRAG: Enhancing Temporal Memory
Retrieval in Large Language Models through Synaptic Mechanisms</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.03347">abs/2505.03347</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">From RAG to Memory: Non-Parametric
Continual Learning for Large Language Models</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45585">ICML</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Streaming Video Question-Answering with
In-context Video KV-Cache Retrieval</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SeCom: On Memory Construction and
Retrieval for Personalized Conversational Agents</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">DuoAttention: Efficient Long-Context LLM
Inference with Retrieval and Streaming Heads</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Provence: Efficient and Robust Context
Pruning for Retrieval-Augmented Generation</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">RazorAttention: Efficient KV Cache
Compression Through Retrieval Heads</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46460">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Not All Heads Matter: A Head-Level KV
Cache Compression Method with Integrated Retrieval and Reasoning</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SePer: Measure Retrieval Utility Through
the Lens of Semantic Perplexity Reduction</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Memory Construction and Retrieval for
Personalized Conversational Agents</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45585">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr>
<h3 id="structured-evidence-kgscene-graph13-篇已去重">6. Structured
Evidence (KG/Scene-Graph)（13 篇，已去重）</h3>
<p>表格</p>
<p>复制</p>
<table>
<colgroup>
<col style="width: 38%">
<col style="width: 3%">
<col style="width: 55%">
<col style="width: 0%">
<col style="width: 0%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">完整题目</th>
<th style="text-align: left;">会议/年份</th>
<th style="text-align: left;">官方链接</th>
<th style="text-align: left;">代码</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EventRAG: Enhancing LLM Generation with
Event Knowledge Graphs</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.234">ACL</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">G-Retriever: Retrieval-Augmented
Generation for Textual Graph Understanding and Question Answering</td>
<td style="text-align: left;">NeurIPS 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95524">NeurIPS</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MMGraphRAG: Bridging Vision and Language
with Interpretable Multimodal Knowledge Graphs</td>
<td style="text-align: left;">arXiv 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Taming the Untamed: Graph-Based Knowledge
Retrieval and Reasoning for MLLMs to Conquer the Unknown</td>
<td style="text-align: left;">ICCV 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Medical Graph RAG: Evidence-based Medical
Large Language Model via Graph Retrieval-Augmented Generation</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">HyKGE: A Hypothesis Knowledge Graph
Enhanced RAG Framework for Accurate and Reliable Medical LLMs
Responses</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Query-Driven Multimodal GraphRAG: Dynamic
Local Knowledge Graph Construction for Online Reasoning</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a href="%5BQuery-Driven%20Multimodal%20GraphRAG:%20Dynamic%20Local%20Knowledge%20Graph%20Construction%20for%20Online%20Reasoning%20-%20ACL%20Anthology%5D(https://aclanthology.org/2025.findings-acl.1100/)">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">CausalRAG: Integrating Causal Graphs into
Retrieval-Augmented Generation</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GNN-RAG: Graph Neural Retrieval for
Efficient Large Language Model Reasoning on Knowledge Graphs</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SimGRAG: Leveraging Similar Subgraphs for
Knowledge Graphs Driven Retrieval-Augmented Generation</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FRAG: A Flexible Modular Framework for
Retrieval-Augmented Generation based on Knowledge Graphs</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Hierarchical Planning for Complex Tasks
with Knowledge Graph-RAG and Symbolic Verification</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43660">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Think-on-Graph 2.0: Deep and Faithful
Large Language Model Reasoning with Knowledge-guided Retrieval Augmented
Generation</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr>
<h3 id="multi-hop-iterative10-篇已去重">7. Multi-hop / Iterative（10
篇，已去重）</h3>
<table>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">完整题目</th>
<th style="text-align: left;">会议/年份</th>
<th style="text-align: left;">官方链接</th>
<th style="text-align: left;">代码</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VideoRAG: Retrieval-Augmented Generation
over Video Corpus</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.13093">abs/2411.13093</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;">主类：Video Segment</td>
</tr>
<tr>
<td style="text-align: left;">VDocRAG: Retrieval-Augmented Generation
over Visually-Rich Documents</td>
<td style="text-align: left;">CVPR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://vdocrag.github.io/">project</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;">主类：Patch/Region</td>
</tr>
<tr>
<td style="text-align: left;">GeAR: Graph-enhanced Agent for
Retrieval-augmented Generation</td>
<td style="text-align: left;">arXiv 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.18431">abs/2412.18431</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">HopRAG: Multi-Hop Reasoning for
Logic-Aware Retrieval Augmented Generation</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">KiRAG: Knowledge-Driven Iterative
Retriever for Enhancing Retrieval-Augmented Generation</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Dialogue-RAG: Enhancing Retrieval for LLMs
via Node-Linking Utterance Rewriting</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Hierarchical Planning for Complex Tasks
with Knowledge Graph-RAG and Symbolic Verification</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43660">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Query-Driven Multimodal GraphRAG: Dynamic
Local Knowledge Graph Construction for Online Reasoning</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">EventRAG: Enhancing LLM Generation with
Event Knowledge Graphs</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.234">ACL</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;">主类：Structured</td>
</tr>
<tr>
<td style="text-align: left;">Think-on-Graph 2.0: Deep and Faithful
Large Language Model Reasoning with Knowledge-guided Retrieval Augmented
Generation</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr>
<h3 id="security-robustness6-篇-1">8. Security &amp; Robustness（6
篇）</h3>
<table>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">完整题目</th>
<th style="text-align: left;">会议/年份</th>
<th style="text-align: left;">官方链接</th>
<th style="text-align: left;">代码</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PoisonedEye: Knowledge Poisoning Attack on
Retrieval-Augmented Generation based Large Vision-Language Models</td>
<td style="text-align: left;">ICML 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46373">ICML</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SafeRAG: Benchmarking Security in
Retrieval-Augmented Generation of Large Language Model</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Safeguarding RAG Pipelines with GMTP: A
Gradient-based Masked Token Probability Method for Poisoned Document
Detection</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Typos that Broke the RAG's Back: Genetic
Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level
Perturbations</td>
<td style="text-align: left;">EMNLP 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2024.emnlp-main.123">ACL</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Mitigating Bias in RAG: Controlling the
Embedder</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">OCR Hinders RAG: Evaluating the Cascading
Impact of OCR on Retrieval-Augmented Generation</td>
<td style="text-align: left;">ICCV 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.20804">abs/2507.20804</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr>
<h3 id="benchmark-evaluation7-篇-1">9. Benchmark / Evaluation（7
篇）</h3>
<table style="width:100%;">
<colgroup>
<col style="width: 59%">
<col style="width: 6%">
<col style="width: 27%">
<col style="width: 1%">
<col style="width: 5%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">完整题目</th>
<th style="text-align: left;">会议/年份</th>
<th style="text-align: left;">官方链接</th>
<th style="text-align: left;">代码</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MRAG-Bench: Vision-Centric Evaluation for
Retrieval-Augmented Multimodal Models</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">REAL-MM-RAG: A Real-World Multi-Modal
Retrieval Benchmark</td>
<td style="text-align: left;">ACL 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://aclanthology.org/2025.acl-main.123">ACL</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MM-EMBED: Universal Multimodal Retrieval
with Multimodal LLMs</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.08056">abs/2501.08056</a></td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;">主类：Unified</td>
</tr>
<tr>
<td style="text-align: left;">Benchmarking Multimodal Retrieval
Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning
Agent</td>
<td style="text-align: left;">ICLR 2025</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">ICML</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">INQUIRE: A Natural World Text-to-Image
Retrieval Benchmark</td>
<td style="text-align: left;">NeurIPS 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97543">NeurIPS</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">VERIFIED: A Video Corpus Moment Retrieval
Benchmark for Fine-Grained Video Understanding</td>
<td style="text-align: left;">NeurIPS 2024</td>
<td style="text-align: left;"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97632">NeurIPS</a></td>
<td style="text-align: left;">N</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">WikiDO: A New Benchmark Evaluating
Cross-Modal Retrieval for Vision-Language Models</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/08/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/long-context/seminar%E8%AE%B0%E5%BD%95%EF%BC%9AFAI-Seminar%20%20MIT%20%E7%8E%8B%E4%B8%80%E9%A3%9E%20%20LLM%E7%9A%84%E9%95%BF%E6%96%87%E6%9C%AC%E5%9B%B0%E5%A2%83%EF%BC%9ANTP%E4%BB%BB%E5%8A%A1%E4%B8%8ETransformer%E6%9E%B6%E6%9E%84%E7%9A%84%E5%86%85%E5%9C%A8%E5%81%8F%E5%B7%AE/" rel="prev" title="seminar记录：FAI-Seminar MIT 王一飞 LLM的长文本困境：NTP任务与Transformer架构的内在偏差">
      <i class="fa fa-chevron-left"></i> seminar记录：FAI-Seminar MIT 王一飞 LLM的长文本困境：NTP任务与Transformer架构的内在偏差
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/08/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/UniRAG%EF%BC%9AUnified%20Query%20Understanding%20Method%20for%20%20Retrieval%20Augmented%20Generation/" rel="next" title="UniRAG：Unified Query Understanding Method for Retrieval Augmented Generation">
      UniRAG：Unified Query Understanding Method for Retrieval Augmented Generation <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#acl-25"><span class="nav-number">1.</span> <span class="nav-text">ACL 25</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#main"><span class="nav-number">1.1.</span> <span class="nav-text">main</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#hybgrag-hybrid-retrieval-augmented-generation-on-textual-and-relational-knowledge-bases"><span class="nav-number">1.1.1.</span> <span class="nav-text">HybGRAG:
Hybrid Retrieval-Augmented Generation on Textual and Relational
Knowledge Bases</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#main-rag-multi-agent-filtering-retrieval-augmented-generation"><span class="nav-number">1.1.2.</span> <span class="nav-text">MAIN-RAG:
Multi-Agent Filtering Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-critic-leveraging-automated-critic-guided-agentic-workflow-for-retrieval-augmented-generation"><span class="nav-number">1.1.3.</span> <span class="nav-text">RAG-Critic:
Leveraging Automated Critic-Guided Agentic Workflow for Retrieval
Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#saferag-benchmarking-security-in-retrieval-augmented-generation-of-large-language-model"><span class="nav-number">1.1.4.</span> <span class="nav-text">SafeRAG:
Benchmarking Security in Retrieval-Augmented Generation of Large
Language Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#on-the-robustness-of-rag-systems-in-educational-question-answering-under-knowledge-discrepancies"><span class="nav-number">1.1.5.</span> <span class="nav-text">On
the Robustness of RAG Systems in Educational Question Answering under
Knowledge Discrepancies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pandoras-box-or-aladdins-lamp-a-comprehensive-analysis-revealing-the-role-of-rag-noise-in-large-language-models"><span class="nav-number">1.1.6.</span> <span class="nav-text">Pandora’s
Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of
RAG Noise in Large Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#neusym-rag-hybrid-neural-symbolic-retrieval-with-multiview-structuring-for-pdf-question-answering"><span class="nav-number">1.1.7.</span> <span class="nav-text">NeuSym-RAG:
Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF
Question Answering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#drag-distilling-rag-for-slms-from-llms-to-transfer-knowledge-and-mitigate-hallucination-via-evidence-and-graph-based-distillation"><span class="nav-number">1.1.8.</span> <span class="nav-text">DRAG:
Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate
Hallucination via Evidence and Graph-based Distillation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rageval-scenario-specific-rag-evaluation-dataset-generation-framework"><span class="nav-number">1.1.9.</span> <span class="nav-text">RAGEval:
Scenario Specific RAG Evaluation Dataset Generation
Framework</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#are-llms-effective-psychological-assessors-leveraging-adaptive-rag-for-interpretable-mental-health-screening-through-psychometric-practice"><span class="nav-number">1.1.10.</span> <span class="nav-text">Are
LLMs effective psychological assessors? Leveraging adaptive RAG for
interpretable mental health screening through psychometric
practice</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#s-rag-a-novel-audit-framework-for-detecting-unauthorized-use-of-personal-data-in-rag-systems"><span class="nav-number">1.1.11.</span> <span class="nav-text">S-RAG:
A Novel Audit Framework for Detecting Unauthorized Use of Personal Data
in RAG Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gainrag-preference-alignment-in-retrieval-augmented-generation-through-gain-signal-synthesis"><span class="nav-number">1.1.12.</span> <span class="nav-text">GainRAG:
Preference Alignment in Retrieval-Augmented Generation through Gain
Signal Synthesis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tcrag-turingcomplete-rags-case-study-on-medical-llm-systems"><span class="nav-number">1.1.13.</span> <span class="nav-text">TC–RAG:
Turing–Complete RAG’s Case study on Medical LLM Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#divide-then-align-honest-alignment-based-on-the-knowledge-boundary-of-rag"><span class="nav-number">1.1.14.</span> <span class="nav-text">Divide-Then-Align:
Honest Alignment based on the Knowledge Boundary of RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hykge-a-hypothesis-knowledge-graph-enhanced-rag-framework-for-accurate-and-reliable-medical-llms-responses"><span class="nav-number">1.1.15.</span> <span class="nav-text">HyKGE:
A Hypothesis Knowledge Graph Enhanced RAG Framework for Accurate and
Reliable Medical LLMs Responses</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wavrag-audio-integrated-retrieval-augmented-generation-for-spoken-dialogue-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E4%B8%8D%E8%BF%87%E6%98%AFaudio%E7%9B%B8%E5%85%B3%E7%9A%84"><span class="nav-number">1.1.16.</span> <span class="nav-text">WavRAG:
Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue
Models（多模态的，不过是audio相关的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unirag-unified-query-understanding-method-for-retrieval-augmented-generation"><span class="nav-number">1.1.17.</span> <span class="nav-text">UniRAG:
Unified Query Understanding Method for Retrieval Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#towards-omni-rag-comprehensive-retrieval-augmented-generation-for-large-language-models-in-medical-applications"><span class="nav-number">1.1.18.</span> <span class="nav-text">Towards
Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large
Language Models in Medical Applications</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#molrag-unlocking-the-power-of-large-language-models-for-molecular-property-prediction"><span class="nav-number">1.1.19.</span> <span class="nav-text">MolRAG:
Unlocking the Power of Large Language Models for Molecular Property
Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#removal-of-hallucination-on-hallucination-debate-augmented-rag"><span class="nav-number">1.1.20.</span> <span class="nav-text">Removal
of Hallucination on Hallucination: Debate-Augmented RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#eventrag-enhancing-llm-generation-with-event-knowledge-graphs"><span class="nav-number">1.1.21.</span> <span class="nav-text">EventRAG:
Enhancing LLM Generation with Event Knowledge Graphs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-distracting-effect-understanding-irrelevant-passages-in-rag"><span class="nav-number">1.1.22.</span> <span class="nav-text">The
Distracting Effect: Understanding Irrelevant Passages in
RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kirag-knowledge-driven-iterative-retriever-for-enhancing-retrieval-augmented-generation"><span class="nav-number">1.1.23.</span> <span class="nav-text">KiRAG:
Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#faithfulrag-fact-level-conflict-modeling-for-context-faithful-retrieval-augmented-generation"><span class="nav-number">1.1.24.</span> <span class="nav-text">FaithfulRAG:
Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#memerag-a-multilingual-end-to-end-meta-evaluation-benchmark-for-retrieval-augmented-generation"><span class="nav-number">1.1.25.</span> <span class="nav-text">MEMERAG:
A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval
Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dialogue-rag-enhancing-retrieval-for-llms-via-node-linking-utterance-rewriting"><span class="nav-number">1.1.26.</span> <span class="nav-text">Dialogue-RAG:
Enhancing Retrieval for LLMs via Node-Linking Utterance
Rewriting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-efficiency-vs.-accuracy-trade-off-optimizing-rag-enhanced-llm-recommender-systems-using-multi-head-early-exit"><span class="nav-number">1.1.27.</span> <span class="nav-text">The
Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM
Recommender Systems Using Multi-Head Early Exit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sgic-a-self-guided-iterative-calibration-framework-for-rag"><span class="nav-number">1.1.28.</span> <span class="nav-text">SGIC:
A Self-Guided Iterative Calibration Framework for RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#medical-graph-rag-evidence-based-medical-large-language-model-via-graph-retrieval-augmented-generation"><span class="nav-number">1.1.29.</span> <span class="nav-text">Medical
Graph RAG: Evidence-based Medical Large Language Model via Graph
Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#astute-rag-overcoming-imperfect-retrieval-augmentation-and-knowledge-conflicts-for-large-language-models"><span class="nav-number">1.1.30.</span> <span class="nav-text">Astute
RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts
for Large Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#real-mm-rag-a-real-world-multi-modal-retrieval-benchmark%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E4%BD%86%E6%98%AFbenchmark"><span class="nav-number">1.1.31.</span> <span class="nav-text">REAL-MM-RAG:
A Real-World Multi-Modal Retrieval
Benchmark（多模态的，但是benchmark）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dualrag-a-dual-process-approach-to-integrate-reasoning-and-retrieval-for-multi-hop-question-answering"><span class="nav-number">1.1.32.</span> <span class="nav-text">DualRAG:
A Dual-Process Approach to Integrate Reasoning and Retrieval for
Multi-Hop Question Answering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#core-mmrag-cross-source-knowledge-reconciliation-for-multimodal-rag"><span class="nav-number">1.1.33.</span> <span class="nav-text">CoRe-MMRAG:
Cross-Source Knowledge Reconciliation for Multimodal RAG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#findings"><span class="nav-number">1.2.</span> <span class="nav-text">findings</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#treerag-unleashing-the-power-of-hierarchical-storage-for-enhanced-knowledge-retrieval-in-long-documents"><span class="nav-number">1.2.1.</span> <span class="nav-text">TreeRAG:
Unleashing the Power of Hierarchical Storage for Enhanced Knowledge
Retrieval in Long Documents</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scrag-hybrid-retrieval-augmented-generation-for-llm-based-cross-tissue-single-cell-annotation"><span class="nav-number">1.2.2.</span> <span class="nav-text">scRAG:
Hybrid Retrieval-Augmented Generation for LLM-based Cross-Tissue
Single-Cell Annotation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hoprag-multi-hop-reasoning-for-logic-aware-retrieval-augmented-generation"><span class="nav-number">1.2.3.</span> <span class="nav-text">HopRAG:
Multi-Hop Reasoning for Logic-Aware Retrieval Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ltrag-enhancing-autoformalization-and-self-refinement-for-logical-reasoning-with-thought-guided-rag"><span class="nav-number">1.2.4.</span> <span class="nav-text">LTRAG:
Enhancing autoformalization and self-refinement for logical reasoning
with Thought-Guided RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#simgrag-leveraging-similar-subgraphs-for-knowledge-graphs-driven-retrieval-augmented-generation"><span class="nav-number">1.2.5.</span> <span class="nav-text">SimGRAG:
Leveraging Similar Subgraphs for Knowledge Graphs Driven
Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#remoterag-a-privacy-preserving-llm-cloud-rag-service"><span class="nav-number">1.2.6.</span> <span class="nav-text">RemoteRAG:
A Privacy-Preserving LLM Cloud RAG Service</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#investigating-language-preference-of-multilingual-rag-systems"><span class="nav-number">1.2.7.</span> <span class="nav-text">Investigating
Language Preference of Multilingual RAG Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#frag-a-flexible-modular-framework-for-retrieval-augmented-generation-based-on-knowledge-graphs"><span class="nav-number">1.2.8.</span> <span class="nav-text">FRAG:
A Flexible Modular Framework for Retrieval-Augmented Generation based on
Knowledge Graphs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#navrag-generating-user-demand-instructions-for-embodied-navigation-through-retrieval-augmented-llm"><span class="nav-number">1.2.9.</span> <span class="nav-text">NavRAG:
Generating User Demand Instructions for Embodied Navigation through
Retrieval-Augmented LLM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#speecht-rag-reliable-depression-detection-in-llms-with-retrieval-augmented-generation-using-speech-timing-information"><span class="nav-number">1.2.10.</span> <span class="nav-text">SpeechT-RAG:
Reliable Depression Detection in LLMs with Retrieval-Augmented
Generation Using Speech Timing Information</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#roserag-robust-retrieval-augmented-generation-with-small-scale-llms-via-margin-aware-preference-optimization"><span class="nav-number">1.2.11.</span> <span class="nav-text">RoseRAG:
Robust Retrieval-augmented Generation with Small-scale LLMs via
Margin-aware Preference Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gnn-rag-graph-neural-retrieval-for-efficient-large-language-model-reasoning-on-knowledge-graphs"><span class="nav-number">1.2.12.</span> <span class="nav-text">GNN-RAG:
Graph Neural Retrieval for Efficient Large Language Model Reasoning on
Knowledge Graphs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#astrid---an-automated-and-scalable-triad-for-the-evaluation-of-rag-based-clinical-question-answering-systems"><span class="nav-number">1.2.13.</span> <span class="nav-text">ASTRID
- An Automated and Scalable TRIaD for the Evaluation of RAG-based
Clinical Question Answering Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#garage-a-benchmark-with-grounding-annotations-for-rag-evaluation"><span class="nav-number">1.2.14.</span> <span class="nav-text">GaRAGe:
A Benchmark with Grounding Annotations for RAG Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-rewardbench-benchmarking-reward-models-in-retrieval-augmented-generation-for-preference-alignment"><span class="nav-number">1.2.15.</span> <span class="nav-text">RAG-RewardBench:
Benchmarking Reward Models in Retrieval Augmented Generation for
Preference Alignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mitigating-bias-in-rag-controlling-the-embedder"><span class="nav-number">1.2.16.</span> <span class="nav-text">Mitigating
Bias in RAG: Controlling the Embedder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#synapticrag-enhancing-temporal-memory-retrieval-in-large-language-models-through-synaptic-mechanisms"><span class="nav-number">1.2.17.</span> <span class="nav-text">SynapticRAG:
Enhancing Temporal Memory Retrieval in Large Language Models through
Synaptic Mechanisms</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#techniquerag-retrieval-augmented-generation-for-adversarial-technique-annotation-in-cyber-threat-intelligence-text"><span class="nav-number">1.2.18.</span> <span class="nav-text">TechniqueRAG:
Retrieval Augmented Generation for Adversarial Technique Annotation in
Cyber Threat Intelligence Text</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#videorag-retrieval-augmented-generation-over-video-corpus%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number">1.2.19.</span> <span class="nav-text">VideoRAG:
Retrieval-Augmented Generation over Video
Corpus（多模态的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#query-driven-multimodal-graphrag-dynamic-local-knowledge-graph-construction-for-online-reasoning"><span class="nav-number">1.2.20.</span> <span class="nav-text">Query-Driven
Multimodal GraphRAG: Dynamic Local Knowledge Graph Construction for
Online Reasoning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#causalrag-integrating-causal-graphs-into-retrieval-augmented-generation"><span class="nav-number">1.2.21.</span> <span class="nav-text">CausalRAG:
Integrating Causal Graphs into Retrieval-Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#eventrag-supportive-event-retrieval-on-hypergraph-for-future-forecasting"><span class="nav-number">1.2.22.</span> <span class="nav-text">EventRAG:
Supportive Event Retrieval on Hypergraph for Future
Forecasting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#safeguarding-rag-pipelines-with-gmtp-a-gradient-based-masked-token-probability-method-for-poisoned-document-detection"><span class="nav-number">1.2.23.</span> <span class="nav-text">Safeguarding
RAG Pipelines with GMTP: A Gradient-based Masked Token Probability
Method for Poisoned Document Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ecorag-evidentiality-guided-compression-for-long-context-rag"><span class="nav-number">1.2.24.</span> <span class="nav-text">ECoRAG:
Evidentiality-guided Compression for Long Context RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hash-rag-bridging-deep-hashing-with-retriever-for-efficient-fine-retrieval-and-augmented-generation"><span class="nav-number">1.2.25.</span> <span class="nav-text">HASH-RAG:
Bridging Deep Hashing with Retriever for Efficient, Fine Retrieval and
Augmented Generation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#naacl-25"><span class="nav-number">2.</span> <span class="nav-text">NAACL 25</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#main-1"><span class="nav-number">2.1.</span> <span class="nav-text">main</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-star-enhancing-deliberative-reasoning-with-retrieval-augmented-verification-and-refinement"><span class="nav-number">2.1.1.</span> <span class="nav-text">RAG-Star:
Enhancing Deliberative Reasoning with Retrieval Augmented Verification
and Refinement</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#simrag-self-improving-retrieval-augmented-generation-for-adapting-large-language-models-to-specialized-domains"><span class="nav-number">2.1.2.</span> <span class="nav-text">SimRAG:
Self-Improving Retrieval-Augmented Generation for Adapting Large
Language Models to Specialized Domains</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#do-rag-systems-cover-what-matters-evaluating-and-optimizing-responses-with-sub-question-coverage"><span class="nav-number">2.1.3.</span> <span class="nav-text">Do
RAG Systems Cover What Matters? Evaluating and Optimizing Responses with
Sub-Question Coverage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-llms-are-not-safer-a-safety-analysis-of-retrieval-augmented-generation-for-large-language-models"><span class="nav-number">2.1.4.</span> <span class="nav-text">RAG
LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation
for Large Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#trag-term-level-retrieval-augmented-generation-for-domain-adaptive-retrieval"><span class="nav-number">2.1.5.</span> <span class="nav-text">tRAG:
Term-level Retrieval-Augmented Generation for Domain-Adaptive
Retrieval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pa-rag-rag-alignment-via-multi-perspective-preference-optimization"><span class="nav-number">2.1.6.</span> <span class="nav-text">PA-RAG:
RAG Alignment via Multi-Perspective Preference
Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#corag-collaborative-retrieval-augmented-generation"><span class="nav-number">2.1.7.</span> <span class="nav-text">CoRAG:
Collaborative Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transform-retrieval-for-textual-entailment-in-rag"><span class="nav-number">2.1.8.</span> <span class="nav-text">Transform
Retrieval for Textual Entailment in RAG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#findings-1"><span class="nav-number">2.2.</span> <span class="nav-text">findings</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#systematic-knowledge-injection-into-large-language-models-via-diverse-augmentation-for-domain-specific-rag"><span class="nav-number">2.2.1.</span> <span class="nav-text">Systematic
Knowledge Injection into Large Language Models via Diverse Augmentation
for Domain-Specific RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#probing-rag-self-probing-to-guide-language-models-in-selective-document-retrieval"><span class="nav-number">2.2.2.</span> <span class="nav-text">Probing-RAG:
Self-Probing to Guide Language Models in Selective Document
Retrieval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mes-rag-bringing-multi-modal-entity-storage-and-secure-enhancements-to-rag%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number">2.2.3.</span> <span class="nav-text">MES-RAG:
Bringing Multi-modal, Entity-Storage, and Secure Enhancements to
RAG（多模态的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#grag-graph-retrieval-augmented-generation"><span class="nav-number">2.2.4.</span> <span class="nav-text">GRAG: Graph
Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coderag-bench-can-retrieval-augment-code-generation"><span class="nav-number">2.2.5.</span> <span class="nav-text">CodeRAG-Bench:
Can Retrieval Augment Code Generation?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unirag-universal-retrieval-augmentation-for-large-vision-language-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number">2.2.6.</span> <span class="nav-text">UniRAG:
Universal Retrieval Augmentation for Large Vision Language
Models（多模态的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#funnelrag-a-coarse-to-fine-progressive-retrieval-paradigm-for-rag"><span class="nav-number">2.2.7.</span> <span class="nav-text">FunnelRAG:
A Coarse-to-Fine Progressive Retrieval Paradigm for RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#grappi-a-retrieve-divide-solve-graphrag-framework-for-large-scale-protein-protein-interaction-exploration"><span class="nav-number">2.2.8.</span> <span class="nav-text">GraPPI:
A Retrieve-Divide-Solve GraphRAG Framework for Large-scale
Protein-protein Interaction Exploration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chain-of-rank-enhancing-large-language-models-for-domain-specific-rag-in-edge-device"><span class="nav-number">2.2.9.</span> <span class="nav-text">Chain-of-Rank:
Enhancing Large Language Models for Domain-Specific RAG in Edge
Device</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#superrag-beyond-rag-with-layout-aware-graph-modeling"><span class="nav-number">2.2.10.</span> <span class="nav-text">SuperRAG:
Beyond RAG with Layout-Aware Graph Modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hypa-rag-a-hybrid-parameter-adaptive-retrieval-augmented-generation-system-for-ai-legal-and-policy-applications"><span class="nav-number">2.2.11.</span> <span class="nav-text">HyPA-RAG:
A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI
Legal and Policy Applications</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#evaluating-the-performance-of-rag-methods-for-conversational-ai-in-the-airport-domain"><span class="nav-number">2.2.12.</span> <span class="nav-text">Evaluating
the Performance of RAG Methods for Conversational AI in the Airport
Domain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dsrag-a-double-stream-retrieval-augmented-generation-framework-for-countless-intent-detection"><span class="nav-number">2.2.13.</span> <span class="nav-text">DSRAG:
A Double-Stream Retrieval-Augmented Generation Framework for Countless
Intent Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#from-generating-answers-to-building-explanations-integrating-multi-round-rag-and-causal-modeling-for-scientific-qa"><span class="nav-number">2.2.14.</span> <span class="nav-text">From
Generating Answers to Building Explanations: Integrating Multi-Round RAG
and Causal Modeling for Scientific QA</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#emnlp-24"><span class="nav-number">3.</span> <span class="nav-text">EMNLP 24</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#main-2"><span class="nav-number">3.1.</span> <span class="nav-text">main</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rule-reliable-multimodal-rag-for-factuality-in-medical-vision-language-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E4%BD%86%E6%98%AF%E5%8C%BB%E5%AD%A6%E7%9B%B8%E5%85%B3%E7%9A%84"><span class="nav-number">3.1.1.</span> <span class="nav-text">RULE:
Reliable Multimodal RAG for Factuality in Medical Vision Language
Models（多模态的，但是医学相关的）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-qa-arena-evaluating-domain-robustness-for-long-form-retrieval-augmented-question-answering"><span class="nav-number">3.1.2.</span> <span class="nav-text">RAG-QA
Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented
Question Answering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#from-rag-to-riches-retrieval-interlaced-with-sequence-generation"><span class="nav-number">3.1.3.</span> <span class="nav-text">From
RAG to Riches: Retrieval Interlaced with Sequence
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-of-a-haystack-a-challenge-to-long-context-llms-and-rag-systems"><span class="nav-number">3.1.4.</span> <span class="nav-text">Summary
of a Haystack: A Challenge to Long-Context LLMs and RAG
Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dynamicer-resolving-emerging-mentions-to-dynamic-entities-for-rag"><span class="nav-number">3.1.5.</span> <span class="nav-text">DynamicER:
Resolving Emerging Mentions to Dynamic Entities for RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deciphering-the-interplay-of-parametric-and-non-parametric-memory-in-rag-models"><span class="nav-number">3.1.6.</span> <span class="nav-text">Deciphering
the Interplay of Parametric and Non-Parametric Memory in RAG
Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#re-rag-improving-open-domain-qa-performance-and-interpretability-with-relevance-estimator-in-retrieval-augmented-generation"><span class="nav-number">3.1.7.</span> <span class="nav-text">RE-RAG:
Improving Open-Domain QA Performance and Interpretability with Relevance
Estimator in Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#longrag-a-dual-perspective-retrieval-augmented-generation-paradigm-for-long-context-question-answering"><span class="nav-number">3.1.8.</span> <span class="nav-text">LongRAG:
A Dual-perspective Retrieval-Augmented Generation Paradigm for
Long-Context Question Answering</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#findings-2"><span class="nav-number">3.2.</span> <span class="nav-text">findings</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rag-studio-towards-in-domain-adaptation-of-retrieval-augmented-generation-through-self-alignment"><span class="nav-number">3.2.1.</span> <span class="nav-text">RAG-Studio:
Towards In-Domain Adaptation Of Retrieval Augmented Generation Through
Self-Alignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rafe-ranking-feedback-improves-query-rewriting-for-rag%E7%9C%9F%E5%B0%8F%E4%BC%97%E5%95%8A%E7%AB%9F%E7%84%B6%E7%A0%94%E7%A9%B6query-rewrite"><span class="nav-number">3.2.2.</span> <span class="nav-text">RaFe:
Ranking Feedback Improves Query Rewriting for
RAG（真小众啊，竟然研究query rewrite）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaptive-selection-for-homogeneous-tools-an-instantiation-in-the-rag-scenario"><span class="nav-number">3.2.3.</span> <span class="nav-text">Adaptive
Selection for Homogeneous Tools: An Instantiation in the RAG
Scenario</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bsharedrag-backbone-shared-retrieval-augmented-generation-for-the-e-commerce-domain"><span class="nav-number">3.2.4.</span> <span class="nav-text">BSharedRAG:
Backbone Shared Retrieval-Augmented Generation for the E-commerce
Domain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#open-rag-enhanced-retrieval-augmented-reasoning-with-open-source-large-language-models"><span class="nav-number">3.2.5.</span> <span class="nav-text">Open-RAG:
Enhanced Retrieval Augmented Reasoning with Open-Source Large Language
Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#typos-that-broke-the-rags-back-genetic-attack-on-rag-pipeline-by-simulating-documents-in-the-wild-via-low-level-perturbations"><span class="nav-number">3.2.6.</span> <span class="nav-text">Typos
that Broke the RAG’s Back: Genetic Attack on RAG Pipeline by Simulating
Documents in the Wild via Low-level Perturbations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#autorag-hp-automatic-online-hyper-parameter-tuning-for-retrieval-augmented-generation"><span class="nav-number">3.2.7.</span> <span class="nav-text">AutoRAG-HP:
Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented
Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#long2rag-evaluating-long-context-long-form-retrieval-augmented-generation-with-key-point-recall"><span class="nav-number">3.2.8.</span> <span class="nav-text">\(LONG^{2}RAG\): Evaluating Long-Context
&amp; Long-Form Retrieval-Augmented Generation with Key Point
Recall</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#icml-25%E6%8C%89retrieval%E5%85%B3%E9%94%AE%E8%AF%8D%E6%A3%80%E7%B4%A2%E7%9A%84%E6%8E%92%E9%99%A4%E4%BA%86ai4sci"><span class="nav-number">4.</span> <span class="nav-text">ICML
25（按retrieval关键词检索的，排除了ai4sci）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#poisonedeye-knowledge-poisoning-attack-on-retrieval-augmented-generation-based-large-vision-language-models%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number">4.1.</span> <span class="nav-text">PoisonedEye: Knowledge
Poisoning Attack on Retrieval-Augmented Generation based Large
Vision-Language Models（多模态的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieval-augmented-zero-shot-enzyme-generation-for-specified-substrate"><span class="nav-number">4.2.</span> <span class="nav-text">Retrieval Augmented
Zero-Shot Enzyme Generation for Specified Substrate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#she-streaming-media-hashing-retrieval%E7%96%91%E4%BC%BC%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number">4.3.</span> <span class="nav-text">SHE: Streaming-media
Hashing Retrieval（疑似多模态的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#contradiction-retrieval-via-contrastive-learning-with-sparsity"><span class="nav-number">4.4.</span> <span class="nav-text">Contradiction Retrieval
via Contrastive Learning with Sparsity</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#llm-alignment-as-retriever-optimization-an-information-retrieval-perspective"><span class="nav-number">4.5.</span> <span class="nav-text">LLM Alignment as
Retriever Optimization: An Information Retrieval Perspective</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#efficient-length-generalizable-attention-via-causal-retrieval-for-long-context-language-modeling"><span class="nav-number">4.6.</span> <span class="nav-text">Efficient
Length-Generalizable Attention via Causal Retrieval for Long-Context
Language Modeling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#phantomwiki-on-demand-datasets-for-reasoning-and-retrieval-evaluation"><span class="nav-number">4.7.</span> <span class="nav-text">PhantomWiki: On-Demand
Datasets for Reasoning and Retrieval Evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#understanding-synthetic-context-extension-via-retrieval-heads"><span class="nav-number">4.8.</span> <span class="nav-text">Understanding Synthetic
Context Extension via Retrieval Heads</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rapid-long-context-inference-with-retrieval-augmented-speculative-decoding"><span class="nav-number">4.9.</span> <span class="nav-text">RAPID: Long-Context
Inference with Retrieval-Augmented Speculative Decoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#c-3po-compact-plug-and-play-proxy-optimization-to-achieve-human-like-retrieval-augmented-generation"><span class="nav-number">4.10.</span> <span class="nav-text">C-3PO: Compact
Plug-and-Play Proxy Optimization to Achieve Human-like
Retrieval-Augmented Generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#on-the-vulnerability-of-applying-retrieval-augmented-generation-within-knowledge-intensive-application-domains"><span class="nav-number">4.11.</span> <span class="nav-text">On the Vulnerability of
Applying Retrieval-Augmented Generation within Knowledge-Intensive
Application Domains</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#in-context-learning-as-conditioned-associative-memory-retrieval"><span class="nav-number">4.12.</span> <span class="nav-text">In-Context Learning as
Conditioned Associative Memory Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#in-context-denoising-with-one-layer-transformers-connections-between-attention-and-associative-memory-retrieval"><span class="nav-number">4.13.</span> <span class="nav-text">In-Context Denoising
with One-Layer Transformers: Connections between Attention and
Associative Memory Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#poqd-performance-oriented-query-decomposer-for-multi-vector-retrieval"><span class="nav-number">4.14.</span> <span class="nav-text">POQD:
Performance-Oriented Query Decomposer for Multi-vector
retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scenir-visual-semantic-clarity-through-unsupervised-scene-graph-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E4%BD%86%E6%98%AF%E6%98%AFscene-graph%E7%9B%B8%E5%85%B3%E7%9A%84"><span class="nav-number">4.15.</span> <span class="nav-text">SCENIR: Visual Semantic
Clarity through Unsupervised Scene Graph
Retrieval（多模态的？但是是scene graph相关的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieval-augmented-time-series-forecasting"><span class="nav-number">4.16.</span> <span class="nav-text">Retrieval Augmented
Time Series Forecasting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#position-retrieval-augmented-systems-can-be-dangerous-medical-communicators"><span class="nav-number">4.17.</span> <span class="nav-text">Position:
Retrieval-augmented systems can be dangerous medical
communicators</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#beyond-cropped-regions-new-benchmark-and-corresponding-baseline-for-chinese-scene-text-retrieval-in-diverse-layouts"><span class="nav-number">4.18.</span> <span class="nav-text">Beyond Cropped Regions:
New Benchmark and Corresponding Baseline for Chinese Scene Text
Retrieval in Diverse Layouts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qure-query-relevant-retrieval-through-hard-negative-sampling-in-composed-image-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number">4.19.</span> <span class="nav-text">QuRe: Query-Relevant
Retrieval through Hard Negative Sampling in Composed Image
Retrieval（多模态的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-attribute-aware-hash-codes-for-fine-grained-image-retrieval-via-query-optimization%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number">4.20.</span> <span class="nav-text">Learning
Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query
Optimization（多模态的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#locality-preserving-markovian-transition-for-instance-retrieval"><span class="nav-number">4.21.</span> <span class="nav-text">Locality Preserving
Markovian Transition for Instance Retrieval</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#visual-abstraction-a-plug-and-play-approach-for-text-visual-retrieval%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84"><span class="nav-number">4.22.</span> <span class="nav-text">Visual Abstraction: A
Plug-and-Play Approach for Text-Visual Retrieval（多模态的）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#docks-rag-optimizing-document-level-relation-extraction-through-llm-enhanced-hybrid-prompt-tuning%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E6%96%87%E6%A1%A3%E7%B1%BB"><span class="nav-number">4.23.</span> <span class="nav-text">DocKS-RAG: Optimizing
Document-Level Relation Extraction through LLM-Enhanced Hybrid Prompt
Tuning（多模态的，文档类）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#from-rag-to-memory-non-parametric-continual-learning-for-large-language-models"><span class="nav-number">4.24.</span> <span class="nav-text">From RAG to Memory:
Non-Parametric Continual Learning for Large Language Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hierarchical-planning-for-complex-tasks-with-knowledge-graph-rag-and-symbolic-verification"><span class="nav-number">4.25.</span> <span class="nav-text">Hierarchical Planning
for Complex Tasks with Knowledge Graph-RAG and Symbolic
Verification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieval-augmented-perception-high-resolution-image-perception-meets-visual-rag%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E7%8E%8B%E4%B8%AD%E7%8E%8B%E7%A5%9E%E4%B8%AD%E7%A5%9E"><span class="nav-number">4.26.</span> <span class="nav-text">Retrieval-Augmented
Perception: High-resolution Image Perception Meets Visual
RAG（多模态的，王中王，神中神）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#realrag-retrieval-augmented-realistic-image-generation-via-self-reflective-contrastive-learning%E4%B9%9F%E7%AE%97%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9A%84%E5%90%A7%E7%BB%93%E5%90%88rag%E4%B8%8Ediffusion"><span class="nav-number">4.27.</span> <span class="nav-text">RealRAG:
Retrieval-augmented Realistic Image Generation via Self-reflective
Contrastive Learning（也算多模态的吧，结合RAG与diffusion）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lara-benchmarking-retrieval-augmented-generation-and-long-context-llms-no-silver-bullet-for-lc-or-rag-routing"><span class="nav-number">4.28.</span> <span class="nav-text">LaRA: Benchmarking
Retrieval-Augmented Generation and Long-Context LLMs – No Silver Bullet
for LC or RAG Routing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ragged-towards-informed-design-of-scalable-and-stable-rag-systems"><span class="nav-number">4.29.</span> <span class="nav-text">RAGGED: Towards
Informed Design of Scalable and Stable RAG Systems</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#iclr-25%E6%8C%89retrieval%E5%85%B3%E9%94%AE%E8%AF%8D%E6%A3%80%E7%B4%A2"><span class="nav-number">5.</span> <span class="nav-text">ICLR
25（按retrieval关键词检索）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#video-multimodal-retrieval"><span class="nav-number">6.</span> <span class="nav-text">3. Video &amp; Multimodal
Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cross-modal-dense-retrieval18-%E7%AF%87"><span class="nav-number">6.0.0.1.</span> <span class="nav-text">1. Cross-modal Dense
Retrieval（18 篇）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fine-grained-patchregion-retrieval15-%E7%AF%87"><span class="nav-number">6.0.0.2.</span> <span class="nav-text">2. Fine-grained
Patch&#x2F;Region Retrieval（15 篇）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#video-segment-retrieval14-%E7%AF%87"><span class="nav-number">6.0.0.3.</span> <span class="nav-text">3. Video Segment Retrieval（14
篇）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unified-multimodal-index12-%E7%AF%87"><span class="nav-number">6.0.1.</span> <span class="nav-text">4. Unified Multimodal Index（12
篇）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#memory-augmented-retrieval11-%E7%AF%87"><span class="nav-number">6.0.2.</span> <span class="nav-text">5. Memory-Augmented
Retrieval（11 篇）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#structured-evidence-kgscene-graph13-%E7%AF%87"><span class="nav-number">6.0.3.</span> <span class="nav-text">6. Structured Evidence
(KG&#x2F;Scene-Graph)（13 篇）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-hop-iterative10-%E7%AF%87"><span class="nav-number">6.0.4.</span> <span class="nav-text">7. Multi-hop &#x2F; Iterative（10 篇）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#security-robustness6-%E7%AF%87"><span class="nav-number">6.0.5.</span> <span class="nav-text">8. Security &amp; Robustness（6
篇）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#benchmark-evaluation7-%E7%AF%87"><span class="nav-number">6.0.6.</span> <span class="nav-text">9. Benchmark &#x2F; Evaluation（7 篇）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8E%BB%E9%87%8D%E4%B9%8B%E5%90%8E%E7%9A%84%E8%AE%BA%E6%96%87%E5%88%97%E8%A1%A8"><span class="nav-number">7.</span> <span class="nav-text">去重之后的论文列表</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cross-modal-dense-retrieval18-%E7%AF%87%E5%B7%B2%E5%8E%BB%E9%87%8D"><span class="nav-number">7.0.1.</span> <span class="nav-text">1. Cross-modal Dense
Retrieval（18 篇，已去重）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fine-grained-patchregion-retrieval15-%E7%AF%87%E5%B7%B2%E5%8E%BB%E9%87%8D"><span class="nav-number">7.0.2.</span> <span class="nav-text">2. Fine-grained
Patch&#x2F;Region Retrieval（15 篇，已去重）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%97%AFfine-grained-regionpatch-retrieval%E8%BF%99%E7%B1%BB%E6%96%B9%E6%B3%95%E7%9A%84%E5%A4%A7%E6%A6%82%E6%80%9D%E8%B7%AF%E5%BA%94%E8%AF%A5%E6%98%AF%E7%94%A8ocr%E6%8A%80%E6%9C%AFdetector%E5%A6%82faster-r-cnn%E5%85%88%E5%9C%A8%E8%BE%93%E5%85%A5%E4%B8%8A%E4%BE%8B%E5%A6%82%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E6%A3%80%E6%B5%8B%E6%AD%A4%E6%97%B6%E4%BC%9A%E6%A3%80%E6%B5%8B%E5%87%BA%E5%BE%88%E5%A4%9A%E7%9A%84%E6%A1%86%E7%84%B6%E5%90%8E%E8%BF%99%E4%BA%9B%E6%A1%86%E4%BC%9A%E8%A2%AB%E5%8A%A0%E5%85%A5%E5%88%B0%E4%B8%80%E4%B8%AA%E5%BB%BA%E5%A5%BD%E7%9A%84%E5%BA%93%E4%B9%8B%E5%90%8E%E4%BC%9A%E7%94%A8%E7%94%A8%E6%88%B7%E7%9A%84query%E8%BF%9B%E8%A1%8C%E6%A3%80%E7%B4%A2%E5%92%8C%E6%88%91%E4%BB%AC%E5%85%B3%E6%B3%A8%E7%9A%84rag%E8%A7%A3%E5%86%B3%E5%A4%9A%E6%A8%A1%E6%80%81%E8%BE%93%E5%85%A5token%E8%BF%87%E5%A4%9A%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%E5%AE%83%E4%BB%AC%E7%9A%84%E5%8C%BA%E5%88%AB%E4%BC%BC%E4%B9%8E%E4%B8%BB%E8%A6%81%E5%9C%A8%E4%BA%8E%E5%A6%82%E4%BD%95%E5%88%92%E5%88%86%E8%BE%93%E5%85%A5%E6%98%AF%E5%90%A6%E4%BC%9A%E5%BB%BA%E5%BA%93%E5%89%8D%E8%80%85%E6%98%AF%E5%8A%A8%E6%80%81%E5%88%92%E5%88%86%E7%9A%84%E4%B8%94%E5%80%99%E9%80%89%E6%A1%86%E5%BE%88%E5%A4%9A%E5%9B%A0%E6%AD%A4%E9%9C%80%E8%A6%81%E5%88%A9%E7%94%A8%E5%BA%93%E6%A3%80%E7%B4%A2%E7%9A%84%E6%97%B6%E5%80%99%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0faiss%E7%AD%89%E8%BF%91%E4%BC%BC%E7%9A%84%E6%8A%80%E6%9C%AF%E8%80%8C%E5%90%8E%E8%80%85%E5%9B%A0%E4%B8%BA%E6%98%AF%E5%9B%BA%E5%AE%9A%E7%9A%84%E7%AD%96%E7%95%A5%E8%BF%9B%E8%A1%8C%E5%88%92%E5%88%86%E5%80%99%E9%80%89%E6%A1%86%E8%BE%83%E5%B0%91%E5%9B%A0%E6%AD%A4%E4%B8%8D%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0%E4%BB%80%E4%B9%88%E8%BF%91%E4%BC%BC%E6%8A%80%E6%9C%AF%E4%B9%9F%E4%B8%8D%E9%9C%80%E8%A6%81%E5%BB%BA%E5%BA%93%E5%8F%AA%E9%9C%80%E8%A6%81%E5%88%92%E5%88%86%E5%AE%8C%E5%90%8E%E6%AD%A3%E5%B8%B8%E7%94%A8query%E8%BF%9B%E8%A1%8C%E6%A3%80%E7%B4%A2-%E5%AE%B9%E6%98%93%E6%83%B3%E5%88%B0%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%9A%84%E6%94%B9%E8%BF%9B%E5%B0%B1%E6%98%AF%E6%8A%8A%E4%B8%A4%E7%A7%8D%E5%88%92%E5%88%86%E6%96%B9%E5%BC%8F%E7%BB%93%E5%90%88%E8%B5%B7%E6%9D%A5%E4%BD%86%E6%98%AF%E5%B7%B2%E7%BB%8F%E6%9C%89%E4%B8%80%E4%BA%9B%E8%AE%BA%E6%96%87%E8%BF%99%E6%A0%B7%E5%81%9A%E4%BA%86%E6%AF%94%E5%A6%82vdocrag%E5%A5%BD%E5%83%8F%E5%B0%B1%E5%B7%B2%E7%BB%8F%E7%BB%93%E5%90%88%E4%BA%86%E5%AE%83%E5%85%88%E7%94%A8%E5%9B%BA%E5%AE%9A%E5%88%92%E5%88%86%E7%AD%96%E7%95%A5%E7%84%B6%E5%90%8E%E6%A3%80%E7%B4%A2%E5%88%B0%E8%8B%A5%E5%B9%B2%E4%B8%AA%E7%B2%97%E7%9A%84%E6%A1%86%E4%B9%8B%E5%90%8E%E5%9C%A8%E8%BF%99%E4%BA%9B%E5%B0%8F%E6%A1%86%E9%87%8C%E9%9D%A2%E7%94%A8ocrdetector%E6%8A%80%E6%9C%AF%E8%BF%9B%E8%A1%8C%E7%BB%86%E8%87%B4%E7%9A%84%E6%A3%80%E7%B4%A2%E8%BF%99%E6%A0%B7%E6%84%9F%E8%A7%89%E4%B8%8A%E6%95%88%E6%9E%9C%E4%BC%9A%E5%A5%BD%E4%B8%80%E4%BA%9B"><span class="nav-number">7.1.</span> <span class="nav-text">####
嗯，fine-grained region&#x2F;patch
retrieval，这类方法的大概思路应该是，用OCR技术&#x2F;detector（如faster
R-CNN），先在输入上（例如图像）进行检测。此时会检测出很多的框。然后这些框会被加入到一个建好的库，之后会用用户的query进行检索。和我们关注的，RAG解决多模态输入token过多的方法相比，它们的区别似乎主要在于如何划分输入，是否会建库。前者是动态划分的，且候选框很多，因此需要利用库，检索的时候需要用到faiss等近似的技术；而后者因为是固定的策略进行划分，候选框较少，因此不需要用到什么近似技术，也不需要建库，只需要划分完后正常用query进行检索容易想到一个小的改进，就是把两种划分方式结合起来。但是已经有一些论文这样做了。比如VDocRAG，好像就已经结合了：它先用固定划分策略，然后检索到若干个粗的框之后，在这些小框里面用OCR&#x2F;detector技术进行细致的检索。这样感觉上效果会好一些</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#video-segment-retrieval14-%E7%AF%87%E5%B7%B2%E5%8E%BB%E9%87%8D"><span class="nav-number">7.1.1.</span> <span class="nav-text">3. Video Segment
Retrieval（14 篇，已去重）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unified-multimodal-index12-%E7%AF%87%E5%B7%B2%E5%8E%BB%E9%87%8D"><span class="nav-number">7.1.2.</span> <span class="nav-text">4. Unified Multimodal
Index（12 篇，已去重）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#memory-augmented-retrieval11-%E7%AF%87%E5%B7%B2%E5%8E%BB%E9%87%8D"><span class="nav-number">7.1.3.</span> <span class="nav-text">5. Memory-Augmented
Retrieval（11 篇，已去重）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#structured-evidence-kgscene-graph13-%E7%AF%87%E5%B7%B2%E5%8E%BB%E9%87%8D"><span class="nav-number">7.1.4.</span> <span class="nav-text">6. Structured
Evidence (KG&#x2F;Scene-Graph)（13 篇，已去重）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-hop-iterative10-%E7%AF%87%E5%B7%B2%E5%8E%BB%E9%87%8D"><span class="nav-number">7.1.5.</span> <span class="nav-text">7. Multi-hop &#x2F; Iterative（10
篇，已去重）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#security-robustness6-%E7%AF%87-1"><span class="nav-number">7.1.6.</span> <span class="nav-text">8. Security &amp; Robustness（6
篇）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#benchmark-evaluation7-%E7%AF%87-1"><span class="nav-number">7.1.7.</span> <span class="nav-text">9. Benchmark &#x2F; Evaluation（7
篇）</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">217</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">67</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">103</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
