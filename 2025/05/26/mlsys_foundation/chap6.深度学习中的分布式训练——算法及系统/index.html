<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="title: chap6.深度学习中的分布式训练——算法及系统date: 2025-05-26 15:51tags: [分布式训练]categories: [mlsys, foundation]1. 分布式训练的必要性随着问题规模的增大（包括数据量和计算量层面的），单一设备处理速度逐渐无法满足我们的需求（比如太慢了，或者是干脆单设备放不下模型了）而用分布式算法，把问题划分为若干小问题后，再经过分">
<meta property="og:type" content="article">
<meta property="og:title" content="bluemouse&#39;s blog">
<meta property="og:url" content="https://blueeemouse.github.io/2025/05/26/mlsys_foundation/chap6.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E2%80%94%E2%80%94%E7%AE%97%E6%B3%95%E5%8F%8A%E7%B3%BB%E7%BB%9F/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="title: chap6.深度学习中的分布式训练——算法及系统date: 2025-05-26 15:51tags: [分布式训练]categories: [mlsys, foundation]1. 分布式训练的必要性随着问题规模的增大（包括数据量和计算量层面的），单一设备处理速度逐渐无法满足我们的需求（比如太慢了，或者是干脆单设备放不下模型了）而用分布式算法，把问题划分为若干小问题后，再经过分">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-05-26T07:08:24.173Z">
<meta property="article:modified_time" content="2025-05-27T07:39:54.923Z">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/2025/05/26/mlsys_foundation/chap6.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E2%80%94%E2%80%94%E7%AE%97%E6%B3%95%E5%8F%8A%E7%B3%BB%E7%BB%9F/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title> | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/05/26/mlsys_foundation/chap6.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E2%80%94%E2%80%94%E7%AE%97%E6%B3%95%E5%8F%8A%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-26 15:08:24" itemprop="dateCreated datePublished" datetime="2025-05-26T15:08:24+08:00">2025-05-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-05-27 15:39:54" itemprop="dateModified" datetime="2025-05-27T15:39:54+08:00">2025-05-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id><a href="#" class="headerlink" title></a></h1><hr>
<h2 id="title-chap6-深度学习中的分布式训练——算法及系统date-2025-05-26-15-51tags-分布式训练-categories-mlsys-foundation"><a href="#title-chap6-深度学习中的分布式训练——算法及系统date-2025-05-26-15-51tags-分布式训练-categories-mlsys-foundation" class="headerlink" title="title: chap6.深度学习中的分布式训练——算法及系统date: 2025-05-26 15:51tags: [分布式训练]categories: [mlsys, foundation]"></a>title: chap6.深度学习中的分布式训练——算法及系统<br>date: 2025-05-26 15:51<br>tags: [分布式训练]<br>categories: [mlsys, foundation]</h2><h1 id="1-分布式训练的必要性"><a href="#1-分布式训练的必要性" class="headerlink" title="1. 分布式训练的必要性"></a>1. 分布式训练的必要性</h1><h2 id="随着问题规模的增大（包括数据量和计算量层面的），单一设备处理速度逐渐无法满足我们的需求（比如太慢了，或者是干脆单设备放不下模型了）而用分布式算法，把问题划分为若干小问题后，再经过分布式系统来执行处理，能做到较快解决较大的问题"><a href="#随着问题规模的增大（包括数据量和计算量层面的），单一设备处理速度逐渐无法满足我们的需求（比如太慢了，或者是干脆单设备放不下模型了）而用分布式算法，把问题划分为若干小问题后，再经过分布式系统来执行处理，能做到较快解决较大的问题" class="headerlink" title="随着问题规模的增大（包括数据量和计算量层面的），单一设备处理速度逐渐无法满足我们的需求（比如太慢了，或者是干脆单设备放不下模型了）而用分布式算法，把问题划分为若干小问题后，再经过分布式系统来执行处理，能做到较快解决较大的问题"></a>随着问题规模的增大（包括数据量和计算量层面的），单一设备处理速度逐渐无法满足我们的需求（比如太慢了，或者是干脆单设备放不下模型了）<br>而用分布式算法，把问题划分为若干小问题后，再经过分布式系统来执行处理，能做到较快解决较大的问题</h2><h2 id="这里还介绍两个并行加速定律，来说明分布式训练的意义。"><a href="#这里还介绍两个并行加速定律，来说明分布式训练的意义。" class="headerlink" title="这里还介绍两个并行加速定律，来说明分布式训练的意义。"></a>这里还介绍两个并行加速定律，来说明分布式训练的意义。</h2><span id="more"></span>
<h3 id="第一个是阿姆达尔定律（Amdahl’s-law），这是一个比较悲观的定律。它认为，用分布式计算，是存在一个加速比的理论上限的，也就是不能仅仅依靠分布式训练来无限制，或者说较大幅度地提高计算效率，最终还是得落到提高串行处理效率，也就是单个硬件的效率上。其公式如下：-S-latency-s-frac-1-1-p-frac-p-s-其中，-S-latency-表示加速比，p代表任务里可以用分布式训练进行加速的比例（例如，可能一半是可以用分布式训练加速的），s表示并行执行单元的个数，显然s越大，理论上加速比例就越大。但根据这个公式，会发现，即使s增大到无穷，加速比也不过是-frac-1-1-p-，也就是有上限的。它的推导也比较简单："><a href="#第一个是阿姆达尔定律（Amdahl’s-law），这是一个比较悲观的定律。它认为，用分布式计算，是存在一个加速比的理论上限的，也就是不能仅仅依靠分布式训练来无限制，或者说较大幅度地提高计算效率，最终还是得落到提高串行处理效率，也就是单个硬件的效率上。其公式如下：-S-latency-s-frac-1-1-p-frac-p-s-其中，-S-latency-表示加速比，p代表任务里可以用分布式训练进行加速的比例（例如，可能一半是可以用分布式训练加速的），s表示并行执行单元的个数，显然s越大，理论上加速比例就越大。但根据这个公式，会发现，即使s增大到无穷，加速比也不过是-frac-1-1-p-，也就是有上限的。它的推导也比较简单：" class="headerlink" title="第一个是阿姆达尔定律（Amdahl’s law），这是一个比较悲观的定律。它认为，用分布式计算，是存在一个加速比的理论上限的，也就是不能仅仅依靠分布式训练来无限制，或者说较大幅度地提高计算效率，最终还是得落到提高串行处理效率，也就是单个硬件的效率上。其公式如下：$$S_{latency}(s)&#x3D;\frac{1}{(1-p)+\frac{p}{s}}$$其中，$S_{latency}$表示加速比，p代表任务里可以用分布式训练进行加速的比例（例如，可能一半是可以用分布式训练加速的），s表示并行执行单元的个数，显然s越大，理论上加速比例就越大。但根据这个公式，会发现，即使s增大到无穷，加速比也不过是$\frac{1}{1-p}$，也就是有上限的。它的推导也比较简单："></a>第一个是阿姆达尔定律（Amdahl’s law），这是一个比较悲观的定律。它认为，用分布式计算，是存在一个加速比的理论上限的，也就是不能仅仅依靠分布式训练来无限制，或者说较大幅度地提高计算效率，最终还是得落到提高串行处理效率，也就是单个硬件的效率上。其公式如下：$$S_{latency}(s)&#x3D;\frac{1}{(1-p)+\frac{p}{s}}$$其中，$S_{latency}$表示加速比，p代表任务里可以用分布式训练进行加速的比例（例如，可能一半是可以用分布式训练加速的），s表示并行执行单元的个数，显然s越大，理论上加速比例就越大。但根据这个公式，会发现，即使s增大到无穷，加速比也不过是$\frac{1}{1-p}$，也就是有上限的。它的推导也比较简单：</h3><h4 id="首先，我们把程序执行时间划分一下：-T-1-W-ser-W-par-（其中-W-ser-表示无法并行执行的部分，下标ser表示serial；-W-par-表示可以并行执行的部分，下标par表示parallel）"><a href="#首先，我们把程序执行时间划分一下：-T-1-W-ser-W-par-（其中-W-ser-表示无法并行执行的部分，下标ser表示serial；-W-par-表示可以并行执行的部分，下标par表示parallel）" class="headerlink" title="首先，我们把程序执行时间划分一下：$T_{1}&#x3D;W_{ser}+W_{par}$（其中$W_{ser}$表示无法并行执行的部分，下标ser表示serial；$W_{par}$表示可以并行执行的部分，下标par表示parallel）"></a>首先，我们把程序执行时间划分一下：$T_{1}&#x3D;W_{ser}+W_{par}$（其中$W_{ser}$表示无法并行执行的部分，下标ser表示serial；$W_{par}$表示可以并行执行的部分，下标par表示parallel）</h4><h4 id="然后，假设我们有s个并行执行单元，则此时理想情况下，可并行的部分的时间，应该能做到变为原来的-frac-1-s-，因此，有：-T-p-geq-W-ser-frac-W-par-s-（其中，-T-p-表示实际的并行执行之后的总时间。右边的部分就是理想的执行时间）"><a href="#然后，假设我们有s个并行执行单元，则此时理想情况下，可并行的部分的时间，应该能做到变为原来的-frac-1-s-，因此，有：-T-p-geq-W-ser-frac-W-par-s-（其中，-T-p-表示实际的并行执行之后的总时间。右边的部分就是理想的执行时间）" class="headerlink" title="然后，假设我们有s个并行执行单元，则此时理想情况下，可并行的部分的时间，应该能做到变为原来的$\frac{1}{s}$，因此，有：$T_{p}\geq W_{ser}+\frac{W_{par}}{s}$（其中，$T_{p}$表示实际的并行执行之后的总时间。右边的部分就是理想的执行时间）"></a>然后，假设我们有s个并行执行单元，则此时理想情况下，可并行的部分的时间，应该能做到变为原来的$\frac{1}{s}$，因此，有：$T_{p}\geq W_{ser}+\frac{W_{par}}{s}$（其中，$T_{p}$表示实际的并行执行之后的总时间。右边的部分就是理想的执行时间）</h4><h6 id="这里多说一下，之所以理想的时间中，并行部分的执行时间恰好就是-frac-1-s-，可以这样直观理解：我们对可并行部分，要把任务分到s个执行单元上去执行。最终我们衡量这部分的运行时间，看的其实不是执行时间最短的那个单元，而是执行时间最长的那个单元。因此，我们现在就来看一下，s个单元里，最短的时间可以是多少呢？这里我们可以把-W-par-理解为某种工作量，工作量是一定的。因此，分到各个执行单元上的时候，如果有某个执行单元的工作量小于平均值-frac-W-par-s-，则一定会有某个执行单元的工作量大于平均值-frac-W-par-s-。则此时并行执行的总时间应该大于-frac-W-par-s-。故，最小的时间确实是发生在我们把任务量平均分到每个执行单元上的时候。"><a href="#这里多说一下，之所以理想的时间中，并行部分的执行时间恰好就是-frac-1-s-，可以这样直观理解：我们对可并行部分，要把任务分到s个执行单元上去执行。最终我们衡量这部分的运行时间，看的其实不是执行时间最短的那个单元，而是执行时间最长的那个单元。因此，我们现在就来看一下，s个单元里，最短的时间可以是多少呢？这里我们可以把-W-par-理解为某种工作量，工作量是一定的。因此，分到各个执行单元上的时候，如果有某个执行单元的工作量小于平均值-frac-W-par-s-，则一定会有某个执行单元的工作量大于平均值-frac-W-par-s-。则此时并行执行的总时间应该大于-frac-W-par-s-。故，最小的时间确实是发生在我们把任务量平均分到每个执行单元上的时候。" class="headerlink" title="这里多说一下，之所以理想的时间中，并行部分的执行时间恰好就是$\frac{1}{s}$，可以这样直观理解：我们对可并行部分，要把任务分到s个执行单元上去执行。最终我们衡量这部分的运行时间，看的其实不是执行时间最短的那个单元，而是执行时间最长的那个单元。因此，我们现在就来看一下，s个单元里，最短的时间可以是多少呢？这里我们可以把$W_{par}$理解为某种工作量，工作量是一定的。因此，分到各个执行单元上的时候，如果有某个执行单元的工作量小于平均值$\frac{W_{par}}{s}$，则一定会有某个执行单元的工作量大于平均值$\frac{W_{par}}{s}$。则此时并行执行的总时间应该大于$\frac{W_{par}}{s}$。故，最小的时间确实是发生在我们把任务量平均分到每个执行单元上的时候。"></a>这里多说一下，之所以理想的时间中，并行部分的执行时间恰好就是$\frac{1}{s}$，可以这样直观理解：我们对可并行部分，要把任务分到s个执行单元上去执行。最终我们衡量这部分的运行时间，看的其实不是执行时间最短的那个单元，而是执行时间最长的那个单元。因此，我们现在就来看一下，s个单元里，最短的时间可以是多少呢？这里我们可以把$W_{par}$理解为某种工作量，工作量是一定的。因此，分到各个执行单元上的时候，如果有某个执行单元的工作量小于平均值$\frac{W_{par}}{s}$，则一定会有某个执行单元的工作量大于平均值$\frac{W_{par}}{s}$。则此时并行执行的总时间应该大于$\frac{W_{par}}{s}$。故，最小的时间确实是发生在我们把任务量平均分到每个执行单元上的时候。</h6><h4 id="此时，我们根据加速比的定义（-frac-T-1-T-p-，即原执行时间比上加速后的时间），可得到：-S-latency-leq-frac-W-ser-W-par-W-ser-frac-W-par-s-进一步的，我们记p为-T-1-中可并行加速的部分的占比，则有：-W-ser-1-p-T-1-，-W-par-pT-1-此时再把上面的不等式整理一下，有：-S-latency-leq-frac-1-p-p-1-p-frac-p-s-frac-1-1-p-frac-p-s"><a href="#此时，我们根据加速比的定义（-frac-T-1-T-p-，即原执行时间比上加速后的时间），可得到：-S-latency-leq-frac-W-ser-W-par-W-ser-frac-W-par-s-进一步的，我们记p为-T-1-中可并行加速的部分的占比，则有：-W-ser-1-p-T-1-，-W-par-pT-1-此时再把上面的不等式整理一下，有：-S-latency-leq-frac-1-p-p-1-p-frac-p-s-frac-1-1-p-frac-p-s" class="headerlink" title="此时，我们根据加速比的定义（$\frac{T_{1}}{T_{p}}$，即原执行时间比上加速后的时间），可得到：$S_{latency}\leq \frac{W_{ser}+W_{par}}{W_{ser}+\frac{W_{par}}{s}}$进一步的，我们记p为$T_{1}$中可并行加速的部分的占比，则有：$W_{ser}&#x3D;(1-p)T_{1}$，$W_{par}&#x3D;pT_{1}$此时再把上面的不等式整理一下，有：$S_{latency}\leq \frac{(1-p)+p}{1-p+\frac{p}{s}} &#x3D; \frac{1}{1-p+\frac{p}{s}}$"></a>此时，我们根据加速比的定义（$\frac{T_{1}}{T_{p}}$，即原执行时间比上加速后的时间），可得到：$S_{latency}\leq \frac{W_{ser}+W_{par}}{W_{ser}+\frac{W_{par}}{s}}$<br>进一步的，我们记p为$T_{1}$中可并行加速的部分的占比，则有：$W_{ser}&#x3D;(1-p)T_{1}$，$W_{par}&#x3D;pT_{1}$<br>此时再把上面的不等式整理一下，有：$S_{latency}\leq \frac{(1-p)+p}{1-p+\frac{p}{s}} &#x3D; \frac{1}{1-p+\frac{p}{s}}$</h4><h3 id="第二个则是古斯塔夫定律（Gustafson’s-law），它类似于对上面阿姆达尔定律的一个反击，是一个乐观的定律。它认为其实线性加速比是有可能的，因此，我们确实是有机会通过增加执行单元的数量来得到比较好的加速比的。其推导其实和上面有些类似，但关键在于一个地方，就是，古斯塔夫定律认为，随着并行度的增加，可解问题的规模也在增加；而上面的阿姆达尔定律则假设问题规模总是固定的："><a href="#第二个则是古斯塔夫定律（Gustafson’s-law），它类似于对上面阿姆达尔定律的一个反击，是一个乐观的定律。它认为其实线性加速比是有可能的，因此，我们确实是有机会通过增加执行单元的数量来得到比较好的加速比的。其推导其实和上面有些类似，但关键在于一个地方，就是，古斯塔夫定律认为，随着并行度的增加，可解问题的规模也在增加；而上面的阿姆达尔定律则假设问题规模总是固定的：" class="headerlink" title="第二个则是古斯塔夫定律（Gustafson’s law），它类似于对上面阿姆达尔定律的一个反击，是一个乐观的定律。它认为其实线性加速比是有可能的，因此，我们确实是有机会通过增加执行单元的数量来得到比较好的加速比的。其推导其实和上面有些类似，但关键在于一个地方，就是，古斯塔夫定律认为，随着并行度的增加，可解问题的规模也在增加；而上面的阿姆达尔定律则假设问题规模总是固定的："></a>第二个则是古斯塔夫定律（Gustafson’s law），它类似于对上面阿姆达尔定律的一个反击，是一个乐观的定律。它认为其实线性加速比是有可能的，因此，我们确实是有机会通过增加执行单元的数量来得到比较好的加速比的。其推导其实和上面有些类似，但关键在于一个地方，就是，古斯塔夫定律认为，随着并行度的增加，可解问题的规模也在增加；而上面的阿姆达尔定律则假设问题规模总是固定的：</h3><h4 id="这里我们记，在拥有s个并行执行单元的机器上，执行时间为：-T-1-W-ser-W-par-，这里初始的时间记号没变，含义和上面一样。下面，是关键的一步：在单处理器上，我们可得到，执行时间为：-T-s-W-ser-sW-par"><a href="#这里我们记，在拥有s个并行执行单元的机器上，执行时间为：-T-1-W-ser-W-par-，这里初始的时间记号没变，含义和上面一样。下面，是关键的一步：在单处理器上，我们可得到，执行时间为：-T-s-W-ser-sW-par" class="headerlink" title="这里我们记，在拥有s个并行执行单元的机器上，执行时间为：$T_{1}&#x3D;W_{ser}+W_{par}$，这里初始的时间记号没变，含义和上面一样。下面，是关键的一步：在单处理器上，我们可得到，执行时间为：$T_{s}&#x3D;W_{ser}+sW_{par}$"></a>这里我们记，在拥有s个并行执行单元的机器上，执行时间为：$T_{1}&#x3D;W_{ser}+W_{par}$，这里初始的时间记号没变，含义和上面一样。下面，是关键的一步：<br>在单处理器上，我们可得到，执行时间为：$T_{s}&#x3D;W_{ser}+sW_{par}$</h4><h4 id="看起来和上面好像没什么区别？但这里，我们是先定义了并行执行的时间。然后，在相同的问题规模下，我们再看，如果要在单处理器上执行，会是多少时间。上面阿姆达尔定律，则是：先看一下在单处理器情况下，执行时间是多少，之后再看用s个并行执行单元后，执行时间又是多少。最大的区别在于，上面阿姆达尔定律，它假设了可并行的部分的问题任务量是一定的，即固定为-W-par-；此处的古斯塔夫定律，它先考察并行执行下的时间，此时可并行部分的任务量是-W-par-，则之后在单处理器上执行的时候，总的并行部分的任务量应该是-SW-par-（？再理解一下）"><a href="#看起来和上面好像没什么区别？但这里，我们是先定义了并行执行的时间。然后，在相同的问题规模下，我们再看，如果要在单处理器上执行，会是多少时间。上面阿姆达尔定律，则是：先看一下在单处理器情况下，执行时间是多少，之后再看用s个并行执行单元后，执行时间又是多少。最大的区别在于，上面阿姆达尔定律，它假设了可并行的部分的问题任务量是一定的，即固定为-W-par-；此处的古斯塔夫定律，它先考察并行执行下的时间，此时可并行部分的任务量是-W-par-，则之后在单处理器上执行的时候，总的并行部分的任务量应该是-SW-par-（？再理解一下）" class="headerlink" title="看起来和上面好像没什么区别？但这里，我们是先定义了并行执行的时间。然后，在相同的问题规模下，我们再看，如果要在单处理器上执行，会是多少时间。上面阿姆达尔定律，则是：先看一下在单处理器情况下，执行时间是多少，之后再看用s个并行执行单元后，执行时间又是多少。最大的区别在于，上面阿姆达尔定律，它假设了可并行的部分的问题任务量是一定的，即固定为$W_{par}$；此处的古斯塔夫定律，它先考察并行执行下的时间，此时可并行部分的任务量是$W_{par}$，则之后在单处理器上执行的时候，总的并行部分的任务量应该是$SW_{par}$（？再理解一下）"></a>看起来和上面好像没什么区别？但这里，我们是先定义了并行执行的时间。然后，在相同的问题规模下，我们再看，如果要在单处理器上执行，会是多少时间。上面阿姆达尔定律，则是：先看一下在单处理器情况下，执行时间是多少，之后再看用s个并行执行单元后，执行时间又是多少。最大的区别在于，上面阿姆达尔定律，它假设了可并行的部分的问题任务量是一定的，即固定为$W_{par}$；此处的古斯塔夫定律，它先考察并行执行下的时间，此时可并行部分的任务量是$W_{par}$，则之后在单处理器上执行的时候，总的并行部分的任务量应该是$SW_{par}$（？再理解一下）</h4><h4 id="此时，我们依然按照加速比的定义，可得到：-S-latency-s-frac-W-ser-sW-par-W-ser-W-par-；依然引入p记号，则有：-S-latency-s-frac-1-p-sp-1-1-p-sp-可以看到，当并行执行单元s增多的时候，且足够大的时候，是有可能可以做到接近线性加速比的"><a href="#此时，我们依然按照加速比的定义，可得到：-S-latency-s-frac-W-ser-sW-par-W-ser-W-par-；依然引入p记号，则有：-S-latency-s-frac-1-p-sp-1-1-p-sp-可以看到，当并行执行单元s增多的时候，且足够大的时候，是有可能可以做到接近线性加速比的" class="headerlink" title="此时，我们依然按照加速比的定义，可得到：$S_{latency}(s)&#x3D;\frac{W_{ser}+sW_{par}}{W_{ser}+W_{par}}$；依然引入p记号，则有：$S_{latency}(s)&#x3D;\frac{1-p+sp}{1}&#x3D;1-p+sp$可以看到，当并行执行单元s增多的时候，且足够大的时候，是有可能可以做到接近线性加速比的"></a>此时，我们依然按照加速比的定义，可得到：$S_{latency}(s)&#x3D;\frac{W_{ser}+sW_{par}}{W_{ser}+W_{par}}$；依然引入p记号，则有：$S_{latency}(s)&#x3D;\frac{1-p+sp}{1}&#x3D;1-p+sp$<br>可以看到，当并行执行单元s增多的时候，且足够大的时候，是有可能可以做到接近线性加速比的</h4><h1 id="2-并行化的基本方案"><a href="#2-并行化的基本方案" class="headerlink" title="2. 并行化的基本方案"></a>2. 并行化的基本方案</h1><h2 id="大体上分算子内并行和算子间并行。算子内并行，没太明白……而算子间并行，大体上分两种，一种是模型并行，一种是数据并行。这两个比较主要"><a href="#大体上分算子内并行和算子间并行。算子内并行，没太明白……而算子间并行，大体上分两种，一种是模型并行，一种是数据并行。这两个比较主要" class="headerlink" title="大体上分算子内并行和算子间并行。算子内并行，没太明白……而算子间并行，大体上分两种，一种是模型并行，一种是数据并行。这两个比较主要"></a>大体上分算子内并行和算子间并行。算子内并行，没太明白……<br>而算子间并行，大体上分两种，一种是模型并行，一种是数据并行。这两个比较主要</h2><h2 id="2-1-模型并行"><a href="#2-1-模型并行" class="headerlink" title="2.1. 模型并行"></a>2.1. 模型并行</h2><h3 id="2-1-1-朴素idea："><a href="#2-1-1-朴素idea：" class="headerlink" title="2.1.1. 朴素idea："></a>2.1.1. 朴素idea：</h3><h4 id="简单说，这个并行方式是要把计算图划分，然后放到不同的设备上进行执行。因此需要跨设备传递中间的激活值，以及每个设备都需要用梯度更新它那部分的模型"><a href="#简单说，这个并行方式是要把计算图划分，然后放到不同的设备上进行执行。因此需要跨设备传递中间的激活值，以及每个设备都需要用梯度更新它那部分的模型" class="headerlink" title="简单说，这个并行方式是要把计算图划分，然后放到不同的设备上进行执行。因此需要跨设备传递中间的激活值，以及每个设备都需要用梯度更新它那部分的模型"></a>简单说，这个并行方式是要把计算图划分，然后放到不同的设备上进行执行。因此需要跨设备传递中间的激活值，以及每个设备都需要用梯度更新它那部分的模型</h4><h3 id="2-1-2-朴素idea的问题："><a href="#2-1-2-朴素idea的问题：" class="headerlink" title="2.1.2. 朴素idea的问题："></a>2.1.2. 朴素idea的问题：</h3><h4 id="但如果只是简单地把计算图划分（具体到代码上，一般是把模型的module之类的给划分，比如，可能模型里有两个sequential，第一个sequential放到cuda0上，第二个sequential放到cuda1上，这样），那其实未必能起到加速的效果。因为，给定一个batch，要前向计算，以上面的两个sequential的情况为例，就是要先在cuda0上经过第一个seq，再把这部分的结果放到cuda1上，经过seq2这样，每个时刻，其实只有一个gpu在运行，且我们还多了跨设备传递数据的步骤，很可能不仅没加速，还比原来用单卡慢了"><a href="#但如果只是简单地把计算图划分（具体到代码上，一般是把模型的module之类的给划分，比如，可能模型里有两个sequential，第一个sequential放到cuda0上，第二个sequential放到cuda1上，这样），那其实未必能起到加速的效果。因为，给定一个batch，要前向计算，以上面的两个sequential的情况为例，就是要先在cuda0上经过第一个seq，再把这部分的结果放到cuda1上，经过seq2这样，每个时刻，其实只有一个gpu在运行，且我们还多了跨设备传递数据的步骤，很可能不仅没加速，还比原来用单卡慢了" class="headerlink" title="但如果只是简单地把计算图划分（具体到代码上，一般是把模型的module之类的给划分，比如，可能模型里有两个sequential，第一个sequential放到cuda0上，第二个sequential放到cuda1上，这样），那其实未必能起到加速的效果。因为，给定一个batch，要前向计算，以上面的两个sequential的情况为例，就是要先在cuda0上经过第一个seq，再把这部分的结果放到cuda1上，经过seq2这样，每个时刻，其实只有一个gpu在运行，且我们还多了跨设备传递数据的步骤，很可能不仅没加速，还比原来用单卡慢了"></a>但如果只是简单地把计算图划分（具体到代码上，一般是把模型的module之类的给划分，比如，可能模型里有两个sequential，第一个sequential放到cuda0上，第二个sequential放到cuda1上，这样），那其实未必能起到加速的效果。因为，给定一个batch，要前向计算，以上面的两个sequential的情况为例，就是要先在cuda0上经过第一个seq，再把这部分的结果放到cuda1上，经过seq2<br>这样，每个时刻，其实只有一个gpu在运行，且我们还多了跨设备传递数据的步骤，很可能不仅没加速，还比原来用单卡慢了</h4><h3 id="2-1-3-优化方式："><a href="#2-1-3-优化方式：" class="headerlink" title="2.1.3. 优化方式："></a>2.1.3. 优化方式：</h3><h4 id="因此需要优化。跨设备的搬移数据，不太好优化，但如果能尽量避免设备的空闲，显然是一个极大的改进。因此，考虑流水线。大概的idea是：我们把一个batch再给切分为若干split，之后每个split挨个进入到cuda设备里。用上面的例子进行说明，假如一个batch被我们划分为4个split，则split0先进入cuda0，然后下一个时刻，split0进入cuda1，split1进入cuda0；再下一个时刻，split0算完了，split1进入cuda1，split2进入cuda2……"><a href="#因此需要优化。跨设备的搬移数据，不太好优化，但如果能尽量避免设备的空闲，显然是一个极大的改进。因此，考虑流水线。大概的idea是：我们把一个batch再给切分为若干split，之后每个split挨个进入到cuda设备里。用上面的例子进行说明，假如一个batch被我们划分为4个split，则split0先进入cuda0，然后下一个时刻，split0进入cuda1，split1进入cuda0；再下一个时刻，split0算完了，split1进入cuda1，split2进入cuda2……" class="headerlink" title="因此需要优化。跨设备的搬移数据，不太好优化，但如果能尽量避免设备的空闲，显然是一个极大的改进。因此，考虑流水线。大概的idea是：我们把一个batch再给切分为若干split，之后每个split挨个进入到cuda设备里。用上面的例子进行说明，假如一个batch被我们划分为4个split，则split0先进入cuda0，然后下一个时刻，split0进入cuda1，split1进入cuda0；再下一个时刻，split0算完了，split1进入cuda1，split2进入cuda2……"></a>因此需要优化。跨设备的搬移数据，不太好优化，但如果能尽量避免设备的空闲，显然是一个极大的改进。因此，考虑流水线。大概的idea是：我们把一个batch再给切分为若干split，之后每个split挨个进入到cuda设备里。用上面的例子进行说明，假如一个batch被我们划分为4个split，则split0先进入cuda0，然后下一个时刻，split0进入cuda1，split1进入cuda0；再下一个时刻，split0算完了，split1进入cuda1，split2进入cuda2……</h4><h3 id="上面的朴素的流水线，一般仅在开头结尾的iteration有gpu的空洞（也就是有设备的空闲），而中间大多数时候，两块gpu都能同时运行，效果比用流水线之前的是要好很多的另一个点是，划分split的时候，并非越多越好，因为跨设备移动数据是比较耗时的（但本身划分较多的split的时候，是相当于流水线级数增多了吗？）"><a href="#上面的朴素的流水线，一般仅在开头结尾的iteration有gpu的空洞（也就是有设备的空闲），而中间大多数时候，两块gpu都能同时运行，效果比用流水线之前的是要好很多的另一个点是，划分split的时候，并非越多越好，因为跨设备移动数据是比较耗时的（但本身划分较多的split的时候，是相当于流水线级数增多了吗？）" class="headerlink" title="上面的朴素的流水线，一般仅在开头结尾的iteration有gpu的空洞（也就是有设备的空闲），而中间大多数时候，两块gpu都能同时运行，效果比用流水线之前的是要好很多的另一个点是，划分split的时候，并非越多越好，因为跨设备移动数据是比较耗时的（但本身划分较多的split的时候，是相当于流水线级数增多了吗？）"></a>上面的朴素的流水线，一般仅在开头结尾的iteration有gpu的空洞（也就是有设备的空闲），而中间大多数时候，两块gpu都能同时运行，效果比用流水线之前的是要好很多的<br>另一个点是，划分split的时候，并非越多越好，因为跨设备移动数据是比较耗时的（但本身划分较多的split的时候，是相当于流水线级数增多了吗？）</h3><h3 id="下面再介绍两种具体一点的算法。第一个是Gpipe算法，第二个是PipeDream算法"><a href="#下面再介绍两种具体一点的算法。第一个是Gpipe算法，第二个是PipeDream算法" class="headerlink" title="下面再介绍两种具体一点的算法。第一个是Gpipe算法，第二个是PipeDream算法"></a>下面再介绍两种具体一点的算法。第一个是Gpipe算法，第二个是PipeDream算法</h3><h3 id="2-1-4-Gpipe算法："><a href="#2-1-4-Gpipe算法：" class="headerlink" title="2.1.4. Gpipe算法："></a>2.1.4. Gpipe算法：</h3><h4 id="它的idea是，对batch进行拆分，从而减少设备空闲（也就是所谓Bubble），且是在一个batch的所有前馈都结束之后，才开始反向传播（这样看，它和上面朴素的流水线改进，有什么区别？）"><a href="#它的idea是，对batch进行拆分，从而减少设备空闲（也就是所谓Bubble），且是在一个batch的所有前馈都结束之后，才开始反向传播（这样看，它和上面朴素的流水线改进，有什么区别？）" class="headerlink" title="它的idea是，对batch进行拆分，从而减少设备空闲（也就是所谓Bubble），且是在一个batch的所有前馈都结束之后，才开始反向传播（这样看，它和上面朴素的流水线改进，有什么区别？）"></a>它的idea是，对batch进行拆分，从而减少设备空闲（也就是所谓Bubble），且是在一个batch的所有前馈都结束之后，才开始反向传播（这样看，它和上面朴素的流水线改进，有什么区别？）</h4><h3 id="2-1-5-PipeDream算法"><a href="#2-1-5-PipeDream算法" class="headerlink" title="2.1.5. PipeDream算法"></a>2.1.5. PipeDream算法</h3><h4 id="它相当于在Gpipe算法上作出一些改进。主要是，上面Gpipe算法要等所有的前馈完成了，才一起反向传播（这样本身是有什么好处吗？如果什么好处都没有，显然一开始不会设计成这样啊？），但这样就很浪费设备，因为有时候得等一些split完成前馈，才能反传。因此，PipeDream就是在这方面作出了改进，第一个split的前馈一旦算完，就立刻开始反向传播。且每个split的反向传播，都被设计为占据两个时隙（可以简单理解为两个时间单位），进而人为制造出bubble，从而让后续的前馈运算能在这个间隙里进行，消除bubble（感觉还是没太明白啊？）"><a href="#它相当于在Gpipe算法上作出一些改进。主要是，上面Gpipe算法要等所有的前馈完成了，才一起反向传播（这样本身是有什么好处吗？如果什么好处都没有，显然一开始不会设计成这样啊？），但这样就很浪费设备，因为有时候得等一些split完成前馈，才能反传。因此，PipeDream就是在这方面作出了改进，第一个split的前馈一旦算完，就立刻开始反向传播。且每个split的反向传播，都被设计为占据两个时隙（可以简单理解为两个时间单位），进而人为制造出bubble，从而让后续的前馈运算能在这个间隙里进行，消除bubble（感觉还是没太明白啊？）" class="headerlink" title="它相当于在Gpipe算法上作出一些改进。主要是，上面Gpipe算法要等所有的前馈完成了，才一起反向传播（这样本身是有什么好处吗？如果什么好处都没有，显然一开始不会设计成这样啊？），但这样就很浪费设备，因为有时候得等一些split完成前馈，才能反传。因此，PipeDream就是在这方面作出了改进，第一个split的前馈一旦算完，就立刻开始反向传播。且每个split的反向传播，都被设计为占据两个时隙（可以简单理解为两个时间单位），进而人为制造出bubble，从而让后续的前馈运算能在这个间隙里进行，消除bubble（感觉还是没太明白啊？）"></a>它相当于在Gpipe算法上作出一些改进。主要是，上面Gpipe算法要等所有的前馈完成了，才一起反向传播（这样本身是有什么好处吗？如果什么好处都没有，显然一开始不会设计成这样啊？），但这样就很浪费设备，因为有时候得等一些split完成前馈，才能反传。因此，PipeDream就是在这方面作出了改进，第一个split的前馈一旦算完，就立刻开始反向传播。且每个split的反向传播，都被设计为占据两个时隙（可以简单理解为两个时间单位），进而人为制造出bubble，从而让后续的前馈运算能在这个间隙里进行，消除bubble（感觉还是没太明白啊？）</h4><h2 id="2-2-数据并行"><a href="#2-2-数据并行" class="headerlink" title="2.2. 数据并行"></a>2.2. 数据并行</h2><h3 id="2-2-1-朴素idea："><a href="#2-2-1-朴素idea：" class="headerlink" title="2.2.1. 朴素idea："></a>2.2.1. 朴素idea：</h3><h4 id="所谓数据并行，就是，不同设备上，执行相同的计算图（可以理解为，我们不切分模型了，每个设备上都放一个完整的模型。然后每个设备上都可以各自算一批数据，完成前向和后向，接着就是跨设备的聚合梯度，并利用聚合后的梯度来更新模型）（但可以看到，这个并行方式其实是适用于“单个模型可以放在单卡上”的情况的，如果模型太大了，导致放不下，其实这个并行方式就不奏效了，可能得两种并行方式结合起来）"><a href="#所谓数据并行，就是，不同设备上，执行相同的计算图（可以理解为，我们不切分模型了，每个设备上都放一个完整的模型。然后每个设备上都可以各自算一批数据，完成前向和后向，接着就是跨设备的聚合梯度，并利用聚合后的梯度来更新模型）（但可以看到，这个并行方式其实是适用于“单个模型可以放在单卡上”的情况的，如果模型太大了，导致放不下，其实这个并行方式就不奏效了，可能得两种并行方式结合起来）" class="headerlink" title="所谓数据并行，就是，不同设备上，执行相同的计算图（可以理解为，我们不切分模型了，每个设备上都放一个完整的模型。然后每个设备上都可以各自算一批数据，完成前向和后向，接着就是跨设备的聚合梯度，并利用聚合后的梯度来更新模型）（但可以看到，这个并行方式其实是适用于“单个模型可以放在单卡上”的情况的，如果模型太大了，导致放不下，其实这个并行方式就不奏效了，可能得两种并行方式结合起来）"></a>所谓数据并行，就是，不同设备上，执行相同的计算图（可以理解为，我们不切分模型了，每个设备上都放一个完整的模型。然后每个设备上都可以各自算一批数据，完成前向和后向，接着就是跨设备的聚合梯度，并利用聚合后的梯度来更新模型）（但可以看到，这个并行方式其实是适用于“单个模型可以放在单卡上”的情况的，如果模型太大了，导致放不下，其实这个并行方式就不奏效了，可能得两种并行方式结合起来）</h4><h3 id="2-2-2-集合通信（collective-communication）中的通信原语（primitive）"><a href="#2-2-2-集合通信（collective-communication）中的通信原语（primitive）" class="headerlink" title="2.2.2. 集合通信（collective communication）中的通信原语（primitive）"></a>2.2.2. 集合通信（collective communication）中的通信原语（primitive）</h3><h4 id="集合通信，可以简单理解为，设备间数据的传输（因为此时每个设备上都有各自的前向和后向，所以需要聚合梯度，也就是需要传递数据）。而通信原语，可以简单理解为一些基本的传输操作"><a href="#集合通信，可以简单理解为，设备间数据的传输（因为此时每个设备上都有各自的前向和后向，所以需要聚合梯度，也就是需要传递数据）。而通信原语，可以简单理解为一些基本的传输操作" class="headerlink" title="集合通信，可以简单理解为，设备间数据的传输（因为此时每个设备上都有各自的前向和后向，所以需要聚合梯度，也就是需要传递数据）。而通信原语，可以简单理解为一些基本的传输操作"></a>集合通信，可以简单理解为，设备间数据的传输（因为此时每个设备上都有各自的前向和后向，所以需要聚合梯度，也就是需要传递数据）。而通信原语，可以简单理解为一些基本的传输操作</h4><h4 id="通信原语，主要分三种：一对多，多对一，多对多"><a href="#通信原语，主要分三种：一对多，多对一，多对多" class="headerlink" title="通信原语，主要分三种：一对多，多对一，多对多"></a>通信原语，主要分三种：一对多，多对一，多对多</h4><h4 id="一对多："><a href="#一对多：" class="headerlink" title="一对多："></a>一对多：</h4><h5 id="主要有Scatter和Broadcast"><a href="#主要有Scatter和Broadcast" class="headerlink" title="主要有Scatter和Broadcast"></a>主要有Scatter和Broadcast</h5><h5 id="Scatter：它就是把主节点的数据进行划分，并分发到其它的指定的节点（节点，可以简单理解成设备，也就是一张gpu。但不同语境下应该有不同的理解）"><a href="#Scatter：它就是把主节点的数据进行划分，并分发到其它的指定的节点（节点，可以简单理解成设备，也就是一张gpu。但不同语境下应该有不同的理解）" class="headerlink" title="Scatter：它就是把主节点的数据进行划分，并分发到其它的指定的节点（节点，可以简单理解成设备，也就是一张gpu。但不同语境下应该有不同的理解）"></a>Scatter：它就是把主节点的数据进行划分，并分发到其它的指定的节点（节点，可以简单理解成设备，也就是一张gpu。但不同语境下应该有不同的理解）</h5><h5 id="Broadcast：它是某个节点，把自身的数据，发送到集群里的其它节点上（注意，这就是Broadcast和Scatter的区别了，Broadcast发送的是自己的数据，而Scatter是把大的数据给拆散，分发到各个设备上）（一般Broadcast操作用于在分布式机器学习中的参数初始化）"><a href="#Broadcast：它是某个节点，把自身的数据，发送到集群里的其它节点上（注意，这就是Broadcast和Scatter的区别了，Broadcast发送的是自己的数据，而Scatter是把大的数据给拆散，分发到各个设备上）（一般Broadcast操作用于在分布式机器学习中的参数初始化）" class="headerlink" title="Broadcast：它是某个节点，把自身的数据，发送到集群里的其它节点上（注意，这就是Broadcast和Scatter的区别了，Broadcast发送的是自己的数据，而Scatter是把大的数据给拆散，分发到各个设备上）（一般Broadcast操作用于在分布式机器学习中的参数初始化）"></a>Broadcast：它是某个节点，把自身的数据，发送到集群里的其它节点上（注意，这就是Broadcast和Scatter的区别了，Broadcast发送的是自己的数据，而Scatter是把大的数据给拆散，分发到各个设备上）（一般Broadcast操作用于在分布式机器学习中的参数初始化）</h5><h4 id="多对一："><a href="#多对一：" class="headerlink" title="多对一："></a>多对一：</h4><h5 id="主要有Reduce和Gather"><a href="#主要有Reduce和Gather" class="headerlink" title="主要有Reduce和Gather"></a>主要有Reduce和Gather</h5><h5 id="Reduce：被称为规约于是暖，是一系列简单运算操作的统称。细分有很多，包括：Sum、Min、Max等。这里不妨以Sum为例，理解一下：例如现在四个设备上各自有一个二维向量，然后要求把这些向量给累加起来，那Reduce操作就可以把四个设备上的二维向量聚集到一个设备上（比如gpu0），然后相加，从而在这个指定的设备上得到结果"><a href="#Reduce：被称为规约于是暖，是一系列简单运算操作的统称。细分有很多，包括：Sum、Min、Max等。这里不妨以Sum为例，理解一下：例如现在四个设备上各自有一个二维向量，然后要求把这些向量给累加起来，那Reduce操作就可以把四个设备上的二维向量聚集到一个设备上（比如gpu0），然后相加，从而在这个指定的设备上得到结果" class="headerlink" title="Reduce：被称为规约于是暖，是一系列简单运算操作的统称。细分有很多，包括：Sum、Min、Max等。这里不妨以Sum为例，理解一下：例如现在四个设备上各自有一个二维向量，然后要求把这些向量给累加起来，那Reduce操作就可以把四个设备上的二维向量聚集到一个设备上（比如gpu0），然后相加，从而在这个指定的设备上得到结果"></a>Reduce：被称为规约于是暖，是一系列简单运算操作的统称。细分有很多，包括：Sum、Min、Max等。这里不妨以Sum为例，理解一下：例如现在四个设备上各自有一个二维向量，然后要求把这些向量给累加起来，那Reduce操作就可以把四个设备上的二维向量聚集到一个设备上（比如gpu0），然后相加，从而在这个指定的设备上得到结果</h5><h5 id="Gather：它是指，把多个节点上的数据收集到单个节点上（注意，这里和上面的Reduce的不同在于，Reduce收集完了是要做一些操作的，但这里Gather只是单纯的收集起来。它可以理解为反向的Scatter）"><a href="#Gather：它是指，把多个节点上的数据收集到单个节点上（注意，这里和上面的Reduce的不同在于，Reduce收集完了是要做一些操作的，但这里Gather只是单纯的收集起来。它可以理解为反向的Scatter）" class="headerlink" title="Gather：它是指，把多个节点上的数据收集到单个节点上（注意，这里和上面的Reduce的不同在于，Reduce收集完了是要做一些操作的，但这里Gather只是单纯的收集起来。它可以理解为反向的Scatter）"></a>Gather：它是指，把多个节点上的数据收集到单个节点上（注意，这里和上面的Reduce的不同在于，Reduce收集完了是要做一些操作的，但这里Gather只是单纯的收集起来。它可以理解为反向的Scatter）</h5><h4 id="多对多："><a href="#多对多：" class="headerlink" title="多对多："></a>多对多：</h4><h5 id="主要有All-Gather和All-Reduce"><a href="#主要有All-Gather和All-Reduce" class="headerlink" title="主要有All-Gather和All-Reduce"></a>主要有All-Gather和All-Reduce</h5><h5 id="All-Gather：可以理解为Gather-Broadcast。也就是，先把各个节点的数据收集起来，放到一个节点上，然后再从这个节点出发，进行Broadcast"><a href="#All-Gather：可以理解为Gather-Broadcast。也就是，先把各个节点的数据收集起来，放到一个节点上，然后再从这个节点出发，进行Broadcast" class="headerlink" title="All-Gather：可以理解为Gather+Broadcast。也就是，先把各个节点的数据收集起来，放到一个节点上，然后再从这个节点出发，进行Broadcast"></a>All-Gather：可以理解为Gather+Broadcast。也就是，先把各个节点的数据收集起来，放到一个节点上，然后再从这个节点出发，进行Broadcast</h5><h5 id="All-Reduce：它的目的是，要在所有的节点上都进行相同的Reduce操作。因此，我们可以看成是Reduce-Broadcast（这样，在一个节点上，我们会先得到Reduce的结果，之后再从这个节点，进行Broadcast，从而每个节点都能得到Reduce后的操作）"><a href="#All-Reduce：它的目的是，要在所有的节点上都进行相同的Reduce操作。因此，我们可以看成是Reduce-Broadcast（这样，在一个节点上，我们会先得到Reduce的结果，之后再从这个节点，进行Broadcast，从而每个节点都能得到Reduce后的操作）" class="headerlink" title="All-Reduce：它的目的是，要在所有的节点上都进行相同的Reduce操作。因此，我们可以看成是Reduce+Broadcast（这样，在一个节点上，我们会先得到Reduce的结果，之后再从这个节点，进行Broadcast，从而每个节点都能得到Reduce后的操作）"></a>All-Reduce：它的目的是，要在所有的节点上都进行相同的Reduce操作。因此，我们可以看成是Reduce+Broadcast（这样，在一个节点上，我们会先得到Reduce的结果，之后再从这个节点，进行Broadcast，从而每个节点都能得到Reduce后的操作）</h5><h6 id="All-Reduce的基础是点对点通信，点对点通信又可以分为同步的和异步的通信（先略。简单说，同步的就是得等接收端那里开始接受数据了，发送端才能结束；异步则不需要这么麻烦，发送完数据，发送端就结束了）"><a href="#All-Reduce的基础是点对点通信，点对点通信又可以分为同步的和异步的通信（先略。简单说，同步的就是得等接收端那里开始接受数据了，发送端才能结束；异步则不需要这么麻烦，发送完数据，发送端就结束了）" class="headerlink" title="All-Reduce的基础是点对点通信，点对点通信又可以分为同步的和异步的通信（先略。简单说，同步的就是得等接收端那里开始接受数据了，发送端才能结束；异步则不需要这么麻烦，发送完数据，发送端就结束了）"></a>All-Reduce的基础是点对点通信，点对点通信又可以分为同步的和异步的通信（先略。简单说，同步的就是得等接收端那里开始接受数据了，发送端才能结束；异步则不需要这么麻烦，发送完数据，发送端就结束了）</h6><h6 id="下面再探讨All-Reduce的实现算法（这个感觉有点重点啊）"><a href="#下面再探讨All-Reduce的实现算法（这个感觉有点重点啊）" class="headerlink" title="下面再探讨All-Reduce的实现算法（这个感觉有点重点啊）"></a>下面再探讨All-Reduce的实现算法（这个感觉有点重点啊）</h6><h6 id="1-Reduce-Broadcast也就是最朴素的实现。它一般是采用一个参数服务器的分布式架构，也就是会有一个节点充当服务器（或者说中心节点），它会先存放Reduce的结果，经过本地计算后，再Broadcast到各个其它节点上如果我们给定如下的记号：-alpha-表示两个通信节点之间的时延latency，S表示数据块大小（Size），B表示两个通信节点之间的带宽（Bandwidth），C表示每字节数据的计算耗时（Computation），N表示节点个数（Number），则总体的耗时为-2-alpha-frac-S-B-NSC-现在我们看一下这个式子，前半的部分表示数据传递上的耗时。节点之间的通信，发出数据需要花时间，接收数据也要花时间，所以有个因子2；而-alpha-是时延，可以简单理解为硬性的时间开销；-frac-S-B-则表示，大小为S的数据，发送要多久（以及接收要多久）；后半部分则是总共N个节点，它们都把数据传到中心节点上，一共是-N-S-的数据量，再乘以C，则得到在计算方面的时间开销这个算法的最大缺点就是，parameter-server的那个节点，它的带宽会成为瓶颈（因为这个节点，它一开始reduce的时候，就得从其它节点那里接收很多数据，这里就得耗时了；之后它又得进行broadcast，要往其它节点那里发送很多数据，也很耗时）（怎么越看越感觉这个公式有点问题？）"><a href="#1-Reduce-Broadcast也就是最朴素的实现。它一般是采用一个参数服务器的分布式架构，也就是会有一个节点充当服务器（或者说中心节点），它会先存放Reduce的结果，经过本地计算后，再Broadcast到各个其它节点上如果我们给定如下的记号：-alpha-表示两个通信节点之间的时延latency，S表示数据块大小（Size），B表示两个通信节点之间的带宽（Bandwidth），C表示每字节数据的计算耗时（Computation），N表示节点个数（Number），则总体的耗时为-2-alpha-frac-S-B-NSC-现在我们看一下这个式子，前半的部分表示数据传递上的耗时。节点之间的通信，发出数据需要花时间，接收数据也要花时间，所以有个因子2；而-alpha-是时延，可以简单理解为硬性的时间开销；-frac-S-B-则表示，大小为S的数据，发送要多久（以及接收要多久）；后半部分则是总共N个节点，它们都把数据传到中心节点上，一共是-N-S-的数据量，再乘以C，则得到在计算方面的时间开销这个算法的最大缺点就是，parameter-server的那个节点，它的带宽会成为瓶颈（因为这个节点，它一开始reduce的时候，就得从其它节点那里接收很多数据，这里就得耗时了；之后它又得进行broadcast，要往其它节点那里发送很多数据，也很耗时）（怎么越看越感觉这个公式有点问题？）" class="headerlink" title="1. Reduce+Broadcast也就是最朴素的实现。它一般是采用一个参数服务器的分布式架构，也就是会有一个节点充当服务器（或者说中心节点），它会先存放Reduce的结果，经过本地计算后，再Broadcast到各个其它节点上如果我们给定如下的记号：$\alpha$表示两个通信节点之间的时延latency，S表示数据块大小（Size），B表示两个通信节点之间的带宽（Bandwidth），C表示每字节数据的计算耗时（Computation），N表示节点个数（Number），则总体的耗时为$$2*(\alpha+\frac{S}{B})+NSC$$现在我们看一下这个式子，前半的部分表示数据传递上的耗时。节点之间的通信，发出数据需要花时间，接收数据也要花时间，所以有个因子2；而$\alpha$是时延，可以简单理解为硬性的时间开销；$\frac{S}{B}$则表示，大小为S的数据，发送要多久（以及接收要多久）；后半部分则是总共N个节点，它们都把数据传到中心节点上，一共是$N*S$的数据量，再乘以C，则得到在计算方面的时间开销这个算法的最大缺点就是，parameter server的那个节点，它的带宽会成为瓶颈（因为这个节点，它一开始reduce的时候，就得从其它节点那里接收很多数据，这里就得耗时了；之后它又得进行broadcast，要往其它节点那里发送很多数据，也很耗时）（怎么越看越感觉这个公式有点问题？）"></a>1. Reduce+Broadcast<br>也就是最朴素的实现。它一般是采用一个参数服务器的分布式架构，也就是会有一个节点充当服务器（或者说中心节点），它会先存放Reduce的结果，经过本地计算后，再Broadcast到各个其它节点上<br>如果我们给定如下的记号：$\alpha$表示两个通信节点之间的时延latency，S表示数据块大小（Size），B表示两个通信节点之间的带宽（Bandwidth），C表示每字节数据的计算耗时（Computation），N表示节点个数（Number），则总体的耗时为$$2*(\alpha+\frac{S}{B})+N<em>S</em>C$$现在我们看一下这个式子，前半的部分表示数据传递上的耗时。节点之间的通信，发出数据需要花时间，接收数据也要花时间，所以有个因子2；而$\alpha$是时延，可以简单理解为硬性的时间开销；$\frac{S}{B}$则表示，大小为S的数据，发送要多久（以及接收要多久）；后半部分则是总共N个节点，它们都把数据传到中心节点上，一共是$N*S$的数据量，再乘以C，则得到在计算方面的时间开销<br>这个算法的最大缺点就是，parameter server的那个节点，它的带宽会成为瓶颈（因为这个节点，它一开始reduce的时候，就得从其它节点那里接收很多数据，这里就得耗时了；之后它又得进行broadcast，要往其它节点那里发送很多数据，也很耗时）（怎么越看越感觉这个公式有点问题？）</h6><h6 id="2-树形递归算法其整体耗时为-log-2-N-alpha-frac-S-B-2SC-log-2-N-alpha-frac-S-B-2log-2-N-alpha-frac-S-B-SC-其中，-log-2-N-可以认为是“层数”，因为我们是两两节点组合到一起；前半部分的时间是聚合数据以及运算数据的时间开销。每一层虽然有很多节点在聚合和计算，但它们都是独立的，互不影响，可以认为是同时进行的，所以每一层的聚合所花的时间都是-alpha-frac-S-B-；而每一层的计算时间开销都是-2BC-（这里有个2，是因为，我们两两组合节点来计算，则需要计算的数据量是-2S-，再乘上计算的“速度“C，即得到计算开销）至于后半部分，则是把结果分发出去的时间开销。可以看到，分发和聚合的时间开销是一样的，且因为后半部分已经算完了，所以不再有计算上的开销"><a href="#2-树形递归算法其整体耗时为-log-2-N-alpha-frac-S-B-2SC-log-2-N-alpha-frac-S-B-2log-2-N-alpha-frac-S-B-SC-其中，-log-2-N-可以认为是“层数”，因为我们是两两节点组合到一起；前半部分的时间是聚合数据以及运算数据的时间开销。每一层虽然有很多节点在聚合和计算，但它们都是独立的，互不影响，可以认为是同时进行的，所以每一层的聚合所花的时间都是-alpha-frac-S-B-；而每一层的计算时间开销都是-2BC-（这里有个2，是因为，我们两两组合节点来计算，则需要计算的数据量是-2S-，再乘上计算的“速度“C，即得到计算开销）至于后半部分，则是把结果分发出去的时间开销。可以看到，分发和聚合的时间开销是一样的，且因为后半部分已经算完了，所以不再有计算上的开销" class="headerlink" title="2. 树形递归算法其整体耗时为$$log_{2}N*(\alpha+\frac{S}{B}+2SC)+log_{2}N*(\alpha+\frac{S}{B})&#x3D;2log_{2}N(\alpha+\frac{S}{B}+SC)$$其中，$log_{2}N$可以认为是“层数”，因为我们是两两节点组合到一起；前半部分的时间是聚合数据以及运算数据的时间开销。每一层虽然有很多节点在聚合和计算，但它们都是独立的，互不影响，可以认为是同时进行的，所以每一层的聚合所花的时间都是$\alpha+\frac{S}{B}$；而每一层的计算时间开销都是$2BC$（这里有个2，是因为，我们两两组合节点来计算，则需要计算的数据量是$2S$，再乘上计算的“速度“C，即得到计算开销）至于后半部分，则是把结果分发出去的时间开销。可以看到，分发和聚合的时间开销是一样的，且因为后半部分已经算完了，所以不再有计算上的开销"></a>2. 树形递归算法<br>其整体耗时为$$log_{2}N*(\alpha+\frac{S}{B}+2<em>S</em>C)+log_{2}N*(\alpha+\frac{S}{B})&#x3D;2<em>log_{2}N</em>(\alpha+\frac{S}{B}+S<em>C)$$其中，$log_{2}N$可以认为是“层数”，因为我们是两两节点组合到一起；前半部分的时间是聚合数据以及运算数据的时间开销。每一层虽然有很多节点在聚合和计算，但它们都是独立的，互不影响，可以认为是同时进行的，所以每一层的聚合所花的时间都是$\alpha+\frac{S}{B}$；而每一层的计算时间开销都是$2</em>B<em>C$（这里有个2，是因为，我们两两组合节点来计算，则需要计算的数据量是$2</em>S$，再乘上计算的“速度“C，即得到计算开销）<br>至于后半部分，则是把结果分发出去的时间开销。可以看到，分发和聚合的时间开销是一样的，且因为后半部分已经算完了，所以不再有计算上的开销</h6><h6 id="上面两种方法都是基于Reduce-Broadcast的。还有一类是基于Scatter-Reduce-All-Gather的操作的，这里以Ring-AllReduce为例"><a href="#上面两种方法都是基于Reduce-Broadcast的。还有一类是基于Scatter-Reduce-All-Gather的操作的，这里以Ring-AllReduce为例" class="headerlink" title="上面两种方法都是基于Reduce+Broadcast的。还有一类是基于Scatter-Reduce+All-Gather的操作的，这里以Ring AllReduce为例"></a>上面两种方法都是基于Reduce+Broadcast的。还有一类是基于Scatter-Reduce+All-Gather的操作的，这里以Ring AllReduce为例</h6><h6 id="3-Ring-AllReduce既然它是基于Scatter-Reduce-All-Gather的，那么它的第一步就是Scatter-Reduce具体来说，每个节点（也就是每张卡，或称为每个worker）上的数据量，我们记作K；然后，我们对每个节点上的数据，都划分为N个chunk（N也是节点的个数）；之后，我们可以给每个chunk一个编号，则有0-N-1。现在，对于chunk-0，我们的第i个gpu，它会向下一个gpu传递它的chunk-i的数据（对于gpu-0，它的上一个gpu，我们当成gpu-N-1）；之后，接收到chunk数据的gpu，就继续把它接收到的chunk数据整合，往下一个gpu的相同chunk上传。这样一直传，传N-1次，则N个gpu上，各自都会有一个chunk上聚合完的数据（且每个gpu只会有一个这种聚合完的chunk）接下来是All-gather。All-gather时，每个gpu都往下一个gpu传递自己的聚合完的那个chunk，且gpu接收了哪个新的chunk之后，也继续往下传递。这样传递N-1次，每个gpu的每个chunk，都是聚合完的。至此，Ring-AllReduce结束分析一下它的耗时。我们可以先分析一下每个worker总共接收-传递了多少的数据量。Scatter-Reduce和All-Gather阶段，每个gpu都是接收-传递了N-1次的，则两个阶段，总共次数是-2-N-1-；且每次都是接收-传递一个chunk的数据量，也就是-frac-K-N-；故每个worker接收-传递的数据总量应为-2-N-1-frac-K-N-我们观察一下这个结果，发现，每个worker的通信数据量（也就是上面反复提到的，接收-传递的数据量）可以近似为-2K-（尤其是N较大的时候）；也就是说，这个通信数据量是近似独立于网络中worker的数量的。这代表什么？这代表即使我们的模型、数据规模越来越大，用到的worker越来越多，但通信上的开销并不会剧增，而是基本差不多（但可能还有一些细节没考虑到吧）；相比之下，上面的朴素做法（例如主从架构），里面的主节点，它的通信量就是-O-N-K-级别的且，每个worker的网络收发负载是均衡的（每个时刻，每个worker既会接收chunk，也会分发chunk，不会出现树形递归算法里，有些worker在傻等着进行分发），网络双向带宽得到充分利用（欸，这么说，接收和分发的带宽，原来不是同一个吗？）"><a href="#3-Ring-AllReduce既然它是基于Scatter-Reduce-All-Gather的，那么它的第一步就是Scatter-Reduce具体来说，每个节点（也就是每张卡，或称为每个worker）上的数据量，我们记作K；然后，我们对每个节点上的数据，都划分为N个chunk（N也是节点的个数）；之后，我们可以给每个chunk一个编号，则有0-N-1。现在，对于chunk-0，我们的第i个gpu，它会向下一个gpu传递它的chunk-i的数据（对于gpu-0，它的上一个gpu，我们当成gpu-N-1）；之后，接收到chunk数据的gpu，就继续把它接收到的chunk数据整合，往下一个gpu的相同chunk上传。这样一直传，传N-1次，则N个gpu上，各自都会有一个chunk上聚合完的数据（且每个gpu只会有一个这种聚合完的chunk）接下来是All-gather。All-gather时，每个gpu都往下一个gpu传递自己的聚合完的那个chunk，且gpu接收了哪个新的chunk之后，也继续往下传递。这样传递N-1次，每个gpu的每个chunk，都是聚合完的。至此，Ring-AllReduce结束分析一下它的耗时。我们可以先分析一下每个worker总共接收-传递了多少的数据量。Scatter-Reduce和All-Gather阶段，每个gpu都是接收-传递了N-1次的，则两个阶段，总共次数是-2-N-1-；且每次都是接收-传递一个chunk的数据量，也就是-frac-K-N-；故每个worker接收-传递的数据总量应为-2-N-1-frac-K-N-我们观察一下这个结果，发现，每个worker的通信数据量（也就是上面反复提到的，接收-传递的数据量）可以近似为-2K-（尤其是N较大的时候）；也就是说，这个通信数据量是近似独立于网络中worker的数量的。这代表什么？这代表即使我们的模型、数据规模越来越大，用到的worker越来越多，但通信上的开销并不会剧增，而是基本差不多（但可能还有一些细节没考虑到吧）；相比之下，上面的朴素做法（例如主从架构），里面的主节点，它的通信量就是-O-N-K-级别的且，每个worker的网络收发负载是均衡的（每个时刻，每个worker既会接收chunk，也会分发chunk，不会出现树形递归算法里，有些worker在傻等着进行分发），网络双向带宽得到充分利用（欸，这么说，接收和分发的带宽，原来不是同一个吗？）" class="headerlink" title="3. Ring AllReduce既然它是基于Scatter-Reduce+All-Gather的，那么它的第一步就是Scatter-Reduce具体来说，每个节点（也就是每张卡，或称为每个worker）上的数据量，我们记作K；然后，我们对每个节点上的数据，都划分为N个chunk（N也是节点的个数）；之后，我们可以给每个chunk一个编号，则有0 ~ N-1。现在，对于chunk 0，我们的第i个gpu，它会向下一个gpu传递它的chunk i的数据（对于gpu 0，它的上一个gpu，我们当成gpu N-1）；之后，接收到chunk数据的gpu，就继续把它接收到的chunk数据整合，往下一个gpu的相同chunk上传。这样一直传，传N-1次，则N个gpu上，各自都会有一个chunk上聚合完的数据（且每个gpu只会有一个这种聚合完的chunk）接下来是All gather。All gather时，每个gpu都往下一个gpu传递自己的聚合完的那个chunk，且gpu接收了哪个新的chunk之后，也继续往下传递。这样传递N-1次，每个gpu的每个chunk，都是聚合完的。至此，Ring AllReduce结束分析一下它的耗时。我们可以先分析一下每个worker总共接收&#x2F;传递了多少的数据量。Scatter-Reduce和All Gather阶段，每个gpu都是接收&#x2F;传递了N-1次的，则两个阶段，总共次数是$2*(N-1)$；且每次都是接收&#x2F;传递一个chunk的数据量，也就是$\frac{K}{N}$；故每个worker接收&#x2F;传递的数据总量应为$2*(N-1)\frac{K}{N}$我们观察一下这个结果，发现，每个worker的通信数据量（也就是上面反复提到的，接收&#x2F;传递的数据量）可以近似为$2K$（尤其是N较大的时候）；也就是说，这个通信数据量是近似独立于网络中worker的数量的。这代表什么？这代表即使我们的模型、数据规模越来越大，用到的worker越来越多，但通信上的开销并不会剧增，而是基本差不多（但可能还有一些细节没考虑到吧）；相比之下，上面的朴素做法（例如主从架构），里面的主节点，它的通信量就是$O(N*K)$级别的且，每个worker的网络收发负载是均衡的（每个时刻，每个worker既会接收chunk，也会分发chunk，不会出现树形递归算法里，有些worker在傻等着进行分发），网络双向带宽得到充分利用（欸，这么说，接收和分发的带宽，原来不是同一个吗？）"></a>3. Ring AllReduce<br>既然它是基于Scatter-Reduce+All-Gather的，那么它的第一步就是Scatter-Reduce<br>具体来说，每个节点（也就是每张卡，或称为每个worker）上的数据量，我们记作K；然后，我们对每个节点上的数据，都划分为N个chunk（N也是节点的个数）；之后，我们可以给每个chunk一个编号，则有0 ~ N-1。现在，对于chunk 0，我们的第i个gpu，它会向下一个gpu传递它的chunk i的数据（对于gpu 0，它的上一个gpu，我们当成gpu N-1）；之后，接收到chunk数据的gpu，就继续把它接收到的chunk数据整合，往下一个gpu的相同chunk上传。这样一直传，传N-1次，则N个gpu上，各自都会有一个chunk上聚合完的数据（且每个gpu只会有一个这种聚合完的chunk）<br>接下来是All gather。All gather时，每个gpu都往下一个gpu传递自己的聚合完的那个chunk，且gpu接收了哪个新的chunk之后，也继续往下传递。这样传递N-1次，每个gpu的每个chunk，都是聚合完的。至此，Ring AllReduce结束<br>分析一下它的耗时。我们可以先分析一下每个worker总共接收&#x2F;传递了多少的数据量。Scatter-Reduce和All Gather阶段，每个gpu都是接收&#x2F;传递了N-1次的，则两个阶段，总共次数是$2*(N-1)$；且每次都是接收&#x2F;传递一个chunk的数据量，也就是$\frac{K}{N}$；故每个worker接收&#x2F;传递的数据总量应为$2*(N-1)<em>\frac{K}{N}$<br>我们观察一下这个结果，发现，每个worker的通信数据量（也就是上面反复提到的，接收&#x2F;传递的数据量）可以近似为$2</em>K$（尤其是N较大的时候）；也就是说，这个通信数据量是近似独立于网络中worker的数量的。这代表什么？这代表即使我们的模型、数据规模越来越大，用到的worker越来越多，但通信上的开销并不会剧增，而是基本差不多（但可能还有一些细节没考虑到吧）；相比之下，上面的朴素做法（例如主从架构），里面的主节点，它的通信量就是$O(N*K)$级别的<br>且，每个worker的网络收发负载是均衡的（每个时刻，每个worker既会接收chunk，也会分发chunk，不会出现树形递归算法里，有些worker在傻等着进行分发），网络双向带宽得到充分利用（欸，这么说，接收和分发的带宽，原来不是同一个吗？）</h6><h3 id="2-2-3-集合通信原语的应用：分布式同步SGD算法"><a href="#2-2-3-集合通信原语的应用：分布式同步SGD算法" class="headerlink" title="2.2.3. 集合通信原语的应用：分布式同步SGD算法"></a>2.2.3. 集合通信原语的应用：分布式同步SGD算法</h3><h4 id="流程倒也简单。每个节点先把自己的梯度初始化为0，然后拿batch-size个数据进行前向-后向，把各自节点上的梯度求个均值（这里可能会用动量式更新地方方法，也即：-G-t-k-leftarrow-G-t-k-frac-1-N-b-nabla-f-x-w-t-）之后，对各个节点的平均梯度，进行All-Reduce（这里就是我们需要通信的地方了，因为我们要传递各个节点上的梯度），得到一个总的梯度，并拿来更新参数，由此，一个iteration结束。后面就是重复这个iteration的操作，直到达到指定的iteration"><a href="#流程倒也简单。每个节点先把自己的梯度初始化为0，然后拿batch-size个数据进行前向-后向，把各自节点上的梯度求个均值（这里可能会用动量式更新地方方法，也即：-G-t-k-leftarrow-G-t-k-frac-1-N-b-nabla-f-x-w-t-）之后，对各个节点的平均梯度，进行All-Reduce（这里就是我们需要通信的地方了，因为我们要传递各个节点上的梯度），得到一个总的梯度，并拿来更新参数，由此，一个iteration结束。后面就是重复这个iteration的操作，直到达到指定的iteration" class="headerlink" title="流程倒也简单。每个节点先把自己的梯度初始化为0，然后拿batch_size个数据进行前向+后向，把各自节点上的梯度求个均值（这里可能会用动量式更新地方方法，也即：$G_{t}^{k}\leftarrow G_{t}^{k}+\frac{1}{N_{b}}\nabla f(x;w_{t})$）之后，对各个节点的平均梯度，进行All-Reduce（这里就是我们需要通信的地方了，因为我们要传递各个节点上的梯度），得到一个总的梯度，并拿来更新参数，由此，一个iteration结束。后面就是重复这个iteration的操作，直到达到指定的iteration"></a>流程倒也简单。每个节点先把自己的梯度初始化为0，然后拿batch_size个数据进行前向+后向，把各自节点上的梯度求个均值（这里可能会用动量式更新地方方法，也即：$G_{t}^{k}\leftarrow G_{t}^{k}+\frac{1}{N_{b}}\nabla f(x;w_{t})$）<br>之后，对各个节点的平均梯度，进行All-Reduce（这里就是我们需要通信的地方了，因为我们要传递各个节点上的梯度），得到一个总的梯度，并拿来更新参数，由此，一个iteration结束。后面就是重复这个iteration的操作，直到达到指定的iteration</h4>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/05/23/algo/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%A2%98/%E5%A0%86%E3%80%81%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97/%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E7%AC%ACK%E4%B8%AA%E6%9C%80%E5%A4%A7%E5%85%83%E7%B4%A0/" rel="prev" title="数组中的第K个最大元素">
      <i class="fa fa-chevron-left"></i> 数组中的第K个最大元素
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/05/26/mlsys_foundation/chap2.Pytorch%E5%9F%BA%E7%A1%80/" rel="next" title="chap2.Pytorch基础">
      chap2.Pytorch基础 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#title-chap6-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E2%80%94%E2%80%94%E7%AE%97%E6%B3%95%E5%8F%8A%E7%B3%BB%E7%BB%9Fdate-2025-05-26-15-51tags-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-categories-mlsys-foundation"><span class="nav-number">1.1.</span> <span class="nav-text">title: chap6.深度学习中的分布式训练——算法及系统date: 2025-05-26 15:51tags: [分布式训练]categories: [mlsys, foundation]</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7"><span class="nav-number">2.</span> <span class="nav-text">1. 分布式训练的必要性</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E7%9D%80%E9%97%AE%E9%A2%98%E8%A7%84%E6%A8%A1%E7%9A%84%E5%A2%9E%E5%A4%A7%EF%BC%88%E5%8C%85%E6%8B%AC%E6%95%B0%E6%8D%AE%E9%87%8F%E5%92%8C%E8%AE%A1%E7%AE%97%E9%87%8F%E5%B1%82%E9%9D%A2%E7%9A%84%EF%BC%89%EF%BC%8C%E5%8D%95%E4%B8%80%E8%AE%BE%E5%A4%87%E5%A4%84%E7%90%86%E9%80%9F%E5%BA%A6%E9%80%90%E6%B8%90%E6%97%A0%E6%B3%95%E6%BB%A1%E8%B6%B3%E6%88%91%E4%BB%AC%E7%9A%84%E9%9C%80%E6%B1%82%EF%BC%88%E6%AF%94%E5%A6%82%E5%A4%AA%E6%85%A2%E4%BA%86%EF%BC%8C%E6%88%96%E8%80%85%E6%98%AF%E5%B9%B2%E8%84%86%E5%8D%95%E8%AE%BE%E5%A4%87%E6%94%BE%E4%B8%8D%E4%B8%8B%E6%A8%A1%E5%9E%8B%E4%BA%86%EF%BC%89%E8%80%8C%E7%94%A8%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%EF%BC%8C%E6%8A%8A%E9%97%AE%E9%A2%98%E5%88%92%E5%88%86%E4%B8%BA%E8%8B%A5%E5%B9%B2%E5%B0%8F%E9%97%AE%E9%A2%98%E5%90%8E%EF%BC%8C%E5%86%8D%E7%BB%8F%E8%BF%87%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%9D%A5%E6%89%A7%E8%A1%8C%E5%A4%84%E7%90%86%EF%BC%8C%E8%83%BD%E5%81%9A%E5%88%B0%E8%BE%83%E5%BF%AB%E8%A7%A3%E5%86%B3%E8%BE%83%E5%A4%A7%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">2.1.</span> <span class="nav-text">随着问题规模的增大（包括数据量和计算量层面的），单一设备处理速度逐渐无法满足我们的需求（比如太慢了，或者是干脆单设备放不下模型了）而用分布式算法，把问题划分为若干小问题后，再经过分布式系统来执行处理，能做到较快解决较大的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%99%E9%87%8C%E8%BF%98%E4%BB%8B%E7%BB%8D%E4%B8%A4%E4%B8%AA%E5%B9%B6%E8%A1%8C%E5%8A%A0%E9%80%9F%E5%AE%9A%E5%BE%8B%EF%BC%8C%E6%9D%A5%E8%AF%B4%E6%98%8E%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E7%9A%84%E6%84%8F%E4%B9%89%E3%80%82"><span class="nav-number">2.2.</span> <span class="nav-text">这里还介绍两个并行加速定律，来说明分布式训练的意义。</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%98%AF%E9%98%BF%E5%A7%86%E8%BE%BE%E5%B0%94%E5%AE%9A%E5%BE%8B%EF%BC%88Amdahl%E2%80%99s-law%EF%BC%89%EF%BC%8C%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%AF%94%E8%BE%83%E6%82%B2%E8%A7%82%E7%9A%84%E5%AE%9A%E5%BE%8B%E3%80%82%E5%AE%83%E8%AE%A4%E4%B8%BA%EF%BC%8C%E7%94%A8%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%EF%BC%8C%E6%98%AF%E5%AD%98%E5%9C%A8%E4%B8%80%E4%B8%AA%E5%8A%A0%E9%80%9F%E6%AF%94%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8A%E9%99%90%E7%9A%84%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E4%B8%8D%E8%83%BD%E4%BB%85%E4%BB%85%E4%BE%9D%E9%9D%A0%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%9D%A5%E6%97%A0%E9%99%90%E5%88%B6%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%E8%BE%83%E5%A4%A7%E5%B9%85%E5%BA%A6%E5%9C%B0%E6%8F%90%E9%AB%98%E8%AE%A1%E7%AE%97%E6%95%88%E7%8E%87%EF%BC%8C%E6%9C%80%E7%BB%88%E8%BF%98%E6%98%AF%E5%BE%97%E8%90%BD%E5%88%B0%E6%8F%90%E9%AB%98%E4%B8%B2%E8%A1%8C%E5%A4%84%E7%90%86%E6%95%88%E7%8E%87%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%8D%95%E4%B8%AA%E7%A1%AC%E4%BB%B6%E7%9A%84%E6%95%88%E7%8E%87%E4%B8%8A%E3%80%82%E5%85%B6%E5%85%AC%E5%BC%8F%E5%A6%82%E4%B8%8B%EF%BC%9A-S-latency-s-frac-1-1-p-frac-p-s-%E5%85%B6%E4%B8%AD%EF%BC%8C-S-latency-%E8%A1%A8%E7%A4%BA%E5%8A%A0%E9%80%9F%E6%AF%94%EF%BC%8Cp%E4%BB%A3%E8%A1%A8%E4%BB%BB%E5%8A%A1%E9%87%8C%E5%8F%AF%E4%BB%A5%E7%94%A8%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E8%BF%9B%E8%A1%8C%E5%8A%A0%E9%80%9F%E7%9A%84%E6%AF%94%E4%BE%8B%EF%BC%88%E4%BE%8B%E5%A6%82%EF%BC%8C%E5%8F%AF%E8%83%BD%E4%B8%80%E5%8D%8A%E6%98%AF%E5%8F%AF%E4%BB%A5%E7%94%A8%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F%E7%9A%84%EF%BC%89%EF%BC%8Cs%E8%A1%A8%E7%A4%BA%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E7%9A%84%E4%B8%AA%E6%95%B0%EF%BC%8C%E6%98%BE%E7%84%B6s%E8%B6%8A%E5%A4%A7%EF%BC%8C%E7%90%86%E8%AE%BA%E4%B8%8A%E5%8A%A0%E9%80%9F%E6%AF%94%E4%BE%8B%E5%B0%B1%E8%B6%8A%E5%A4%A7%E3%80%82%E4%BD%86%E6%A0%B9%E6%8D%AE%E8%BF%99%E4%B8%AA%E5%85%AC%E5%BC%8F%EF%BC%8C%E4%BC%9A%E5%8F%91%E7%8E%B0%EF%BC%8C%E5%8D%B3%E4%BD%BFs%E5%A2%9E%E5%A4%A7%E5%88%B0%E6%97%A0%E7%A9%B7%EF%BC%8C%E5%8A%A0%E9%80%9F%E6%AF%94%E4%B9%9F%E4%B8%8D%E8%BF%87%E6%98%AF-frac-1-1-p-%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%9C%89%E4%B8%8A%E9%99%90%E7%9A%84%E3%80%82%E5%AE%83%E7%9A%84%E6%8E%A8%E5%AF%BC%E4%B9%9F%E6%AF%94%E8%BE%83%E7%AE%80%E5%8D%95%EF%BC%9A"><span class="nav-number">2.2.1.</span> <span class="nav-text">第一个是阿姆达尔定律（Amdahl’s law），这是一个比较悲观的定律。它认为，用分布式计算，是存在一个加速比的理论上限的，也就是不能仅仅依靠分布式训练来无限制，或者说较大幅度地提高计算效率，最终还是得落到提高串行处理效率，也就是单个硬件的效率上。其公式如下：$$S_{latency}(s)&#x3D;\frac{1}{(1-p)+\frac{p}{s}}$$其中，$S_{latency}$表示加速比，p代表任务里可以用分布式训练进行加速的比例（例如，可能一半是可以用分布式训练加速的），s表示并行执行单元的个数，显然s越大，理论上加速比例就越大。但根据这个公式，会发现，即使s增大到无穷，加速比也不过是$\frac{1}{1-p}$，也就是有上限的。它的推导也比较简单：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A6%96%E5%85%88%EF%BC%8C%E6%88%91%E4%BB%AC%E6%8A%8A%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%E5%88%92%E5%88%86%E4%B8%80%E4%B8%8B%EF%BC%9A-T-1-W-ser-W-par-%EF%BC%88%E5%85%B6%E4%B8%AD-W-ser-%E8%A1%A8%E7%A4%BA%E6%97%A0%E6%B3%95%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%8C%E4%B8%8B%E6%A0%87ser%E8%A1%A8%E7%A4%BAserial%EF%BC%9B-W-par-%E8%A1%A8%E7%A4%BA%E5%8F%AF%E4%BB%A5%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%8C%E4%B8%8B%E6%A0%87par%E8%A1%A8%E7%A4%BAparallel%EF%BC%89"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">首先，我们把程序执行时间划分一下：$T_{1}&#x3D;W_{ser}+W_{par}$（其中$W_{ser}$表示无法并行执行的部分，下标ser表示serial；$W_{par}$表示可以并行执行的部分，下标par表示parallel）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%84%B6%E5%90%8E%EF%BC%8C%E5%81%87%E8%AE%BE%E6%88%91%E4%BB%AC%E6%9C%89s%E4%B8%AA%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%EF%BC%8C%E5%88%99%E6%AD%A4%E6%97%B6%E7%90%86%E6%83%B3%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%8F%AF%E5%B9%B6%E8%A1%8C%E7%9A%84%E9%83%A8%E5%88%86%E7%9A%84%E6%97%B6%E9%97%B4%EF%BC%8C%E5%BA%94%E8%AF%A5%E8%83%BD%E5%81%9A%E5%88%B0%E5%8F%98%E4%B8%BA%E5%8E%9F%E6%9D%A5%E7%9A%84-frac-1-s-%EF%BC%8C%E5%9B%A0%E6%AD%A4%EF%BC%8C%E6%9C%89%EF%BC%9A-T-p-geq-W-ser-frac-W-par-s-%EF%BC%88%E5%85%B6%E4%B8%AD%EF%BC%8C-T-p-%E8%A1%A8%E7%A4%BA%E5%AE%9E%E9%99%85%E7%9A%84%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E4%B9%8B%E5%90%8E%E7%9A%84%E6%80%BB%E6%97%B6%E9%97%B4%E3%80%82%E5%8F%B3%E8%BE%B9%E7%9A%84%E9%83%A8%E5%88%86%E5%B0%B1%E6%98%AF%E7%90%86%E6%83%B3%E7%9A%84%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%EF%BC%89"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">然后，假设我们有s个并行执行单元，则此时理想情况下，可并行的部分的时间，应该能做到变为原来的$\frac{1}{s}$，因此，有：$T_{p}\geq W_{ser}+\frac{W_{par}}{s}$（其中，$T_{p}$表示实际的并行执行之后的总时间。右边的部分就是理想的执行时间）</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%BF%99%E9%87%8C%E5%A4%9A%E8%AF%B4%E4%B8%80%E4%B8%8B%EF%BC%8C%E4%B9%8B%E6%89%80%E4%BB%A5%E7%90%86%E6%83%B3%E7%9A%84%E6%97%B6%E9%97%B4%E4%B8%AD%EF%BC%8C%E5%B9%B6%E8%A1%8C%E9%83%A8%E5%88%86%E7%9A%84%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%E6%81%B0%E5%A5%BD%E5%B0%B1%E6%98%AF-frac-1-s-%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%BF%99%E6%A0%B7%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%EF%BC%9A%E6%88%91%E4%BB%AC%E5%AF%B9%E5%8F%AF%E5%B9%B6%E8%A1%8C%E9%83%A8%E5%88%86%EF%BC%8C%E8%A6%81%E6%8A%8A%E4%BB%BB%E5%8A%A1%E5%88%86%E5%88%B0s%E4%B8%AA%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E4%B8%8A%E5%8E%BB%E6%89%A7%E8%A1%8C%E3%80%82%E6%9C%80%E7%BB%88%E6%88%91%E4%BB%AC%E8%A1%A1%E9%87%8F%E8%BF%99%E9%83%A8%E5%88%86%E7%9A%84%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4%EF%BC%8C%E7%9C%8B%E7%9A%84%E5%85%B6%E5%AE%9E%E4%B8%8D%E6%98%AF%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%E6%9C%80%E7%9F%AD%E7%9A%84%E9%82%A3%E4%B8%AA%E5%8D%95%E5%85%83%EF%BC%8C%E8%80%8C%E6%98%AF%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%E6%9C%80%E9%95%BF%E7%9A%84%E9%82%A3%E4%B8%AA%E5%8D%95%E5%85%83%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E6%88%91%E4%BB%AC%E7%8E%B0%E5%9C%A8%E5%B0%B1%E6%9D%A5%E7%9C%8B%E4%B8%80%E4%B8%8B%EF%BC%8Cs%E4%B8%AA%E5%8D%95%E5%85%83%E9%87%8C%EF%BC%8C%E6%9C%80%E7%9F%AD%E7%9A%84%E6%97%B6%E9%97%B4%E5%8F%AF%E4%BB%A5%E6%98%AF%E5%A4%9A%E5%B0%91%E5%91%A2%EF%BC%9F%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E6%8A%8A-W-par-%E7%90%86%E8%A7%A3%E4%B8%BA%E6%9F%90%E7%A7%8D%E5%B7%A5%E4%BD%9C%E9%87%8F%EF%BC%8C%E5%B7%A5%E4%BD%9C%E9%87%8F%E6%98%AF%E4%B8%80%E5%AE%9A%E7%9A%84%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E5%88%86%E5%88%B0%E5%90%84%E4%B8%AA%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E4%B8%8A%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%9C%89%E6%9F%90%E4%B8%AA%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E7%9A%84%E5%B7%A5%E4%BD%9C%E9%87%8F%E5%B0%8F%E4%BA%8E%E5%B9%B3%E5%9D%87%E5%80%BC-frac-W-par-s-%EF%BC%8C%E5%88%99%E4%B8%80%E5%AE%9A%E4%BC%9A%E6%9C%89%E6%9F%90%E4%B8%AA%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E7%9A%84%E5%B7%A5%E4%BD%9C%E9%87%8F%E5%A4%A7%E4%BA%8E%E5%B9%B3%E5%9D%87%E5%80%BC-frac-W-par-s-%E3%80%82%E5%88%99%E6%AD%A4%E6%97%B6%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E7%9A%84%E6%80%BB%E6%97%B6%E9%97%B4%E5%BA%94%E8%AF%A5%E5%A4%A7%E4%BA%8E-frac-W-par-s-%E3%80%82%E6%95%85%EF%BC%8C%E6%9C%80%E5%B0%8F%E7%9A%84%E6%97%B6%E9%97%B4%E7%A1%AE%E5%AE%9E%E6%98%AF%E5%8F%91%E7%94%9F%E5%9C%A8%E6%88%91%E4%BB%AC%E6%8A%8A%E4%BB%BB%E5%8A%A1%E9%87%8F%E5%B9%B3%E5%9D%87%E5%88%86%E5%88%B0%E6%AF%8F%E4%B8%AA%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E4%B8%8A%E7%9A%84%E6%97%B6%E5%80%99%E3%80%82"><span class="nav-number">2.2.1.2.0.1.</span> <span class="nav-text">这里多说一下，之所以理想的时间中，并行部分的执行时间恰好就是$\frac{1}{s}$，可以这样直观理解：我们对可并行部分，要把任务分到s个执行单元上去执行。最终我们衡量这部分的运行时间，看的其实不是执行时间最短的那个单元，而是执行时间最长的那个单元。因此，我们现在就来看一下，s个单元里，最短的时间可以是多少呢？这里我们可以把$W_{par}$理解为某种工作量，工作量是一定的。因此，分到各个执行单元上的时候，如果有某个执行单元的工作量小于平均值$\frac{W_{par}}{s}$，则一定会有某个执行单元的工作量大于平均值$\frac{W_{par}}{s}$。则此时并行执行的总时间应该大于$\frac{W_{par}}{s}$。故，最小的时间确实是发生在我们把任务量平均分到每个执行单元上的时候。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A4%E6%97%B6%EF%BC%8C%E6%88%91%E4%BB%AC%E6%A0%B9%E6%8D%AE%E5%8A%A0%E9%80%9F%E6%AF%94%E7%9A%84%E5%AE%9A%E4%B9%89%EF%BC%88-frac-T-1-T-p-%EF%BC%8C%E5%8D%B3%E5%8E%9F%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%E6%AF%94%E4%B8%8A%E5%8A%A0%E9%80%9F%E5%90%8E%E7%9A%84%E6%97%B6%E9%97%B4%EF%BC%89%EF%BC%8C%E5%8F%AF%E5%BE%97%E5%88%B0%EF%BC%9A-S-latency-leq-frac-W-ser-W-par-W-ser-frac-W-par-s-%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%EF%BC%8C%E6%88%91%E4%BB%AC%E8%AE%B0p%E4%B8%BA-T-1-%E4%B8%AD%E5%8F%AF%E5%B9%B6%E8%A1%8C%E5%8A%A0%E9%80%9F%E7%9A%84%E9%83%A8%E5%88%86%E7%9A%84%E5%8D%A0%E6%AF%94%EF%BC%8C%E5%88%99%E6%9C%89%EF%BC%9A-W-ser-1-p-T-1-%EF%BC%8C-W-par-pT-1-%E6%AD%A4%E6%97%B6%E5%86%8D%E6%8A%8A%E4%B8%8A%E9%9D%A2%E7%9A%84%E4%B8%8D%E7%AD%89%E5%BC%8F%E6%95%B4%E7%90%86%E4%B8%80%E4%B8%8B%EF%BC%8C%E6%9C%89%EF%BC%9A-S-latency-leq-frac-1-p-p-1-p-frac-p-s-frac-1-1-p-frac-p-s"><span class="nav-number">2.2.1.3.</span> <span class="nav-text">此时，我们根据加速比的定义（$\frac{T_{1}}{T_{p}}$，即原执行时间比上加速后的时间），可得到：$S_{latency}\leq \frac{W_{ser}+W_{par}}{W_{ser}+\frac{W_{par}}{s}}$进一步的，我们记p为$T_{1}$中可并行加速的部分的占比，则有：$W_{ser}&#x3D;(1-p)T_{1}$，$W_{par}&#x3D;pT_{1}$此时再把上面的不等式整理一下，有：$S_{latency}\leq \frac{(1-p)+p}{1-p+\frac{p}{s}} &#x3D; \frac{1}{1-p+\frac{p}{s}}$</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E4%B8%AA%E5%88%99%E6%98%AF%E5%8F%A4%E6%96%AF%E5%A1%94%E5%A4%AB%E5%AE%9A%E5%BE%8B%EF%BC%88Gustafson%E2%80%99s-law%EF%BC%89%EF%BC%8C%E5%AE%83%E7%B1%BB%E4%BC%BC%E4%BA%8E%E5%AF%B9%E4%B8%8A%E9%9D%A2%E9%98%BF%E5%A7%86%E8%BE%BE%E5%B0%94%E5%AE%9A%E5%BE%8B%E7%9A%84%E4%B8%80%E4%B8%AA%E5%8F%8D%E5%87%BB%EF%BC%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E4%B9%90%E8%A7%82%E7%9A%84%E5%AE%9A%E5%BE%8B%E3%80%82%E5%AE%83%E8%AE%A4%E4%B8%BA%E5%85%B6%E5%AE%9E%E7%BA%BF%E6%80%A7%E5%8A%A0%E9%80%9F%E6%AF%94%E6%98%AF%E6%9C%89%E5%8F%AF%E8%83%BD%E7%9A%84%EF%BC%8C%E5%9B%A0%E6%AD%A4%EF%BC%8C%E6%88%91%E4%BB%AC%E7%A1%AE%E5%AE%9E%E6%98%AF%E6%9C%89%E6%9C%BA%E4%BC%9A%E9%80%9A%E8%BF%87%E5%A2%9E%E5%8A%A0%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E7%9A%84%E6%95%B0%E9%87%8F%E6%9D%A5%E5%BE%97%E5%88%B0%E6%AF%94%E8%BE%83%E5%A5%BD%E7%9A%84%E5%8A%A0%E9%80%9F%E6%AF%94%E7%9A%84%E3%80%82%E5%85%B6%E6%8E%A8%E5%AF%BC%E5%85%B6%E5%AE%9E%E5%92%8C%E4%B8%8A%E9%9D%A2%E6%9C%89%E4%BA%9B%E7%B1%BB%E4%BC%BC%EF%BC%8C%E4%BD%86%E5%85%B3%E9%94%AE%E5%9C%A8%E4%BA%8E%E4%B8%80%E4%B8%AA%E5%9C%B0%E6%96%B9%EF%BC%8C%E5%B0%B1%E6%98%AF%EF%BC%8C%E5%8F%A4%E6%96%AF%E5%A1%94%E5%A4%AB%E5%AE%9A%E5%BE%8B%E8%AE%A4%E4%B8%BA%EF%BC%8C%E9%9A%8F%E7%9D%80%E5%B9%B6%E8%A1%8C%E5%BA%A6%E7%9A%84%E5%A2%9E%E5%8A%A0%EF%BC%8C%E5%8F%AF%E8%A7%A3%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%84%E6%A8%A1%E4%B9%9F%E5%9C%A8%E5%A2%9E%E5%8A%A0%EF%BC%9B%E8%80%8C%E4%B8%8A%E9%9D%A2%E7%9A%84%E9%98%BF%E5%A7%86%E8%BE%BE%E5%B0%94%E5%AE%9A%E5%BE%8B%E5%88%99%E5%81%87%E8%AE%BE%E9%97%AE%E9%A2%98%E8%A7%84%E6%A8%A1%E6%80%BB%E6%98%AF%E5%9B%BA%E5%AE%9A%E7%9A%84%EF%BC%9A"><span class="nav-number">2.2.2.</span> <span class="nav-text">第二个则是古斯塔夫定律（Gustafson’s law），它类似于对上面阿姆达尔定律的一个反击，是一个乐观的定律。它认为其实线性加速比是有可能的，因此，我们确实是有机会通过增加执行单元的数量来得到比较好的加速比的。其推导其实和上面有些类似，但关键在于一个地方，就是，古斯塔夫定律认为，随着并行度的增加，可解问题的规模也在增加；而上面的阿姆达尔定律则假设问题规模总是固定的：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E8%AE%B0%EF%BC%8C%E5%9C%A8%E6%8B%A5%E6%9C%89s%E4%B8%AA%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E7%9A%84%E6%9C%BA%E5%99%A8%E4%B8%8A%EF%BC%8C%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%E4%B8%BA%EF%BC%9A-T-1-W-ser-W-par-%EF%BC%8C%E8%BF%99%E9%87%8C%E5%88%9D%E5%A7%8B%E7%9A%84%E6%97%B6%E9%97%B4%E8%AE%B0%E5%8F%B7%E6%B2%A1%E5%8F%98%EF%BC%8C%E5%90%AB%E4%B9%89%E5%92%8C%E4%B8%8A%E9%9D%A2%E4%B8%80%E6%A0%B7%E3%80%82%E4%B8%8B%E9%9D%A2%EF%BC%8C%E6%98%AF%E5%85%B3%E9%94%AE%E7%9A%84%E4%B8%80%E6%AD%A5%EF%BC%9A%E5%9C%A8%E5%8D%95%E5%A4%84%E7%90%86%E5%99%A8%E4%B8%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E5%BE%97%E5%88%B0%EF%BC%8C%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%E4%B8%BA%EF%BC%9A-T-s-W-ser-sW-par"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">这里我们记，在拥有s个并行执行单元的机器上，执行时间为：$T_{1}&#x3D;W_{ser}+W_{par}$，这里初始的时间记号没变，含义和上面一样。下面，是关键的一步：在单处理器上，我们可得到，执行时间为：$T_{s}&#x3D;W_{ser}+sW_{par}$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9C%8B%E8%B5%B7%E6%9D%A5%E5%92%8C%E4%B8%8A%E9%9D%A2%E5%A5%BD%E5%83%8F%E6%B2%A1%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F%E4%BD%86%E8%BF%99%E9%87%8C%EF%BC%8C%E6%88%91%E4%BB%AC%E6%98%AF%E5%85%88%E5%AE%9A%E4%B9%89%E4%BA%86%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E7%9A%84%E6%97%B6%E9%97%B4%E3%80%82%E7%84%B6%E5%90%8E%EF%BC%8C%E5%9C%A8%E7%9B%B8%E5%90%8C%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%84%E6%A8%A1%E4%B8%8B%EF%BC%8C%E6%88%91%E4%BB%AC%E5%86%8D%E7%9C%8B%EF%BC%8C%E5%A6%82%E6%9E%9C%E8%A6%81%E5%9C%A8%E5%8D%95%E5%A4%84%E7%90%86%E5%99%A8%E4%B8%8A%E6%89%A7%E8%A1%8C%EF%BC%8C%E4%BC%9A%E6%98%AF%E5%A4%9A%E5%B0%91%E6%97%B6%E9%97%B4%E3%80%82%E4%B8%8A%E9%9D%A2%E9%98%BF%E5%A7%86%E8%BE%BE%E5%B0%94%E5%AE%9A%E5%BE%8B%EF%BC%8C%E5%88%99%E6%98%AF%EF%BC%9A%E5%85%88%E7%9C%8B%E4%B8%80%E4%B8%8B%E5%9C%A8%E5%8D%95%E5%A4%84%E7%90%86%E5%99%A8%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%E6%98%AF%E5%A4%9A%E5%B0%91%EF%BC%8C%E4%B9%8B%E5%90%8E%E5%86%8D%E7%9C%8B%E7%94%A8s%E4%B8%AA%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E5%90%8E%EF%BC%8C%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4%E5%8F%88%E6%98%AF%E5%A4%9A%E5%B0%91%E3%80%82%E6%9C%80%E5%A4%A7%E7%9A%84%E5%8C%BA%E5%88%AB%E5%9C%A8%E4%BA%8E%EF%BC%8C%E4%B8%8A%E9%9D%A2%E9%98%BF%E5%A7%86%E8%BE%BE%E5%B0%94%E5%AE%9A%E5%BE%8B%EF%BC%8C%E5%AE%83%E5%81%87%E8%AE%BE%E4%BA%86%E5%8F%AF%E5%B9%B6%E8%A1%8C%E7%9A%84%E9%83%A8%E5%88%86%E7%9A%84%E9%97%AE%E9%A2%98%E4%BB%BB%E5%8A%A1%E9%87%8F%E6%98%AF%E4%B8%80%E5%AE%9A%E7%9A%84%EF%BC%8C%E5%8D%B3%E5%9B%BA%E5%AE%9A%E4%B8%BA-W-par-%EF%BC%9B%E6%AD%A4%E5%A4%84%E7%9A%84%E5%8F%A4%E6%96%AF%E5%A1%94%E5%A4%AB%E5%AE%9A%E5%BE%8B%EF%BC%8C%E5%AE%83%E5%85%88%E8%80%83%E5%AF%9F%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E4%B8%8B%E7%9A%84%E6%97%B6%E9%97%B4%EF%BC%8C%E6%AD%A4%E6%97%B6%E5%8F%AF%E5%B9%B6%E8%A1%8C%E9%83%A8%E5%88%86%E7%9A%84%E4%BB%BB%E5%8A%A1%E9%87%8F%E6%98%AF-W-par-%EF%BC%8C%E5%88%99%E4%B9%8B%E5%90%8E%E5%9C%A8%E5%8D%95%E5%A4%84%E7%90%86%E5%99%A8%E4%B8%8A%E6%89%A7%E8%A1%8C%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%80%BB%E7%9A%84%E5%B9%B6%E8%A1%8C%E9%83%A8%E5%88%86%E7%9A%84%E4%BB%BB%E5%8A%A1%E9%87%8F%E5%BA%94%E8%AF%A5%E6%98%AF-SW-par-%EF%BC%88%EF%BC%9F%E5%86%8D%E7%90%86%E8%A7%A3%E4%B8%80%E4%B8%8B%EF%BC%89"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">看起来和上面好像没什么区别？但这里，我们是先定义了并行执行的时间。然后，在相同的问题规模下，我们再看，如果要在单处理器上执行，会是多少时间。上面阿姆达尔定律，则是：先看一下在单处理器情况下，执行时间是多少，之后再看用s个并行执行单元后，执行时间又是多少。最大的区别在于，上面阿姆达尔定律，它假设了可并行的部分的问题任务量是一定的，即固定为$W_{par}$；此处的古斯塔夫定律，它先考察并行执行下的时间，此时可并行部分的任务量是$W_{par}$，则之后在单处理器上执行的时候，总的并行部分的任务量应该是$SW_{par}$（？再理解一下）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A4%E6%97%B6%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BE%9D%E7%84%B6%E6%8C%89%E7%85%A7%E5%8A%A0%E9%80%9F%E6%AF%94%E7%9A%84%E5%AE%9A%E4%B9%89%EF%BC%8C%E5%8F%AF%E5%BE%97%E5%88%B0%EF%BC%9A-S-latency-s-frac-W-ser-sW-par-W-ser-W-par-%EF%BC%9B%E4%BE%9D%E7%84%B6%E5%BC%95%E5%85%A5p%E8%AE%B0%E5%8F%B7%EF%BC%8C%E5%88%99%E6%9C%89%EF%BC%9A-S-latency-s-frac-1-p-sp-1-1-p-sp-%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E5%BD%93%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83s%E5%A2%9E%E5%A4%9A%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E4%B8%94%E8%B6%B3%E5%A4%9F%E5%A4%A7%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%98%AF%E6%9C%89%E5%8F%AF%E8%83%BD%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%88%B0%E6%8E%A5%E8%BF%91%E7%BA%BF%E6%80%A7%E5%8A%A0%E9%80%9F%E6%AF%94%E7%9A%84"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">此时，我们依然按照加速比的定义，可得到：$S_{latency}(s)&#x3D;\frac{W_{ser}+sW_{par}}{W_{ser}+W_{par}}$；依然引入p记号，则有：$S_{latency}(s)&#x3D;\frac{1-p+sp}{1}&#x3D;1-p+sp$可以看到，当并行执行单元s增多的时候，且足够大的时候，是有可能可以做到接近线性加速比的</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E5%B9%B6%E8%A1%8C%E5%8C%96%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%A1%88"><span class="nav-number">3.</span> <span class="nav-text">2. 并行化的基本方案</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E4%BD%93%E4%B8%8A%E5%88%86%E7%AE%97%E5%AD%90%E5%86%85%E5%B9%B6%E8%A1%8C%E5%92%8C%E7%AE%97%E5%AD%90%E9%97%B4%E5%B9%B6%E8%A1%8C%E3%80%82%E7%AE%97%E5%AD%90%E5%86%85%E5%B9%B6%E8%A1%8C%EF%BC%8C%E6%B2%A1%E5%A4%AA%E6%98%8E%E7%99%BD%E2%80%A6%E2%80%A6%E8%80%8C%E7%AE%97%E5%AD%90%E9%97%B4%E5%B9%B6%E8%A1%8C%EF%BC%8C%E5%A4%A7%E4%BD%93%E4%B8%8A%E5%88%86%E4%B8%A4%E7%A7%8D%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%98%AF%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%98%AF%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E3%80%82%E8%BF%99%E4%B8%A4%E4%B8%AA%E6%AF%94%E8%BE%83%E4%B8%BB%E8%A6%81"><span class="nav-number">3.1.</span> <span class="nav-text">大体上分算子内并行和算子间并行。算子内并行，没太明白……而算子间并行，大体上分两种，一种是模型并行，一种是数据并行。这两个比较主要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C"><span class="nav-number">3.2.</span> <span class="nav-text">2.1. 模型并行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-%E6%9C%B4%E7%B4%A0idea%EF%BC%9A"><span class="nav-number">3.2.1.</span> <span class="nav-text">2.1.1. 朴素idea：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E8%AF%B4%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%B9%B6%E8%A1%8C%E6%96%B9%E5%BC%8F%E6%98%AF%E8%A6%81%E6%8A%8A%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%88%92%E5%88%86%EF%BC%8C%E7%84%B6%E5%90%8E%E6%94%BE%E5%88%B0%E4%B8%8D%E5%90%8C%E7%9A%84%E8%AE%BE%E5%A4%87%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%89%A7%E8%A1%8C%E3%80%82%E5%9B%A0%E6%AD%A4%E9%9C%80%E8%A6%81%E8%B7%A8%E8%AE%BE%E5%A4%87%E4%BC%A0%E9%80%92%E4%B8%AD%E9%97%B4%E7%9A%84%E6%BF%80%E6%B4%BB%E5%80%BC%EF%BC%8C%E4%BB%A5%E5%8F%8A%E6%AF%8F%E4%B8%AA%E8%AE%BE%E5%A4%87%E9%83%BD%E9%9C%80%E8%A6%81%E7%94%A8%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E5%AE%83%E9%82%A3%E9%83%A8%E5%88%86%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">简单说，这个并行方式是要把计算图划分，然后放到不同的设备上进行执行。因此需要跨设备传递中间的激活值，以及每个设备都需要用梯度更新它那部分的模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-%E6%9C%B4%E7%B4%A0idea%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A"><span class="nav-number">3.2.2.</span> <span class="nav-text">2.1.2. 朴素idea的问题：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%86%E5%A6%82%E6%9E%9C%E5%8F%AA%E6%98%AF%E7%AE%80%E5%8D%95%E5%9C%B0%E6%8A%8A%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%88%92%E5%88%86%EF%BC%88%E5%85%B7%E4%BD%93%E5%88%B0%E4%BB%A3%E7%A0%81%E4%B8%8A%EF%BC%8C%E4%B8%80%E8%88%AC%E6%98%AF%E6%8A%8A%E6%A8%A1%E5%9E%8B%E7%9A%84module%E4%B9%8B%E7%B1%BB%E7%9A%84%E7%BB%99%E5%88%92%E5%88%86%EF%BC%8C%E6%AF%94%E5%A6%82%EF%BC%8C%E5%8F%AF%E8%83%BD%E6%A8%A1%E5%9E%8B%E9%87%8C%E6%9C%89%E4%B8%A4%E4%B8%AAsequential%EF%BC%8C%E7%AC%AC%E4%B8%80%E4%B8%AAsequential%E6%94%BE%E5%88%B0cuda0%E4%B8%8A%EF%BC%8C%E7%AC%AC%E4%BA%8C%E4%B8%AAsequential%E6%94%BE%E5%88%B0cuda1%E4%B8%8A%EF%BC%8C%E8%BF%99%E6%A0%B7%EF%BC%89%EF%BC%8C%E9%82%A3%E5%85%B6%E5%AE%9E%E6%9C%AA%E5%BF%85%E8%83%BD%E8%B5%B7%E5%88%B0%E5%8A%A0%E9%80%9F%E7%9A%84%E6%95%88%E6%9E%9C%E3%80%82%E5%9B%A0%E4%B8%BA%EF%BC%8C%E7%BB%99%E5%AE%9A%E4%B8%80%E4%B8%AAbatch%EF%BC%8C%E8%A6%81%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%EF%BC%8C%E4%BB%A5%E4%B8%8A%E9%9D%A2%E7%9A%84%E4%B8%A4%E4%B8%AAsequential%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%BA%E4%BE%8B%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%A6%81%E5%85%88%E5%9C%A8cuda0%E4%B8%8A%E7%BB%8F%E8%BF%87%E7%AC%AC%E4%B8%80%E4%B8%AAseq%EF%BC%8C%E5%86%8D%E6%8A%8A%E8%BF%99%E9%83%A8%E5%88%86%E7%9A%84%E7%BB%93%E6%9E%9C%E6%94%BE%E5%88%B0cuda1%E4%B8%8A%EF%BC%8C%E7%BB%8F%E8%BF%87seq2%E8%BF%99%E6%A0%B7%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%97%B6%E5%88%BB%EF%BC%8C%E5%85%B6%E5%AE%9E%E5%8F%AA%E6%9C%89%E4%B8%80%E4%B8%AAgpu%E5%9C%A8%E8%BF%90%E8%A1%8C%EF%BC%8C%E4%B8%94%E6%88%91%E4%BB%AC%E8%BF%98%E5%A4%9A%E4%BA%86%E8%B7%A8%E8%AE%BE%E5%A4%87%E4%BC%A0%E9%80%92%E6%95%B0%E6%8D%AE%E7%9A%84%E6%AD%A5%E9%AA%A4%EF%BC%8C%E5%BE%88%E5%8F%AF%E8%83%BD%E4%B8%8D%E4%BB%85%E6%B2%A1%E5%8A%A0%E9%80%9F%EF%BC%8C%E8%BF%98%E6%AF%94%E5%8E%9F%E6%9D%A5%E7%94%A8%E5%8D%95%E5%8D%A1%E6%85%A2%E4%BA%86"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">但如果只是简单地把计算图划分（具体到代码上，一般是把模型的module之类的给划分，比如，可能模型里有两个sequential，第一个sequential放到cuda0上，第二个sequential放到cuda1上，这样），那其实未必能起到加速的效果。因为，给定一个batch，要前向计算，以上面的两个sequential的情况为例，就是要先在cuda0上经过第一个seq，再把这部分的结果放到cuda1上，经过seq2这样，每个时刻，其实只有一个gpu在运行，且我们还多了跨设备传递数据的步骤，很可能不仅没加速，还比原来用单卡慢了</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-3-%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F%EF%BC%9A"><span class="nav-number">3.2.3.</span> <span class="nav-text">2.1.3. 优化方式：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%A0%E6%AD%A4%E9%9C%80%E8%A6%81%E4%BC%98%E5%8C%96%E3%80%82%E8%B7%A8%E8%AE%BE%E5%A4%87%E7%9A%84%E6%90%AC%E7%A7%BB%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%B8%8D%E5%A4%AA%E5%A5%BD%E4%BC%98%E5%8C%96%EF%BC%8C%E4%BD%86%E5%A6%82%E6%9E%9C%E8%83%BD%E5%B0%BD%E9%87%8F%E9%81%BF%E5%85%8D%E8%AE%BE%E5%A4%87%E7%9A%84%E7%A9%BA%E9%97%B2%EF%BC%8C%E6%98%BE%E7%84%B6%E6%98%AF%E4%B8%80%E4%B8%AA%E6%9E%81%E5%A4%A7%E7%9A%84%E6%94%B9%E8%BF%9B%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E8%80%83%E8%99%91%E6%B5%81%E6%B0%B4%E7%BA%BF%E3%80%82%E5%A4%A7%E6%A6%82%E7%9A%84idea%E6%98%AF%EF%BC%9A%E6%88%91%E4%BB%AC%E6%8A%8A%E4%B8%80%E4%B8%AAbatch%E5%86%8D%E7%BB%99%E5%88%87%E5%88%86%E4%B8%BA%E8%8B%A5%E5%B9%B2split%EF%BC%8C%E4%B9%8B%E5%90%8E%E6%AF%8F%E4%B8%AAsplit%E6%8C%A8%E4%B8%AA%E8%BF%9B%E5%85%A5%E5%88%B0cuda%E8%AE%BE%E5%A4%87%E9%87%8C%E3%80%82%E7%94%A8%E4%B8%8A%E9%9D%A2%E7%9A%84%E4%BE%8B%E5%AD%90%E8%BF%9B%E8%A1%8C%E8%AF%B4%E6%98%8E%EF%BC%8C%E5%81%87%E5%A6%82%E4%B8%80%E4%B8%AAbatch%E8%A2%AB%E6%88%91%E4%BB%AC%E5%88%92%E5%88%86%E4%B8%BA4%E4%B8%AAsplit%EF%BC%8C%E5%88%99split0%E5%85%88%E8%BF%9B%E5%85%A5cuda0%EF%BC%8C%E7%84%B6%E5%90%8E%E4%B8%8B%E4%B8%80%E4%B8%AA%E6%97%B6%E5%88%BB%EF%BC%8Csplit0%E8%BF%9B%E5%85%A5cuda1%EF%BC%8Csplit1%E8%BF%9B%E5%85%A5cuda0%EF%BC%9B%E5%86%8D%E4%B8%8B%E4%B8%80%E4%B8%AA%E6%97%B6%E5%88%BB%EF%BC%8Csplit0%E7%AE%97%E5%AE%8C%E4%BA%86%EF%BC%8Csplit1%E8%BF%9B%E5%85%A5cuda1%EF%BC%8Csplit2%E8%BF%9B%E5%85%A5cuda2%E2%80%A6%E2%80%A6"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">因此需要优化。跨设备的搬移数据，不太好优化，但如果能尽量避免设备的空闲，显然是一个极大的改进。因此，考虑流水线。大概的idea是：我们把一个batch再给切分为若干split，之后每个split挨个进入到cuda设备里。用上面的例子进行说明，假如一个batch被我们划分为4个split，则split0先进入cuda0，然后下一个时刻，split0进入cuda1，split1进入cuda0；再下一个时刻，split0算完了，split1进入cuda1，split2进入cuda2……</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8A%E9%9D%A2%E7%9A%84%E6%9C%B4%E7%B4%A0%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF%EF%BC%8C%E4%B8%80%E8%88%AC%E4%BB%85%E5%9C%A8%E5%BC%80%E5%A4%B4%E7%BB%93%E5%B0%BE%E7%9A%84iteration%E6%9C%89gpu%E7%9A%84%E7%A9%BA%E6%B4%9E%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%9C%89%E8%AE%BE%E5%A4%87%E7%9A%84%E7%A9%BA%E9%97%B2%EF%BC%89%EF%BC%8C%E8%80%8C%E4%B8%AD%E9%97%B4%E5%A4%A7%E5%A4%9A%E6%95%B0%E6%97%B6%E5%80%99%EF%BC%8C%E4%B8%A4%E5%9D%97gpu%E9%83%BD%E8%83%BD%E5%90%8C%E6%97%B6%E8%BF%90%E8%A1%8C%EF%BC%8C%E6%95%88%E6%9E%9C%E6%AF%94%E7%94%A8%E6%B5%81%E6%B0%B4%E7%BA%BF%E4%B9%8B%E5%89%8D%E7%9A%84%E6%98%AF%E8%A6%81%E5%A5%BD%E5%BE%88%E5%A4%9A%E7%9A%84%E5%8F%A6%E4%B8%80%E4%B8%AA%E7%82%B9%E6%98%AF%EF%BC%8C%E5%88%92%E5%88%86split%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%B9%B6%E9%9D%9E%E8%B6%8A%E5%A4%9A%E8%B6%8A%E5%A5%BD%EF%BC%8C%E5%9B%A0%E4%B8%BA%E8%B7%A8%E8%AE%BE%E5%A4%87%E7%A7%BB%E5%8A%A8%E6%95%B0%E6%8D%AE%E6%98%AF%E6%AF%94%E8%BE%83%E8%80%97%E6%97%B6%E7%9A%84%EF%BC%88%E4%BD%86%E6%9C%AC%E8%BA%AB%E5%88%92%E5%88%86%E8%BE%83%E5%A4%9A%E7%9A%84split%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%98%AF%E7%9B%B8%E5%BD%93%E4%BA%8E%E6%B5%81%E6%B0%B4%E7%BA%BF%E7%BA%A7%E6%95%B0%E5%A2%9E%E5%A4%9A%E4%BA%86%E5%90%97%EF%BC%9F%EF%BC%89"><span class="nav-number">3.2.4.</span> <span class="nav-text">上面的朴素的流水线，一般仅在开头结尾的iteration有gpu的空洞（也就是有设备的空闲），而中间大多数时候，两块gpu都能同时运行，效果比用流水线之前的是要好很多的另一个点是，划分split的时候，并非越多越好，因为跨设备移动数据是比较耗时的（但本身划分较多的split的时候，是相当于流水线级数增多了吗？）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E9%9D%A2%E5%86%8D%E4%BB%8B%E7%BB%8D%E4%B8%A4%E7%A7%8D%E5%85%B7%E4%BD%93%E4%B8%80%E7%82%B9%E7%9A%84%E7%AE%97%E6%B3%95%E3%80%82%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%98%AFGpipe%E7%AE%97%E6%B3%95%EF%BC%8C%E7%AC%AC%E4%BA%8C%E4%B8%AA%E6%98%AFPipeDream%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.5.</span> <span class="nav-text">下面再介绍两种具体一点的算法。第一个是Gpipe算法，第二个是PipeDream算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-4-Gpipe%E7%AE%97%E6%B3%95%EF%BC%9A"><span class="nav-number">3.2.6.</span> <span class="nav-text">2.1.4. Gpipe算法：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%83%E7%9A%84idea%E6%98%AF%EF%BC%8C%E5%AF%B9batch%E8%BF%9B%E8%A1%8C%E6%8B%86%E5%88%86%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%87%8F%E5%B0%91%E8%AE%BE%E5%A4%87%E7%A9%BA%E9%97%B2%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%89%80%E8%B0%93Bubble%EF%BC%89%EF%BC%8C%E4%B8%94%E6%98%AF%E5%9C%A8%E4%B8%80%E4%B8%AAbatch%E7%9A%84%E6%89%80%E6%9C%89%E5%89%8D%E9%A6%88%E9%83%BD%E7%BB%93%E6%9D%9F%E4%B9%8B%E5%90%8E%EF%BC%8C%E6%89%8D%E5%BC%80%E5%A7%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88%E8%BF%99%E6%A0%B7%E7%9C%8B%EF%BC%8C%E5%AE%83%E5%92%8C%E4%B8%8A%E9%9D%A2%E6%9C%B4%E7%B4%A0%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%94%B9%E8%BF%9B%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F%EF%BC%89"><span class="nav-number">3.2.6.1.</span> <span class="nav-text">它的idea是，对batch进行拆分，从而减少设备空闲（也就是所谓Bubble），且是在一个batch的所有前馈都结束之后，才开始反向传播（这样看，它和上面朴素的流水线改进，有什么区别？）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-5-PipeDream%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.7.</span> <span class="nav-text">2.1.5. PipeDream算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%83%E7%9B%B8%E5%BD%93%E4%BA%8E%E5%9C%A8Gpipe%E7%AE%97%E6%B3%95%E4%B8%8A%E4%BD%9C%E5%87%BA%E4%B8%80%E4%BA%9B%E6%94%B9%E8%BF%9B%E3%80%82%E4%B8%BB%E8%A6%81%E6%98%AF%EF%BC%8C%E4%B8%8A%E9%9D%A2Gpipe%E7%AE%97%E6%B3%95%E8%A6%81%E7%AD%89%E6%89%80%E6%9C%89%E7%9A%84%E5%89%8D%E9%A6%88%E5%AE%8C%E6%88%90%E4%BA%86%EF%BC%8C%E6%89%8D%E4%B8%80%E8%B5%B7%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88%E8%BF%99%E6%A0%B7%E6%9C%AC%E8%BA%AB%E6%98%AF%E6%9C%89%E4%BB%80%E4%B9%88%E5%A5%BD%E5%A4%84%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E4%BB%80%E4%B9%88%E5%A5%BD%E5%A4%84%E9%83%BD%E6%B2%A1%E6%9C%89%EF%BC%8C%E6%98%BE%E7%84%B6%E4%B8%80%E5%BC%80%E5%A7%8B%E4%B8%8D%E4%BC%9A%E8%AE%BE%E8%AE%A1%E6%88%90%E8%BF%99%E6%A0%B7%E5%95%8A%EF%BC%9F%EF%BC%89%EF%BC%8C%E4%BD%86%E8%BF%99%E6%A0%B7%E5%B0%B1%E5%BE%88%E6%B5%AA%E8%B4%B9%E8%AE%BE%E5%A4%87%EF%BC%8C%E5%9B%A0%E4%B8%BA%E6%9C%89%E6%97%B6%E5%80%99%E5%BE%97%E7%AD%89%E4%B8%80%E4%BA%9Bsplit%E5%AE%8C%E6%88%90%E5%89%8D%E9%A6%88%EF%BC%8C%E6%89%8D%E8%83%BD%E5%8F%8D%E4%BC%A0%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8CPipeDream%E5%B0%B1%E6%98%AF%E5%9C%A8%E8%BF%99%E6%96%B9%E9%9D%A2%E4%BD%9C%E5%87%BA%E4%BA%86%E6%94%B9%E8%BF%9B%EF%BC%8C%E7%AC%AC%E4%B8%80%E4%B8%AAsplit%E7%9A%84%E5%89%8D%E9%A6%88%E4%B8%80%E6%97%A6%E7%AE%97%E5%AE%8C%EF%BC%8C%E5%B0%B1%E7%AB%8B%E5%88%BB%E5%BC%80%E5%A7%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%82%E4%B8%94%E6%AF%8F%E4%B8%AAsplit%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%8C%E9%83%BD%E8%A2%AB%E8%AE%BE%E8%AE%A1%E4%B8%BA%E5%8D%A0%E6%8D%AE%E4%B8%A4%E4%B8%AA%E6%97%B6%E9%9A%99%EF%BC%88%E5%8F%AF%E4%BB%A5%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3%E4%B8%BA%E4%B8%A4%E4%B8%AA%E6%97%B6%E9%97%B4%E5%8D%95%E4%BD%8D%EF%BC%89%EF%BC%8C%E8%BF%9B%E8%80%8C%E4%BA%BA%E4%B8%BA%E5%88%B6%E9%80%A0%E5%87%BAbubble%EF%BC%8C%E4%BB%8E%E8%80%8C%E8%AE%A9%E5%90%8E%E7%BB%AD%E7%9A%84%E5%89%8D%E9%A6%88%E8%BF%90%E7%AE%97%E8%83%BD%E5%9C%A8%E8%BF%99%E4%B8%AA%E9%97%B4%E9%9A%99%E9%87%8C%E8%BF%9B%E8%A1%8C%EF%BC%8C%E6%B6%88%E9%99%A4bubble%EF%BC%88%E6%84%9F%E8%A7%89%E8%BF%98%E6%98%AF%E6%B2%A1%E5%A4%AA%E6%98%8E%E7%99%BD%E5%95%8A%EF%BC%9F%EF%BC%89"><span class="nav-number">3.2.7.1.</span> <span class="nav-text">它相当于在Gpipe算法上作出一些改进。主要是，上面Gpipe算法要等所有的前馈完成了，才一起反向传播（这样本身是有什么好处吗？如果什么好处都没有，显然一开始不会设计成这样啊？），但这样就很浪费设备，因为有时候得等一些split完成前馈，才能反传。因此，PipeDream就是在这方面作出了改进，第一个split的前馈一旦算完，就立刻开始反向传播。且每个split的反向传播，都被设计为占据两个时隙（可以简单理解为两个时间单位），进而人为制造出bubble，从而让后续的前馈运算能在这个间隙里进行，消除bubble（感觉还是没太明白啊？）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="nav-number">3.3.</span> <span class="nav-text">2.2. 数据并行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E6%9C%B4%E7%B4%A0idea%EF%BC%9A"><span class="nav-number">3.3.1.</span> <span class="nav-text">2.2.1. 朴素idea：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%80%E8%B0%93%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%EF%BC%8C%E5%B0%B1%E6%98%AF%EF%BC%8C%E4%B8%8D%E5%90%8C%E8%AE%BE%E5%A4%87%E4%B8%8A%EF%BC%8C%E6%89%A7%E8%A1%8C%E7%9B%B8%E5%90%8C%E7%9A%84%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%88%E5%8F%AF%E4%BB%A5%E7%90%86%E8%A7%A3%E4%B8%BA%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%8D%E5%88%87%E5%88%86%E6%A8%A1%E5%9E%8B%E4%BA%86%EF%BC%8C%E6%AF%8F%E4%B8%AA%E8%AE%BE%E5%A4%87%E4%B8%8A%E9%83%BD%E6%94%BE%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E3%80%82%E7%84%B6%E5%90%8E%E6%AF%8F%E4%B8%AA%E8%AE%BE%E5%A4%87%E4%B8%8A%E9%83%BD%E5%8F%AF%E4%BB%A5%E5%90%84%E8%87%AA%E7%AE%97%E4%B8%80%E6%89%B9%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%AE%8C%E6%88%90%E5%89%8D%E5%90%91%E5%92%8C%E5%90%8E%E5%90%91%EF%BC%8C%E6%8E%A5%E7%9D%80%E5%B0%B1%E6%98%AF%E8%B7%A8%E8%AE%BE%E5%A4%87%E7%9A%84%E8%81%9A%E5%90%88%E6%A2%AF%E5%BA%A6%EF%BC%8C%E5%B9%B6%E5%88%A9%E7%94%A8%E8%81%9A%E5%90%88%E5%90%8E%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%9D%A5%E6%9B%B4%E6%96%B0%E6%A8%A1%E5%9E%8B%EF%BC%89%EF%BC%88%E4%BD%86%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%B9%B6%E8%A1%8C%E6%96%B9%E5%BC%8F%E5%85%B6%E5%AE%9E%E6%98%AF%E9%80%82%E7%94%A8%E4%BA%8E%E2%80%9C%E5%8D%95%E4%B8%AA%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E6%94%BE%E5%9C%A8%E5%8D%95%E5%8D%A1%E4%B8%8A%E2%80%9D%E7%9A%84%E6%83%85%E5%86%B5%E7%9A%84%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%A8%A1%E5%9E%8B%E5%A4%AA%E5%A4%A7%E4%BA%86%EF%BC%8C%E5%AF%BC%E8%87%B4%E6%94%BE%E4%B8%8D%E4%B8%8B%EF%BC%8C%E5%85%B6%E5%AE%9E%E8%BF%99%E4%B8%AA%E5%B9%B6%E8%A1%8C%E6%96%B9%E5%BC%8F%E5%B0%B1%E4%B8%8D%E5%A5%8F%E6%95%88%E4%BA%86%EF%BC%8C%E5%8F%AF%E8%83%BD%E5%BE%97%E4%B8%A4%E7%A7%8D%E5%B9%B6%E8%A1%8C%E6%96%B9%E5%BC%8F%E7%BB%93%E5%90%88%E8%B5%B7%E6%9D%A5%EF%BC%89"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">所谓数据并行，就是，不同设备上，执行相同的计算图（可以理解为，我们不切分模型了，每个设备上都放一个完整的模型。然后每个设备上都可以各自算一批数据，完成前向和后向，接着就是跨设备的聚合梯度，并利用聚合后的梯度来更新模型）（但可以看到，这个并行方式其实是适用于“单个模型可以放在单卡上”的情况的，如果模型太大了，导致放不下，其实这个并行方式就不奏效了，可能得两种并行方式结合起来）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%EF%BC%88collective-communication%EF%BC%89%E4%B8%AD%E7%9A%84%E9%80%9A%E4%BF%A1%E5%8E%9F%E8%AF%AD%EF%BC%88primitive%EF%BC%89"><span class="nav-number">3.3.2.</span> <span class="nav-text">2.2.2. 集合通信（collective communication）中的通信原语（primitive）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3%E4%B8%BA%EF%BC%8C%E8%AE%BE%E5%A4%87%E9%97%B4%E6%95%B0%E6%8D%AE%E7%9A%84%E4%BC%A0%E8%BE%93%EF%BC%88%E5%9B%A0%E4%B8%BA%E6%AD%A4%E6%97%B6%E6%AF%8F%E4%B8%AA%E8%AE%BE%E5%A4%87%E4%B8%8A%E9%83%BD%E6%9C%89%E5%90%84%E8%87%AA%E7%9A%84%E5%89%8D%E5%90%91%E5%92%8C%E5%90%8E%E5%90%91%EF%BC%8C%E6%89%80%E4%BB%A5%E9%9C%80%E8%A6%81%E8%81%9A%E5%90%88%E6%A2%AF%E5%BA%A6%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E9%9C%80%E8%A6%81%E4%BC%A0%E9%80%92%E6%95%B0%E6%8D%AE%EF%BC%89%E3%80%82%E8%80%8C%E9%80%9A%E4%BF%A1%E5%8E%9F%E8%AF%AD%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3%E4%B8%BA%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E7%9A%84%E4%BC%A0%E8%BE%93%E6%93%8D%E4%BD%9C"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">集合通信，可以简单理解为，设备间数据的传输（因为此时每个设备上都有各自的前向和后向，所以需要聚合梯度，也就是需要传递数据）。而通信原语，可以简单理解为一些基本的传输操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E4%BF%A1%E5%8E%9F%E8%AF%AD%EF%BC%8C%E4%B8%BB%E8%A6%81%E5%88%86%E4%B8%89%E7%A7%8D%EF%BC%9A%E4%B8%80%E5%AF%B9%E5%A4%9A%EF%BC%8C%E5%A4%9A%E5%AF%B9%E4%B8%80%EF%BC%8C%E5%A4%9A%E5%AF%B9%E5%A4%9A"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">通信原语，主要分三种：一对多，多对一，多对多</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E5%AF%B9%E5%A4%9A%EF%BC%9A"><span class="nav-number">3.3.2.3.</span> <span class="nav-text">一对多：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E6%9C%89Scatter%E5%92%8CBroadcast"><span class="nav-number">3.3.2.3.1.</span> <span class="nav-text">主要有Scatter和Broadcast</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Scatter%EF%BC%9A%E5%AE%83%E5%B0%B1%E6%98%AF%E6%8A%8A%E4%B8%BB%E8%8A%82%E7%82%B9%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%88%92%E5%88%86%EF%BC%8C%E5%B9%B6%E5%88%86%E5%8F%91%E5%88%B0%E5%85%B6%E5%AE%83%E7%9A%84%E6%8C%87%E5%AE%9A%E7%9A%84%E8%8A%82%E7%82%B9%EF%BC%88%E8%8A%82%E7%82%B9%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3%E6%88%90%E8%AE%BE%E5%A4%87%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E4%B8%80%E5%BC%A0gpu%E3%80%82%E4%BD%86%E4%B8%8D%E5%90%8C%E8%AF%AD%E5%A2%83%E4%B8%8B%E5%BA%94%E8%AF%A5%E6%9C%89%E4%B8%8D%E5%90%8C%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%89"><span class="nav-number">3.3.2.3.2.</span> <span class="nav-text">Scatter：它就是把主节点的数据进行划分，并分发到其它的指定的节点（节点，可以简单理解成设备，也就是一张gpu。但不同语境下应该有不同的理解）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Broadcast%EF%BC%9A%E5%AE%83%E6%98%AF%E6%9F%90%E4%B8%AA%E8%8A%82%E7%82%B9%EF%BC%8C%E6%8A%8A%E8%87%AA%E8%BA%AB%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%8F%91%E9%80%81%E5%88%B0%E9%9B%86%E7%BE%A4%E9%87%8C%E7%9A%84%E5%85%B6%E5%AE%83%E8%8A%82%E7%82%B9%E4%B8%8A%EF%BC%88%E6%B3%A8%E6%84%8F%EF%BC%8C%E8%BF%99%E5%B0%B1%E6%98%AFBroadcast%E5%92%8CScatter%E7%9A%84%E5%8C%BA%E5%88%AB%E4%BA%86%EF%BC%8CBroadcast%E5%8F%91%E9%80%81%E7%9A%84%E6%98%AF%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E8%80%8CScatter%E6%98%AF%E6%8A%8A%E5%A4%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%99%E6%8B%86%E6%95%A3%EF%BC%8C%E5%88%86%E5%8F%91%E5%88%B0%E5%90%84%E4%B8%AA%E8%AE%BE%E5%A4%87%E4%B8%8A%EF%BC%89%EF%BC%88%E4%B8%80%E8%88%ACBroadcast%E6%93%8D%E4%BD%9C%E7%94%A8%E4%BA%8E%E5%9C%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%89"><span class="nav-number">3.3.2.3.3.</span> <span class="nav-text">Broadcast：它是某个节点，把自身的数据，发送到集群里的其它节点上（注意，这就是Broadcast和Scatter的区别了，Broadcast发送的是自己的数据，而Scatter是把大的数据给拆散，分发到各个设备上）（一般Broadcast操作用于在分布式机器学习中的参数初始化）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E4%B8%80%EF%BC%9A"><span class="nav-number">3.3.2.4.</span> <span class="nav-text">多对一：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E6%9C%89Reduce%E5%92%8CGather"><span class="nav-number">3.3.2.4.1.</span> <span class="nav-text">主要有Reduce和Gather</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Reduce%EF%BC%9A%E8%A2%AB%E7%A7%B0%E4%B8%BA%E8%A7%84%E7%BA%A6%E4%BA%8E%E6%98%AF%E6%9A%96%EF%BC%8C%E6%98%AF%E4%B8%80%E7%B3%BB%E5%88%97%E7%AE%80%E5%8D%95%E8%BF%90%E7%AE%97%E6%93%8D%E4%BD%9C%E7%9A%84%E7%BB%9F%E7%A7%B0%E3%80%82%E7%BB%86%E5%88%86%E6%9C%89%E5%BE%88%E5%A4%9A%EF%BC%8C%E5%8C%85%E6%8B%AC%EF%BC%9ASum%E3%80%81Min%E3%80%81Max%E7%AD%89%E3%80%82%E8%BF%99%E9%87%8C%E4%B8%8D%E5%A6%A8%E4%BB%A5Sum%E4%B8%BA%E4%BE%8B%EF%BC%8C%E7%90%86%E8%A7%A3%E4%B8%80%E4%B8%8B%EF%BC%9A%E4%BE%8B%E5%A6%82%E7%8E%B0%E5%9C%A8%E5%9B%9B%E4%B8%AA%E8%AE%BE%E5%A4%87%E4%B8%8A%E5%90%84%E8%87%AA%E6%9C%89%E4%B8%80%E4%B8%AA%E4%BA%8C%E7%BB%B4%E5%90%91%E9%87%8F%EF%BC%8C%E7%84%B6%E5%90%8E%E8%A6%81%E6%B1%82%E6%8A%8A%E8%BF%99%E4%BA%9B%E5%90%91%E9%87%8F%E7%BB%99%E7%B4%AF%E5%8A%A0%E8%B5%B7%E6%9D%A5%EF%BC%8C%E9%82%A3Reduce%E6%93%8D%E4%BD%9C%E5%B0%B1%E5%8F%AF%E4%BB%A5%E6%8A%8A%E5%9B%9B%E4%B8%AA%E8%AE%BE%E5%A4%87%E4%B8%8A%E7%9A%84%E4%BA%8C%E7%BB%B4%E5%90%91%E9%87%8F%E8%81%9A%E9%9B%86%E5%88%B0%E4%B8%80%E4%B8%AA%E8%AE%BE%E5%A4%87%E4%B8%8A%EF%BC%88%E6%AF%94%E5%A6%82gpu0%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E7%9B%B8%E5%8A%A0%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%9C%A8%E8%BF%99%E4%B8%AA%E6%8C%87%E5%AE%9A%E7%9A%84%E8%AE%BE%E5%A4%87%E4%B8%8A%E5%BE%97%E5%88%B0%E7%BB%93%E6%9E%9C"><span class="nav-number">3.3.2.4.2.</span> <span class="nav-text">Reduce：被称为规约于是暖，是一系列简单运算操作的统称。细分有很多，包括：Sum、Min、Max等。这里不妨以Sum为例，理解一下：例如现在四个设备上各自有一个二维向量，然后要求把这些向量给累加起来，那Reduce操作就可以把四个设备上的二维向量聚集到一个设备上（比如gpu0），然后相加，从而在这个指定的设备上得到结果</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Gather%EF%BC%9A%E5%AE%83%E6%98%AF%E6%8C%87%EF%BC%8C%E6%8A%8A%E5%A4%9A%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%B0%E5%8D%95%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%EF%BC%88%E6%B3%A8%E6%84%8F%EF%BC%8C%E8%BF%99%E9%87%8C%E5%92%8C%E4%B8%8A%E9%9D%A2%E7%9A%84Reduce%E7%9A%84%E4%B8%8D%E5%90%8C%E5%9C%A8%E4%BA%8E%EF%BC%8CReduce%E6%94%B6%E9%9B%86%E5%AE%8C%E4%BA%86%E6%98%AF%E8%A6%81%E5%81%9A%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C%E7%9A%84%EF%BC%8C%E4%BD%86%E8%BF%99%E9%87%8CGather%E5%8F%AA%E6%98%AF%E5%8D%95%E7%BA%AF%E7%9A%84%E6%94%B6%E9%9B%86%E8%B5%B7%E6%9D%A5%E3%80%82%E5%AE%83%E5%8F%AF%E4%BB%A5%E7%90%86%E8%A7%A3%E4%B8%BA%E5%8F%8D%E5%90%91%E7%9A%84Scatter%EF%BC%89"><span class="nav-number">3.3.2.4.3.</span> <span class="nav-text">Gather：它是指，把多个节点上的数据收集到单个节点上（注意，这里和上面的Reduce的不同在于，Reduce收集完了是要做一些操作的，但这里Gather只是单纯的收集起来。它可以理解为反向的Scatter）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E5%A4%9A%EF%BC%9A"><span class="nav-number">3.3.2.5.</span> <span class="nav-text">多对多：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E6%9C%89All-Gather%E5%92%8CAll-Reduce"><span class="nav-number">3.3.2.5.1.</span> <span class="nav-text">主要有All-Gather和All-Reduce</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#All-Gather%EF%BC%9A%E5%8F%AF%E4%BB%A5%E7%90%86%E8%A7%A3%E4%B8%BAGather-Broadcast%E3%80%82%E4%B9%9F%E5%B0%B1%E6%98%AF%EF%BC%8C%E5%85%88%E6%8A%8A%E5%90%84%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E8%B5%B7%E6%9D%A5%EF%BC%8C%E6%94%BE%E5%88%B0%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%EF%BC%8C%E7%84%B6%E5%90%8E%E5%86%8D%E4%BB%8E%E8%BF%99%E4%B8%AA%E8%8A%82%E7%82%B9%E5%87%BA%E5%8F%91%EF%BC%8C%E8%BF%9B%E8%A1%8CBroadcast"><span class="nav-number">3.3.2.5.2.</span> <span class="nav-text">All-Gather：可以理解为Gather+Broadcast。也就是，先把各个节点的数据收集起来，放到一个节点上，然后再从这个节点出发，进行Broadcast</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#All-Reduce%EF%BC%9A%E5%AE%83%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%EF%BC%8C%E8%A6%81%E5%9C%A8%E6%89%80%E6%9C%89%E7%9A%84%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%BD%E8%BF%9B%E8%A1%8C%E7%9B%B8%E5%90%8C%E7%9A%84Reduce%E6%93%8D%E4%BD%9C%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%9C%8B%E6%88%90%E6%98%AFReduce-Broadcast%EF%BC%88%E8%BF%99%E6%A0%B7%EF%BC%8C%E5%9C%A8%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BC%9A%E5%85%88%E5%BE%97%E5%88%B0Reduce%E7%9A%84%E7%BB%93%E6%9E%9C%EF%BC%8C%E4%B9%8B%E5%90%8E%E5%86%8D%E4%BB%8E%E8%BF%99%E4%B8%AA%E8%8A%82%E7%82%B9%EF%BC%8C%E8%BF%9B%E8%A1%8CBroadcast%EF%BC%8C%E4%BB%8E%E8%80%8C%E6%AF%8F%E4%B8%AA%E8%8A%82%E7%82%B9%E9%83%BD%E8%83%BD%E5%BE%97%E5%88%B0Reduce%E5%90%8E%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%89"><span class="nav-number">3.3.2.5.3.</span> <span class="nav-text">All-Reduce：它的目的是，要在所有的节点上都进行相同的Reduce操作。因此，我们可以看成是Reduce+Broadcast（这样，在一个节点上，我们会先得到Reduce的结果，之后再从这个节点，进行Broadcast，从而每个节点都能得到Reduce后的操作）</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#All-Reduce%E7%9A%84%E5%9F%BA%E7%A1%80%E6%98%AF%E7%82%B9%E5%AF%B9%E7%82%B9%E9%80%9A%E4%BF%A1%EF%BC%8C%E7%82%B9%E5%AF%B9%E7%82%B9%E9%80%9A%E4%BF%A1%E5%8F%88%E5%8F%AF%E4%BB%A5%E5%88%86%E4%B8%BA%E5%90%8C%E6%AD%A5%E7%9A%84%E5%92%8C%E5%BC%82%E6%AD%A5%E7%9A%84%E9%80%9A%E4%BF%A1%EF%BC%88%E5%85%88%E7%95%A5%E3%80%82%E7%AE%80%E5%8D%95%E8%AF%B4%EF%BC%8C%E5%90%8C%E6%AD%A5%E7%9A%84%E5%B0%B1%E6%98%AF%E5%BE%97%E7%AD%89%E6%8E%A5%E6%94%B6%E7%AB%AF%E9%82%A3%E9%87%8C%E5%BC%80%E5%A7%8B%E6%8E%A5%E5%8F%97%E6%95%B0%E6%8D%AE%E4%BA%86%EF%BC%8C%E5%8F%91%E9%80%81%E7%AB%AF%E6%89%8D%E8%83%BD%E7%BB%93%E6%9D%9F%EF%BC%9B%E5%BC%82%E6%AD%A5%E5%88%99%E4%B8%8D%E9%9C%80%E8%A6%81%E8%BF%99%E4%B9%88%E9%BA%BB%E7%83%A6%EF%BC%8C%E5%8F%91%E9%80%81%E5%AE%8C%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%8F%91%E9%80%81%E7%AB%AF%E5%B0%B1%E7%BB%93%E6%9D%9F%E4%BA%86%EF%BC%89"><span class="nav-number">3.3.2.5.3.1.</span> <span class="nav-text">All-Reduce的基础是点对点通信，点对点通信又可以分为同步的和异步的通信（先略。简单说，同步的就是得等接收端那里开始接受数据了，发送端才能结束；异步则不需要这么麻烦，发送完数据，发送端就结束了）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%B8%8B%E9%9D%A2%E5%86%8D%E6%8E%A2%E8%AE%A8All-Reduce%E7%9A%84%E5%AE%9E%E7%8E%B0%E7%AE%97%E6%B3%95%EF%BC%88%E8%BF%99%E4%B8%AA%E6%84%9F%E8%A7%89%E6%9C%89%E7%82%B9%E9%87%8D%E7%82%B9%E5%95%8A%EF%BC%89"><span class="nav-number">3.3.2.5.3.2.</span> <span class="nav-text">下面再探讨All-Reduce的实现算法（这个感觉有点重点啊）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-Reduce-Broadcast%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%9C%80%E6%9C%B4%E7%B4%A0%E7%9A%84%E5%AE%9E%E7%8E%B0%E3%80%82%E5%AE%83%E4%B8%80%E8%88%AC%E6%98%AF%E9%87%87%E7%94%A8%E4%B8%80%E4%B8%AA%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E4%BC%9A%E6%9C%89%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%E5%85%85%E5%BD%93%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%88%E6%88%96%E8%80%85%E8%AF%B4%E4%B8%AD%E5%BF%83%E8%8A%82%E7%82%B9%EF%BC%89%EF%BC%8C%E5%AE%83%E4%BC%9A%E5%85%88%E5%AD%98%E6%94%BEReduce%E7%9A%84%E7%BB%93%E6%9E%9C%EF%BC%8C%E7%BB%8F%E8%BF%87%E6%9C%AC%E5%9C%B0%E8%AE%A1%E7%AE%97%E5%90%8E%EF%BC%8C%E5%86%8DBroadcast%E5%88%B0%E5%90%84%E4%B8%AA%E5%85%B6%E5%AE%83%E8%8A%82%E7%82%B9%E4%B8%8A%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E7%BB%99%E5%AE%9A%E5%A6%82%E4%B8%8B%E7%9A%84%E8%AE%B0%E5%8F%B7%EF%BC%9A-alpha-%E8%A1%A8%E7%A4%BA%E4%B8%A4%E4%B8%AA%E9%80%9A%E4%BF%A1%E8%8A%82%E7%82%B9%E4%B9%8B%E9%97%B4%E7%9A%84%E6%97%B6%E5%BB%B6latency%EF%BC%8CS%E8%A1%A8%E7%A4%BA%E6%95%B0%E6%8D%AE%E5%9D%97%E5%A4%A7%E5%B0%8F%EF%BC%88Size%EF%BC%89%EF%BC%8CB%E8%A1%A8%E7%A4%BA%E4%B8%A4%E4%B8%AA%E9%80%9A%E4%BF%A1%E8%8A%82%E7%82%B9%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B8%A6%E5%AE%BD%EF%BC%88Bandwidth%EF%BC%89%EF%BC%8CC%E8%A1%A8%E7%A4%BA%E6%AF%8F%E5%AD%97%E8%8A%82%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AE%A1%E7%AE%97%E8%80%97%E6%97%B6%EF%BC%88Computation%EF%BC%89%EF%BC%8CN%E8%A1%A8%E7%A4%BA%E8%8A%82%E7%82%B9%E4%B8%AA%E6%95%B0%EF%BC%88Number%EF%BC%89%EF%BC%8C%E5%88%99%E6%80%BB%E4%BD%93%E7%9A%84%E8%80%97%E6%97%B6%E4%B8%BA-2-alpha-frac-S-B-NSC-%E7%8E%B0%E5%9C%A8%E6%88%91%E4%BB%AC%E7%9C%8B%E4%B8%80%E4%B8%8B%E8%BF%99%E4%B8%AA%E5%BC%8F%E5%AD%90%EF%BC%8C%E5%89%8D%E5%8D%8A%E7%9A%84%E9%83%A8%E5%88%86%E8%A1%A8%E7%A4%BA%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E4%B8%8A%E7%9A%84%E8%80%97%E6%97%B6%E3%80%82%E8%8A%82%E7%82%B9%E4%B9%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1%EF%BC%8C%E5%8F%91%E5%87%BA%E6%95%B0%E6%8D%AE%E9%9C%80%E8%A6%81%E8%8A%B1%E6%97%B6%E9%97%B4%EF%BC%8C%E6%8E%A5%E6%94%B6%E6%95%B0%E6%8D%AE%E4%B9%9F%E8%A6%81%E8%8A%B1%E6%97%B6%E9%97%B4%EF%BC%8C%E6%89%80%E4%BB%A5%E6%9C%89%E4%B8%AA%E5%9B%A0%E5%AD%902%EF%BC%9B%E8%80%8C-alpha-%E6%98%AF%E6%97%B6%E5%BB%B6%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3%E4%B8%BA%E7%A1%AC%E6%80%A7%E7%9A%84%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%EF%BC%9B-frac-S-B-%E5%88%99%E8%A1%A8%E7%A4%BA%EF%BC%8C%E5%A4%A7%E5%B0%8F%E4%B8%BAS%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%8F%91%E9%80%81%E8%A6%81%E5%A4%9A%E4%B9%85%EF%BC%88%E4%BB%A5%E5%8F%8A%E6%8E%A5%E6%94%B6%E8%A6%81%E5%A4%9A%E4%B9%85%EF%BC%89%EF%BC%9B%E5%90%8E%E5%8D%8A%E9%83%A8%E5%88%86%E5%88%99%E6%98%AF%E6%80%BB%E5%85%B1N%E4%B8%AA%E8%8A%82%E7%82%B9%EF%BC%8C%E5%AE%83%E4%BB%AC%E9%83%BD%E6%8A%8A%E6%95%B0%E6%8D%AE%E4%BC%A0%E5%88%B0%E4%B8%AD%E5%BF%83%E8%8A%82%E7%82%B9%E4%B8%8A%EF%BC%8C%E4%B8%80%E5%85%B1%E6%98%AF-N-S-%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%EF%BC%8C%E5%86%8D%E4%B9%98%E4%BB%A5C%EF%BC%8C%E5%88%99%E5%BE%97%E5%88%B0%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%96%B9%E9%9D%A2%E7%9A%84%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%E8%BF%99%E4%B8%AA%E7%AE%97%E6%B3%95%E7%9A%84%E6%9C%80%E5%A4%A7%E7%BC%BA%E7%82%B9%E5%B0%B1%E6%98%AF%EF%BC%8Cparameter-server%E7%9A%84%E9%82%A3%E4%B8%AA%E8%8A%82%E7%82%B9%EF%BC%8C%E5%AE%83%E7%9A%84%E5%B8%A6%E5%AE%BD%E4%BC%9A%E6%88%90%E4%B8%BA%E7%93%B6%E9%A2%88%EF%BC%88%E5%9B%A0%E4%B8%BA%E8%BF%99%E4%B8%AA%E8%8A%82%E7%82%B9%EF%BC%8C%E5%AE%83%E4%B8%80%E5%BC%80%E5%A7%8Breduce%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%B0%B1%E5%BE%97%E4%BB%8E%E5%85%B6%E5%AE%83%E8%8A%82%E7%82%B9%E9%82%A3%E9%87%8C%E6%8E%A5%E6%94%B6%E5%BE%88%E5%A4%9A%E6%95%B0%E6%8D%AE%EF%BC%8C%E8%BF%99%E9%87%8C%E5%B0%B1%E5%BE%97%E8%80%97%E6%97%B6%E4%BA%86%EF%BC%9B%E4%B9%8B%E5%90%8E%E5%AE%83%E5%8F%88%E5%BE%97%E8%BF%9B%E8%A1%8Cbroadcast%EF%BC%8C%E8%A6%81%E5%BE%80%E5%85%B6%E5%AE%83%E8%8A%82%E7%82%B9%E9%82%A3%E9%87%8C%E5%8F%91%E9%80%81%E5%BE%88%E5%A4%9A%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%B9%9F%E5%BE%88%E8%80%97%E6%97%B6%EF%BC%89%EF%BC%88%E6%80%8E%E4%B9%88%E8%B6%8A%E7%9C%8B%E8%B6%8A%E6%84%9F%E8%A7%89%E8%BF%99%E4%B8%AA%E5%85%AC%E5%BC%8F%E6%9C%89%E7%82%B9%E9%97%AE%E9%A2%98%EF%BC%9F%EF%BC%89"><span class="nav-number">3.3.2.5.3.3.</span> <span class="nav-text">1. Reduce+Broadcast也就是最朴素的实现。它一般是采用一个参数服务器的分布式架构，也就是会有一个节点充当服务器（或者说中心节点），它会先存放Reduce的结果，经过本地计算后，再Broadcast到各个其它节点上如果我们给定如下的记号：$\alpha$表示两个通信节点之间的时延latency，S表示数据块大小（Size），B表示两个通信节点之间的带宽（Bandwidth），C表示每字节数据的计算耗时（Computation），N表示节点个数（Number），则总体的耗时为$$2*(\alpha+\frac{S}{B})+NSC$$现在我们看一下这个式子，前半的部分表示数据传递上的耗时。节点之间的通信，发出数据需要花时间，接收数据也要花时间，所以有个因子2；而$\alpha$是时延，可以简单理解为硬性的时间开销；$\frac{S}{B}$则表示，大小为S的数据，发送要多久（以及接收要多久）；后半部分则是总共N个节点，它们都把数据传到中心节点上，一共是$N*S$的数据量，再乘以C，则得到在计算方面的时间开销这个算法的最大缺点就是，parameter server的那个节点，它的带宽会成为瓶颈（因为这个节点，它一开始reduce的时候，就得从其它节点那里接收很多数据，这里就得耗时了；之后它又得进行broadcast，要往其它节点那里发送很多数据，也很耗时）（怎么越看越感觉这个公式有点问题？）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-%E6%A0%91%E5%BD%A2%E9%80%92%E5%BD%92%E7%AE%97%E6%B3%95%E5%85%B6%E6%95%B4%E4%BD%93%E8%80%97%E6%97%B6%E4%B8%BA-log-2-N-alpha-frac-S-B-2SC-log-2-N-alpha-frac-S-B-2log-2-N-alpha-frac-S-B-SC-%E5%85%B6%E4%B8%AD%EF%BC%8C-log-2-N-%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E6%98%AF%E2%80%9C%E5%B1%82%E6%95%B0%E2%80%9D%EF%BC%8C%E5%9B%A0%E4%B8%BA%E6%88%91%E4%BB%AC%E6%98%AF%E4%B8%A4%E4%B8%A4%E8%8A%82%E7%82%B9%E7%BB%84%E5%90%88%E5%88%B0%E4%B8%80%E8%B5%B7%EF%BC%9B%E5%89%8D%E5%8D%8A%E9%83%A8%E5%88%86%E7%9A%84%E6%97%B6%E9%97%B4%E6%98%AF%E8%81%9A%E5%90%88%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8A%E8%BF%90%E7%AE%97%E6%95%B0%E6%8D%AE%E7%9A%84%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%E3%80%82%E6%AF%8F%E4%B8%80%E5%B1%82%E8%99%BD%E7%84%B6%E6%9C%89%E5%BE%88%E5%A4%9A%E8%8A%82%E7%82%B9%E5%9C%A8%E8%81%9A%E5%90%88%E5%92%8C%E8%AE%A1%E7%AE%97%EF%BC%8C%E4%BD%86%E5%AE%83%E4%BB%AC%E9%83%BD%E6%98%AF%E7%8B%AC%E7%AB%8B%E7%9A%84%EF%BC%8C%E4%BA%92%E4%B8%8D%E5%BD%B1%E5%93%8D%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E6%98%AF%E5%90%8C%E6%97%B6%E8%BF%9B%E8%A1%8C%E7%9A%84%EF%BC%8C%E6%89%80%E4%BB%A5%E6%AF%8F%E4%B8%80%E5%B1%82%E7%9A%84%E8%81%9A%E5%90%88%E6%89%80%E8%8A%B1%E7%9A%84%E6%97%B6%E9%97%B4%E9%83%BD%E6%98%AF-alpha-frac-S-B-%EF%BC%9B%E8%80%8C%E6%AF%8F%E4%B8%80%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%E9%83%BD%E6%98%AF-2BC-%EF%BC%88%E8%BF%99%E9%87%8C%E6%9C%89%E4%B8%AA2%EF%BC%8C%E6%98%AF%E5%9B%A0%E4%B8%BA%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%A4%E4%B8%A4%E7%BB%84%E5%90%88%E8%8A%82%E7%82%B9%E6%9D%A5%E8%AE%A1%E7%AE%97%EF%BC%8C%E5%88%99%E9%9C%80%E8%A6%81%E8%AE%A1%E7%AE%97%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%E6%98%AF-2S-%EF%BC%8C%E5%86%8D%E4%B9%98%E4%B8%8A%E8%AE%A1%E7%AE%97%E7%9A%84%E2%80%9C%E9%80%9F%E5%BA%A6%E2%80%9CC%EF%BC%8C%E5%8D%B3%E5%BE%97%E5%88%B0%E8%AE%A1%E7%AE%97%E5%BC%80%E9%94%80%EF%BC%89%E8%87%B3%E4%BA%8E%E5%90%8E%E5%8D%8A%E9%83%A8%E5%88%86%EF%BC%8C%E5%88%99%E6%98%AF%E6%8A%8A%E7%BB%93%E6%9E%9C%E5%88%86%E5%8F%91%E5%87%BA%E5%8E%BB%E7%9A%84%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%E3%80%82%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E5%88%86%E5%8F%91%E5%92%8C%E8%81%9A%E5%90%88%E7%9A%84%E6%97%B6%E9%97%B4%E5%BC%80%E9%94%80%E6%98%AF%E4%B8%80%E6%A0%B7%E7%9A%84%EF%BC%8C%E4%B8%94%E5%9B%A0%E4%B8%BA%E5%90%8E%E5%8D%8A%E9%83%A8%E5%88%86%E5%B7%B2%E7%BB%8F%E7%AE%97%E5%AE%8C%E4%BA%86%EF%BC%8C%E6%89%80%E4%BB%A5%E4%B8%8D%E5%86%8D%E6%9C%89%E8%AE%A1%E7%AE%97%E4%B8%8A%E7%9A%84%E5%BC%80%E9%94%80"><span class="nav-number">3.3.2.5.3.4.</span> <span class="nav-text">2. 树形递归算法其整体耗时为$$log_{2}N*(\alpha+\frac{S}{B}+2SC)+log_{2}N*(\alpha+\frac{S}{B})&#x3D;2log_{2}N(\alpha+\frac{S}{B}+SC)$$其中，$log_{2}N$可以认为是“层数”，因为我们是两两节点组合到一起；前半部分的时间是聚合数据以及运算数据的时间开销。每一层虽然有很多节点在聚合和计算，但它们都是独立的，互不影响，可以认为是同时进行的，所以每一层的聚合所花的时间都是$\alpha+\frac{S}{B}$；而每一层的计算时间开销都是$2BC$（这里有个2，是因为，我们两两组合节点来计算，则需要计算的数据量是$2S$，再乘上计算的“速度“C，即得到计算开销）至于后半部分，则是把结果分发出去的时间开销。可以看到，分发和聚合的时间开销是一样的，且因为后半部分已经算完了，所以不再有计算上的开销</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%B8%8A%E9%9D%A2%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%E9%83%BD%E6%98%AF%E5%9F%BA%E4%BA%8EReduce-Broadcast%E7%9A%84%E3%80%82%E8%BF%98%E6%9C%89%E4%B8%80%E7%B1%BB%E6%98%AF%E5%9F%BA%E4%BA%8EScatter-Reduce-All-Gather%E7%9A%84%E6%93%8D%E4%BD%9C%E7%9A%84%EF%BC%8C%E8%BF%99%E9%87%8C%E4%BB%A5Ring-AllReduce%E4%B8%BA%E4%BE%8B"><span class="nav-number">3.3.2.5.3.5.</span> <span class="nav-text">上面两种方法都是基于Reduce+Broadcast的。还有一类是基于Scatter-Reduce+All-Gather的操作的，这里以Ring AllReduce为例</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-Ring-AllReduce%E6%97%A2%E7%84%B6%E5%AE%83%E6%98%AF%E5%9F%BA%E4%BA%8EScatter-Reduce-All-Gather%E7%9A%84%EF%BC%8C%E9%82%A3%E4%B9%88%E5%AE%83%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AD%A5%E5%B0%B1%E6%98%AFScatter-Reduce%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E6%AF%8F%E4%B8%AA%E8%8A%82%E7%82%B9%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%AF%8F%E5%BC%A0%E5%8D%A1%EF%BC%8C%E6%88%96%E7%A7%B0%E4%B8%BA%E6%AF%8F%E4%B8%AAworker%EF%BC%89%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%EF%BC%8C%E6%88%91%E4%BB%AC%E8%AE%B0%E4%BD%9CK%EF%BC%9B%E7%84%B6%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AF%B9%E6%AF%8F%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E9%83%BD%E5%88%92%E5%88%86%E4%B8%BAN%E4%B8%AAchunk%EF%BC%88N%E4%B9%9F%E6%98%AF%E8%8A%82%E7%82%B9%E7%9A%84%E4%B8%AA%E6%95%B0%EF%BC%89%EF%BC%9B%E4%B9%8B%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%BB%99%E6%AF%8F%E4%B8%AAchunk%E4%B8%80%E4%B8%AA%E7%BC%96%E5%8F%B7%EF%BC%8C%E5%88%99%E6%9C%890-N-1%E3%80%82%E7%8E%B0%E5%9C%A8%EF%BC%8C%E5%AF%B9%E4%BA%8Echunk-0%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E7%AC%ACi%E4%B8%AAgpu%EF%BC%8C%E5%AE%83%E4%BC%9A%E5%90%91%E4%B8%8B%E4%B8%80%E4%B8%AAgpu%E4%BC%A0%E9%80%92%E5%AE%83%E7%9A%84chunk-i%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%88%E5%AF%B9%E4%BA%8Egpu-0%EF%BC%8C%E5%AE%83%E7%9A%84%E4%B8%8A%E4%B8%80%E4%B8%AAgpu%EF%BC%8C%E6%88%91%E4%BB%AC%E5%BD%93%E6%88%90gpu-N-1%EF%BC%89%EF%BC%9B%E4%B9%8B%E5%90%8E%EF%BC%8C%E6%8E%A5%E6%94%B6%E5%88%B0chunk%E6%95%B0%E6%8D%AE%E7%9A%84gpu%EF%BC%8C%E5%B0%B1%E7%BB%A7%E7%BB%AD%E6%8A%8A%E5%AE%83%E6%8E%A5%E6%94%B6%E5%88%B0%E7%9A%84chunk%E6%95%B0%E6%8D%AE%E6%95%B4%E5%90%88%EF%BC%8C%E5%BE%80%E4%B8%8B%E4%B8%80%E4%B8%AAgpu%E7%9A%84%E7%9B%B8%E5%90%8Cchunk%E4%B8%8A%E4%BC%A0%E3%80%82%E8%BF%99%E6%A0%B7%E4%B8%80%E7%9B%B4%E4%BC%A0%EF%BC%8C%E4%BC%A0N-1%E6%AC%A1%EF%BC%8C%E5%88%99N%E4%B8%AAgpu%E4%B8%8A%EF%BC%8C%E5%90%84%E8%87%AA%E9%83%BD%E4%BC%9A%E6%9C%89%E4%B8%80%E4%B8%AAchunk%E4%B8%8A%E8%81%9A%E5%90%88%E5%AE%8C%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%88%E4%B8%94%E6%AF%8F%E4%B8%AAgpu%E5%8F%AA%E4%BC%9A%E6%9C%89%E4%B8%80%E4%B8%AA%E8%BF%99%E7%A7%8D%E8%81%9A%E5%90%88%E5%AE%8C%E7%9A%84chunk%EF%BC%89%E6%8E%A5%E4%B8%8B%E6%9D%A5%E6%98%AFAll-gather%E3%80%82All-gather%E6%97%B6%EF%BC%8C%E6%AF%8F%E4%B8%AAgpu%E9%83%BD%E5%BE%80%E4%B8%8B%E4%B8%80%E4%B8%AAgpu%E4%BC%A0%E9%80%92%E8%87%AA%E5%B7%B1%E7%9A%84%E8%81%9A%E5%90%88%E5%AE%8C%E7%9A%84%E9%82%A3%E4%B8%AAchunk%EF%BC%8C%E4%B8%94gpu%E6%8E%A5%E6%94%B6%E4%BA%86%E5%93%AA%E4%B8%AA%E6%96%B0%E7%9A%84chunk%E4%B9%8B%E5%90%8E%EF%BC%8C%E4%B9%9F%E7%BB%A7%E7%BB%AD%E5%BE%80%E4%B8%8B%E4%BC%A0%E9%80%92%E3%80%82%E8%BF%99%E6%A0%B7%E4%BC%A0%E9%80%92N-1%E6%AC%A1%EF%BC%8C%E6%AF%8F%E4%B8%AAgpu%E7%9A%84%E6%AF%8F%E4%B8%AAchunk%EF%BC%8C%E9%83%BD%E6%98%AF%E8%81%9A%E5%90%88%E5%AE%8C%E7%9A%84%E3%80%82%E8%87%B3%E6%AD%A4%EF%BC%8CRing-AllReduce%E7%BB%93%E6%9D%9F%E5%88%86%E6%9E%90%E4%B8%80%E4%B8%8B%E5%AE%83%E7%9A%84%E8%80%97%E6%97%B6%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%85%88%E5%88%86%E6%9E%90%E4%B8%80%E4%B8%8B%E6%AF%8F%E4%B8%AAworker%E6%80%BB%E5%85%B1%E6%8E%A5%E6%94%B6-%E4%BC%A0%E9%80%92%E4%BA%86%E5%A4%9A%E5%B0%91%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%E3%80%82Scatter-Reduce%E5%92%8CAll-Gather%E9%98%B6%E6%AE%B5%EF%BC%8C%E6%AF%8F%E4%B8%AAgpu%E9%83%BD%E6%98%AF%E6%8E%A5%E6%94%B6-%E4%BC%A0%E9%80%92%E4%BA%86N-1%E6%AC%A1%E7%9A%84%EF%BC%8C%E5%88%99%E4%B8%A4%E4%B8%AA%E9%98%B6%E6%AE%B5%EF%BC%8C%E6%80%BB%E5%85%B1%E6%AC%A1%E6%95%B0%E6%98%AF-2-N-1-%EF%BC%9B%E4%B8%94%E6%AF%8F%E6%AC%A1%E9%83%BD%E6%98%AF%E6%8E%A5%E6%94%B6-%E4%BC%A0%E9%80%92%E4%B8%80%E4%B8%AAchunk%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF-frac-K-N-%EF%BC%9B%E6%95%85%E6%AF%8F%E4%B8%AAworker%E6%8E%A5%E6%94%B6-%E4%BC%A0%E9%80%92%E7%9A%84%E6%95%B0%E6%8D%AE%E6%80%BB%E9%87%8F%E5%BA%94%E4%B8%BA-2-N-1-frac-K-N-%E6%88%91%E4%BB%AC%E8%A7%82%E5%AF%9F%E4%B8%80%E4%B8%8B%E8%BF%99%E4%B8%AA%E7%BB%93%E6%9E%9C%EF%BC%8C%E5%8F%91%E7%8E%B0%EF%BC%8C%E6%AF%8F%E4%B8%AAworker%E7%9A%84%E9%80%9A%E4%BF%A1%E6%95%B0%E6%8D%AE%E9%87%8F%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E4%B8%8A%E9%9D%A2%E5%8F%8D%E5%A4%8D%E6%8F%90%E5%88%B0%E7%9A%84%EF%BC%8C%E6%8E%A5%E6%94%B6-%E4%BC%A0%E9%80%92%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%EF%BC%89%E5%8F%AF%E4%BB%A5%E8%BF%91%E4%BC%BC%E4%B8%BA-2K-%EF%BC%88%E5%B0%A4%E5%85%B6%E6%98%AFN%E8%BE%83%E5%A4%A7%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%89%EF%BC%9B%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E8%BF%99%E4%B8%AA%E9%80%9A%E4%BF%A1%E6%95%B0%E6%8D%AE%E9%87%8F%E6%98%AF%E8%BF%91%E4%BC%BC%E7%8B%AC%E7%AB%8B%E4%BA%8E%E7%BD%91%E7%BB%9C%E4%B8%ADworker%E7%9A%84%E6%95%B0%E9%87%8F%E7%9A%84%E3%80%82%E8%BF%99%E4%BB%A3%E8%A1%A8%E4%BB%80%E4%B9%88%EF%BC%9F%E8%BF%99%E4%BB%A3%E8%A1%A8%E5%8D%B3%E4%BD%BF%E6%88%91%E4%BB%AC%E7%9A%84%E6%A8%A1%E5%9E%8B%E3%80%81%E6%95%B0%E6%8D%AE%E8%A7%84%E6%A8%A1%E8%B6%8A%E6%9D%A5%E8%B6%8A%E5%A4%A7%EF%BC%8C%E7%94%A8%E5%88%B0%E7%9A%84worker%E8%B6%8A%E6%9D%A5%E8%B6%8A%E5%A4%9A%EF%BC%8C%E4%BD%86%E9%80%9A%E4%BF%A1%E4%B8%8A%E7%9A%84%E5%BC%80%E9%94%80%E5%B9%B6%E4%B8%8D%E4%BC%9A%E5%89%A7%E5%A2%9E%EF%BC%8C%E8%80%8C%E6%98%AF%E5%9F%BA%E6%9C%AC%E5%B7%AE%E4%B8%8D%E5%A4%9A%EF%BC%88%E4%BD%86%E5%8F%AF%E8%83%BD%E8%BF%98%E6%9C%89%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82%E6%B2%A1%E8%80%83%E8%99%91%E5%88%B0%E5%90%A7%EF%BC%89%EF%BC%9B%E7%9B%B8%E6%AF%94%E4%B9%8B%E4%B8%8B%EF%BC%8C%E4%B8%8A%E9%9D%A2%E7%9A%84%E6%9C%B4%E7%B4%A0%E5%81%9A%E6%B3%95%EF%BC%88%E4%BE%8B%E5%A6%82%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84%EF%BC%89%EF%BC%8C%E9%87%8C%E9%9D%A2%E7%9A%84%E4%B8%BB%E8%8A%82%E7%82%B9%EF%BC%8C%E5%AE%83%E7%9A%84%E9%80%9A%E4%BF%A1%E9%87%8F%E5%B0%B1%E6%98%AF-O-N-K-%E7%BA%A7%E5%88%AB%E7%9A%84%E4%B8%94%EF%BC%8C%E6%AF%8F%E4%B8%AAworker%E7%9A%84%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E8%B4%9F%E8%BD%BD%E6%98%AF%E5%9D%87%E8%A1%A1%E7%9A%84%EF%BC%88%E6%AF%8F%E4%B8%AA%E6%97%B6%E5%88%BB%EF%BC%8C%E6%AF%8F%E4%B8%AAworker%E6%97%A2%E4%BC%9A%E6%8E%A5%E6%94%B6chunk%EF%BC%8C%E4%B9%9F%E4%BC%9A%E5%88%86%E5%8F%91chunk%EF%BC%8C%E4%B8%8D%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%A0%91%E5%BD%A2%E9%80%92%E5%BD%92%E7%AE%97%E6%B3%95%E9%87%8C%EF%BC%8C%E6%9C%89%E4%BA%9Bworker%E5%9C%A8%E5%82%BB%E7%AD%89%E7%9D%80%E8%BF%9B%E8%A1%8C%E5%88%86%E5%8F%91%EF%BC%89%EF%BC%8C%E7%BD%91%E7%BB%9C%E5%8F%8C%E5%90%91%E5%B8%A6%E5%AE%BD%E5%BE%97%E5%88%B0%E5%85%85%E5%88%86%E5%88%A9%E7%94%A8%EF%BC%88%E6%AC%B8%EF%BC%8C%E8%BF%99%E4%B9%88%E8%AF%B4%EF%BC%8C%E6%8E%A5%E6%94%B6%E5%92%8C%E5%88%86%E5%8F%91%E7%9A%84%E5%B8%A6%E5%AE%BD%EF%BC%8C%E5%8E%9F%E6%9D%A5%E4%B8%8D%E6%98%AF%E5%90%8C%E4%B8%80%E4%B8%AA%E5%90%97%EF%BC%9F%EF%BC%89"><span class="nav-number">3.3.2.5.3.6.</span> <span class="nav-text">3. Ring AllReduce既然它是基于Scatter-Reduce+All-Gather的，那么它的第一步就是Scatter-Reduce具体来说，每个节点（也就是每张卡，或称为每个worker）上的数据量，我们记作K；然后，我们对每个节点上的数据，都划分为N个chunk（N也是节点的个数）；之后，我们可以给每个chunk一个编号，则有0 ~ N-1。现在，对于chunk 0，我们的第i个gpu，它会向下一个gpu传递它的chunk i的数据（对于gpu 0，它的上一个gpu，我们当成gpu N-1）；之后，接收到chunk数据的gpu，就继续把它接收到的chunk数据整合，往下一个gpu的相同chunk上传。这样一直传，传N-1次，则N个gpu上，各自都会有一个chunk上聚合完的数据（且每个gpu只会有一个这种聚合完的chunk）接下来是All gather。All gather时，每个gpu都往下一个gpu传递自己的聚合完的那个chunk，且gpu接收了哪个新的chunk之后，也继续往下传递。这样传递N-1次，每个gpu的每个chunk，都是聚合完的。至此，Ring AllReduce结束分析一下它的耗时。我们可以先分析一下每个worker总共接收&#x2F;传递了多少的数据量。Scatter-Reduce和All Gather阶段，每个gpu都是接收&#x2F;传递了N-1次的，则两个阶段，总共次数是$2*(N-1)$；且每次都是接收&#x2F;传递一个chunk的数据量，也就是$\frac{K}{N}$；故每个worker接收&#x2F;传递的数据总量应为$2*(N-1)\frac{K}{N}$我们观察一下这个结果，发现，每个worker的通信数据量（也就是上面反复提到的，接收&#x2F;传递的数据量）可以近似为$2K$（尤其是N较大的时候）；也就是说，这个通信数据量是近似独立于网络中worker的数量的。这代表什么？这代表即使我们的模型、数据规模越来越大，用到的worker越来越多，但通信上的开销并不会剧增，而是基本差不多（但可能还有一些细节没考虑到吧）；相比之下，上面的朴素做法（例如主从架构），里面的主节点，它的通信量就是$O(N*K)$级别的且，每个worker的网络收发负载是均衡的（每个时刻，每个worker既会接收chunk，也会分发chunk，不会出现树形递归算法里，有些worker在傻等着进行分发），网络双向带宽得到充分利用（欸，这么说，接收和分发的带宽，原来不是同一个吗？）</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E5%8E%9F%E8%AF%AD%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%90%8C%E6%AD%A5SGD%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.3.</span> <span class="nav-text">2.2.3. 集合通信原语的应用：分布式同步SGD算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%81%E7%A8%8B%E5%80%92%E4%B9%9F%E7%AE%80%E5%8D%95%E3%80%82%E6%AF%8F%E4%B8%AA%E8%8A%82%E7%82%B9%E5%85%88%E6%8A%8A%E8%87%AA%E5%B7%B1%E7%9A%84%E6%A2%AF%E5%BA%A6%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA0%EF%BC%8C%E7%84%B6%E5%90%8E%E6%8B%BFbatch-size%E4%B8%AA%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%89%8D%E5%90%91-%E5%90%8E%E5%90%91%EF%BC%8C%E6%8A%8A%E5%90%84%E8%87%AA%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B1%82%E4%B8%AA%E5%9D%87%E5%80%BC%EF%BC%88%E8%BF%99%E9%87%8C%E5%8F%AF%E8%83%BD%E4%BC%9A%E7%94%A8%E5%8A%A8%E9%87%8F%E5%BC%8F%E6%9B%B4%E6%96%B0%E5%9C%B0%E6%96%B9%E6%96%B9%E6%B3%95%EF%BC%8C%E4%B9%9F%E5%8D%B3%EF%BC%9A-G-t-k-leftarrow-G-t-k-frac-1-N-b-nabla-f-x-w-t-%EF%BC%89%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%AF%B9%E5%90%84%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E5%B9%B3%E5%9D%87%E6%A2%AF%E5%BA%A6%EF%BC%8C%E8%BF%9B%E8%A1%8CAll-Reduce%EF%BC%88%E8%BF%99%E9%87%8C%E5%B0%B1%E6%98%AF%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E9%80%9A%E4%BF%A1%E7%9A%84%E5%9C%B0%E6%96%B9%E4%BA%86%EF%BC%8C%E5%9B%A0%E4%B8%BA%E6%88%91%E4%BB%AC%E8%A6%81%E4%BC%A0%E9%80%92%E5%90%84%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84%E6%A2%AF%E5%BA%A6%EF%BC%89%EF%BC%8C%E5%BE%97%E5%88%B0%E4%B8%80%E4%B8%AA%E6%80%BB%E7%9A%84%E6%A2%AF%E5%BA%A6%EF%BC%8C%E5%B9%B6%E6%8B%BF%E6%9D%A5%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%EF%BC%8C%E7%94%B1%E6%AD%A4%EF%BC%8C%E4%B8%80%E4%B8%AAiteration%E7%BB%93%E6%9D%9F%E3%80%82%E5%90%8E%E9%9D%A2%E5%B0%B1%E6%98%AF%E9%87%8D%E5%A4%8D%E8%BF%99%E4%B8%AAiteration%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E7%9B%B4%E5%88%B0%E8%BE%BE%E5%88%B0%E6%8C%87%E5%AE%9A%E7%9A%84iteration"><span class="nav-number">3.3.3.1.</span> <span class="nav-text">流程倒也简单。每个节点先把自己的梯度初始化为0，然后拿batch_size个数据进行前向+后向，把各自节点上的梯度求个均值（这里可能会用动量式更新地方方法，也即：$G_{t}^{k}\leftarrow G_{t}^{k}+\frac{1}{N_{b}}\nabla f(x;w_{t})$）之后，对各个节点的平均梯度，进行All-Reduce（这里就是我们需要通信的地方了，因为我们要传递各个节点上的梯度），得到一个总的梯度，并拿来更新参数，由此，一个iteration结束。后面就是重复这个iteration的操作，直到达到指定的iteration</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">134</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
