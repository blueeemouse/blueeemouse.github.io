<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="原文链接在这">
<meta property="og:type" content="article">
<meta property="og:title" content="blog reading：OpenAI o1 技术初探1：整体框架，利用Test-Time Scaling Law提升逻辑推理能力">
<meta property="og:url" content="https://blueeemouse.github.io/2025/05/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/reasoning/blog%20reading%EF%BC%9AOpenAI%20o1%20%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A21%EF%BC%9A%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%EF%BC%8C%E5%88%A9%E7%94%A8Test-Time%20Scaling%20Law%E6%8F%90%E5%8D%87%E9%80%BB%E8%BE%91%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:description" content="原文链接在这">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-05-30T09:32:00.000Z">
<meta property="article:modified_time" content="2025-06-01T16:15:33.916Z">
<meta property="article:author" content="bluemouse">
<meta property="article:tag" content="TTS">
<meta property="article:tag" content="reasoning">
<meta property="article:tag" content="o1">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/2025/05/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/reasoning/blog%20reading%EF%BC%9AOpenAI%20o1%20%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A21%EF%BC%9A%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%EF%BC%8C%E5%88%A9%E7%94%A8Test-Time%20Scaling%20Law%E6%8F%90%E5%8D%87%E9%80%BB%E8%BE%91%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>blog reading：OpenAI o1 技术初探1：整体框架，利用Test-Time Scaling Law提升逻辑推理能力 | bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/05/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/reasoning/blog%20reading%EF%BC%9AOpenAI%20o1%20%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A21%EF%BC%9A%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%EF%BC%8C%E5%88%A9%E7%94%A8Test-Time%20Scaling%20Law%E6%8F%90%E5%8D%87%E9%80%BB%E8%BE%91%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          blog reading：OpenAI o1 技术初探1：整体框架，利用Test-Time Scaling Law提升逻辑推理能力
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-30 17:32:00" itemprop="dateCreated datePublished" datetime="2025-05-30T17:32:00+08:00">2025-05-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-06-02 00:15:33" itemprop="dateModified" datetime="2025-06-02T00:15:33+08:00">2025-06-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blog%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">blog阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blog%E9%98%85%E8%AF%BB/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="原文链接在这"><a href="#原文链接在这" class="headerlink" title="原文链接在这"></a>原文链接<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/773907223">在这</a></h1><span id="more"></span>
<h1 id="篇幅还蛮长的，但讲的很好，作为入门很合适（非常感谢。本文仅用于个人学习的记录与思考，无商业用途）"><a href="#篇幅还蛮长的，但讲的很好，作为入门很合适（非常感谢。本文仅用于个人学习的记录与思考，无商业用途）" class="headerlink" title="篇幅还蛮长的，但讲的很好，作为入门很合适（非常感谢。本文仅用于个人学习的记录与思考，无商业用途）"></a>篇幅还蛮长的，但讲的很好，作为入门很合适（非常感谢。本文仅用于个人学习的记录与思考，无商业用途）</h1><h1 id="一、什么是Test-Inferenc-time-Scaling-Law？"><a href="#一、什么是Test-Inferenc-time-Scaling-Law？" class="headerlink" title="一、什么是Test&#x2F;Inferenc-time Scaling Law？"></a>一、什么是Test&#x2F;Inferenc-time Scaling Law？</h1><h2 id="首先明确一下，现在是研究reasoning的场景"><a href="#首先明确一下，现在是研究reasoning的场景" class="headerlink" title="首先明确一下，现在是研究reasoning的场景"></a>首先明确一下，现在是研究reasoning的场景</h2><h2 id="最简单的提高推理能力的方法，就是设计prompt，比如在prompt里给出一些例题，且例题里有一步一步的思考过程，让模型进行参考（但每个问题都设计prompt，太麻烦了。所以我们想，能否自动做这个过程？也就是让模型自动开展CoT的过程）进一步的，如果我们有一大批高质量数据，既有结果，也有详细和正确的中间步骤，那我们直接用这些数据来对模型进行sft（也就是一种post-training），训练完，模型的输出就能带上中间思考过程和步骤了；又或者，我们先训练模型，让它掌握格式，要输出中间步骤（先确保有中间步骤），之后再单独训练一个奖励模型，采用类似rlhf的操作，让奖励模型去评估中间步骤，从而提高中间步骤的质量。最后在实际推理的时候，我们让模型一步一步探索，然后再用奖励模型来判断哪些步骤是比较好的，我们就选择这一步，并继续向后走"><a href="#最简单的提高推理能力的方法，就是设计prompt，比如在prompt里给出一些例题，且例题里有一步一步的思考过程，让模型进行参考（但每个问题都设计prompt，太麻烦了。所以我们想，能否自动做这个过程？也就是让模型自动开展CoT的过程）进一步的，如果我们有一大批高质量数据，既有结果，也有详细和正确的中间步骤，那我们直接用这些数据来对模型进行sft（也就是一种post-training），训练完，模型的输出就能带上中间思考过程和步骤了；又或者，我们先训练模型，让它掌握格式，要输出中间步骤（先确保有中间步骤），之后再单独训练一个奖励模型，采用类似rlhf的操作，让奖励模型去评估中间步骤，从而提高中间步骤的质量。最后在实际推理的时候，我们让模型一步一步探索，然后再用奖励模型来判断哪些步骤是比较好的，我们就选择这一步，并继续向后走" class="headerlink" title="最简单的提高推理能力的方法，就是设计prompt，比如在prompt里给出一些例题，且例题里有一步一步的思考过程，让模型进行参考（但每个问题都设计prompt，太麻烦了。所以我们想，能否自动做这个过程？也就是让模型自动开展CoT的过程）进一步的，如果我们有一大批高质量数据，既有结果，也有详细和正确的中间步骤，那我们直接用这些数据来对模型进行sft（也就是一种post-training），训练完，模型的输出就能带上中间思考过程和步骤了；又或者，我们先训练模型，让它掌握格式，要输出中间步骤（先确保有中间步骤），之后再单独训练一个奖励模型，采用类似rlhf的操作，让奖励模型去评估中间步骤，从而提高中间步骤的质量。最后在实际推理的时候，我们让模型一步一步探索，然后再用奖励模型来判断哪些步骤是比较好的，我们就选择这一步，并继续向后走"></a>最简单的提高推理能力的方法，就是设计prompt，比如在prompt里给出一些例题，且例题里有一步一步的思考过程，让模型进行参考（但每个问题都设计prompt，太麻烦了。所以我们想，能否自动做这个过程？也就是让模型自动开展CoT的过程）<br>进一步的，如果我们有一大批高质量数据，既有结果，也有详细和正确的中间步骤，那我们直接用这些数据来对模型进行sft（也就是一种post-training），训练完，模型的输出就能带上中间思考过程和步骤了；<br>又或者，我们先训练模型，让它掌握格式，要输出中间步骤（先确保有中间步骤），之后再单独训练一个奖励模型，采用类似rlhf的操作，让奖励模型去评估中间步骤，从而提高中间步骤的质量。最后在实际推理的时候，我们让模型一步一步探索，然后再用奖励模型来判断哪些步骤是比较好的，我们就选择这一步，并继续向后走</h2><h2 id="但是上述内容里，除了设计prompt的方法，其它的其实都是post-training，还算不上inference。inference的出现，可以认为是想要进一步改进而导致的："><a href="#但是上述内容里，除了设计prompt的方法，其它的其实都是post-training，还算不上inference。inference的出现，可以认为是想要进一步改进而导致的：" class="headerlink" title="但是上述内容里，除了设计prompt的方法，其它的其实都是post-training，还算不上inference。inference的出现，可以认为是想要进一步改进而导致的："></a>但是上述内容里，除了设计prompt的方法，其它的其实都是post-training，还算不上inference。inference的出现，可以认为是想要进一步改进而导致的：</h2><h3 id="假使我们post-training完，得到的模型能输出中间过程，且质量也不错。但我们也不能保证，这些中间结果和答案一定是最好的。我们可能还是要做一些类似搜索的操作，比如，引入能评估中间步骤的verifier，然后让模型进行N次attempts（一次attempt就是从开始，经过完整的中间步骤，最终得到结果），之后再用verifier对这N次attempts进行评估，从里面选出最好的一次attempt作为结果"><a href="#假使我们post-training完，得到的模型能输出中间过程，且质量也不错。但我们也不能保证，这些中间结果和答案一定是最好的。我们可能还是要做一些类似搜索的操作，比如，引入能评估中间步骤的verifier，然后让模型进行N次attempts（一次attempt就是从开始，经过完整的中间步骤，最终得到结果），之后再用verifier对这N次attempts进行评估，从里面选出最好的一次attempt作为结果" class="headerlink" title="假使我们post-training完，得到的模型能输出中间过程，且质量也不错。但我们也不能保证，这些中间结果和答案一定是最好的。我们可能还是要做一些类似搜索的操作，比如，引入能评估中间步骤的verifier，然后让模型进行N次attempts（一次attempt就是从开始，经过完整的中间步骤，最终得到结果），之后再用verifier对这N次attempts进行评估，从里面选出最好的一次attempt作为结果"></a>假使我们post-training完，得到的模型能输出中间过程，且质量也不错。但我们也不能保证，这些中间结果和答案一定是最好的。我们可能还是要做一些类似搜索的操作，比如，引入能评估中间步骤的verifier，然后让模型进行N次attempts（一次attempt就是从开始，经过完整的中间步骤，最终得到结果），之后再用verifier对这N次attempts进行评估，从里面选出最好的一次attempt作为结果</h3><h3 id="上面的做法是基于：模型能输出过程，可能过程的质量还不错，但也许还能更好，这么一种情况。若在post-training的时候，我们就引入verifier来指导模型生成的中间步骤，可能可以直接得到很高质量的中间过程了。也就是说，post-training的时候，我们就引入verifier，来评价中间过程并改善模型输出的中间过程的质量"><a href="#上面的做法是基于：模型能输出过程，可能过程的质量还不错，但也许还能更好，这么一种情况。若在post-training的时候，我们就引入verifier来指导模型生成的中间步骤，可能可以直接得到很高质量的中间过程了。也就是说，post-training的时候，我们就引入verifier，来评价中间过程并改善模型输出的中间过程的质量" class="headerlink" title="上面的做法是基于：模型能输出过程，可能过程的质量还不错，但也许还能更好，这么一种情况。若在post-training的时候，我们就引入verifier来指导模型生成的中间步骤，可能可以直接得到很高质量的中间过程了。也就是说，post-training的时候，我们就引入verifier，来评价中间过程并改善模型输出的中间过程的质量"></a>上面的做法是基于：模型能输出过程，可能过程的质量还不错，但也许还能更好，这么一种情况。若在post-training的时候，我们就引入verifier来指导模型生成的中间步骤，可能可以直接得到很高质量的中间过程了。也就是说，post-training的时候，我们就引入verifier，来评价中间过程并改善模型输出的中间过程的质量</h3><h2 id="至此，我们可以总结一下常见的，用于提高模型推理能力的三套框架："><a href="#至此，我们可以总结一下常见的，用于提高模型推理能力的三套框架：" class="headerlink" title="至此，我们可以总结一下常见的，用于提高模型推理能力的三套框架："></a>至此，我们可以总结一下常见的，用于提高模型推理能力的三套框架：</h2><h3 id="框架1-base-generator-inference"><a href="#框架1-base-generator-inference" class="headerlink" title="框架1. base generator+inference"></a>框架1. base generator+inference</h3><h5 id="具体来说，就是，我们训练好模型后，不再进行post-training，只在推理的时候做一些操作。也就是，我们会用一个训练好的verifier（这里我们不把训练verifier当成post-training的一部分。毕竟，训练verifier，并不是去训练我们的llm）（verifier的类型也有很多，常见的有PRM，Process-supervised-Reward-Model），在推理的时候，辅助我们的llm进行探索。主要是用一些搜索的方法（如best-of-N，beam-search等）"><a href="#具体来说，就是，我们训练好模型后，不再进行post-training，只在推理的时候做一些操作。也就是，我们会用一个训练好的verifier（这里我们不把训练verifier当成post-training的一部分。毕竟，训练verifier，并不是去训练我们的llm）（verifier的类型也有很多，常见的有PRM，Process-supervised-Reward-Model），在推理的时候，辅助我们的llm进行探索。主要是用一些搜索的方法（如best-of-N，beam-search等）" class="headerlink" title="具体来说，就是，我们训练好模型后，不再进行post-training，只在推理的时候做一些操作。也就是，我们会用一个训练好的verifier（这里我们不把训练verifier当成post-training的一部分。毕竟，训练verifier，并不是去训练我们的llm）（verifier的类型也有很多，常见的有PRM，Process-supervised Reward Model），在推理的时候，辅助我们的llm进行探索。主要是用一些搜索的方法（如best-of-N，beam search等）"></a>具体来说，就是，我们训练好模型后，不再进行post-training，只在推理的时候做一些操作。也就是，我们会用一个训练好的verifier（这里我们不把训练verifier当成post-training的一部分。毕竟，训练verifier，并不是去训练我们的llm）（verifier的类型也有很多，常见的有PRM，Process-supervised Reward Model），在推理的时候，辅助我们的llm进行探索。主要是用一些搜索的方法（如best-of-N，beam search等）</h5><h3 id="框架2-base-generator-formatted-post-training-inference"><a href="#框架2-base-generator-formatted-post-training-inference" class="headerlink" title="框架2. base generator+formatted post-training+inference"></a>框架2. base generator+formatted post-training+inference</h3><h5 id="这套框架的步骤是，先对训练好的llm，进行一些post-training，让它掌握格式，知道推理的时候进行中间步骤的思考，之后再用于inference（此时我们依然可以用那些在框架1里用到的，帮助提高模型推理能力的方法）这个post-training，基础的要求是让模型知道产生结果之余，还得产生中间步骤。具体的训练方法，可以是sft，也可以是一些rl算法。但如果手头有很高质量的数据，比如带中间步骤，且中间步骤不仅规范，正确率也很高，那训练的时候，也潜移默化地提高了模型生成的中间过程的质量。这显然是好的，只不过对数据要求比较高而已"><a href="#这套框架的步骤是，先对训练好的llm，进行一些post-training，让它掌握格式，知道推理的时候进行中间步骤的思考，之后再用于inference（此时我们依然可以用那些在框架1里用到的，帮助提高模型推理能力的方法）这个post-training，基础的要求是让模型知道产生结果之余，还得产生中间步骤。具体的训练方法，可以是sft，也可以是一些rl算法。但如果手头有很高质量的数据，比如带中间步骤，且中间步骤不仅规范，正确率也很高，那训练的时候，也潜移默化地提高了模型生成的中间过程的质量。这显然是好的，只不过对数据要求比较高而已" class="headerlink" title="这套框架的步骤是，先对训练好的llm，进行一些post-training，让它掌握格式，知道推理的时候进行中间步骤的思考，之后再用于inference（此时我们依然可以用那些在框架1里用到的，帮助提高模型推理能力的方法）这个post-training，基础的要求是让模型知道产生结果之余，还得产生中间步骤。具体的训练方法，可以是sft，也可以是一些rl算法。但如果手头有很高质量的数据，比如带中间步骤，且中间步骤不仅规范，正确率也很高，那训练的时候，也潜移默化地提高了模型生成的中间过程的质量。这显然是好的，只不过对数据要求比较高而已"></a>这套框架的步骤是，先对训练好的llm，进行一些post-training，让它掌握格式，知道推理的时候进行中间步骤的思考，之后再用于inference（此时我们依然可以用那些在框架1里用到的，帮助提高模型推理能力的方法）<br>这个post-training，基础的要求是让模型知道产生结果之余，还得产生中间步骤。具体的训练方法，可以是sft，也可以是一些rl算法。但如果手头有很高质量的数据，比如带中间步骤，且中间步骤不仅规范，正确率也很高，那训练的时候，也潜移默化地提高了模型生成的中间过程的质量。这显然是好的，只不过对数据要求比较高而已</h5><h5 id="如果对比一下框架2和框架1，不难发现，主要是多了一个post-training，且这个post-training至少是可以帮助模型规范过程，学到要输出中间步骤（根据数据质量，还有可能提高中间步骤的质量）。因为进行了post-training，所以模型输出时会自动带上中间过程；相比之下，框架1里的模型，没有经过post-trainig，所以它就不太会知道要输出中间过程，故我们在框架1下，推理时需要带上一些prompt，指导模型按格式输出）"><a href="#如果对比一下框架2和框架1，不难发现，主要是多了一个post-training，且这个post-training至少是可以帮助模型规范过程，学到要输出中间步骤（根据数据质量，还有可能提高中间步骤的质量）。因为进行了post-training，所以模型输出时会自动带上中间过程；相比之下，框架1里的模型，没有经过post-trainig，所以它就不太会知道要输出中间过程，故我们在框架1下，推理时需要带上一些prompt，指导模型按格式输出）" class="headerlink" title="如果对比一下框架2和框架1，不难发现，主要是多了一个post-training，且这个post-training至少是可以帮助模型规范过程，学到要输出中间步骤（根据数据质量，还有可能提高中间步骤的质量）。因为进行了post-training，所以模型输出时会自动带上中间过程；相比之下，框架1里的模型，没有经过post-trainig，所以它就不太会知道要输出中间过程，故我们在框架1下，推理时需要带上一些prompt，指导模型按格式输出）"></a>如果对比一下框架2和框架1，不难发现，主要是多了一个post-training，且这个post-training至少是可以帮助模型规范过程，学到要输出中间步骤（根据数据质量，还有可能提高中间步骤的质量）。因为进行了post-training，所以模型输出时会自动带上中间过程；相比之下，框架1里的模型，没有经过post-trainig，所以它就不太会知道要输出中间过程，故我们在框架1下，推理时需要带上一些prompt，指导模型按格式输出）</h5><h3 id="框架3-base-generator-formatted-post-training（with-inference-filtering-method）-inference（selective）"><a href="#框架3-base-generator-formatted-post-training（with-inference-filtering-method）-inference（selective）" class="headerlink" title="框架3. base generator+formatted post-training（with inference filtering method）+inference（selective）"></a>框架3. base generator+formatted post-training（with inference filtering method）+inference（selective）</h3><h5 id="这套框架其实和框架2是有些类似的，只不过数据来源不同。框架2里post-training的数据是我们额外准备好的，而框架3里的post-trainig的数据，是llm自生成的。当然，为了保证这些数据的质量，我们会从中筛选出高质量的数据，再用这些数据进行post-training（但具体怎么指导llm生成数据，这里我猜测，是需要设计一些prompt的）因为我们已经从自生成的数据里进行筛选了，所以，可以认为这些筛出来的数据的质量已经比较高了，因此在推理的时候我们可以考虑不加上那些搜索的推理技巧（毕竟可以剩下推理时的计算量嘛）；但以防万一，或者为了追求性能，我们依然可以在推理阶段用上前述的各种技巧"><a href="#这套框架其实和框架2是有些类似的，只不过数据来源不同。框架2里post-training的数据是我们额外准备好的，而框架3里的post-trainig的数据，是llm自生成的。当然，为了保证这些数据的质量，我们会从中筛选出高质量的数据，再用这些数据进行post-training（但具体怎么指导llm生成数据，这里我猜测，是需要设计一些prompt的）因为我们已经从自生成的数据里进行筛选了，所以，可以认为这些筛出来的数据的质量已经比较高了，因此在推理的时候我们可以考虑不加上那些搜索的推理技巧（毕竟可以剩下推理时的计算量嘛）；但以防万一，或者为了追求性能，我们依然可以在推理阶段用上前述的各种技巧" class="headerlink" title="这套框架其实和框架2是有些类似的，只不过数据来源不同。框架2里post-training的数据是我们额外准备好的，而框架3里的post-trainig的数据，是llm自生成的。当然，为了保证这些数据的质量，我们会从中筛选出高质量的数据，再用这些数据进行post-training（但具体怎么指导llm生成数据，这里我猜测，是需要设计一些prompt的）因为我们已经从自生成的数据里进行筛选了，所以，可以认为这些筛出来的数据的质量已经比较高了，因此在推理的时候我们可以考虑不加上那些搜索的推理技巧（毕竟可以剩下推理时的计算量嘛）；但以防万一，或者为了追求性能，我们依然可以在推理阶段用上前述的各种技巧"></a>这套框架其实和框架2是有些类似的，只不过数据来源不同。框架2里post-training的数据是我们额外准备好的，而框架3里的post-trainig的数据，是llm自生成的。当然，为了保证这些数据的质量，我们会从中筛选出高质量的数据，再用这些数据进行post-training（但具体怎么指导llm生成数据，这里我猜测，是需要设计一些prompt的）<br>因为我们已经从自生成的数据里进行筛选了，所以，可以认为这些筛出来的数据的质量已经比较高了，因此在推理的时候我们可以考虑不加上那些搜索的推理技巧（毕竟可以剩下推理时的计算量嘛）；但以防万一，或者为了追求性能，我们依然可以在推理阶段用上前述的各种技巧</h5><h2 id="基于对上述框架的理解，我们再来看deepmind基于框架采取的两种方案："><a href="#基于对上述框架的理解，我们再来看deepmind基于框架采取的两种方案：" class="headerlink" title="基于对上述框架的理解，我们再来看deepmind基于框架采取的两种方案："></a>基于对上述框架的理解，我们再来看deepmind基于框架采取的两种方案：</h2><h3 id="方案一：利用PRM指引搜索（具体来说，先post-training，再进行推理时的优化post-trainig的时候，只引导模型在格式方面对齐；再额外训练一个verifier，推理时用这个verifier指导搜索）（也就是框架2了）"><a href="#方案一：利用PRM指引搜索（具体来说，先post-training，再进行推理时的优化post-trainig的时候，只引导模型在格式方面对齐；再额外训练一个verifier，推理时用这个verifier指导搜索）（也就是框架2了）" class="headerlink" title="方案一：利用PRM指引搜索（具体来说，先post-training，再进行推理时的优化post-trainig的时候，只引导模型在格式方面对齐；再额外训练一个verifier，推理时用这个verifier指导搜索）（也就是框架2了）"></a>方案一：利用PRM指引搜索（具体来说，先post-training，再进行推理时的优化<br>post-trainig的时候，只引导模型在格式方面对齐；再额外训练一个verifier，推理时用这个verifier指导搜索）（也就是框架2了）</h3><h3 id="方案二：通过sft，直接改变模型的输出分布（Revise-proposal-distribution）也就是，先用sft进行post-training，既让模型学会输出中间步骤，同时也确保中间步骤的质量；此后，同样需要训练verifier（PRM），并在推理过程中指导搜索（我们发现，还是框架2。只不过这里可能数据质量高一点，所以对中间过程的质量也有所保证）"><a href="#方案二：通过sft，直接改变模型的输出分布（Revise-proposal-distribution）也就是，先用sft进行post-training，既让模型学会输出中间步骤，同时也确保中间步骤的质量；此后，同样需要训练verifier（PRM），并在推理过程中指导搜索（我们发现，还是框架2。只不过这里可能数据质量高一点，所以对中间过程的质量也有所保证）" class="headerlink" title="方案二：通过sft，直接改变模型的输出分布（Revise proposal distribution）也就是，先用sft进行post-training，既让模型学会输出中间步骤，同时也确保中间步骤的质量；此后，同样需要训练verifier（PRM），并在推理过程中指导搜索（我们发现，还是框架2。只不过这里可能数据质量高一点，所以对中间过程的质量也有所保证）"></a>方案二：通过sft，直接改变模型的输出分布（Revise proposal distribution）<br>也就是，先用sft进行post-training，既让模型学会输出中间步骤，同时也确保中间步骤的质量；此后，同样需要训练verifier（PRM），并在推理过程中指导搜索（我们发现，还是框架2。只不过这里可能数据质量高一点，所以对中间过程的质量也有所保证）</h3><h2 id="下面详细讲一下两种具体的，改善模型推理能力的方案"><a href="#下面详细讲一下两种具体的，改善模型推理能力的方案" class="headerlink" title="下面详细讲一下两种具体的，改善模型推理能力的方案"></a>下面详细讲一下两种具体的，改善模型推理能力的方案</h2><h1 id="二、方案一"><a href="#二、方案一" class="headerlink" title="二、方案一"></a>二、方案一</h1><h2 id="根据上述的过程，核心的步骤是以下三步：post-training（让模型掌握格式）训练verifier（确保这个verifier真的能作出合理正确的判断）推理时的指导搜搜（要把verifier真的用起来，并提高推理质量）下面详细看一下这三步要怎么做"><a href="#根据上述的过程，核心的步骤是以下三步：post-training（让模型掌握格式）训练verifier（确保这个verifier真的能作出合理正确的判断）推理时的指导搜搜（要把verifier真的用起来，并提高推理质量）下面详细看一下这三步要怎么做" class="headerlink" title="根据上述的过程，核心的步骤是以下三步：post-training（让模型掌握格式）训练verifier（确保这个verifier真的能作出合理正确的判断）推理时的指导搜搜（要把verifier真的用起来，并提高推理质量）下面详细看一下这三步要怎么做"></a>根据上述的过程，核心的步骤是以下三步：<br>post-training（让模型掌握格式）<br>训练verifier（确保这个verifier真的能作出合理正确的判断）<br>推理时的指导搜搜（要把verifier真的用起来，并提高推理质量）<br>下面详细看一下这三步要怎么做</h2><h2 id="2-1-格式训练"><a href="#2-1-格式训练" class="headerlink" title="2.1. 格式训练"></a>2.1. 格式训练</h2><h3 id="大体上讲，我们是利用prompt，让模型自生产数据，从而用来训练模型，让它掌握输出格式"><a href="#大体上讲，我们是利用prompt，让模型自生产数据，从而用来训练模型，让它掌握输出格式" class="headerlink" title="大体上讲，我们是利用prompt，让模型自生产数据，从而用来训练模型，让它掌握输出格式"></a>大体上讲，我们是利用prompt，让模型自生产数据，从而用来训练模型，让它掌握输出格式</h3><h5 id="具体来说，我们可以考虑，在prompt中，加入有正确格式的例子，引导模型生成一批结果（都是有正确格式的），之后把这批生成数据加到sft的数据集里，然后再用这些sft的数据集微调模型。训练完，模型理应能掌握正确的格式了（此时我们没有做太多的操作，仅仅是通过prompt引导模型生成带格式的数据，也没去管这些数据的质量。所以现在模型的中间步骤还并不可信）"><a href="#具体来说，我们可以考虑，在prompt中，加入有正确格式的例子，引导模型生成一批结果（都是有正确格式的），之后把这批生成数据加到sft的数据集里，然后再用这些sft的数据集微调模型。训练完，模型理应能掌握正确的格式了（此时我们没有做太多的操作，仅仅是通过prompt引导模型生成带格式的数据，也没去管这些数据的质量。所以现在模型的中间步骤还并不可信）" class="headerlink" title="具体来说，我们可以考虑，在prompt中，加入有正确格式的例子，引导模型生成一批结果（都是有正确格式的），之后把这批生成数据加到sft的数据集里，然后再用这些sft的数据集微调模型。训练完，模型理应能掌握正确的格式了（此时我们没有做太多的操作，仅仅是通过prompt引导模型生成带格式的数据，也没去管这些数据的质量。所以现在模型的中间步骤还并不可信）"></a>具体来说，我们可以考虑，在prompt中，加入有正确格式的例子，引导模型生成一批结果（都是有正确格式的），之后把这批生成数据加到sft的数据集里，然后再用这些sft的数据集微调模型。训练完，模型理应能掌握正确的格式了（此时我们没有做太多的操作，仅仅是通过prompt引导模型生成带格式的数据，也没去管这些数据的质量。所以现在模型的中间步骤还并不可信）</h5><h2 id="2-2-训练PRM"><a href="#2-2-训练PRM" class="headerlink" title="2.2. 训练PRM"></a>2.2. 训练PRM</h2><h3 id="这一步，我们的目的是训练出一个可以评估中间步骤好坏的奖励模型（即此处的PRM）。这个奖励模型的输入，就应该是问题-中间步骤，输出则是中间步骤的好坏（比如用一个分数进行量化，或者若干等级的标签进行量化）。那么，就需要获得带标签的数据。数据如何获取就是一个关键的问题。根据经费的多少，有三种方法可选："><a href="#这一步，我们的目的是训练出一个可以评估中间步骤好坏的奖励模型（即此处的PRM）。这个奖励模型的输入，就应该是问题-中间步骤，输出则是中间步骤的好坏（比如用一个分数进行量化，或者若干等级的标签进行量化）。那么，就需要获得带标签的数据。数据如何获取就是一个关键的问题。根据经费的多少，有三种方法可选：" class="headerlink" title="这一步，我们的目的是训练出一个可以评估中间步骤好坏的奖励模型（即此处的PRM）。这个奖励模型的输入，就应该是问题+中间步骤，输出则是中间步骤的好坏（比如用一个分数进行量化，或者若干等级的标签进行量化）。那么，就需要获得带标签的数据。数据如何获取就是一个关键的问题。根据经费的多少，有三种方法可选："></a>这一步，我们的目的是训练出一个可以评估中间步骤好坏的奖励模型（即此处的PRM）。这个奖励模型的输入，就应该是问题+中间步骤，输出则是中间步骤的好坏（比如用一个分数进行量化，或者若干等级的标签进行量化）。那么，就需要获得带标签的数据。数据如何获取就是一个关键的问题。根据经费的多少，有三种方法可选：</h3><h5 id="方法一：大量标注。此时我们让generator（也就是预训练完，且经过了格式post-training的llm）生成一大批带中间步骤和答案的数据，之后找人对中间步骤进行标注，完了拿来训练PRM。效果应当是非常好的，就是成本很高而已"><a href="#方法一：大量标注。此时我们让generator（也就是预训练完，且经过了格式post-training的llm）生成一大批带中间步骤和答案的数据，之后找人对中间步骤进行标注，完了拿来训练PRM。效果应当是非常好的，就是成本很高而已" class="headerlink" title="方法一：大量标注。此时我们让generator（也就是预训练完，且经过了格式post-training的llm）生成一大批带中间步骤和答案的数据，之后找人对中间步骤进行标注，完了拿来训练PRM。效果应当是非常好的，就是成本很高而已"></a>方法一：大量标注。此时我们让generator（也就是预训练完，且经过了格式post-training的llm）生成一大批带中间步骤和答案的数据，之后找人对中间步骤进行标注，完了拿来训练PRM。效果应当是非常好的，就是成本很高而已</h5><h5 id="方法二：筛出一部分数据进行标注。我们还是让generator生成一批数据（此时可能就不是非常大的量级，因为现在考虑的是经费不那么多的情况），然后进行若干的筛选（例如，把格式错误的答案筛掉），之后把筛出来的数据拿去打标签，打完就先训一次PRM。此时，我们要用训练完的PRM，筛出它不擅长的例子，再让人去标注这些它不擅长的例子（或者说是难的例子），起到针对性训练的效果具体来说，我们训练完PRM后，让generator再生成一批带过程和答案的数据，然后让PRM打分。此时，我们就挑出那些：”PRM给整体过程打分很高，但最终答案错误“的数据。这些就是PRM的hard-example挑出hard-example后，人工对这些数据进行标注，并用这些数据再来训练PRM，之后重复上述过程，迭代地训练PRM"><a href="#方法二：筛出一部分数据进行标注。我们还是让generator生成一批数据（此时可能就不是非常大的量级，因为现在考虑的是经费不那么多的情况），然后进行若干的筛选（例如，把格式错误的答案筛掉），之后把筛出来的数据拿去打标签，打完就先训一次PRM。此时，我们要用训练完的PRM，筛出它不擅长的例子，再让人去标注这些它不擅长的例子（或者说是难的例子），起到针对性训练的效果具体来说，我们训练完PRM后，让generator再生成一批带过程和答案的数据，然后让PRM打分。此时，我们就挑出那些：”PRM给整体过程打分很高，但最终答案错误“的数据。这些就是PRM的hard-example挑出hard-example后，人工对这些数据进行标注，并用这些数据再来训练PRM，之后重复上述过程，迭代地训练PRM" class="headerlink" title="方法二：筛出一部分数据进行标注。我们还是让generator生成一批数据（此时可能就不是非常大的量级，因为现在考虑的是经费不那么多的情况），然后进行若干的筛选（例如，把格式错误的答案筛掉），之后把筛出来的数据拿去打标签，打完就先训一次PRM。此时，我们要用训练完的PRM，筛出它不擅长的例子，再让人去标注这些它不擅长的例子（或者说是难的例子），起到针对性训练的效果具体来说，我们训练完PRM后，让generator再生成一批带过程和答案的数据，然后让PRM打分。此时，我们就挑出那些：”PRM给整体过程打分很高，但最终答案错误“的数据。这些就是PRM的hard example挑出hard example后，人工对这些数据进行标注，并用这些数据再来训练PRM，之后重复上述过程，迭代地训练PRM"></a>方法二：筛出一部分数据进行标注。我们还是让generator生成一批数据（此时可能就不是非常大的量级，因为现在考虑的是经费不那么多的情况），然后进行若干的筛选（例如，把格式错误的答案筛掉），之后把筛出来的数据拿去打标签，打完就先训一次PRM。此时，我们要用训练完的PRM，筛出它不擅长的例子，再让人去标注这些它不擅长的例子（或者说是难的例子），起到针对性训练的效果<br>具体来说，我们训练完PRM后，让generator再生成一批带过程和答案的数据，然后让PRM打分。此时，我们就挑出那些：”PRM给整体过程打分很高，但最终答案错误“的数据。这些就是PRM的hard example<br>挑出hard example后，人工对这些数据进行标注，并用这些数据再来训练PRM，之后重复上述过程，迭代地训练PRM</h5><h6 id="（大体上，这是有道理的，因为答案错了，很大程度上就是因为中间过程有误，那么中间过程的打分理应不那么高。但既然有这些答案错了，中间过程却分很高的数据，我们就认为PRM对这些数据的打分不太正确，因此它们是hard-example）（但其实这也并不完全正确。也许也存在这种情况：中间过程确实挺对的，单纯是generator在最后输出答案的时候出错了。不过可能还是得有真实的数据才能验证这一点吧。如果确实有这个现象，那或许是一个可以改进的小点）"><a href="#（大体上，这是有道理的，因为答案错了，很大程度上就是因为中间过程有误，那么中间过程的打分理应不那么高。但既然有这些答案错了，中间过程却分很高的数据，我们就认为PRM对这些数据的打分不太正确，因此它们是hard-example）（但其实这也并不完全正确。也许也存在这种情况：中间过程确实挺对的，单纯是generator在最后输出答案的时候出错了。不过可能还是得有真实的数据才能验证这一点吧。如果确实有这个现象，那或许是一个可以改进的小点）" class="headerlink" title="（大体上，这是有道理的，因为答案错了，很大程度上就是因为中间过程有误，那么中间过程的打分理应不那么高。但既然有这些答案错了，中间过程却分很高的数据，我们就认为PRM对这些数据的打分不太正确，因此它们是hard example）（但其实这也并不完全正确。也许也存在这种情况：中间过程确实挺对的，单纯是generator在最后输出答案的时候出错了。不过可能还是得有真实的数据才能验证这一点吧。如果确实有这个现象，那或许是一个可以改进的小点）"></a>（大体上，这是有道理的，因为答案错了，很大程度上就是因为中间过程有误，那么中间过程的打分理应不那么高。但既然有这些答案错了，中间过程却分很高的数据，我们就认为PRM对这些数据的打分不太正确，因此它们是hard example）（但其实这也并不完全正确。也许也存在这种情况：中间过程确实挺对的，单纯是generator在最后输出答案的时候出错了。不过可能还是得有真实的数据才能验证这一点吧。如果确实有这个现象，那或许是一个可以改进的小点）</h6><h6 id="另外，上面提到了让PRM对中间步骤进行整体打分的操作（因为PRM本身的输出是针对一个步骤的）。这里有若干的选择，常见的有：连乘式（prod）、最小式（min）、最后一步式（last-step）。prod和min是openAI的lets-verify-step-by-step论文里探索过的，last-step则是deepmind在Scaling-LLM-Test-Time-Compute-Optimally-can-be-More-Effective-than-Scaling-Model-Parameters里使用的（看博客是这么说的，笔者还没读呢，回头读读确认下）。哪一种其实都是有点道理的，选择并不绝对。重点是，选择了某一种方式后，我们就能用verifier，对中间的步骤进行一个整体的打分（prod就是，让verifier对每个step都打分，然后把所有的得分乘起来，得到的就是整个中间步骤的整体得分；min就是，让verifier对每个step打分，之后取出其中最小的得分，作为整体的得分。这种方式背后的思想是，如果中间过程里有一步很可能错了，那整体也很可能错了；反之，如果中间每一步都很对，那整体大概率也是对的；last-step则是，让verifier对每个step打分，再取最后一个步骤的得分，作为整体的得分。这是因为，假设一个问题p，模型输出了k个步骤，则当我们要对第i个步骤进行打分的时候，输入给verifier的其实是问题p加上前i个步骤。因为评价一个步骤不能孤立地看，而是要结合上文的。因此，我们对最后一个step打分的时候，要输入的其实就是问题加所有的步骤。那么，对最后一步的评分，其实也可以看成是对整体的评分了。这就类似于，LSTM里，最后的一个隐状态，理论上是包含了前面的所有信息的）"><a href="#另外，上面提到了让PRM对中间步骤进行整体打分的操作（因为PRM本身的输出是针对一个步骤的）。这里有若干的选择，常见的有：连乘式（prod）、最小式（min）、最后一步式（last-step）。prod和min是openAI的lets-verify-step-by-step论文里探索过的，last-step则是deepmind在Scaling-LLM-Test-Time-Compute-Optimally-can-be-More-Effective-than-Scaling-Model-Parameters里使用的（看博客是这么说的，笔者还没读呢，回头读读确认下）。哪一种其实都是有点道理的，选择并不绝对。重点是，选择了某一种方式后，我们就能用verifier，对中间的步骤进行一个整体的打分（prod就是，让verifier对每个step都打分，然后把所有的得分乘起来，得到的就是整个中间步骤的整体得分；min就是，让verifier对每个step打分，之后取出其中最小的得分，作为整体的得分。这种方式背后的思想是，如果中间过程里有一步很可能错了，那整体也很可能错了；反之，如果中间每一步都很对，那整体大概率也是对的；last-step则是，让verifier对每个step打分，再取最后一个步骤的得分，作为整体的得分。这是因为，假设一个问题p，模型输出了k个步骤，则当我们要对第i个步骤进行打分的时候，输入给verifier的其实是问题p加上前i个步骤。因为评价一个步骤不能孤立地看，而是要结合上文的。因此，我们对最后一个step打分的时候，要输入的其实就是问题加所有的步骤。那么，对最后一步的评分，其实也可以看成是对整体的评分了。这就类似于，LSTM里，最后的一个隐状态，理论上是包含了前面的所有信息的）" class="headerlink" title="另外，上面提到了让PRM对中间步骤进行整体打分的操作（因为PRM本身的输出是针对一个步骤的）。这里有若干的选择，常见的有：连乘式（prod）、最小式（min）、最后一步式（last step）。prod和min是openAI的lets verify step by step论文里探索过的，last step则是deepmind在Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters里使用的（看博客是这么说的，笔者还没读呢，回头读读确认下）。哪一种其实都是有点道理的，选择并不绝对。重点是，选择了某一种方式后，我们就能用verifier，对中间的步骤进行一个整体的打分（prod就是，让verifier对每个step都打分，然后把所有的得分乘起来，得到的就是整个中间步骤的整体得分；min就是，让verifier对每个step打分，之后取出其中最小的得分，作为整体的得分。这种方式背后的思想是，如果中间过程里有一步很可能错了，那整体也很可能错了；反之，如果中间每一步都很对，那整体大概率也是对的；last step则是，让verifier对每个step打分，再取最后一个步骤的得分，作为整体的得分。这是因为，假设一个问题p，模型输出了k个步骤，则当我们要对第i个步骤进行打分的时候，输入给verifier的其实是问题p加上前i个步骤。因为评价一个步骤不能孤立地看，而是要结合上文的。因此，我们对最后一个step打分的时候，要输入的其实就是问题加所有的步骤。那么，对最后一步的评分，其实也可以看成是对整体的评分了。这就类似于，LSTM里，最后的一个隐状态，理论上是包含了前面的所有信息的）"></a>另外，上面提到了让PRM对中间步骤进行整体打分的操作（因为PRM本身的输出是针对一个步骤的）。这里有若干的选择，常见的有：连乘式（prod）、最小式（min）、最后一步式（last step）。prod和min是openAI的<em><strong>lets verify step by step</strong></em>论文里探索过的，last step则是deepmind在<em><strong>Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</strong></em>里使用的（看博客是这么说的，笔者还没读呢，回头读读确认下）。哪一种其实都是有点道理的，选择并不绝对。重点是，选择了某一种方式后，我们就能用verifier，对中间的步骤进行一个整体的打分<br>（prod就是，让verifier对每个step都打分，然后把所有的得分乘起来，得到的就是整个中间步骤的整体得分；<br>min就是，让verifier对每个step打分，之后取出其中最小的得分，作为整体的得分。这种方式背后的思想是，如果中间过程里有一步很可能错了，那整体也很可能错了；反之，如果中间每一步都很对，那整体大概率也是对的；<br>last step则是，让verifier对每个step打分，再取最后一个步骤的得分，作为整体的得分。这是因为，假设一个问题p，模型输出了k个步骤，则当我们要对第i个步骤进行打分的时候，输入给verifier的其实是问题p加上前i个步骤。因为评价一个步骤不能孤立地看，而是要结合上文的。因此，我们对最后一个step打分的时候，要输入的其实就是问题加所有的步骤。那么，对最后一步的评分，其实也可以看成是对整体的评分了。这就类似于，LSTM里，最后的一个隐状态，理论上是包含了前面的所有信息的）</h6><h5 id="方法三：完全自动生成数据。此时我们完全不进行人工标注。用模型生成的数据进行打分具体来说，我们先让generator生成一批带过程和答案的数据。此时，我们要对这些过程进行打分，然后用打了分的数据来训练PRM。（但关于这个具体的训练方式，笔者有点问题：我们得到的是soft-label，也就是连续的数值，为什么博客说，训练的时候，PRM还是一个分类模型？这不太对吧？得看了原论文才来回答一下了）怎么打分呢？这里的方法，类似于rl里的Monte-Carlo方法（话说它其实就是教N-Monte-Carlo-rollouts）。操作如下（但这个是根据博客内容总结的，而博客里提到，这个方法本身在论文里并没有得到详细的介绍。所以带有主观的猜测。同样的，还是得等到笔者读一读原文之后才能给出准确的回答。此处先姑且用作理解）："><a href="#方法三：完全自动生成数据。此时我们完全不进行人工标注。用模型生成的数据进行打分具体来说，我们先让generator生成一批带过程和答案的数据。此时，我们要对这些过程进行打分，然后用打了分的数据来训练PRM。（但关于这个具体的训练方式，笔者有点问题：我们得到的是soft-label，也就是连续的数值，为什么博客说，训练的时候，PRM还是一个分类模型？这不太对吧？得看了原论文才来回答一下了）怎么打分呢？这里的方法，类似于rl里的Monte-Carlo方法（话说它其实就是教N-Monte-Carlo-rollouts）。操作如下（但这个是根据博客内容总结的，而博客里提到，这个方法本身在论文里并没有得到详细的介绍。所以带有主观的猜测。同样的，还是得等到笔者读一读原文之后才能给出准确的回答。此处先姑且用作理解）：" class="headerlink" title="方法三：完全自动生成数据。此时我们完全不进行人工标注。用模型生成的数据进行打分具体来说，我们先让generator生成一批带过程和答案的数据。此时，我们要对这些过程进行打分，然后用打了分的数据来训练PRM。（但关于这个具体的训练方式，笔者有点问题：我们得到的是soft label，也就是连续的数值，为什么博客说，训练的时候，PRM还是一个分类模型？这不太对吧？得看了原论文才来回答一下了）怎么打分呢？这里的方法，类似于rl里的Monte Carlo方法（话说它其实就是教N Monte-Carlo rollouts）。操作如下（但这个是根据博客内容总结的，而博客里提到，这个方法本身在论文里并没有得到详细的介绍。所以带有主观的猜测。同样的，还是得等到笔者读一读原文之后才能给出准确的回答。此处先姑且用作理解）："></a>方法三：完全自动生成数据。此时我们完全不进行人工标注。用模型生成的数据进行打分<br>具体来说，我们先让generator生成一批带过程和答案的数据。此时，我们要对这些过程进行打分，然后用打了分的数据来训练PRM。（但关于这个具体的训练方式，笔者有点问题：我们得到的是soft label，也就是连续的数值，为什么博客说，训练的时候，PRM还是一个分类模型？这不太对吧？得看了原论文才来回答一下了）<br>怎么打分呢？这里的方法，类似于rl里的Monte Carlo方法（话说它其实就是教N Monte-Carlo rollouts）。操作如下（但这个是根据博客内容总结的，而博客里提到，这个方法本身在论文里并没有得到详细的介绍。所以带有主观的猜测。同样的，还是得等到笔者读一读原文之后才能给出准确的回答。此处先姑且用作理解）：</h5><h6 id="假如现在有一个sample（它是指完整的中间步骤加答案的数据），我们想对其中每个步骤进行评分。不妨先考虑对step1进行评分。我们把问题和step1给generator，让它继续采样N个samples（也就是，让模型从问题和step1出发，来尝试各种路径解决问题并给出答案），然后我们统计这N个samples里有多少个是回答对了的，这个回答正确的比例，我们就认为代表了step1的评分（所以，这是个连续值，也就是soft-label）对其它步骤，也是类似的评分方式（可以看到，其实计算量还蛮大的。毕竟是Monte-Carlo的方法）"><a href="#假如现在有一个sample（它是指完整的中间步骤加答案的数据），我们想对其中每个步骤进行评分。不妨先考虑对step1进行评分。我们把问题和step1给generator，让它继续采样N个samples（也就是，让模型从问题和step1出发，来尝试各种路径解决问题并给出答案），然后我们统计这N个samples里有多少个是回答对了的，这个回答正确的比例，我们就认为代表了step1的评分（所以，这是个连续值，也就是soft-label）对其它步骤，也是类似的评分方式（可以看到，其实计算量还蛮大的。毕竟是Monte-Carlo的方法）" class="headerlink" title="假如现在有一个sample（它是指完整的中间步骤加答案的数据），我们想对其中每个步骤进行评分。不妨先考虑对step1进行评分。我们把问题和step1给generator，让它继续采样N个samples（也就是，让模型从问题和step1出发，来尝试各种路径解决问题并给出答案），然后我们统计这N个samples里有多少个是回答对了的，这个回答正确的比例，我们就认为代表了step1的评分（所以，这是个连续值，也就是soft label）对其它步骤，也是类似的评分方式（可以看到，其实计算量还蛮大的。毕竟是Monte Carlo的方法）"></a>假如现在有一个sample（它是指完整的中间步骤加答案的数据），我们想对其中每个步骤进行评分。不妨先考虑对step1进行评分。我们把问题和step1给generator，让它继续采样N个samples（也就是，让模型从问题和step1出发，来尝试各种路径解决问题并给出答案），然后我们统计这N个samples里有多少个是回答对了的，这个回答正确的比例，我们就认为代表了step1的评分（所以，这是个连续值，也就是soft label）<br>对其它步骤，也是类似的评分方式（可以看到，其实计算量还蛮大的。毕竟是Monte Carlo的方法）</h6><h2 id="2-3-使用PRM指导搜索过程"><a href="#2-3-使用PRM指导搜索过程" class="headerlink" title="2.3. 使用PRM指导搜索过程"></a>2.3. 使用PRM指导搜索过程</h2><h3 id="概括地讲，我们现在有了generator（只有格式，质量无法保证）和verifier（可以对中间过程进行评分），然后希望用verifier指导模型生成结果，筛出高质量的回答。用到的方法是一些搜索的方法。主要是best-of-N、beam-search以及lookahead-search"><a href="#概括地讲，我们现在有了generator（只有格式，质量无法保证）和verifier（可以对中间过程进行评分），然后希望用verifier指导模型生成结果，筛出高质量的回答。用到的方法是一些搜索的方法。主要是best-of-N、beam-search以及lookahead-search" class="headerlink" title="概括地讲，我们现在有了generator（只有格式，质量无法保证）和verifier（可以对中间过程进行评分），然后希望用verifier指导模型生成结果，筛出高质量的回答。用到的方法是一些搜索的方法。主要是best-of-N、beam search以及lookahead search"></a>概括地讲，我们现在有了generator（只有格式，质量无法保证）和verifier（可以对中间过程进行评分），然后希望用verifier指导模型生成结果，筛出高质量的回答。用到的方法是一些搜索的方法。主要是best-of-N、beam search以及lookahead search</h3><h3 id="2-3-1-best-of-N"><a href="#2-3-1-best-of-N" class="headerlink" title="2.3.1. best-of-N"></a>2.3.1. best-of-N</h3><h4 id="这个搜索方法比较简单直白，也常被用做baseline进行比较。它是说，让generator针对问题，生成N个完整回答（此时当然包含中间步骤），再用训练好的verifier对这N个回答的中间步骤都进行整体的打分（回忆上面提到的，三种进行整体打分的方式：prod，min，last-step），挑出整体打分最高的那个回答，作为最终回答"><a href="#这个搜索方法比较简单直白，也常被用做baseline进行比较。它是说，让generator针对问题，生成N个完整回答（此时当然包含中间步骤），再用训练好的verifier对这N个回答的中间步骤都进行整体的打分（回忆上面提到的，三种进行整体打分的方式：prod，min，last-step），挑出整体打分最高的那个回答，作为最终回答" class="headerlink" title="这个搜索方法比较简单直白，也常被用做baseline进行比较。它是说，让generator针对问题，生成N个完整回答（此时当然包含中间步骤），再用训练好的verifier对这N个回答的中间步骤都进行整体的打分（回忆上面提到的，三种进行整体打分的方式：prod，min，last step），挑出整体打分最高的那个回答，作为最终回答"></a>这个搜索方法比较简单直白，也常被用做baseline进行比较。它是说，让generator针对问题，生成N个完整回答（此时当然包含中间步骤），再用训练好的verifier对这N个回答的中间步骤都进行整体的打分（回忆上面提到的，三种进行整体打分的方式：prod，min，last step），挑出整体打分最高的那个回答，作为最终回答</h4><h3 id="2-3-2-beam-search"><a href="#2-3-2-beam-search" class="headerlink" title="2.3.2. beam search"></a>2.3.2. beam search</h3><h4 id="这个方法有点走一步看一步的意思。针对一个问题，我们先并行地采样N个step1，再用verifier对这N个步骤进行打分，从中挑出得分前M高的step1，继续往下走。每个选出来的step1，都会再采样出N个step2，同样的，打完分之后挑出其中得分前M高的step2，继续往下走，直至达到指定的停止要求（比如，达到指定的搜索深度了）（别忘了，评估第i个步骤的得分的时候，是要把问题和前i个步骤都输入给verifier的）"><a href="#这个方法有点走一步看一步的意思。针对一个问题，我们先并行地采样N个step1，再用verifier对这N个步骤进行打分，从中挑出得分前M高的step1，继续往下走。每个选出来的step1，都会再采样出N个step2，同样的，打完分之后挑出其中得分前M高的step2，继续往下走，直至达到指定的停止要求（比如，达到指定的搜索深度了）（别忘了，评估第i个步骤的得分的时候，是要把问题和前i个步骤都输入给verifier的）" class="headerlink" title="这个方法有点走一步看一步的意思。针对一个问题，我们先并行地采样N个step1，再用verifier对这N个步骤进行打分，从中挑出得分前M高的step1，继续往下走。每个选出来的step1，都会再采样出N个step2，同样的，打完分之后挑出其中得分前M高的step2，继续往下走，直至达到指定的停止要求（比如，达到指定的搜索深度了）（别忘了，评估第i个步骤的得分的时候，是要把问题和前i个步骤都输入给verifier的）"></a>这个方法有点走一步看一步的意思。针对一个问题，我们先并行地采样N个step1，再用verifier对这N个步骤进行打分，从中挑出得分前M高的step1，继续往下走。每个选出来的step1，都会再采样出N个step2，同样的，打完分之后挑出其中得分前M高的step2，继续往下走，直至达到指定的停止要求（比如，达到指定的搜索深度了）（别忘了，评估第i个步骤的得分的时候，是要把问题和前i个步骤都输入给verifier的）</h4><h3 id="2-3-3-lookahead-search"><a href="#2-3-3-lookahead-search" class="headerlink" title="2.3.3. lookahead search"></a>2.3.3. lookahead search</h3><h4 id="这个其实和上面的beam-search的思想是类似的，或者说，它是beam-search的推广，beam-search是lookahead-search的特例。我们对一个问题，同样先并行采样N个step1，然后还是要挑出得分前M高的step1往下走，之后的步骤也是类似的。唯一的不同是在于打分上。此时我们对一个step-i的打分，不是直接用verifier的输出，而是，从这个当前步骤i出发，往后走K步，然后把这走了K步之后的步骤拿去给verifier打分，并把这个得分-score-i-k-，视为当前步骤i的得分（这个lookahead-search和beam-search的关系，就类似于rl中的TD-n-方法和TD-0-之间的关系）"><a href="#这个其实和上面的beam-search的思想是类似的，或者说，它是beam-search的推广，beam-search是lookahead-search的特例。我们对一个问题，同样先并行采样N个step1，然后还是要挑出得分前M高的step1往下走，之后的步骤也是类似的。唯一的不同是在于打分上。此时我们对一个step-i的打分，不是直接用verifier的输出，而是，从这个当前步骤i出发，往后走K步，然后把这走了K步之后的步骤拿去给verifier打分，并把这个得分-score-i-k-，视为当前步骤i的得分（这个lookahead-search和beam-search的关系，就类似于rl中的TD-n-方法和TD-0-之间的关系）" class="headerlink" title="这个其实和上面的beam search的思想是类似的，或者说，它是beam search的推广，beam search是lookahead search的特例。我们对一个问题，同样先并行采样N个step1，然后还是要挑出得分前M高的step1往下走，之后的步骤也是类似的。唯一的不同是在于打分上。此时我们对一个step i的打分，不是直接用verifier的输出，而是，从这个当前步骤i出发，往后走K步，然后把这走了K步之后的步骤拿去给verifier打分，并把这个得分$score_{i+k}$，视为当前步骤i的得分（这个lookahead search和beam search的关系，就类似于rl中的TD(n)方法和TD(0)之间的关系）"></a>这个其实和上面的beam search的思想是类似的，或者说，它是beam search的推广，beam search是lookahead search的特例。我们对一个问题，同样先并行采样N个step1，然后还是要挑出得分前M高的step1往下走，之后的步骤也是类似的。唯一的不同是在于打分上。此时我们对一个step i的打分，不是直接用verifier的输出，而是，从这个当前步骤i出发，往后走K步，然后把这走了K步之后的步骤拿去给verifier打分，并把这个得分$score_{i+k}$，视为当前步骤i的得分（这个lookahead search和beam search的关系，就类似于rl中的TD(n)方法和TD(0)之间的关系）</h4><h2 id="2-4-如何选择最佳搜索方法"><a href="#2-4-如何选择最佳搜索方法" class="headerlink" title="2.4. 如何选择最佳搜索方法"></a>2.4. 如何选择最佳搜索方法</h2><h3 id="上面介绍了三种辅助推理的搜索方法。自然，我们会想，有没有什么方法能挑出当前最适合、或者说最优的搜索方法？这里要考虑一些条件，比如：搜索预算（generation-budget）：也就是并行采样数量N。因为算力是有限的问题难度（difficulty）：之所以考虑这个因素，是因为，简单的问题，可能其实不需要搜索的辅助，llm也能回答得很好，此时再去搜索，且不说是否影响结果，就算不影响，可能也是白费算力；而对于难的问题，可能就很需要搜索方法，而且需要精挑细选搜索方法"><a href="#上面介绍了三种辅助推理的搜索方法。自然，我们会想，有没有什么方法能挑出当前最适合、或者说最优的搜索方法？这里要考虑一些条件，比如：搜索预算（generation-budget）：也就是并行采样数量N。因为算力是有限的问题难度（difficulty）：之所以考虑这个因素，是因为，简单的问题，可能其实不需要搜索的辅助，llm也能回答得很好，此时再去搜索，且不说是否影响结果，就算不影响，可能也是白费算力；而对于难的问题，可能就很需要搜索方法，而且需要精挑细选搜索方法" class="headerlink" title="上面介绍了三种辅助推理的搜索方法。自然，我们会想，有没有什么方法能挑出当前最适合、或者说最优的搜索方法？这里要考虑一些条件，比如：搜索预算（generation budget）：也就是并行采样数量N。因为算力是有限的问题难度（difficulty）：之所以考虑这个因素，是因为，简单的问题，可能其实不需要搜索的辅助，llm也能回答得很好，此时再去搜索，且不说是否影响结果，就算不影响，可能也是白费算力；而对于难的问题，可能就很需要搜索方法，而且需要精挑细选搜索方法"></a>上面介绍了三种辅助推理的搜索方法。自然，我们会想，有没有什么方法能挑出当前最适合、或者说最优的搜索方法？这里要考虑一些条件，比如：<br>搜索预算（generation budget）：也就是并行采样数量N。因为算力是有限的<br>问题难度（difficulty）：之所以考虑这个因素，是因为，简单的问题，可能其实不需要搜索的辅助，llm也能回答得很好，此时再去搜索，且不说是否影响结果，就算不影响，可能也是白费算力；而对于难的问题，可能就很需要搜索方法，而且需要精挑细选搜索方法</h3><h3 id="这个问题确实很值得探索。事实上，我们还可能可以想到，选定一种搜索算法以后，它对推理能力的提高能有多大？随着我们在推理时的搜索上花费的算力的增加，推理能力能提高到什么程度？什么时候会达到上限？还是说没有上限？（这样可能就会想到类似Can-1B-LLM-Surpass-405B-LLM-Rethinking-Compute-Optimal-Test-Time-Scaling这篇论文的idea了，或者说就会和它一样，去探讨类似的问题）"><a href="#这个问题确实很值得探索。事实上，我们还可能可以想到，选定一种搜索算法以后，它对推理能力的提高能有多大？随着我们在推理时的搜索上花费的算力的增加，推理能力能提高到什么程度？什么时候会达到上限？还是说没有上限？（这样可能就会想到类似Can-1B-LLM-Surpass-405B-LLM-Rethinking-Compute-Optimal-Test-Time-Scaling这篇论文的idea了，或者说就会和它一样，去探讨类似的问题）" class="headerlink" title="这个问题确实很值得探索。事实上，我们还可能可以想到，选定一种搜索算法以后，它对推理能力的提高能有多大？随着我们在推理时的搜索上花费的算力的增加，推理能力能提高到什么程度？什么时候会达到上限？还是说没有上限？（这样可能就会想到类似Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling这篇论文的idea了，或者说就会和它一样，去探讨类似的问题）"></a>这个问题确实很值得探索。事实上，我们还可能可以想到，选定一种搜索算法以后，它对推理能力的提高能有多大？随着我们在推理时的搜索上花费的算力的增加，推理能力能提高到什么程度？什么时候会达到上限？还是说没有上限？（这样可能就会想到类似<em><strong>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</strong></em>这篇论文的idea了，或者说就会和它一样，去探讨类似的问题）</h3><h3 id="说回来，这里可以直接参考原论文的实验结果。不妨先摆上结论："><a href="#说回来，这里可以直接参考原论文的实验结果。不妨先摆上结论：" class="headerlink" title="说回来，这里可以直接参考原论文的实验结果。不妨先摆上结论："></a>说回来，这里可以直接参考原论文的实验结果。不妨先摆上结论：</h3><h4 id="搜索效果是受到搜索预算和问题难度的影响的（这是比较显然的）"><a href="#搜索效果是受到搜索预算和问题难度的影响的（这是比较显然的）" class="headerlink" title="搜索效果是受到搜索预算和问题难度的影响的（这是比较显然的）"></a>搜索效果是受到搜索预算和问题难度的影响的（这是比较显然的）</h4><h4 id="当搜索预算较小，问题较难时，更适合beam-search，但要注意超参的调节"><a href="#当搜索预算较小，问题较难时，更适合beam-search，但要注意超参的调节" class="headerlink" title="当搜索预算较小，问题较难时，更适合beam search，但要注意超参的调节"></a>当搜索预算较小，问题较难时，更适合beam search，但要注意超参的调节</h4><h4 id="当搜索预算较大，问题较简单时，更适合best-of-N方法"><a href="#当搜索预算较大，问题较简单时，更适合best-of-N方法" class="headerlink" title="当搜索预算较大，问题较简单时，更适合best-of-N方法"></a>当搜索预算较大，问题较简单时，更适合best-of-N方法</h4><h4 id="当PRM训练得足够好时，较复杂的搜索方法（比如lookahead-search）的表现可能并不会特别好"><a href="#当PRM训练得足够好时，较复杂的搜索方法（比如lookahead-search）的表现可能并不会特别好" class="headerlink" title="当PRM训练得足够好时，较复杂的搜索方法（比如lookahead search）的表现可能并不会特别好"></a>当PRM训练得足够好时，较复杂的搜索方法（比如lookahead search）的表现可能并不会特别好</h4><h4 id="当问题特别难时，test-time-scaling-law可能效果有限。此时更有效的做法可能还是回到pretrain阶段，从这里入手，通过数据调配、pretrain-scaling等方法，给模型注入更多知识，这样可能会更有效"><a href="#当问题特别难时，test-time-scaling-law可能效果有限。此时更有效的做法可能还是回到pretrain阶段，从这里入手，通过数据调配、pretrain-scaling等方法，给模型注入更多知识，这样可能会更有效" class="headerlink" title="当问题特别难时，test-time scaling law可能效果有限。此时更有效的做法可能还是回到pretrain阶段，从这里入手，通过数据调配、pretrain scaling等方法，给模型注入更多知识，这样可能会更有效"></a>当问题特别难时，test-time scaling law可能效果有限。此时更有效的做法可能还是回到pretrain阶段，从这里入手，通过数据调配、pretrain scaling等方法，给模型注入更多知识，这样可能会更有效</h4><h1 id="三、"><a href="#三、" class="headerlink" title="三、"></a>三、</h1><h1 id="四、Pretrain还是Inference？"><a href="#四、Pretrain还是Inference？" class="headerlink" title="四、Pretrain还是Inference？"></a>四、Pretrain还是Inference？</h1><h1 id><a href="#" class="headerlink" title></a></h1>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/TTS/" rel="tag"># TTS</a>
              <a href="/tags/reasoning/" rel="tag"># reasoning</a>
              <a href="/tags/o1/" rel="tag"># o1</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/05/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/reasoning/Learning%20to%20reason%20with%20LLMs/" rel="prev" title="Learning to reason with LLMs">
      <i class="fa fa-chevron-left"></i> Learning to reason with LLMs
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/06/01/algo/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/%E5%AE%9A%E9%95%BF%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/%E5%AD%90%E6%95%B0%E7%BB%84%E6%9C%80%E5%A4%A7%E5%B9%B3%E5%9D%87%E6%95%B0%E2%85%A0/" rel="next" title="子数组最大平均数Ⅰ">
      子数组最大平均数Ⅰ <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8E%9F%E6%96%87%E9%93%BE%E6%8E%A5%E5%9C%A8%E8%BF%99"><span class="nav-number">1.</span> <span class="nav-text">原文链接在这</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AF%87%E5%B9%85%E8%BF%98%E8%9B%AE%E9%95%BF%E7%9A%84%EF%BC%8C%E4%BD%86%E8%AE%B2%E7%9A%84%E5%BE%88%E5%A5%BD%EF%BC%8C%E4%BD%9C%E4%B8%BA%E5%85%A5%E9%97%A8%E5%BE%88%E5%90%88%E9%80%82%EF%BC%88%E9%9D%9E%E5%B8%B8%E6%84%9F%E8%B0%A2%E3%80%82%E6%9C%AC%E6%96%87%E4%BB%85%E7%94%A8%E4%BA%8E%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%AE%B0%E5%BD%95%E4%B8%8E%E6%80%9D%E8%80%83%EF%BC%8C%E6%97%A0%E5%95%86%E4%B8%9A%E7%94%A8%E9%80%94%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">篇幅还蛮长的，但讲的很好，作为入门很合适（非常感谢。本文仅用于个人学习的记录与思考，无商业用途）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AFTest-Inferenc-time-Scaling-Law%EF%BC%9F"><span class="nav-number">3.</span> <span class="nav-text">一、什么是Test&#x2F;Inferenc-time Scaling Law？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A6%96%E5%85%88%E6%98%8E%E7%A1%AE%E4%B8%80%E4%B8%8B%EF%BC%8C%E7%8E%B0%E5%9C%A8%E6%98%AF%E7%A0%94%E7%A9%B6reasoning%E7%9A%84%E5%9C%BA%E6%99%AF"><span class="nav-number">3.1.</span> <span class="nav-text">首先明确一下，现在是研究reasoning的场景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E6%8F%90%E9%AB%98%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%AE%BE%E8%AE%A1prompt%EF%BC%8C%E6%AF%94%E5%A6%82%E5%9C%A8prompt%E9%87%8C%E7%BB%99%E5%87%BA%E4%B8%80%E4%BA%9B%E4%BE%8B%E9%A2%98%EF%BC%8C%E4%B8%94%E4%BE%8B%E9%A2%98%E9%87%8C%E6%9C%89%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E7%9A%84%E6%80%9D%E8%80%83%E8%BF%87%E7%A8%8B%EF%BC%8C%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%8F%82%E8%80%83%EF%BC%88%E4%BD%86%E6%AF%8F%E4%B8%AA%E9%97%AE%E9%A2%98%E9%83%BD%E8%AE%BE%E8%AE%A1prompt%EF%BC%8C%E5%A4%AA%E9%BA%BB%E7%83%A6%E4%BA%86%E3%80%82%E6%89%80%E4%BB%A5%E6%88%91%E4%BB%AC%E6%83%B3%EF%BC%8C%E8%83%BD%E5%90%A6%E8%87%AA%E5%8A%A8%E5%81%9A%E8%BF%99%E4%B8%AA%E8%BF%87%E7%A8%8B%EF%BC%9F%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%87%AA%E5%8A%A8%E5%BC%80%E5%B1%95CoT%E7%9A%84%E8%BF%87%E7%A8%8B%EF%BC%89%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E6%9C%89%E4%B8%80%E5%A4%A7%E6%89%B9%E9%AB%98%E8%B4%A8%E9%87%8F%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%97%A2%E6%9C%89%E7%BB%93%E6%9E%9C%EF%BC%8C%E4%B9%9F%E6%9C%89%E8%AF%A6%E7%BB%86%E5%92%8C%E6%AD%A3%E7%A1%AE%E7%9A%84%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%8C%E9%82%A3%E6%88%91%E4%BB%AC%E7%9B%B4%E6%8E%A5%E7%94%A8%E8%BF%99%E4%BA%9B%E6%95%B0%E6%8D%AE%E6%9D%A5%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8Csft%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E4%B8%80%E7%A7%8Dpost-training%EF%BC%89%EF%BC%8C%E8%AE%AD%E7%BB%83%E5%AE%8C%EF%BC%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BE%93%E5%87%BA%E5%B0%B1%E8%83%BD%E5%B8%A6%E4%B8%8A%E4%B8%AD%E9%97%B4%E6%80%9D%E8%80%83%E8%BF%87%E7%A8%8B%E5%92%8C%E6%AD%A5%E9%AA%A4%E4%BA%86%EF%BC%9B%E5%8F%88%E6%88%96%E8%80%85%EF%BC%8C%E6%88%91%E4%BB%AC%E5%85%88%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%AE%A9%E5%AE%83%E6%8E%8C%E6%8F%A1%E6%A0%BC%E5%BC%8F%EF%BC%8C%E8%A6%81%E8%BE%93%E5%87%BA%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%88%E5%85%88%E7%A1%AE%E4%BF%9D%E6%9C%89%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%89%EF%BC%8C%E4%B9%8B%E5%90%8E%E5%86%8D%E5%8D%95%E7%8B%AC%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%EF%BC%8C%E9%87%87%E7%94%A8%E7%B1%BB%E4%BC%BCrlhf%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E8%AE%A9%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E5%8E%BB%E8%AF%84%E4%BC%B0%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%8C%E4%BB%8E%E8%80%8C%E6%8F%90%E9%AB%98%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E7%9A%84%E8%B4%A8%E9%87%8F%E3%80%82%E6%9C%80%E5%90%8E%E5%9C%A8%E5%AE%9E%E9%99%85%E6%8E%A8%E7%90%86%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%88%91%E4%BB%AC%E8%AE%A9%E6%A8%A1%E5%9E%8B%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%8E%A2%E7%B4%A2%EF%BC%8C%E7%84%B6%E5%90%8E%E5%86%8D%E7%94%A8%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%88%A4%E6%96%AD%E5%93%AA%E4%BA%9B%E6%AD%A5%E9%AA%A4%E6%98%AF%E6%AF%94%E8%BE%83%E5%A5%BD%E7%9A%84%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E9%80%89%E6%8B%A9%E8%BF%99%E4%B8%80%E6%AD%A5%EF%BC%8C%E5%B9%B6%E7%BB%A7%E7%BB%AD%E5%90%91%E5%90%8E%E8%B5%B0"><span class="nav-number">3.2.</span> <span class="nav-text">最简单的提高推理能力的方法，就是设计prompt，比如在prompt里给出一些例题，且例题里有一步一步的思考过程，让模型进行参考（但每个问题都设计prompt，太麻烦了。所以我们想，能否自动做这个过程？也就是让模型自动开展CoT的过程）进一步的，如果我们有一大批高质量数据，既有结果，也有详细和正确的中间步骤，那我们直接用这些数据来对模型进行sft（也就是一种post-training），训练完，模型的输出就能带上中间思考过程和步骤了；又或者，我们先训练模型，让它掌握格式，要输出中间步骤（先确保有中间步骤），之后再单独训练一个奖励模型，采用类似rlhf的操作，让奖励模型去评估中间步骤，从而提高中间步骤的质量。最后在实际推理的时候，我们让模型一步一步探索，然后再用奖励模型来判断哪些步骤是比较好的，我们就选择这一步，并继续向后走</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%86%E6%98%AF%E4%B8%8A%E8%BF%B0%E5%86%85%E5%AE%B9%E9%87%8C%EF%BC%8C%E9%99%A4%E4%BA%86%E8%AE%BE%E8%AE%A1prompt%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E5%85%B6%E5%AE%83%E7%9A%84%E5%85%B6%E5%AE%9E%E9%83%BD%E6%98%AFpost-training%EF%BC%8C%E8%BF%98%E7%AE%97%E4%B8%8D%E4%B8%8Ainference%E3%80%82inference%E7%9A%84%E5%87%BA%E7%8E%B0%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E6%98%AF%E6%83%B3%E8%A6%81%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%94%B9%E8%BF%9B%E8%80%8C%E5%AF%BC%E8%87%B4%E7%9A%84%EF%BC%9A"><span class="nav-number">3.3.</span> <span class="nav-text">但是上述内容里，除了设计prompt的方法，其它的其实都是post-training，还算不上inference。inference的出现，可以认为是想要进一步改进而导致的：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%81%87%E4%BD%BF%E6%88%91%E4%BB%ACpost-training%E5%AE%8C%EF%BC%8C%E5%BE%97%E5%88%B0%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%83%BD%E8%BE%93%E5%87%BA%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%EF%BC%8C%E4%B8%94%E8%B4%A8%E9%87%8F%E4%B9%9F%E4%B8%8D%E9%94%99%E3%80%82%E4%BD%86%E6%88%91%E4%BB%AC%E4%B9%9F%E4%B8%8D%E8%83%BD%E4%BF%9D%E8%AF%81%EF%BC%8C%E8%BF%99%E4%BA%9B%E4%B8%AD%E9%97%B4%E7%BB%93%E6%9E%9C%E5%92%8C%E7%AD%94%E6%A1%88%E4%B8%80%E5%AE%9A%E6%98%AF%E6%9C%80%E5%A5%BD%E7%9A%84%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%AF%E8%83%BD%E8%BF%98%E6%98%AF%E8%A6%81%E5%81%9A%E4%B8%80%E4%BA%9B%E7%B1%BB%E4%BC%BC%E6%90%9C%E7%B4%A2%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%AF%94%E5%A6%82%EF%BC%8C%E5%BC%95%E5%85%A5%E8%83%BD%E8%AF%84%E4%BC%B0%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E7%9A%84verifier%EF%BC%8C%E7%84%B6%E5%90%8E%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8CN%E6%AC%A1attempts%EF%BC%88%E4%B8%80%E6%AC%A1attempt%E5%B0%B1%E6%98%AF%E4%BB%8E%E5%BC%80%E5%A7%8B%EF%BC%8C%E7%BB%8F%E8%BF%87%E5%AE%8C%E6%95%B4%E7%9A%84%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%8C%E6%9C%80%E7%BB%88%E5%BE%97%E5%88%B0%E7%BB%93%E6%9E%9C%EF%BC%89%EF%BC%8C%E4%B9%8B%E5%90%8E%E5%86%8D%E7%94%A8verifier%E5%AF%B9%E8%BF%99N%E6%AC%A1attempts%E8%BF%9B%E8%A1%8C%E8%AF%84%E4%BC%B0%EF%BC%8C%E4%BB%8E%E9%87%8C%E9%9D%A2%E9%80%89%E5%87%BA%E6%9C%80%E5%A5%BD%E7%9A%84%E4%B8%80%E6%AC%A1attempt%E4%BD%9C%E4%B8%BA%E7%BB%93%E6%9E%9C"><span class="nav-number">3.3.1.</span> <span class="nav-text">假使我们post-training完，得到的模型能输出中间过程，且质量也不错。但我们也不能保证，这些中间结果和答案一定是最好的。我们可能还是要做一些类似搜索的操作，比如，引入能评估中间步骤的verifier，然后让模型进行N次attempts（一次attempt就是从开始，经过完整的中间步骤，最终得到结果），之后再用verifier对这N次attempts进行评估，从里面选出最好的一次attempt作为结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8A%E9%9D%A2%E7%9A%84%E5%81%9A%E6%B3%95%E6%98%AF%E5%9F%BA%E4%BA%8E%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%83%BD%E8%BE%93%E5%87%BA%E8%BF%87%E7%A8%8B%EF%BC%8C%E5%8F%AF%E8%83%BD%E8%BF%87%E7%A8%8B%E7%9A%84%E8%B4%A8%E9%87%8F%E8%BF%98%E4%B8%8D%E9%94%99%EF%BC%8C%E4%BD%86%E4%B9%9F%E8%AE%B8%E8%BF%98%E8%83%BD%E6%9B%B4%E5%A5%BD%EF%BC%8C%E8%BF%99%E4%B9%88%E4%B8%80%E7%A7%8D%E6%83%85%E5%86%B5%E3%80%82%E8%8B%A5%E5%9C%A8post-training%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E5%BC%95%E5%85%A5verifier%E6%9D%A5%E6%8C%87%E5%AF%BC%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E7%9A%84%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%8C%E5%8F%AF%E8%83%BD%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E5%BE%97%E5%88%B0%E5%BE%88%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E4%BA%86%E3%80%82%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8Cpost-training%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E5%BC%95%E5%85%A5verifier%EF%BC%8C%E6%9D%A5%E8%AF%84%E4%BB%B7%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E5%B9%B6%E6%94%B9%E5%96%84%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E7%9A%84%E8%B4%A8%E9%87%8F"><span class="nav-number">3.3.2.</span> <span class="nav-text">上面的做法是基于：模型能输出过程，可能过程的质量还不错，但也许还能更好，这么一种情况。若在post-training的时候，我们就引入verifier来指导模型生成的中间步骤，可能可以直接得到很高质量的中间过程了。也就是说，post-training的时候，我们就引入verifier，来评价中间过程并改善模型输出的中间过程的质量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%B3%E6%AD%A4%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E6%80%BB%E7%BB%93%E4%B8%80%E4%B8%8B%E5%B8%B8%E8%A7%81%E7%9A%84%EF%BC%8C%E7%94%A8%E4%BA%8E%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E4%B8%89%E5%A5%97%E6%A1%86%E6%9E%B6%EF%BC%9A"><span class="nav-number">3.4.</span> <span class="nav-text">至此，我们可以总结一下常见的，用于提高模型推理能力的三套框架：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%86%E6%9E%B61-base-generator-inference"><span class="nav-number">3.4.1.</span> <span class="nav-text">框架1. base generator+inference</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%B0%B1%E6%98%AF%EF%BC%8C%E6%88%91%E4%BB%AC%E8%AE%AD%E7%BB%83%E5%A5%BD%E6%A8%A1%E5%9E%8B%E5%90%8E%EF%BC%8C%E4%B8%8D%E5%86%8D%E8%BF%9B%E8%A1%8Cpost-training%EF%BC%8C%E5%8F%AA%E5%9C%A8%E6%8E%A8%E7%90%86%E7%9A%84%E6%97%B6%E5%80%99%E5%81%9A%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C%E3%80%82%E4%B9%9F%E5%B0%B1%E6%98%AF%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BC%9A%E7%94%A8%E4%B8%80%E4%B8%AA%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84verifier%EF%BC%88%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E4%B8%8D%E6%8A%8A%E8%AE%AD%E7%BB%83verifier%E5%BD%93%E6%88%90post-training%E7%9A%84%E4%B8%80%E9%83%A8%E5%88%86%E3%80%82%E6%AF%95%E7%AB%9F%EF%BC%8C%E8%AE%AD%E7%BB%83verifier%EF%BC%8C%E5%B9%B6%E4%B8%8D%E6%98%AF%E5%8E%BB%E8%AE%AD%E7%BB%83%E6%88%91%E4%BB%AC%E7%9A%84llm%EF%BC%89%EF%BC%88verifier%E7%9A%84%E7%B1%BB%E5%9E%8B%E4%B9%9F%E6%9C%89%E5%BE%88%E5%A4%9A%EF%BC%8C%E5%B8%B8%E8%A7%81%E7%9A%84%E6%9C%89PRM%EF%BC%8CProcess-supervised-Reward-Model%EF%BC%89%EF%BC%8C%E5%9C%A8%E6%8E%A8%E7%90%86%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E8%BE%85%E5%8A%A9%E6%88%91%E4%BB%AC%E7%9A%84llm%E8%BF%9B%E8%A1%8C%E6%8E%A2%E7%B4%A2%E3%80%82%E4%B8%BB%E8%A6%81%E6%98%AF%E7%94%A8%E4%B8%80%E4%BA%9B%E6%90%9C%E7%B4%A2%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E5%A6%82best-of-N%EF%BC%8Cbeam-search%E7%AD%89%EF%BC%89"><span class="nav-number">3.4.1.0.1.</span> <span class="nav-text">具体来说，就是，我们训练好模型后，不再进行post-training，只在推理的时候做一些操作。也就是，我们会用一个训练好的verifier（这里我们不把训练verifier当成post-training的一部分。毕竟，训练verifier，并不是去训练我们的llm）（verifier的类型也有很多，常见的有PRM，Process-supervised Reward Model），在推理的时候，辅助我们的llm进行探索。主要是用一些搜索的方法（如best-of-N，beam search等）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%86%E6%9E%B62-base-generator-formatted-post-training-inference"><span class="nav-number">3.4.2.</span> <span class="nav-text">框架2. base generator+formatted post-training+inference</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%99%E5%A5%97%E6%A1%86%E6%9E%B6%E7%9A%84%E6%AD%A5%E9%AA%A4%E6%98%AF%EF%BC%8C%E5%85%88%E5%AF%B9%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84llm%EF%BC%8C%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%BA%9Bpost-training%EF%BC%8C%E8%AE%A9%E5%AE%83%E6%8E%8C%E6%8F%A1%E6%A0%BC%E5%BC%8F%EF%BC%8C%E7%9F%A5%E9%81%93%E6%8E%A8%E7%90%86%E7%9A%84%E6%97%B6%E5%80%99%E8%BF%9B%E8%A1%8C%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E7%9A%84%E6%80%9D%E8%80%83%EF%BC%8C%E4%B9%8B%E5%90%8E%E5%86%8D%E7%94%A8%E4%BA%8Einference%EF%BC%88%E6%AD%A4%E6%97%B6%E6%88%91%E4%BB%AC%E4%BE%9D%E7%84%B6%E5%8F%AF%E4%BB%A5%E7%94%A8%E9%82%A3%E4%BA%9B%E5%9C%A8%E6%A1%86%E6%9E%B61%E9%87%8C%E7%94%A8%E5%88%B0%E7%9A%84%EF%BC%8C%E5%B8%AE%E5%8A%A9%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%89%E8%BF%99%E4%B8%AApost-training%EF%BC%8C%E5%9F%BA%E7%A1%80%E7%9A%84%E8%A6%81%E6%B1%82%E6%98%AF%E8%AE%A9%E6%A8%A1%E5%9E%8B%E7%9F%A5%E9%81%93%E4%BA%A7%E7%94%9F%E7%BB%93%E6%9E%9C%E4%B9%8B%E4%BD%99%EF%BC%8C%E8%BF%98%E5%BE%97%E4%BA%A7%E7%94%9F%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E3%80%82%E5%85%B7%E4%BD%93%E7%9A%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%AFsft%EF%BC%8C%E4%B9%9F%E5%8F%AF%E4%BB%A5%E6%98%AF%E4%B8%80%E4%BA%9Brl%E7%AE%97%E6%B3%95%E3%80%82%E4%BD%86%E5%A6%82%E6%9E%9C%E6%89%8B%E5%A4%B4%E6%9C%89%E5%BE%88%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%AF%94%E5%A6%82%E5%B8%A6%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%8C%E4%B8%94%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E4%B8%8D%E4%BB%85%E8%A7%84%E8%8C%83%EF%BC%8C%E6%AD%A3%E7%A1%AE%E7%8E%87%E4%B9%9F%E5%BE%88%E9%AB%98%EF%BC%8C%E9%82%A3%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E4%B9%9F%E6%BD%9C%E7%A7%BB%E9%BB%98%E5%8C%96%E5%9C%B0%E6%8F%90%E9%AB%98%E4%BA%86%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E7%9A%84%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E7%9A%84%E8%B4%A8%E9%87%8F%E3%80%82%E8%BF%99%E6%98%BE%E7%84%B6%E6%98%AF%E5%A5%BD%E7%9A%84%EF%BC%8C%E5%8F%AA%E4%B8%8D%E8%BF%87%E5%AF%B9%E6%95%B0%E6%8D%AE%E8%A6%81%E6%B1%82%E6%AF%94%E8%BE%83%E9%AB%98%E8%80%8C%E5%B7%B2"><span class="nav-number">3.4.2.0.1.</span> <span class="nav-text">这套框架的步骤是，先对训练好的llm，进行一些post-training，让它掌握格式，知道推理的时候进行中间步骤的思考，之后再用于inference（此时我们依然可以用那些在框架1里用到的，帮助提高模型推理能力的方法）这个post-training，基础的要求是让模型知道产生结果之余，还得产生中间步骤。具体的训练方法，可以是sft，也可以是一些rl算法。但如果手头有很高质量的数据，比如带中间步骤，且中间步骤不仅规范，正确率也很高，那训练的时候，也潜移默化地提高了模型生成的中间过程的质量。这显然是好的，只不过对数据要求比较高而已</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A6%82%E6%9E%9C%E5%AF%B9%E6%AF%94%E4%B8%80%E4%B8%8B%E6%A1%86%E6%9E%B62%E5%92%8C%E6%A1%86%E6%9E%B61%EF%BC%8C%E4%B8%8D%E9%9A%BE%E5%8F%91%E7%8E%B0%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E5%A4%9A%E4%BA%86%E4%B8%80%E4%B8%AApost-training%EF%BC%8C%E4%B8%94%E8%BF%99%E4%B8%AApost-training%E8%87%B3%E5%B0%91%E6%98%AF%E5%8F%AF%E4%BB%A5%E5%B8%AE%E5%8A%A9%E6%A8%A1%E5%9E%8B%E8%A7%84%E8%8C%83%E8%BF%87%E7%A8%8B%EF%BC%8C%E5%AD%A6%E5%88%B0%E8%A6%81%E8%BE%93%E5%87%BA%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%88%E6%A0%B9%E6%8D%AE%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%EF%BC%8C%E8%BF%98%E6%9C%89%E5%8F%AF%E8%83%BD%E6%8F%90%E9%AB%98%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E7%9A%84%E8%B4%A8%E9%87%8F%EF%BC%89%E3%80%82%E5%9B%A0%E4%B8%BA%E8%BF%9B%E8%A1%8C%E4%BA%86post-training%EF%BC%8C%E6%89%80%E4%BB%A5%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E6%97%B6%E4%BC%9A%E8%87%AA%E5%8A%A8%E5%B8%A6%E4%B8%8A%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%EF%BC%9B%E7%9B%B8%E6%AF%94%E4%B9%8B%E4%B8%8B%EF%BC%8C%E6%A1%86%E6%9E%B61%E9%87%8C%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%8C%E6%B2%A1%E6%9C%89%E7%BB%8F%E8%BF%87post-trainig%EF%BC%8C%E6%89%80%E4%BB%A5%E5%AE%83%E5%B0%B1%E4%B8%8D%E5%A4%AA%E4%BC%9A%E7%9F%A5%E9%81%93%E8%A6%81%E8%BE%93%E5%87%BA%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%EF%BC%8C%E6%95%85%E6%88%91%E4%BB%AC%E5%9C%A8%E6%A1%86%E6%9E%B61%E4%B8%8B%EF%BC%8C%E6%8E%A8%E7%90%86%E6%97%B6%E9%9C%80%E8%A6%81%E5%B8%A6%E4%B8%8A%E4%B8%80%E4%BA%9Bprompt%EF%BC%8C%E6%8C%87%E5%AF%BC%E6%A8%A1%E5%9E%8B%E6%8C%89%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%EF%BC%89"><span class="nav-number">3.4.2.0.2.</span> <span class="nav-text">如果对比一下框架2和框架1，不难发现，主要是多了一个post-training，且这个post-training至少是可以帮助模型规范过程，学到要输出中间步骤（根据数据质量，还有可能提高中间步骤的质量）。因为进行了post-training，所以模型输出时会自动带上中间过程；相比之下，框架1里的模型，没有经过post-trainig，所以它就不太会知道要输出中间过程，故我们在框架1下，推理时需要带上一些prompt，指导模型按格式输出）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%86%E6%9E%B63-base-generator-formatted-post-training%EF%BC%88with-inference-filtering-method%EF%BC%89-inference%EF%BC%88selective%EF%BC%89"><span class="nav-number">3.4.3.</span> <span class="nav-text">框架3. base generator+formatted post-training（with inference filtering method）+inference（selective）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%99%E5%A5%97%E6%A1%86%E6%9E%B6%E5%85%B6%E5%AE%9E%E5%92%8C%E6%A1%86%E6%9E%B62%E6%98%AF%E6%9C%89%E4%BA%9B%E7%B1%BB%E4%BC%BC%E7%9A%84%EF%BC%8C%E5%8F%AA%E4%B8%8D%E8%BF%87%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90%E4%B8%8D%E5%90%8C%E3%80%82%E6%A1%86%E6%9E%B62%E9%87%8Cpost-training%E7%9A%84%E6%95%B0%E6%8D%AE%E6%98%AF%E6%88%91%E4%BB%AC%E9%A2%9D%E5%A4%96%E5%87%86%E5%A4%87%E5%A5%BD%E7%9A%84%EF%BC%8C%E8%80%8C%E6%A1%86%E6%9E%B63%E9%87%8C%E7%9A%84post-trainig%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%98%AFllm%E8%87%AA%E7%94%9F%E6%88%90%E7%9A%84%E3%80%82%E5%BD%93%E7%84%B6%EF%BC%8C%E4%B8%BA%E4%BA%86%E4%BF%9D%E8%AF%81%E8%BF%99%E4%BA%9B%E6%95%B0%E6%8D%AE%E7%9A%84%E8%B4%A8%E9%87%8F%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BC%9A%E4%BB%8E%E4%B8%AD%E7%AD%9B%E9%80%89%E5%87%BA%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%86%8D%E7%94%A8%E8%BF%99%E4%BA%9B%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8Cpost-training%EF%BC%88%E4%BD%86%E5%85%B7%E4%BD%93%E6%80%8E%E4%B9%88%E6%8C%87%E5%AF%BCllm%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%EF%BC%8C%E8%BF%99%E9%87%8C%E6%88%91%E7%8C%9C%E6%B5%8B%EF%BC%8C%E6%98%AF%E9%9C%80%E8%A6%81%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%BA%9Bprompt%E7%9A%84%EF%BC%89%E5%9B%A0%E4%B8%BA%E6%88%91%E4%BB%AC%E5%B7%B2%E7%BB%8F%E4%BB%8E%E8%87%AA%E7%94%9F%E6%88%90%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8C%E8%BF%9B%E8%A1%8C%E7%AD%9B%E9%80%89%E4%BA%86%EF%BC%8C%E6%89%80%E4%BB%A5%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E8%BF%99%E4%BA%9B%E7%AD%9B%E5%87%BA%E6%9D%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9A%84%E8%B4%A8%E9%87%8F%E5%B7%B2%E7%BB%8F%E6%AF%94%E8%BE%83%E9%AB%98%E4%BA%86%EF%BC%8C%E5%9B%A0%E6%AD%A4%E5%9C%A8%E6%8E%A8%E7%90%86%E7%9A%84%E6%97%B6%E5%80%99%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E4%B8%8D%E5%8A%A0%E4%B8%8A%E9%82%A3%E4%BA%9B%E6%90%9C%E7%B4%A2%E7%9A%84%E6%8E%A8%E7%90%86%E6%8A%80%E5%B7%A7%EF%BC%88%E6%AF%95%E7%AB%9F%E5%8F%AF%E4%BB%A5%E5%89%A9%E4%B8%8B%E6%8E%A8%E7%90%86%E6%97%B6%E7%9A%84%E8%AE%A1%E7%AE%97%E9%87%8F%E5%98%9B%EF%BC%89%EF%BC%9B%E4%BD%86%E4%BB%A5%E9%98%B2%E4%B8%87%E4%B8%80%EF%BC%8C%E6%88%96%E8%80%85%E4%B8%BA%E4%BA%86%E8%BF%BD%E6%B1%82%E6%80%A7%E8%83%BD%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BE%9D%E7%84%B6%E5%8F%AF%E4%BB%A5%E5%9C%A8%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%E7%94%A8%E4%B8%8A%E5%89%8D%E8%BF%B0%E7%9A%84%E5%90%84%E7%A7%8D%E6%8A%80%E5%B7%A7"><span class="nav-number">3.4.3.0.1.</span> <span class="nav-text">这套框架其实和框架2是有些类似的，只不过数据来源不同。框架2里post-training的数据是我们额外准备好的，而框架3里的post-trainig的数据，是llm自生成的。当然，为了保证这些数据的质量，我们会从中筛选出高质量的数据，再用这些数据进行post-training（但具体怎么指导llm生成数据，这里我猜测，是需要设计一些prompt的）因为我们已经从自生成的数据里进行筛选了，所以，可以认为这些筛出来的数据的质量已经比较高了，因此在推理的时候我们可以考虑不加上那些搜索的推理技巧（毕竟可以剩下推理时的计算量嘛）；但以防万一，或者为了追求性能，我们依然可以在推理阶段用上前述的各种技巧</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%AF%B9%E4%B8%8A%E8%BF%B0%E6%A1%86%E6%9E%B6%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%8C%E6%88%91%E4%BB%AC%E5%86%8D%E6%9D%A5%E7%9C%8Bdeepmind%E5%9F%BA%E4%BA%8E%E6%A1%86%E6%9E%B6%E9%87%87%E5%8F%96%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%A1%88%EF%BC%9A"><span class="nav-number">3.5.</span> <span class="nav-text">基于对上述框架的理解，我们再来看deepmind基于框架采取的两种方案：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%A1%88%E4%B8%80%EF%BC%9A%E5%88%A9%E7%94%A8PRM%E6%8C%87%E5%BC%95%E6%90%9C%E7%B4%A2%EF%BC%88%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%85%88post-training%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E6%8E%A8%E7%90%86%E6%97%B6%E7%9A%84%E4%BC%98%E5%8C%96post-trainig%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%8F%AA%E5%BC%95%E5%AF%BC%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%A0%BC%E5%BC%8F%E6%96%B9%E9%9D%A2%E5%AF%B9%E9%BD%90%EF%BC%9B%E5%86%8D%E9%A2%9D%E5%A4%96%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAverifier%EF%BC%8C%E6%8E%A8%E7%90%86%E6%97%B6%E7%94%A8%E8%BF%99%E4%B8%AAverifier%E6%8C%87%E5%AF%BC%E6%90%9C%E7%B4%A2%EF%BC%89%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%A1%86%E6%9E%B62%E4%BA%86%EF%BC%89"><span class="nav-number">3.5.1.</span> <span class="nav-text">方案一：利用PRM指引搜索（具体来说，先post-training，再进行推理时的优化post-trainig的时候，只引导模型在格式方面对齐；再额外训练一个verifier，推理时用这个verifier指导搜索）（也就是框架2了）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%A1%88%E4%BA%8C%EF%BC%9A%E9%80%9A%E8%BF%87sft%EF%BC%8C%E7%9B%B4%E6%8E%A5%E6%94%B9%E5%8F%98%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BE%93%E5%87%BA%E5%88%86%E5%B8%83%EF%BC%88Revise-proposal-distribution%EF%BC%89%E4%B9%9F%E5%B0%B1%E6%98%AF%EF%BC%8C%E5%85%88%E7%94%A8sft%E8%BF%9B%E8%A1%8Cpost-training%EF%BC%8C%E6%97%A2%E8%AE%A9%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%BC%9A%E8%BE%93%E5%87%BA%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%8C%E5%90%8C%E6%97%B6%E4%B9%9F%E7%A1%AE%E4%BF%9D%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E7%9A%84%E8%B4%A8%E9%87%8F%EF%BC%9B%E6%AD%A4%E5%90%8E%EF%BC%8C%E5%90%8C%E6%A0%B7%E9%9C%80%E8%A6%81%E8%AE%AD%E7%BB%83verifier%EF%BC%88PRM%EF%BC%89%EF%BC%8C%E5%B9%B6%E5%9C%A8%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%8C%87%E5%AF%BC%E6%90%9C%E7%B4%A2%EF%BC%88%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0%EF%BC%8C%E8%BF%98%E6%98%AF%E6%A1%86%E6%9E%B62%E3%80%82%E5%8F%AA%E4%B8%8D%E8%BF%87%E8%BF%99%E9%87%8C%E5%8F%AF%E8%83%BD%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E9%AB%98%E4%B8%80%E7%82%B9%EF%BC%8C%E6%89%80%E4%BB%A5%E5%AF%B9%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E7%9A%84%E8%B4%A8%E9%87%8F%E4%B9%9F%E6%9C%89%E6%89%80%E4%BF%9D%E8%AF%81%EF%BC%89"><span class="nav-number">3.5.2.</span> <span class="nav-text">方案二：通过sft，直接改变模型的输出分布（Revise proposal distribution）也就是，先用sft进行post-training，既让模型学会输出中间步骤，同时也确保中间步骤的质量；此后，同样需要训练verifier（PRM），并在推理过程中指导搜索（我们发现，还是框架2。只不过这里可能数据质量高一点，所以对中间过程的质量也有所保证）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E9%9D%A2%E8%AF%A6%E7%BB%86%E8%AE%B2%E4%B8%80%E4%B8%8B%E4%B8%A4%E7%A7%8D%E5%85%B7%E4%BD%93%E7%9A%84%EF%BC%8C%E6%94%B9%E5%96%84%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E6%96%B9%E6%A1%88"><span class="nav-number">3.6.</span> <span class="nav-text">下面详细讲一下两种具体的，改善模型推理能力的方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E6%96%B9%E6%A1%88%E4%B8%80"><span class="nav-number">4.</span> <span class="nav-text">二、方案一</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E4%B8%8A%E8%BF%B0%E7%9A%84%E8%BF%87%E7%A8%8B%EF%BC%8C%E6%A0%B8%E5%BF%83%E7%9A%84%E6%AD%A5%E9%AA%A4%E6%98%AF%E4%BB%A5%E4%B8%8B%E4%B8%89%E6%AD%A5%EF%BC%9Apost-training%EF%BC%88%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%8E%8C%E6%8F%A1%E6%A0%BC%E5%BC%8F%EF%BC%89%E8%AE%AD%E7%BB%83verifier%EF%BC%88%E7%A1%AE%E4%BF%9D%E8%BF%99%E4%B8%AAverifier%E7%9C%9F%E7%9A%84%E8%83%BD%E4%BD%9C%E5%87%BA%E5%90%88%E7%90%86%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%88%A4%E6%96%AD%EF%BC%89%E6%8E%A8%E7%90%86%E6%97%B6%E7%9A%84%E6%8C%87%E5%AF%BC%E6%90%9C%E6%90%9C%EF%BC%88%E8%A6%81%E6%8A%8Averifier%E7%9C%9F%E7%9A%84%E7%94%A8%E8%B5%B7%E6%9D%A5%EF%BC%8C%E5%B9%B6%E6%8F%90%E9%AB%98%E6%8E%A8%E7%90%86%E8%B4%A8%E9%87%8F%EF%BC%89%E4%B8%8B%E9%9D%A2%E8%AF%A6%E7%BB%86%E7%9C%8B%E4%B8%80%E4%B8%8B%E8%BF%99%E4%B8%89%E6%AD%A5%E8%A6%81%E6%80%8E%E4%B9%88%E5%81%9A"><span class="nav-number">4.1.</span> <span class="nav-text">根据上述的过程，核心的步骤是以下三步：post-training（让模型掌握格式）训练verifier（确保这个verifier真的能作出合理正确的判断）推理时的指导搜搜（要把verifier真的用起来，并提高推理质量）下面详细看一下这三步要怎么做</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E6%A0%BC%E5%BC%8F%E8%AE%AD%E7%BB%83"><span class="nav-number">4.2.</span> <span class="nav-text">2.1. 格式训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%A7%E4%BD%93%E4%B8%8A%E8%AE%B2%EF%BC%8C%E6%88%91%E4%BB%AC%E6%98%AF%E5%88%A9%E7%94%A8prompt%EF%BC%8C%E8%AE%A9%E6%A8%A1%E5%9E%8B%E8%87%AA%E7%94%9F%E4%BA%A7%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%BB%8E%E8%80%8C%E7%94%A8%E6%9D%A5%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%AE%A9%E5%AE%83%E6%8E%8C%E6%8F%A1%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F"><span class="nav-number">4.2.1.</span> <span class="nav-text">大体上讲，我们是利用prompt，让模型自生产数据，从而用来训练模型，让它掌握输出格式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%EF%BC%8C%E5%9C%A8prompt%E4%B8%AD%EF%BC%8C%E5%8A%A0%E5%85%A5%E6%9C%89%E6%AD%A3%E7%A1%AE%E6%A0%BC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90%EF%BC%8C%E5%BC%95%E5%AF%BC%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E4%B8%80%E6%89%B9%E7%BB%93%E6%9E%9C%EF%BC%88%E9%83%BD%E6%98%AF%E6%9C%89%E6%AD%A3%E7%A1%AE%E6%A0%BC%E5%BC%8F%E7%9A%84%EF%BC%89%EF%BC%8C%E4%B9%8B%E5%90%8E%E6%8A%8A%E8%BF%99%E6%89%B9%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%88%B0sft%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E9%87%8C%EF%BC%8C%E7%84%B6%E5%90%8E%E5%86%8D%E7%94%A8%E8%BF%99%E4%BA%9Bsft%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E3%80%82%E8%AE%AD%E7%BB%83%E5%AE%8C%EF%BC%8C%E6%A8%A1%E5%9E%8B%E7%90%86%E5%BA%94%E8%83%BD%E6%8E%8C%E6%8F%A1%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%A0%BC%E5%BC%8F%E4%BA%86%EF%BC%88%E6%AD%A4%E6%97%B6%E6%88%91%E4%BB%AC%E6%B2%A1%E6%9C%89%E5%81%9A%E5%A4%AA%E5%A4%9A%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E4%BB%85%E4%BB%85%E6%98%AF%E9%80%9A%E8%BF%87prompt%E5%BC%95%E5%AF%BC%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E5%B8%A6%E6%A0%BC%E5%BC%8F%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%B9%9F%E6%B2%A1%E5%8E%BB%E7%AE%A1%E8%BF%99%E4%BA%9B%E6%95%B0%E6%8D%AE%E7%9A%84%E8%B4%A8%E9%87%8F%E3%80%82%E6%89%80%E4%BB%A5%E7%8E%B0%E5%9C%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E8%BF%98%E5%B9%B6%E4%B8%8D%E5%8F%AF%E4%BF%A1%EF%BC%89"><span class="nav-number">4.2.1.0.1.</span> <span class="nav-text">具体来说，我们可以考虑，在prompt中，加入有正确格式的例子，引导模型生成一批结果（都是有正确格式的），之后把这批生成数据加到sft的数据集里，然后再用这些sft的数据集微调模型。训练完，模型理应能掌握正确的格式了（此时我们没有做太多的操作，仅仅是通过prompt引导模型生成带格式的数据，也没去管这些数据的质量。所以现在模型的中间步骤还并不可信）</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E8%AE%AD%E7%BB%83PRM"><span class="nav-number">4.3.</span> <span class="nav-text">2.2. 训练PRM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%99%E4%B8%80%E6%AD%A5%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E8%AE%AD%E7%BB%83%E5%87%BA%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BB%A5%E8%AF%84%E4%BC%B0%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E5%A5%BD%E5%9D%8F%E7%9A%84%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8D%B3%E6%AD%A4%E5%A4%84%E7%9A%84PRM%EF%BC%89%E3%80%82%E8%BF%99%E4%B8%AA%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BE%93%E5%85%A5%EF%BC%8C%E5%B0%B1%E5%BA%94%E8%AF%A5%E6%98%AF%E9%97%AE%E9%A2%98-%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%8C%E8%BE%93%E5%87%BA%E5%88%99%E6%98%AF%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E7%9A%84%E5%A5%BD%E5%9D%8F%EF%BC%88%E6%AF%94%E5%A6%82%E7%94%A8%E4%B8%80%E4%B8%AA%E5%88%86%E6%95%B0%E8%BF%9B%E8%A1%8C%E9%87%8F%E5%8C%96%EF%BC%8C%E6%88%96%E8%80%85%E8%8B%A5%E5%B9%B2%E7%AD%89%E7%BA%A7%E7%9A%84%E6%A0%87%E7%AD%BE%E8%BF%9B%E8%A1%8C%E9%87%8F%E5%8C%96%EF%BC%89%E3%80%82%E9%82%A3%E4%B9%88%EF%BC%8C%E5%B0%B1%E9%9C%80%E8%A6%81%E8%8E%B7%E5%BE%97%E5%B8%A6%E6%A0%87%E7%AD%BE%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%82%E6%95%B0%E6%8D%AE%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E5%B0%B1%E6%98%AF%E4%B8%80%E4%B8%AA%E5%85%B3%E9%94%AE%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82%E6%A0%B9%E6%8D%AE%E7%BB%8F%E8%B4%B9%E7%9A%84%E5%A4%9A%E5%B0%91%EF%BC%8C%E6%9C%89%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%E5%8F%AF%E9%80%89%EF%BC%9A"><span class="nav-number">4.3.1.</span> <span class="nav-text">这一步，我们的目的是训练出一个可以评估中间步骤好坏的奖励模型（即此处的PRM）。这个奖励模型的输入，就应该是问题+中间步骤，输出则是中间步骤的好坏（比如用一个分数进行量化，或者若干等级的标签进行量化）。那么，就需要获得带标签的数据。数据如何获取就是一个关键的问题。根据经费的多少，有三种方法可选：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E5%A4%A7%E9%87%8F%E6%A0%87%E6%B3%A8%E3%80%82%E6%AD%A4%E6%97%B6%E6%88%91%E4%BB%AC%E8%AE%A9generator%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83%E5%AE%8C%EF%BC%8C%E4%B8%94%E7%BB%8F%E8%BF%87%E4%BA%86%E6%A0%BC%E5%BC%8Fpost-training%E7%9A%84llm%EF%BC%89%E7%94%9F%E6%88%90%E4%B8%80%E5%A4%A7%E6%89%B9%E5%B8%A6%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E5%92%8C%E7%AD%94%E6%A1%88%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%B9%8B%E5%90%8E%E6%89%BE%E4%BA%BA%E5%AF%B9%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E8%BF%9B%E8%A1%8C%E6%A0%87%E6%B3%A8%EF%BC%8C%E5%AE%8C%E4%BA%86%E6%8B%BF%E6%9D%A5%E8%AE%AD%E7%BB%83PRM%E3%80%82%E6%95%88%E6%9E%9C%E5%BA%94%E5%BD%93%E6%98%AF%E9%9D%9E%E5%B8%B8%E5%A5%BD%E7%9A%84%EF%BC%8C%E5%B0%B1%E6%98%AF%E6%88%90%E6%9C%AC%E5%BE%88%E9%AB%98%E8%80%8C%E5%B7%B2"><span class="nav-number">4.3.1.0.1.</span> <span class="nav-text">方法一：大量标注。此时我们让generator（也就是预训练完，且经过了格式post-training的llm）生成一大批带中间步骤和答案的数据，之后找人对中间步骤进行标注，完了拿来训练PRM。效果应当是非常好的，就是成本很高而已</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E7%AD%9B%E5%87%BA%E4%B8%80%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%A0%87%E6%B3%A8%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E6%98%AF%E8%AE%A9generator%E7%94%9F%E6%88%90%E4%B8%80%E6%89%B9%E6%95%B0%E6%8D%AE%EF%BC%88%E6%AD%A4%E6%97%B6%E5%8F%AF%E8%83%BD%E5%B0%B1%E4%B8%8D%E6%98%AF%E9%9D%9E%E5%B8%B8%E5%A4%A7%E7%9A%84%E9%87%8F%E7%BA%A7%EF%BC%8C%E5%9B%A0%E4%B8%BA%E7%8E%B0%E5%9C%A8%E8%80%83%E8%99%91%E7%9A%84%E6%98%AF%E7%BB%8F%E8%B4%B9%E4%B8%8D%E9%82%A3%E4%B9%88%E5%A4%9A%E7%9A%84%E6%83%85%E5%86%B5%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E8%BF%9B%E8%A1%8C%E8%8B%A5%E5%B9%B2%E7%9A%84%E7%AD%9B%E9%80%89%EF%BC%88%E4%BE%8B%E5%A6%82%EF%BC%8C%E6%8A%8A%E6%A0%BC%E5%BC%8F%E9%94%99%E8%AF%AF%E7%9A%84%E7%AD%94%E6%A1%88%E7%AD%9B%E6%8E%89%EF%BC%89%EF%BC%8C%E4%B9%8B%E5%90%8E%E6%8A%8A%E7%AD%9B%E5%87%BA%E6%9D%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8B%BF%E5%8E%BB%E6%89%93%E6%A0%87%E7%AD%BE%EF%BC%8C%E6%89%93%E5%AE%8C%E5%B0%B1%E5%85%88%E8%AE%AD%E4%B8%80%E6%AC%A1PRM%E3%80%82%E6%AD%A4%E6%97%B6%EF%BC%8C%E6%88%91%E4%BB%AC%E8%A6%81%E7%94%A8%E8%AE%AD%E7%BB%83%E5%AE%8C%E7%9A%84PRM%EF%BC%8C%E7%AD%9B%E5%87%BA%E5%AE%83%E4%B8%8D%E6%93%85%E9%95%BF%E7%9A%84%E4%BE%8B%E5%AD%90%EF%BC%8C%E5%86%8D%E8%AE%A9%E4%BA%BA%E5%8E%BB%E6%A0%87%E6%B3%A8%E8%BF%99%E4%BA%9B%E5%AE%83%E4%B8%8D%E6%93%85%E9%95%BF%E7%9A%84%E4%BE%8B%E5%AD%90%EF%BC%88%E6%88%96%E8%80%85%E8%AF%B4%E6%98%AF%E9%9A%BE%E7%9A%84%E4%BE%8B%E5%AD%90%EF%BC%89%EF%BC%8C%E8%B5%B7%E5%88%B0%E9%92%88%E5%AF%B9%E6%80%A7%E8%AE%AD%E7%BB%83%E7%9A%84%E6%95%88%E6%9E%9C%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E8%AE%AD%E7%BB%83%E5%AE%8CPRM%E5%90%8E%EF%BC%8C%E8%AE%A9generator%E5%86%8D%E7%94%9F%E6%88%90%E4%B8%80%E6%89%B9%E5%B8%A6%E8%BF%87%E7%A8%8B%E5%92%8C%E7%AD%94%E6%A1%88%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E7%84%B6%E5%90%8E%E8%AE%A9PRM%E6%89%93%E5%88%86%E3%80%82%E6%AD%A4%E6%97%B6%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E6%8C%91%E5%87%BA%E9%82%A3%E4%BA%9B%EF%BC%9A%E2%80%9DPRM%E7%BB%99%E6%95%B4%E4%BD%93%E8%BF%87%E7%A8%8B%E6%89%93%E5%88%86%E5%BE%88%E9%AB%98%EF%BC%8C%E4%BD%86%E6%9C%80%E7%BB%88%E7%AD%94%E6%A1%88%E9%94%99%E8%AF%AF%E2%80%9C%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%82%E8%BF%99%E4%BA%9B%E5%B0%B1%E6%98%AFPRM%E7%9A%84hard-example%E6%8C%91%E5%87%BAhard-example%E5%90%8E%EF%BC%8C%E4%BA%BA%E5%B7%A5%E5%AF%B9%E8%BF%99%E4%BA%9B%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%A0%87%E6%B3%A8%EF%BC%8C%E5%B9%B6%E7%94%A8%E8%BF%99%E4%BA%9B%E6%95%B0%E6%8D%AE%E5%86%8D%E6%9D%A5%E8%AE%AD%E7%BB%83PRM%EF%BC%8C%E4%B9%8B%E5%90%8E%E9%87%8D%E5%A4%8D%E4%B8%8A%E8%BF%B0%E8%BF%87%E7%A8%8B%EF%BC%8C%E8%BF%AD%E4%BB%A3%E5%9C%B0%E8%AE%AD%E7%BB%83PRM"><span class="nav-number">4.3.1.0.2.</span> <span class="nav-text">方法二：筛出一部分数据进行标注。我们还是让generator生成一批数据（此时可能就不是非常大的量级，因为现在考虑的是经费不那么多的情况），然后进行若干的筛选（例如，把格式错误的答案筛掉），之后把筛出来的数据拿去打标签，打完就先训一次PRM。此时，我们要用训练完的PRM，筛出它不擅长的例子，再让人去标注这些它不擅长的例子（或者说是难的例子），起到针对性训练的效果具体来说，我们训练完PRM后，让generator再生成一批带过程和答案的数据，然后让PRM打分。此时，我们就挑出那些：”PRM给整体过程打分很高，但最终答案错误“的数据。这些就是PRM的hard example挑出hard example后，人工对这些数据进行标注，并用这些数据再来训练PRM，之后重复上述过程，迭代地训练PRM</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%EF%BC%88%E5%A4%A7%E4%BD%93%E4%B8%8A%EF%BC%8C%E8%BF%99%E6%98%AF%E6%9C%89%E9%81%93%E7%90%86%E7%9A%84%EF%BC%8C%E5%9B%A0%E4%B8%BA%E7%AD%94%E6%A1%88%E9%94%99%E4%BA%86%EF%BC%8C%E5%BE%88%E5%A4%A7%E7%A8%8B%E5%BA%A6%E4%B8%8A%E5%B0%B1%E6%98%AF%E5%9B%A0%E4%B8%BA%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E6%9C%89%E8%AF%AF%EF%BC%8C%E9%82%A3%E4%B9%88%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E7%9A%84%E6%89%93%E5%88%86%E7%90%86%E5%BA%94%E4%B8%8D%E9%82%A3%E4%B9%88%E9%AB%98%E3%80%82%E4%BD%86%E6%97%A2%E7%84%B6%E6%9C%89%E8%BF%99%E4%BA%9B%E7%AD%94%E6%A1%88%E9%94%99%E4%BA%86%EF%BC%8C%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E5%8D%B4%E5%88%86%E5%BE%88%E9%AB%98%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E8%AE%A4%E4%B8%BAPRM%E5%AF%B9%E8%BF%99%E4%BA%9B%E6%95%B0%E6%8D%AE%E7%9A%84%E6%89%93%E5%88%86%E4%B8%8D%E5%A4%AA%E6%AD%A3%E7%A1%AE%EF%BC%8C%E5%9B%A0%E6%AD%A4%E5%AE%83%E4%BB%AC%E6%98%AFhard-example%EF%BC%89%EF%BC%88%E4%BD%86%E5%85%B6%E5%AE%9E%E8%BF%99%E4%B9%9F%E5%B9%B6%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%AD%A3%E7%A1%AE%E3%80%82%E4%B9%9F%E8%AE%B8%E4%B9%9F%E5%AD%98%E5%9C%A8%E8%BF%99%E7%A7%8D%E6%83%85%E5%86%B5%EF%BC%9A%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E7%A1%AE%E5%AE%9E%E6%8C%BA%E5%AF%B9%E7%9A%84%EF%BC%8C%E5%8D%95%E7%BA%AF%E6%98%AFgenerator%E5%9C%A8%E6%9C%80%E5%90%8E%E8%BE%93%E5%87%BA%E7%AD%94%E6%A1%88%E7%9A%84%E6%97%B6%E5%80%99%E5%87%BA%E9%94%99%E4%BA%86%E3%80%82%E4%B8%8D%E8%BF%87%E5%8F%AF%E8%83%BD%E8%BF%98%E6%98%AF%E5%BE%97%E6%9C%89%E7%9C%9F%E5%AE%9E%E7%9A%84%E6%95%B0%E6%8D%AE%E6%89%8D%E8%83%BD%E9%AA%8C%E8%AF%81%E8%BF%99%E4%B8%80%E7%82%B9%E5%90%A7%E3%80%82%E5%A6%82%E6%9E%9C%E7%A1%AE%E5%AE%9E%E6%9C%89%E8%BF%99%E4%B8%AA%E7%8E%B0%E8%B1%A1%EF%BC%8C%E9%82%A3%E6%88%96%E8%AE%B8%E6%98%AF%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BB%A5%E6%94%B9%E8%BF%9B%E7%9A%84%E5%B0%8F%E7%82%B9%EF%BC%89"><span class="nav-number">4.3.1.0.2.1.</span> <span class="nav-text">（大体上，这是有道理的，因为答案错了，很大程度上就是因为中间过程有误，那么中间过程的打分理应不那么高。但既然有这些答案错了，中间过程却分很高的数据，我们就认为PRM对这些数据的打分不太正确，因此它们是hard example）（但其实这也并不完全正确。也许也存在这种情况：中间过程确实挺对的，单纯是generator在最后输出答案的时候出错了。不过可能还是得有真实的数据才能验证这一点吧。如果确实有这个现象，那或许是一个可以改进的小点）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8F%A6%E5%A4%96%EF%BC%8C%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E4%BA%86%E8%AE%A9PRM%E5%AF%B9%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E8%BF%9B%E8%A1%8C%E6%95%B4%E4%BD%93%E6%89%93%E5%88%86%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%88%E5%9B%A0%E4%B8%BAPRM%E6%9C%AC%E8%BA%AB%E7%9A%84%E8%BE%93%E5%87%BA%E6%98%AF%E9%92%88%E5%AF%B9%E4%B8%80%E4%B8%AA%E6%AD%A5%E9%AA%A4%E7%9A%84%EF%BC%89%E3%80%82%E8%BF%99%E9%87%8C%E6%9C%89%E8%8B%A5%E5%B9%B2%E7%9A%84%E9%80%89%E6%8B%A9%EF%BC%8C%E5%B8%B8%E8%A7%81%E7%9A%84%E6%9C%89%EF%BC%9A%E8%BF%9E%E4%B9%98%E5%BC%8F%EF%BC%88prod%EF%BC%89%E3%80%81%E6%9C%80%E5%B0%8F%E5%BC%8F%EF%BC%88min%EF%BC%89%E3%80%81%E6%9C%80%E5%90%8E%E4%B8%80%E6%AD%A5%E5%BC%8F%EF%BC%88last-step%EF%BC%89%E3%80%82prod%E5%92%8Cmin%E6%98%AFopenAI%E7%9A%84lets-verify-step-by-step%E8%AE%BA%E6%96%87%E9%87%8C%E6%8E%A2%E7%B4%A2%E8%BF%87%E7%9A%84%EF%BC%8Clast-step%E5%88%99%E6%98%AFdeepmind%E5%9C%A8Scaling-LLM-Test-Time-Compute-Optimally-can-be-More-Effective-than-Scaling-Model-Parameters%E9%87%8C%E4%BD%BF%E7%94%A8%E7%9A%84%EF%BC%88%E7%9C%8B%E5%8D%9A%E5%AE%A2%E6%98%AF%E8%BF%99%E4%B9%88%E8%AF%B4%E7%9A%84%EF%BC%8C%E7%AC%94%E8%80%85%E8%BF%98%E6%B2%A1%E8%AF%BB%E5%91%A2%EF%BC%8C%E5%9B%9E%E5%A4%B4%E8%AF%BB%E8%AF%BB%E7%A1%AE%E8%AE%A4%E4%B8%8B%EF%BC%89%E3%80%82%E5%93%AA%E4%B8%80%E7%A7%8D%E5%85%B6%E5%AE%9E%E9%83%BD%E6%98%AF%E6%9C%89%E7%82%B9%E9%81%93%E7%90%86%E7%9A%84%EF%BC%8C%E9%80%89%E6%8B%A9%E5%B9%B6%E4%B8%8D%E7%BB%9D%E5%AF%B9%E3%80%82%E9%87%8D%E7%82%B9%E6%98%AF%EF%BC%8C%E9%80%89%E6%8B%A9%E4%BA%86%E6%9F%90%E4%B8%80%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E8%83%BD%E7%94%A8verifier%EF%BC%8C%E5%AF%B9%E4%B8%AD%E9%97%B4%E7%9A%84%E6%AD%A5%E9%AA%A4%E8%BF%9B%E8%A1%8C%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%E7%9A%84%E6%89%93%E5%88%86%EF%BC%88prod%E5%B0%B1%E6%98%AF%EF%BC%8C%E8%AE%A9verifier%E5%AF%B9%E6%AF%8F%E4%B8%AAstep%E9%83%BD%E6%89%93%E5%88%86%EF%BC%8C%E7%84%B6%E5%90%8E%E6%8A%8A%E6%89%80%E6%9C%89%E7%9A%84%E5%BE%97%E5%88%86%E4%B9%98%E8%B5%B7%E6%9D%A5%EF%BC%8C%E5%BE%97%E5%88%B0%E7%9A%84%E5%B0%B1%E6%98%AF%E6%95%B4%E4%B8%AA%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E7%9A%84%E6%95%B4%E4%BD%93%E5%BE%97%E5%88%86%EF%BC%9Bmin%E5%B0%B1%E6%98%AF%EF%BC%8C%E8%AE%A9verifier%E5%AF%B9%E6%AF%8F%E4%B8%AAstep%E6%89%93%E5%88%86%EF%BC%8C%E4%B9%8B%E5%90%8E%E5%8F%96%E5%87%BA%E5%85%B6%E4%B8%AD%E6%9C%80%E5%B0%8F%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E6%95%B4%E4%BD%93%E7%9A%84%E5%BE%97%E5%88%86%E3%80%82%E8%BF%99%E7%A7%8D%E6%96%B9%E5%BC%8F%E8%83%8C%E5%90%8E%E7%9A%84%E6%80%9D%E6%83%B3%E6%98%AF%EF%BC%8C%E5%A6%82%E6%9E%9C%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E9%87%8C%E6%9C%89%E4%B8%80%E6%AD%A5%E5%BE%88%E5%8F%AF%E8%83%BD%E9%94%99%E4%BA%86%EF%BC%8C%E9%82%A3%E6%95%B4%E4%BD%93%E4%B9%9F%E5%BE%88%E5%8F%AF%E8%83%BD%E9%94%99%E4%BA%86%EF%BC%9B%E5%8F%8D%E4%B9%8B%EF%BC%8C%E5%A6%82%E6%9E%9C%E4%B8%AD%E9%97%B4%E6%AF%8F%E4%B8%80%E6%AD%A5%E9%83%BD%E5%BE%88%E5%AF%B9%EF%BC%8C%E9%82%A3%E6%95%B4%E4%BD%93%E5%A4%A7%E6%A6%82%E7%8E%87%E4%B9%9F%E6%98%AF%E5%AF%B9%E7%9A%84%EF%BC%9Blast-step%E5%88%99%E6%98%AF%EF%BC%8C%E8%AE%A9verifier%E5%AF%B9%E6%AF%8F%E4%B8%AAstep%E6%89%93%E5%88%86%EF%BC%8C%E5%86%8D%E5%8F%96%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E6%AD%A5%E9%AA%A4%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E6%95%B4%E4%BD%93%E7%9A%84%E5%BE%97%E5%88%86%E3%80%82%E8%BF%99%E6%98%AF%E5%9B%A0%E4%B8%BA%EF%BC%8C%E5%81%87%E8%AE%BE%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98p%EF%BC%8C%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E4%BA%86k%E4%B8%AA%E6%AD%A5%E9%AA%A4%EF%BC%8C%E5%88%99%E5%BD%93%E6%88%91%E4%BB%AC%E8%A6%81%E5%AF%B9%E7%AC%ACi%E4%B8%AA%E6%AD%A5%E9%AA%A4%E8%BF%9B%E8%A1%8C%E6%89%93%E5%88%86%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E8%BE%93%E5%85%A5%E7%BB%99verifier%E7%9A%84%E5%85%B6%E5%AE%9E%E6%98%AF%E9%97%AE%E9%A2%98p%E5%8A%A0%E4%B8%8A%E5%89%8Di%E4%B8%AA%E6%AD%A5%E9%AA%A4%E3%80%82%E5%9B%A0%E4%B8%BA%E8%AF%84%E4%BB%B7%E4%B8%80%E4%B8%AA%E6%AD%A5%E9%AA%A4%E4%B8%8D%E8%83%BD%E5%AD%A4%E7%AB%8B%E5%9C%B0%E7%9C%8B%EF%BC%8C%E8%80%8C%E6%98%AF%E8%A6%81%E7%BB%93%E5%90%88%E4%B8%8A%E6%96%87%E7%9A%84%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AF%B9%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AAstep%E6%89%93%E5%88%86%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E8%A6%81%E8%BE%93%E5%85%A5%E7%9A%84%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E9%97%AE%E9%A2%98%E5%8A%A0%E6%89%80%E6%9C%89%E7%9A%84%E6%AD%A5%E9%AA%A4%E3%80%82%E9%82%A3%E4%B9%88%EF%BC%8C%E5%AF%B9%E6%9C%80%E5%90%8E%E4%B8%80%E6%AD%A5%E7%9A%84%E8%AF%84%E5%88%86%EF%BC%8C%E5%85%B6%E5%AE%9E%E4%B9%9F%E5%8F%AF%E4%BB%A5%E7%9C%8B%E6%88%90%E6%98%AF%E5%AF%B9%E6%95%B4%E4%BD%93%E7%9A%84%E8%AF%84%E5%88%86%E4%BA%86%E3%80%82%E8%BF%99%E5%B0%B1%E7%B1%BB%E4%BC%BC%E4%BA%8E%EF%BC%8CLSTM%E9%87%8C%EF%BC%8C%E6%9C%80%E5%90%8E%E7%9A%84%E4%B8%80%E4%B8%AA%E9%9A%90%E7%8A%B6%E6%80%81%EF%BC%8C%E7%90%86%E8%AE%BA%E4%B8%8A%E6%98%AF%E5%8C%85%E5%90%AB%E4%BA%86%E5%89%8D%E9%9D%A2%E7%9A%84%E6%89%80%E6%9C%89%E4%BF%A1%E6%81%AF%E7%9A%84%EF%BC%89"><span class="nav-number">4.3.1.0.2.2.</span> <span class="nav-text">另外，上面提到了让PRM对中间步骤进行整体打分的操作（因为PRM本身的输出是针对一个步骤的）。这里有若干的选择，常见的有：连乘式（prod）、最小式（min）、最后一步式（last step）。prod和min是openAI的lets verify step by step论文里探索过的，last step则是deepmind在Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters里使用的（看博客是这么说的，笔者还没读呢，回头读读确认下）。哪一种其实都是有点道理的，选择并不绝对。重点是，选择了某一种方式后，我们就能用verifier，对中间的步骤进行一个整体的打分（prod就是，让verifier对每个step都打分，然后把所有的得分乘起来，得到的就是整个中间步骤的整体得分；min就是，让verifier对每个step打分，之后取出其中最小的得分，作为整体的得分。这种方式背后的思想是，如果中间过程里有一步很可能错了，那整体也很可能错了；反之，如果中间每一步都很对，那整体大概率也是对的；last step则是，让verifier对每个step打分，再取最后一个步骤的得分，作为整体的得分。这是因为，假设一个问题p，模型输出了k个步骤，则当我们要对第i个步骤进行打分的时候，输入给verifier的其实是问题p加上前i个步骤。因为评价一个步骤不能孤立地看，而是要结合上文的。因此，我们对最后一个step打分的时候，要输入的其实就是问题加所有的步骤。那么，对最后一步的评分，其实也可以看成是对整体的评分了。这就类似于，LSTM里，最后的一个隐状态，理论上是包含了前面的所有信息的）</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%B8%89%EF%BC%9A%E5%AE%8C%E5%85%A8%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E3%80%82%E6%AD%A4%E6%97%B6%E6%88%91%E4%BB%AC%E5%AE%8C%E5%85%A8%E4%B8%8D%E8%BF%9B%E8%A1%8C%E4%BA%BA%E5%B7%A5%E6%A0%87%E6%B3%A8%E3%80%82%E7%94%A8%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%89%93%E5%88%86%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E5%85%88%E8%AE%A9generator%E7%94%9F%E6%88%90%E4%B8%80%E6%89%B9%E5%B8%A6%E8%BF%87%E7%A8%8B%E5%92%8C%E7%AD%94%E6%A1%88%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%82%E6%AD%A4%E6%97%B6%EF%BC%8C%E6%88%91%E4%BB%AC%E8%A6%81%E5%AF%B9%E8%BF%99%E4%BA%9B%E8%BF%87%E7%A8%8B%E8%BF%9B%E8%A1%8C%E6%89%93%E5%88%86%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%E6%89%93%E4%BA%86%E5%88%86%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9D%A5%E8%AE%AD%E7%BB%83PRM%E3%80%82%EF%BC%88%E4%BD%86%E5%85%B3%E4%BA%8E%E8%BF%99%E4%B8%AA%E5%85%B7%E4%BD%93%E7%9A%84%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F%EF%BC%8C%E7%AC%94%E8%80%85%E6%9C%89%E7%82%B9%E9%97%AE%E9%A2%98%EF%BC%9A%E6%88%91%E4%BB%AC%E5%BE%97%E5%88%B0%E7%9A%84%E6%98%AFsoft-label%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%BF%9E%E7%BB%AD%E7%9A%84%E6%95%B0%E5%80%BC%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8D%9A%E5%AE%A2%E8%AF%B4%EF%BC%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8CPRM%E8%BF%98%E6%98%AF%E4%B8%80%E4%B8%AA%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%EF%BC%9F%E8%BF%99%E4%B8%8D%E5%A4%AA%E5%AF%B9%E5%90%A7%EF%BC%9F%E5%BE%97%E7%9C%8B%E4%BA%86%E5%8E%9F%E8%AE%BA%E6%96%87%E6%89%8D%E6%9D%A5%E5%9B%9E%E7%AD%94%E4%B8%80%E4%B8%8B%E4%BA%86%EF%BC%89%E6%80%8E%E4%B9%88%E6%89%93%E5%88%86%E5%91%A2%EF%BC%9F%E8%BF%99%E9%87%8C%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E7%B1%BB%E4%BC%BC%E4%BA%8Erl%E9%87%8C%E7%9A%84Monte-Carlo%E6%96%B9%E6%B3%95%EF%BC%88%E8%AF%9D%E8%AF%B4%E5%AE%83%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E6%95%99N-Monte-Carlo-rollouts%EF%BC%89%E3%80%82%E6%93%8D%E4%BD%9C%E5%A6%82%E4%B8%8B%EF%BC%88%E4%BD%86%E8%BF%99%E4%B8%AA%E6%98%AF%E6%A0%B9%E6%8D%AE%E5%8D%9A%E5%AE%A2%E5%86%85%E5%AE%B9%E6%80%BB%E7%BB%93%E7%9A%84%EF%BC%8C%E8%80%8C%E5%8D%9A%E5%AE%A2%E9%87%8C%E6%8F%90%E5%88%B0%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%96%B9%E6%B3%95%E6%9C%AC%E8%BA%AB%E5%9C%A8%E8%AE%BA%E6%96%87%E9%87%8C%E5%B9%B6%E6%B2%A1%E6%9C%89%E5%BE%97%E5%88%B0%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BB%8B%E7%BB%8D%E3%80%82%E6%89%80%E4%BB%A5%E5%B8%A6%E6%9C%89%E4%B8%BB%E8%A7%82%E7%9A%84%E7%8C%9C%E6%B5%8B%E3%80%82%E5%90%8C%E6%A0%B7%E7%9A%84%EF%BC%8C%E8%BF%98%E6%98%AF%E5%BE%97%E7%AD%89%E5%88%B0%E7%AC%94%E8%80%85%E8%AF%BB%E4%B8%80%E8%AF%BB%E5%8E%9F%E6%96%87%E4%B9%8B%E5%90%8E%E6%89%8D%E8%83%BD%E7%BB%99%E5%87%BA%E5%87%86%E7%A1%AE%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82%E6%AD%A4%E5%A4%84%E5%85%88%E5%A7%91%E4%B8%94%E7%94%A8%E4%BD%9C%E7%90%86%E8%A7%A3%EF%BC%89%EF%BC%9A"><span class="nav-number">4.3.1.0.3.</span> <span class="nav-text">方法三：完全自动生成数据。此时我们完全不进行人工标注。用模型生成的数据进行打分具体来说，我们先让generator生成一批带过程和答案的数据。此时，我们要对这些过程进行打分，然后用打了分的数据来训练PRM。（但关于这个具体的训练方式，笔者有点问题：我们得到的是soft label，也就是连续的数值，为什么博客说，训练的时候，PRM还是一个分类模型？这不太对吧？得看了原论文才来回答一下了）怎么打分呢？这里的方法，类似于rl里的Monte Carlo方法（话说它其实就是教N Monte-Carlo rollouts）。操作如下（但这个是根据博客内容总结的，而博客里提到，这个方法本身在论文里并没有得到详细的介绍。所以带有主观的猜测。同样的，还是得等到笔者读一读原文之后才能给出准确的回答。此处先姑且用作理解）：</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%81%87%E5%A6%82%E7%8E%B0%E5%9C%A8%E6%9C%89%E4%B8%80%E4%B8%AAsample%EF%BC%88%E5%AE%83%E6%98%AF%E6%8C%87%E5%AE%8C%E6%95%B4%E7%9A%84%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E5%8A%A0%E7%AD%94%E6%A1%88%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%89%EF%BC%8C%E6%88%91%E4%BB%AC%E6%83%B3%E5%AF%B9%E5%85%B6%E4%B8%AD%E6%AF%8F%E4%B8%AA%E6%AD%A5%E9%AA%A4%E8%BF%9B%E8%A1%8C%E8%AF%84%E5%88%86%E3%80%82%E4%B8%8D%E5%A6%A8%E5%85%88%E8%80%83%E8%99%91%E5%AF%B9step1%E8%BF%9B%E8%A1%8C%E8%AF%84%E5%88%86%E3%80%82%E6%88%91%E4%BB%AC%E6%8A%8A%E9%97%AE%E9%A2%98%E5%92%8Cstep1%E7%BB%99generator%EF%BC%8C%E8%AE%A9%E5%AE%83%E7%BB%A7%E7%BB%AD%E9%87%87%E6%A0%B7N%E4%B8%AAsamples%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%EF%BC%8C%E8%AE%A9%E6%A8%A1%E5%9E%8B%E4%BB%8E%E9%97%AE%E9%A2%98%E5%92%8Cstep1%E5%87%BA%E5%8F%91%EF%BC%8C%E6%9D%A5%E5%B0%9D%E8%AF%95%E5%90%84%E7%A7%8D%E8%B7%AF%E5%BE%84%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E5%B9%B6%E7%BB%99%E5%87%BA%E7%AD%94%E6%A1%88%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E6%88%91%E4%BB%AC%E7%BB%9F%E8%AE%A1%E8%BF%99N%E4%B8%AAsamples%E9%87%8C%E6%9C%89%E5%A4%9A%E5%B0%91%E4%B8%AA%E6%98%AF%E5%9B%9E%E7%AD%94%E5%AF%B9%E4%BA%86%E7%9A%84%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%9B%9E%E7%AD%94%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%AF%94%E4%BE%8B%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E8%AE%A4%E4%B8%BA%E4%BB%A3%E8%A1%A8%E4%BA%86step1%E7%9A%84%E8%AF%84%E5%88%86%EF%BC%88%E6%89%80%E4%BB%A5%EF%BC%8C%E8%BF%99%E6%98%AF%E4%B8%AA%E8%BF%9E%E7%BB%AD%E5%80%BC%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AFsoft-label%EF%BC%89%E5%AF%B9%E5%85%B6%E5%AE%83%E6%AD%A5%E9%AA%A4%EF%BC%8C%E4%B9%9F%E6%98%AF%E7%B1%BB%E4%BC%BC%E7%9A%84%E8%AF%84%E5%88%86%E6%96%B9%E5%BC%8F%EF%BC%88%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E5%85%B6%E5%AE%9E%E8%AE%A1%E7%AE%97%E9%87%8F%E8%BF%98%E8%9B%AE%E5%A4%A7%E7%9A%84%E3%80%82%E6%AF%95%E7%AB%9F%E6%98%AFMonte-Carlo%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%89"><span class="nav-number">4.3.1.0.3.1.</span> <span class="nav-text">假如现在有一个sample（它是指完整的中间步骤加答案的数据），我们想对其中每个步骤进行评分。不妨先考虑对step1进行评分。我们把问题和step1给generator，让它继续采样N个samples（也就是，让模型从问题和step1出发，来尝试各种路径解决问题并给出答案），然后我们统计这N个samples里有多少个是回答对了的，这个回答正确的比例，我们就认为代表了step1的评分（所以，这是个连续值，也就是soft label）对其它步骤，也是类似的评分方式（可以看到，其实计算量还蛮大的。毕竟是Monte Carlo的方法）</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E4%BD%BF%E7%94%A8PRM%E6%8C%87%E5%AF%BC%E6%90%9C%E7%B4%A2%E8%BF%87%E7%A8%8B"><span class="nav-number">4.4.</span> <span class="nav-text">2.3. 使用PRM指导搜索过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E6%8B%AC%E5%9C%B0%E8%AE%B2%EF%BC%8C%E6%88%91%E4%BB%AC%E7%8E%B0%E5%9C%A8%E6%9C%89%E4%BA%86generator%EF%BC%88%E5%8F%AA%E6%9C%89%E6%A0%BC%E5%BC%8F%EF%BC%8C%E8%B4%A8%E9%87%8F%E6%97%A0%E6%B3%95%E4%BF%9D%E8%AF%81%EF%BC%89%E5%92%8Cverifier%EF%BC%88%E5%8F%AF%E4%BB%A5%E5%AF%B9%E4%B8%AD%E9%97%B4%E8%BF%87%E7%A8%8B%E8%BF%9B%E8%A1%8C%E8%AF%84%E5%88%86%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E5%B8%8C%E6%9C%9B%E7%94%A8verifier%E6%8C%87%E5%AF%BC%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E7%BB%93%E6%9E%9C%EF%BC%8C%E7%AD%9B%E5%87%BA%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82%E7%94%A8%E5%88%B0%E7%9A%84%E6%96%B9%E6%B3%95%E6%98%AF%E4%B8%80%E4%BA%9B%E6%90%9C%E7%B4%A2%E7%9A%84%E6%96%B9%E6%B3%95%E3%80%82%E4%B8%BB%E8%A6%81%E6%98%AFbest-of-N%E3%80%81beam-search%E4%BB%A5%E5%8F%8Alookahead-search"><span class="nav-number">4.4.1.</span> <span class="nav-text">概括地讲，我们现在有了generator（只有格式，质量无法保证）和verifier（可以对中间过程进行评分），然后希望用verifier指导模型生成结果，筛出高质量的回答。用到的方法是一些搜索的方法。主要是best-of-N、beam search以及lookahead search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-best-of-N"><span class="nav-number">4.4.2.</span> <span class="nav-text">2.3.1. best-of-N</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E4%B8%AA%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95%E6%AF%94%E8%BE%83%E7%AE%80%E5%8D%95%E7%9B%B4%E7%99%BD%EF%BC%8C%E4%B9%9F%E5%B8%B8%E8%A2%AB%E7%94%A8%E5%81%9Abaseline%E8%BF%9B%E8%A1%8C%E6%AF%94%E8%BE%83%E3%80%82%E5%AE%83%E6%98%AF%E8%AF%B4%EF%BC%8C%E8%AE%A9generator%E9%92%88%E5%AF%B9%E9%97%AE%E9%A2%98%EF%BC%8C%E7%94%9F%E6%88%90N%E4%B8%AA%E5%AE%8C%E6%95%B4%E5%9B%9E%E7%AD%94%EF%BC%88%E6%AD%A4%E6%97%B6%E5%BD%93%E7%84%B6%E5%8C%85%E5%90%AB%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%EF%BC%89%EF%BC%8C%E5%86%8D%E7%94%A8%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84verifier%E5%AF%B9%E8%BF%99N%E4%B8%AA%E5%9B%9E%E7%AD%94%E7%9A%84%E4%B8%AD%E9%97%B4%E6%AD%A5%E9%AA%A4%E9%83%BD%E8%BF%9B%E8%A1%8C%E6%95%B4%E4%BD%93%E7%9A%84%E6%89%93%E5%88%86%EF%BC%88%E5%9B%9E%E5%BF%86%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84%EF%BC%8C%E4%B8%89%E7%A7%8D%E8%BF%9B%E8%A1%8C%E6%95%B4%E4%BD%93%E6%89%93%E5%88%86%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%9Aprod%EF%BC%8Cmin%EF%BC%8Clast-step%EF%BC%89%EF%BC%8C%E6%8C%91%E5%87%BA%E6%95%B4%E4%BD%93%E6%89%93%E5%88%86%E6%9C%80%E9%AB%98%E7%9A%84%E9%82%A3%E4%B8%AA%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%BD%9C%E4%B8%BA%E6%9C%80%E7%BB%88%E5%9B%9E%E7%AD%94"><span class="nav-number">4.4.2.1.</span> <span class="nav-text">这个搜索方法比较简单直白，也常被用做baseline进行比较。它是说，让generator针对问题，生成N个完整回答（此时当然包含中间步骤），再用训练好的verifier对这N个回答的中间步骤都进行整体的打分（回忆上面提到的，三种进行整体打分的方式：prod，min，last step），挑出整体打分最高的那个回答，作为最终回答</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-beam-search"><span class="nav-number">4.4.3.</span> <span class="nav-text">2.3.2. beam search</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E4%B8%AA%E6%96%B9%E6%B3%95%E6%9C%89%E7%82%B9%E8%B5%B0%E4%B8%80%E6%AD%A5%E7%9C%8B%E4%B8%80%E6%AD%A5%E7%9A%84%E6%84%8F%E6%80%9D%E3%80%82%E9%92%88%E5%AF%B9%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%EF%BC%8C%E6%88%91%E4%BB%AC%E5%85%88%E5%B9%B6%E8%A1%8C%E5%9C%B0%E9%87%87%E6%A0%B7N%E4%B8%AAstep1%EF%BC%8C%E5%86%8D%E7%94%A8verifier%E5%AF%B9%E8%BF%99N%E4%B8%AA%E6%AD%A5%E9%AA%A4%E8%BF%9B%E8%A1%8C%E6%89%93%E5%88%86%EF%BC%8C%E4%BB%8E%E4%B8%AD%E6%8C%91%E5%87%BA%E5%BE%97%E5%88%86%E5%89%8DM%E9%AB%98%E7%9A%84step1%EF%BC%8C%E7%BB%A7%E7%BB%AD%E5%BE%80%E4%B8%8B%E8%B5%B0%E3%80%82%E6%AF%8F%E4%B8%AA%E9%80%89%E5%87%BA%E6%9D%A5%E7%9A%84step1%EF%BC%8C%E9%83%BD%E4%BC%9A%E5%86%8D%E9%87%87%E6%A0%B7%E5%87%BAN%E4%B8%AAstep2%EF%BC%8C%E5%90%8C%E6%A0%B7%E7%9A%84%EF%BC%8C%E6%89%93%E5%AE%8C%E5%88%86%E4%B9%8B%E5%90%8E%E6%8C%91%E5%87%BA%E5%85%B6%E4%B8%AD%E5%BE%97%E5%88%86%E5%89%8DM%E9%AB%98%E7%9A%84step2%EF%BC%8C%E7%BB%A7%E7%BB%AD%E5%BE%80%E4%B8%8B%E8%B5%B0%EF%BC%8C%E7%9B%B4%E8%87%B3%E8%BE%BE%E5%88%B0%E6%8C%87%E5%AE%9A%E7%9A%84%E5%81%9C%E6%AD%A2%E8%A6%81%E6%B1%82%EF%BC%88%E6%AF%94%E5%A6%82%EF%BC%8C%E8%BE%BE%E5%88%B0%E6%8C%87%E5%AE%9A%E7%9A%84%E6%90%9C%E7%B4%A2%E6%B7%B1%E5%BA%A6%E4%BA%86%EF%BC%89%EF%BC%88%E5%88%AB%E5%BF%98%E4%BA%86%EF%BC%8C%E8%AF%84%E4%BC%B0%E7%AC%ACi%E4%B8%AA%E6%AD%A5%E9%AA%A4%E7%9A%84%E5%BE%97%E5%88%86%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%98%AF%E8%A6%81%E6%8A%8A%E9%97%AE%E9%A2%98%E5%92%8C%E5%89%8Di%E4%B8%AA%E6%AD%A5%E9%AA%A4%E9%83%BD%E8%BE%93%E5%85%A5%E7%BB%99verifier%E7%9A%84%EF%BC%89"><span class="nav-number">4.4.3.1.</span> <span class="nav-text">这个方法有点走一步看一步的意思。针对一个问题，我们先并行地采样N个step1，再用verifier对这N个步骤进行打分，从中挑出得分前M高的step1，继续往下走。每个选出来的step1，都会再采样出N个step2，同样的，打完分之后挑出其中得分前M高的step2，继续往下走，直至达到指定的停止要求（比如，达到指定的搜索深度了）（别忘了，评估第i个步骤的得分的时候，是要把问题和前i个步骤都输入给verifier的）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-lookahead-search"><span class="nav-number">4.4.4.</span> <span class="nav-text">2.3.3. lookahead search</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E4%B8%AA%E5%85%B6%E5%AE%9E%E5%92%8C%E4%B8%8A%E9%9D%A2%E7%9A%84beam-search%E7%9A%84%E6%80%9D%E6%83%B3%E6%98%AF%E7%B1%BB%E4%BC%BC%E7%9A%84%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%EF%BC%8C%E5%AE%83%E6%98%AFbeam-search%E7%9A%84%E6%8E%A8%E5%B9%BF%EF%BC%8Cbeam-search%E6%98%AFlookahead-search%E7%9A%84%E7%89%B9%E4%BE%8B%E3%80%82%E6%88%91%E4%BB%AC%E5%AF%B9%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%EF%BC%8C%E5%90%8C%E6%A0%B7%E5%85%88%E5%B9%B6%E8%A1%8C%E9%87%87%E6%A0%B7N%E4%B8%AAstep1%EF%BC%8C%E7%84%B6%E5%90%8E%E8%BF%98%E6%98%AF%E8%A6%81%E6%8C%91%E5%87%BA%E5%BE%97%E5%88%86%E5%89%8DM%E9%AB%98%E7%9A%84step1%E5%BE%80%E4%B8%8B%E8%B5%B0%EF%BC%8C%E4%B9%8B%E5%90%8E%E7%9A%84%E6%AD%A5%E9%AA%A4%E4%B9%9F%E6%98%AF%E7%B1%BB%E4%BC%BC%E7%9A%84%E3%80%82%E5%94%AF%E4%B8%80%E7%9A%84%E4%B8%8D%E5%90%8C%E6%98%AF%E5%9C%A8%E4%BA%8E%E6%89%93%E5%88%86%E4%B8%8A%E3%80%82%E6%AD%A4%E6%97%B6%E6%88%91%E4%BB%AC%E5%AF%B9%E4%B8%80%E4%B8%AAstep-i%E7%9A%84%E6%89%93%E5%88%86%EF%BC%8C%E4%B8%8D%E6%98%AF%E7%9B%B4%E6%8E%A5%E7%94%A8verifier%E7%9A%84%E8%BE%93%E5%87%BA%EF%BC%8C%E8%80%8C%E6%98%AF%EF%BC%8C%E4%BB%8E%E8%BF%99%E4%B8%AA%E5%BD%93%E5%89%8D%E6%AD%A5%E9%AA%A4i%E5%87%BA%E5%8F%91%EF%BC%8C%E5%BE%80%E5%90%8E%E8%B5%B0K%E6%AD%A5%EF%BC%8C%E7%84%B6%E5%90%8E%E6%8A%8A%E8%BF%99%E8%B5%B0%E4%BA%86K%E6%AD%A5%E4%B9%8B%E5%90%8E%E7%9A%84%E6%AD%A5%E9%AA%A4%E6%8B%BF%E5%8E%BB%E7%BB%99verifier%E6%89%93%E5%88%86%EF%BC%8C%E5%B9%B6%E6%8A%8A%E8%BF%99%E4%B8%AA%E5%BE%97%E5%88%86-score-i-k-%EF%BC%8C%E8%A7%86%E4%B8%BA%E5%BD%93%E5%89%8D%E6%AD%A5%E9%AA%A4i%E7%9A%84%E5%BE%97%E5%88%86%EF%BC%88%E8%BF%99%E4%B8%AAlookahead-search%E5%92%8Cbeam-search%E7%9A%84%E5%85%B3%E7%B3%BB%EF%BC%8C%E5%B0%B1%E7%B1%BB%E4%BC%BC%E4%BA%8Erl%E4%B8%AD%E7%9A%84TD-n-%E6%96%B9%E6%B3%95%E5%92%8CTD-0-%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%EF%BC%89"><span class="nav-number">4.4.4.1.</span> <span class="nav-text">这个其实和上面的beam search的思想是类似的，或者说，它是beam search的推广，beam search是lookahead search的特例。我们对一个问题，同样先并行采样N个step1，然后还是要挑出得分前M高的step1往下走，之后的步骤也是类似的。唯一的不同是在于打分上。此时我们对一个step i的打分，不是直接用verifier的输出，而是，从这个当前步骤i出发，往后走K步，然后把这走了K步之后的步骤拿去给verifier打分，并把这个得分$score_{i+k}$，视为当前步骤i的得分（这个lookahead search和beam search的关系，就类似于rl中的TD(n)方法和TD(0)之间的关系）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%9C%80%E4%BD%B3%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95"><span class="nav-number">4.5.</span> <span class="nav-text">2.4. 如何选择最佳搜索方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8A%E9%9D%A2%E4%BB%8B%E7%BB%8D%E4%BA%86%E4%B8%89%E7%A7%8D%E8%BE%85%E5%8A%A9%E6%8E%A8%E7%90%86%E7%9A%84%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95%E3%80%82%E8%87%AA%E7%84%B6%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BC%9A%E6%83%B3%EF%BC%8C%E6%9C%89%E6%B2%A1%E6%9C%89%E4%BB%80%E4%B9%88%E6%96%B9%E6%B3%95%E8%83%BD%E6%8C%91%E5%87%BA%E5%BD%93%E5%89%8D%E6%9C%80%E9%80%82%E5%90%88%E3%80%81%E6%88%96%E8%80%85%E8%AF%B4%E6%9C%80%E4%BC%98%E7%9A%84%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95%EF%BC%9F%E8%BF%99%E9%87%8C%E8%A6%81%E8%80%83%E8%99%91%E4%B8%80%E4%BA%9B%E6%9D%A1%E4%BB%B6%EF%BC%8C%E6%AF%94%E5%A6%82%EF%BC%9A%E6%90%9C%E7%B4%A2%E9%A2%84%E7%AE%97%EF%BC%88generation-budget%EF%BC%89%EF%BC%9A%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%B9%B6%E8%A1%8C%E9%87%87%E6%A0%B7%E6%95%B0%E9%87%8FN%E3%80%82%E5%9B%A0%E4%B8%BA%E7%AE%97%E5%8A%9B%E6%98%AF%E6%9C%89%E9%99%90%E7%9A%84%E9%97%AE%E9%A2%98%E9%9A%BE%E5%BA%A6%EF%BC%88difficulty%EF%BC%89%EF%BC%9A%E4%B9%8B%E6%89%80%E4%BB%A5%E8%80%83%E8%99%91%E8%BF%99%E4%B8%AA%E5%9B%A0%E7%B4%A0%EF%BC%8C%E6%98%AF%E5%9B%A0%E4%B8%BA%EF%BC%8C%E7%AE%80%E5%8D%95%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E5%8F%AF%E8%83%BD%E5%85%B6%E5%AE%9E%E4%B8%8D%E9%9C%80%E8%A6%81%E6%90%9C%E7%B4%A2%E7%9A%84%E8%BE%85%E5%8A%A9%EF%BC%8Cllm%E4%B9%9F%E8%83%BD%E5%9B%9E%E7%AD%94%E5%BE%97%E5%BE%88%E5%A5%BD%EF%BC%8C%E6%AD%A4%E6%97%B6%E5%86%8D%E5%8E%BB%E6%90%9C%E7%B4%A2%EF%BC%8C%E4%B8%94%E4%B8%8D%E8%AF%B4%E6%98%AF%E5%90%A6%E5%BD%B1%E5%93%8D%E7%BB%93%E6%9E%9C%EF%BC%8C%E5%B0%B1%E7%AE%97%E4%B8%8D%E5%BD%B1%E5%93%8D%EF%BC%8C%E5%8F%AF%E8%83%BD%E4%B9%9F%E6%98%AF%E7%99%BD%E8%B4%B9%E7%AE%97%E5%8A%9B%EF%BC%9B%E8%80%8C%E5%AF%B9%E4%BA%8E%E9%9A%BE%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E5%8F%AF%E8%83%BD%E5%B0%B1%E5%BE%88%E9%9C%80%E8%A6%81%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95%EF%BC%8C%E8%80%8C%E4%B8%94%E9%9C%80%E8%A6%81%E7%B2%BE%E6%8C%91%E7%BB%86%E9%80%89%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95"><span class="nav-number">4.5.1.</span> <span class="nav-text">上面介绍了三种辅助推理的搜索方法。自然，我们会想，有没有什么方法能挑出当前最适合、或者说最优的搜索方法？这里要考虑一些条件，比如：搜索预算（generation budget）：也就是并行采样数量N。因为算力是有限的问题难度（difficulty）：之所以考虑这个因素，是因为，简单的问题，可能其实不需要搜索的辅助，llm也能回答得很好，此时再去搜索，且不说是否影响结果，就算不影响，可能也是白费算力；而对于难的问题，可能就很需要搜索方法，而且需要精挑细选搜索方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%99%E4%B8%AA%E9%97%AE%E9%A2%98%E7%A1%AE%E5%AE%9E%E5%BE%88%E5%80%BC%E5%BE%97%E6%8E%A2%E7%B4%A2%E3%80%82%E4%BA%8B%E5%AE%9E%E4%B8%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E8%BF%98%E5%8F%AF%E8%83%BD%E5%8F%AF%E4%BB%A5%E6%83%B3%E5%88%B0%EF%BC%8C%E9%80%89%E5%AE%9A%E4%B8%80%E7%A7%8D%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E4%BB%A5%E5%90%8E%EF%BC%8C%E5%AE%83%E5%AF%B9%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E6%8F%90%E9%AB%98%E8%83%BD%E6%9C%89%E5%A4%9A%E5%A4%A7%EF%BC%9F%E9%9A%8F%E7%9D%80%E6%88%91%E4%BB%AC%E5%9C%A8%E6%8E%A8%E7%90%86%E6%97%B6%E7%9A%84%E6%90%9C%E7%B4%A2%E4%B8%8A%E8%8A%B1%E8%B4%B9%E7%9A%84%E7%AE%97%E5%8A%9B%E7%9A%84%E5%A2%9E%E5%8A%A0%EF%BC%8C%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E8%83%BD%E6%8F%90%E9%AB%98%E5%88%B0%E4%BB%80%E4%B9%88%E7%A8%8B%E5%BA%A6%EF%BC%9F%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E4%BC%9A%E8%BE%BE%E5%88%B0%E4%B8%8A%E9%99%90%EF%BC%9F%E8%BF%98%E6%98%AF%E8%AF%B4%E6%B2%A1%E6%9C%89%E4%B8%8A%E9%99%90%EF%BC%9F%EF%BC%88%E8%BF%99%E6%A0%B7%E5%8F%AF%E8%83%BD%E5%B0%B1%E4%BC%9A%E6%83%B3%E5%88%B0%E7%B1%BB%E4%BC%BCCan-1B-LLM-Surpass-405B-LLM-Rethinking-Compute-Optimal-Test-Time-Scaling%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E7%9A%84idea%E4%BA%86%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%E5%B0%B1%E4%BC%9A%E5%92%8C%E5%AE%83%E4%B8%80%E6%A0%B7%EF%BC%8C%E5%8E%BB%E6%8E%A2%E8%AE%A8%E7%B1%BB%E4%BC%BC%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%89"><span class="nav-number">4.5.2.</span> <span class="nav-text">这个问题确实很值得探索。事实上，我们还可能可以想到，选定一种搜索算法以后，它对推理能力的提高能有多大？随着我们在推理时的搜索上花费的算力的增加，推理能力能提高到什么程度？什么时候会达到上限？还是说没有上限？（这样可能就会想到类似Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling这篇论文的idea了，或者说就会和它一样，去探讨类似的问题）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%B4%E5%9B%9E%E6%9D%A5%EF%BC%8C%E8%BF%99%E9%87%8C%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E5%8F%82%E8%80%83%E5%8E%9F%E8%AE%BA%E6%96%87%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E3%80%82%E4%B8%8D%E5%A6%A8%E5%85%88%E6%91%86%E4%B8%8A%E7%BB%93%E8%AE%BA%EF%BC%9A"><span class="nav-number">4.5.3.</span> <span class="nav-text">说回来，这里可以直接参考原论文的实验结果。不妨先摆上结论：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E6%95%88%E6%9E%9C%E6%98%AF%E5%8F%97%E5%88%B0%E6%90%9C%E7%B4%A2%E9%A2%84%E7%AE%97%E5%92%8C%E9%97%AE%E9%A2%98%E9%9A%BE%E5%BA%A6%E7%9A%84%E5%BD%B1%E5%93%8D%E7%9A%84%EF%BC%88%E8%BF%99%E6%98%AF%E6%AF%94%E8%BE%83%E6%98%BE%E7%84%B6%E7%9A%84%EF%BC%89"><span class="nav-number">4.5.3.1.</span> <span class="nav-text">搜索效果是受到搜索预算和问题难度的影响的（这是比较显然的）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BD%93%E6%90%9C%E7%B4%A2%E9%A2%84%E7%AE%97%E8%BE%83%E5%B0%8F%EF%BC%8C%E9%97%AE%E9%A2%98%E8%BE%83%E9%9A%BE%E6%97%B6%EF%BC%8C%E6%9B%B4%E9%80%82%E5%90%88beam-search%EF%BC%8C%E4%BD%86%E8%A6%81%E6%B3%A8%E6%84%8F%E8%B6%85%E5%8F%82%E7%9A%84%E8%B0%83%E8%8A%82"><span class="nav-number">4.5.3.2.</span> <span class="nav-text">当搜索预算较小，问题较难时，更适合beam search，但要注意超参的调节</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BD%93%E6%90%9C%E7%B4%A2%E9%A2%84%E7%AE%97%E8%BE%83%E5%A4%A7%EF%BC%8C%E9%97%AE%E9%A2%98%E8%BE%83%E7%AE%80%E5%8D%95%E6%97%B6%EF%BC%8C%E6%9B%B4%E9%80%82%E5%90%88best-of-N%E6%96%B9%E6%B3%95"><span class="nav-number">4.5.3.3.</span> <span class="nav-text">当搜索预算较大，问题较简单时，更适合best-of-N方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BD%93PRM%E8%AE%AD%E7%BB%83%E5%BE%97%E8%B6%B3%E5%A4%9F%E5%A5%BD%E6%97%B6%EF%BC%8C%E8%BE%83%E5%A4%8D%E6%9D%82%E7%9A%84%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95%EF%BC%88%E6%AF%94%E5%A6%82lookahead-search%EF%BC%89%E7%9A%84%E8%A1%A8%E7%8E%B0%E5%8F%AF%E8%83%BD%E5%B9%B6%E4%B8%8D%E4%BC%9A%E7%89%B9%E5%88%AB%E5%A5%BD"><span class="nav-number">4.5.3.4.</span> <span class="nav-text">当PRM训练得足够好时，较复杂的搜索方法（比如lookahead search）的表现可能并不会特别好</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BD%93%E9%97%AE%E9%A2%98%E7%89%B9%E5%88%AB%E9%9A%BE%E6%97%B6%EF%BC%8Ctest-time-scaling-law%E5%8F%AF%E8%83%BD%E6%95%88%E6%9E%9C%E6%9C%89%E9%99%90%E3%80%82%E6%AD%A4%E6%97%B6%E6%9B%B4%E6%9C%89%E6%95%88%E7%9A%84%E5%81%9A%E6%B3%95%E5%8F%AF%E8%83%BD%E8%BF%98%E6%98%AF%E5%9B%9E%E5%88%B0pretrain%E9%98%B6%E6%AE%B5%EF%BC%8C%E4%BB%8E%E8%BF%99%E9%87%8C%E5%85%A5%E6%89%8B%EF%BC%8C%E9%80%9A%E8%BF%87%E6%95%B0%E6%8D%AE%E8%B0%83%E9%85%8D%E3%80%81pretrain-scaling%E7%AD%89%E6%96%B9%E6%B3%95%EF%BC%8C%E7%BB%99%E6%A8%A1%E5%9E%8B%E6%B3%A8%E5%85%A5%E6%9B%B4%E5%A4%9A%E7%9F%A5%E8%AF%86%EF%BC%8C%E8%BF%99%E6%A0%B7%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%9B%B4%E6%9C%89%E6%95%88"><span class="nav-number">4.5.3.5.</span> <span class="nav-text">当问题特别难时，test-time scaling law可能效果有限。此时更有效的做法可能还是回到pretrain阶段，从这里入手，通过数据调配、pretrain scaling等方法，给模型注入更多知识，这样可能会更有效</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%89%E3%80%81"><span class="nav-number">5.</span> <span class="nav-text">三、</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Pretrain%E8%BF%98%E6%98%AFInference%EF%BC%9F"><span class="nav-number">6.</span> <span class="nav-text">四、Pretrain还是Inference？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">7.</span> <span class="nav-text"></span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">134</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
