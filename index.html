<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blueeemouse.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="bluemouse&#39;s blog">
<meta property="og:url" content="https://blueeemouse.github.io/index.html">
<meta property="og:site_name" content="bluemouse&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="bluemouse">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blueeemouse.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>bluemouse's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bluemouse's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/efficiency/Ada-KV%EF%BC%9AOptimizing%20KV%20Cache%20Eviction%20by%20Adaptive%20Budget%20Allocation%20%20for%20Efficient%20LLM%20Inference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/efficiency/Ada-KV%EF%BC%9AOptimizing%20KV%20Cache%20Eviction%20by%20Adaptive%20Budget%20Allocation%20%20for%20Efficient%20LLM%20Inference/" class="post-title-link" itemprop="url">Ada-KV：Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-09-23 11:18:00" itemprop="dateCreated datePublished" datetime="2025-09-23T11:18:00+08:00">2025-09-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-09-24 11:18:36" itemprop="dateModified" datetime="2025-09-24T11:18:36+08:00">2025-09-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/efficiency/" itemprop="url" rel="index"><span itemprop="name">efficiency</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/efficiency/kv-cache/" itemprop="url" rel="index"><span itemprop="name">kv-cache</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="venuenips-25">Venue：NIPS 25</h1>
<h1 id="date2025-01-26">date：2025-01-26</h1>
<h1 id="背景">背景：</h1>
<h2 id="得先简单介绍一下kv-cache自回归模型生成下一个token的时候需要利用当前的最后一个token所聚合到的信息变换得到一个概率分布从中采样得到下一个token而聚合信息会用到注意力机制注意力机制就会要求token之间的q和k进行交互计算从而得到token之间的关系attention最后和v一起进行加权求和-假设现有的prompt对应k个token的序列我们要生成token那么聚合之后根据第k个token的信息因为它和前面的所有token都有进行聚合所以是前面信息的总体的表征得到第k1个token也就是回答里的第一个token的概率分布之后要继续生成直到生成了eos为止那生成第k2个token的时候需要让第k1个token计算出qkv然后和前k个token的k进行交互得到attn完了利用前k个token加上第k1个token自己的v进行加权求和得到聚合后的信息此时我们发现生成第k1个token的时候用到了前k个token的k和v生成第k2个token的时候用到了前k1个token的k和v依次类推可以发现我们生成第i1个token的时候应该是要用到前i个token的k和v的因此如果我们把中间生成的token的k和v存下来后续就能一直复用不需要每一次都算显然速度上会更快但因为要存k和v所以这是一种用空间换时间的操作而且还有一个问题是随着我们生成的序列不断变长要存的k和v也越来越多甚至于这可能反而会成为显存开销里的大头因此有必要对存储的kv进行一些压缩剔除之类的操作否则开销无法承担">得先简单介绍一下kv
cache。自回归模型，生成下一个token的时候，需要利用当前的最后一个token所聚合到的信息，变换得到一个概率分布，从中采样得到下一个token。而聚合信息会用到注意力机制。注意力机制就会要求token之间的q和k进行交互计算，从而得到token之间的关系（attention），最后和v一起进行加权求和<br>假设现有的prompt对应k个token的序列。我们要生成token。那么，聚合之后，根据第k个token的信息（因为它和前面的所有token都有进行聚合，所以是前面信息的总体的表征），得到第k+1个token（也就是回答里的第一个token）的概率分布。之后要继续生成，直到生成了<code>&lt;eos&gt;</code>为止。那生成第k+2个token的时候，需要让第k+1个token计算出q，k，v，然后和前k个token的k进行交互，得到attn，完了利用前k个token加上第k+1个token自己的v，进行加权求和，得到聚合后的信息。此时，我们发现，生成第k+1个token的时候，用到了前k个token的k和v；生成第k+2个token的时候，用到了前k+1个token的k和v。依次类推，可以发现，我们生成第i+1个token的时候，应该是要用到前i个token的k和v的。因此，如果我们把中间生成的token的k和v存下来，后续就能一直复用，不需要每一次都算。显然，速度上会更快，但因为要存k和v，所以这是一种用空间换时间的操作。而且还有一个问题是，随着我们生成的序列不断变长，要存的k和v也越来越多。甚至于，这可能反而会成为显存开销里的大头。因此，有必要对存储的kv进行一些压缩、剔除之类的操作，否则开销无法承担</h2>
<h3 id="另由上面的描述我们可以发现之所以没有所谓q-cache是因为往后生成的时候我们用不到前面的token的q没有复用的必要所以不需要存">（另，由上面的描述，我们可以发现，之所以没有所谓q-cache，是因为往后生成的时候，我们用不到前面的token的q，没有复用的必要，所以不需要存）</h3>
<h2 id="关于prefill和decode">关于prefill和decode：</h2>
<h3 id="这是自回归模型推理的时候需要先后经历的两个阶段-首先是prefill阶段它相当于是处理好输入的prompt具体来说是计算并存储prompt-token中的kv便于后续生成回答-第二步是decode阶段它就是基于prefill处理好的prompt-kv-cache进行自回归式的生成">这是自回归模型推理的时候需要先后经历的两个阶段<br>首先是prefill阶段。它相当于是处理好输入的prompt（具体来说是计算并存储prompt
token中的kv），便于后续生成回答；<br>第二步是decode阶段，它就是基于prefill处理好的prompt
kv cache，进行自回归式的生成</h3>
<h3 id="prefill已经讲清楚了而对于decode阶段稍微详细一点就是它是先用prefill阶段的kv生成第一个token然后把这生成的第一个token拼接到token序列里并存储其kv也就是追加到已有的kv-cache中再生成第二个token以此类推当然这里讲的是最朴素的做法所有token的kv都直接存没有什么kv-cache-compression的操作">prefill已经讲清楚了。而对于decode阶段，稍微详细一点，就是，它是先用prefill阶段的kv，生成第一个token；然后把这生成的第一个token拼接到token序列里，并存储其kv（也就是追加到已有的kv
cache中），再生成第二个token，以此类推（当然，这里讲的是最朴素的做法，所有token的kv都直接存，没有什么kv
cache compression的操作）</h3>
<h3 id="如果分析一下两个阶段的一些性质可以发现一些不同这也是后续的一些研究的出发点prefill阶段会一次性处理所有的输入prompt-token所以它的计算量是om2假设输入prompt-token个数为m-而decode阶段它的计算量则是on这表示一次decode的计算复杂度也就是生成一个token的计算复杂度其中我们记当前时刻的历史序列长度为n可以想见decode需要进行多次因为我们一般都要生成多个token每次生成token之后下一次的计算量就会更大因为历史序列更长了此处我们仅考虑最朴素的情况没有什么token-eviction的操作那么假设prompt-token数量为m一共我们要生成n个token则总的decode计算量应该是ontimes-mfracnn-12当生成的token数量很多的时候主导项就应该是ofracnn-12也就是此时的计算量是平方级的了如果prompt很长则总的计算量近似为onm">如果分析一下两个阶段的一些性质，可以发现一些不同。这也是后续的一些研究的出发点：prefill阶段会一次性处理所有的输入prompt
token，所以它的计算量是<span class="math inline">\(O(m^{2})\)</span>（假设输入prompt
token个数为m）；<br>而decode阶段，它的计算量则是<span class="math inline">\(O(n)\)</span>（这表示一次decode的计算复杂度。也就是生成一个token的计算复杂度。其中，我们记当前时刻的历史序列长度为n）。可以想见，decode需要进行多次（因为我们一般都要生成多个token）。每次生成token之后，下一次的计算量就会更大（因为历史序列更长了。此处我们仅考虑最朴素的情况，没有什么token
eviction的操作）。那么，假设prompt
token数量为m，一共我们要生成n个token，则总的decode计算量应该是<span class="math inline">\(O(n\times
m+\frac{n(n-1)}{2})\)</span>。当生成的token数量很多的时候，主导项就应该是<span class="math inline">\(O(\frac{n(n-1)}{2})\)</span>，也就是此时的计算量是平方级的了；如果prompt很长，则总的计算量近似为<span class="math inline">\(O(nm)\)</span></h3>
<h2 id="关于论文中提到的识别key-token的window-size">关于论文中提到的，识别key
token的window size：</h2>
<h3 id="现有方法大部分都是top-k-eviction类的方法它们的思想是保留历史序列里的top-k重要的token而怎么挑选呢最朴素的方法是说我们看看当前的最后一个token它和之前的各个token之间的注意力大小挑出前k大的这些就是关键token它们的kv就是要cache起来的-但这样的问题在于我们仅依赖于一个token来观察可能有偶然性为此如果我们能看多个token比如看最后32个token和前面的token之间的注意力然后通过一些方式汇总到一起例如池化均值等再来看看哪些token是重要的这样的方式就会更加稳定一些而我们用多少个token来观察这个token的数量就是所谓window-size">现有方法大部分都是top-k
eviction类的方法。它们的思想是，保留历史序列里的top-k重要的token。而怎么挑选呢？最朴素的方法是说，我们看看当前的最后一个token，它和之前的各个token之间的注意力大小，挑出前k大的。这些就是关键token，它们的kv就是要cache起来的<br>但这样的问题在于，我们仅依赖于一个token来观察，可能有偶然性。为此，如果我们能看多个token，比如看最后32个token和前面的token之间的注意力，然后通过一些方式汇总到一起（例如池化、均值等），再来看看哪些token是重要的，这样的方式就会更加稳定一些。而我们用多少个token来观察，这个token的数量就是所谓window
size</h3>
<h1 id="动机">动机：</h1>
<h2 id="section"></h2>
<h1 id="insight">insight：</h1>
<h1 id="contribution">contribution：</h1>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/09/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/efficiency/Ada-KV%EF%BC%9AOptimizing%20KV%20Cache%20Eviction%20by%20Adaptive%20Budget%20Allocation%20%20for%20Efficient%20LLM%20Inference/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/rl-based/VRAG-RL%EF%BC%9AEmpower%20Vision-Perception-Based%20RAG%20for%20Visually%20Rich%20Information%20Understanding%20via%20Iterative%20Reasoning%20with%20Reinforcement%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/rl-based/VRAG-RL%EF%BC%9AEmpower%20Vision-Perception-Based%20RAG%20for%20Visually%20Rich%20Information%20Understanding%20via%20Iterative%20Reasoning%20with%20Reinforcement%20Learning/" class="post-title-link" itemprop="url">VRAG-RL：Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-09-20 11:28:00 / Modified: 11:30:53" itemprop="dateCreated datePublished" datetime="2025-09-20T11:28:00+08:00">2025-09-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-RAG/" itemprop="url" rel="index"><span itemprop="name">multi-modal RAG</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="venuearxiv">Venue：arxiv</h1>
<h1 id="date2025-06-03">date：2025-06-03</h1>
<h1 id="动机">动机：</h1>
<h2 id="section"></h2>
<h1 id="insight">insight：</h1>
<h1 id="contribution">contribution：</h1>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/rl-based/VRAG-RL%EF%BC%9AEmpower%20Vision-Perception-Based%20RAG%20for%20Visually%20Rich%20Information%20Understanding%20via%20Iterative%20Reasoning%20with%20Reinforcement%20Learning/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4%E5%A4%9A%E6%A8%A1%E6%80%81RAG%20paper%E8%B0%83%E7%A0%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/%E8%BF%91%E5%B9%B4%E5%A4%9A%E6%A8%A1%E6%80%81RAG%20paper%E8%B0%83%E7%A0%94/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-09-11 21:20:10 / Modified: 21:35:05" itemprop="dateCreated datePublished" datetime="2025-09-11T21:20:10+08:00">2025-09-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="wavrag-audio-integrated-retrieval-augmented-generation-for-spoken-dialogue-models多模态的不过是audio相关的"><strong>WavRAG:
Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue
Models</strong>（多模态的，不过是audio相关的）</h3>
<h3 id="real-mm-rag-a-real-world-multi-modal-retrieval-benchmark多模态的但是benchmark"><strong>REAL-MM-RAG:
A Real-World Multi-Modal Retrieval
Benchmark</strong>（多模态的，但是benchmark）</h3>
<h3 id="videorag-retrieval-augmented-generation-over-video-corpus多模态的video相关"><strong>VideoRAG:
Retrieval-Augmented Generation over Video
Corpus</strong>（多模态的）（video相关）</h3>
<h3 id="mes-rag-bringing-multi-modal-entity-storage-and-secure-enhancements-to-rag多模态的"><strong>MES-RAG:
Bringing Multi-modal, Entity-Storage, and Secure Enhancements to
RAG</strong>（多模态的）</h3>
<h3 id="unirag-universal-retrieval-augmentation-for-large-vision-language-models多模态的关于vlm"><strong>UniRAG:
Universal Retrieval Augmentation for Large Vision Language
Models</strong>（多模态的，关于VLM）</h3>
<h3 id="rule-reliable-multimodal-rag-for-factuality-in-medical-vision-language-models多模态的但是医学相关的"><strong>RULE:
Reliable Multimodal RAG for Factuality in Medical Vision Language
Models</strong>（多模态的，但是医学相关的）</h3>
<h2 id="poisonedeye-knowledge-poisoning-attack-on-retrieval-augmented-generation-based-large-vision-language-models多模态的关于vlm"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46373">PoisonedEye: Knowledge
Poisoning Attack on Retrieval-Augmented Generation based Large
Vision-Language Models</a>（多模态的）（关于VLM）</h2>
<h2 id="she-streaming-media-hashing-retrieval疑似多模态的"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45670">SHE: Streaming-media
Hashing Retrieval</a>（疑似多模态的）</h2>
<h2 id="qure-query-relevant-retrieval-through-hard-negative-sampling-in-composed-image-retrieval多模态的composed-image-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43476">QuRe: Query-Relevant
Retrieval through Hard Negative Sampling in Composed Image
Retrieval</a>（多模态的，composed Image retrieval）</h2>
<h2 id="learning-attribute-aware-hash-codes-for-fine-grained-image-retrieval-via-query-optimization多模态的又是image-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43868">Learning
Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query
Optimization</a>（多模态的）（又是Image retrieval）</h2>
<h2 id="visual-abstraction-a-plug-and-play-approach-for-text-visual-retrieval多模态的text-visual-retrieval"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46021">Visual Abstraction: A
Plug-and-Play Approach for Text-Visual
Retrieval</a>（多模态的）（text-visual retrieval）</h2>
<h2 id="docks-rag-optimizing-document-level-relation-extraction-through-llm-enhanced-hybrid-prompt-tuning多模态的文档类"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45220">DocKS-RAG: Optimizing
Document-Level Relation Extraction through LLM-Enhanced Hybrid Prompt
Tuning</a>（多模态的，文档类）</h2>
<h2 id="retrieval-augmented-perception-high-resolution-image-perception-meets-visual-rag多模态的王中王神中神high-resolution-image-perception"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44979">Retrieval-Augmented
Perception: High-resolution Image Perception Meets Visual
RAG</a>（多模态的，王中王，神中神）（High-resolution Image
Perception）</h2>
<h2 id="realrag-retrieval-augmented-realistic-image-generation-via-self-reflective-contrastive-learning也算多模态的吧结合rag与diffusion"><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44615">RealRAG:
Retrieval-augmented Realistic Image Generation via Self-reflective
Contrastive Learning</a>（也算多模态的吧，结合RAG与diffusion）</h2>
<h2 id="colpali-efficient-document-retrieval-with-vision-language-models优先多模态的关于vlm的">-
ColPali: Efficient Document Retrieval with Vision Language
Models（优先）（多模态的）（关于VLM的）</h2>
<h2 id="visrag-vision-based-retrieval-augmented-generation-on-multi-modality-documents优先多模态的visual-rag">-
VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality
Documents（优先）（多模态的）（visual RAG）</h2>
<h2 id="ra-tta-retrieval-augmented-test-time-adaptation-for-vision-language-models多模态的关于vlm的tta">RA-TTA:
Retrieval-Augmented Test-Time Adaptation for Vision-Language
Models（多模态的）（关于VLM的TTA）</h2>
<h2 id="streaming-video-question-answering-with-in-context-video-kv-cache-retrieval多模态的video-qa">-
Streaming Video Question-Answering with In-context Video KV-Cache
Retrieval（多模态的）（video qa）</h2>
<h2 id="mai-a-multi-turn-aggregation-iteration-model-for-composed-image-retrieval多模态的composed-image-retrieval">MAI:
A Multi-turn Aggregation-Iteration Model for Composed Image
Retrieval（多模态的）（composed Image retrieval）</h2>
<h2 id="bridging-information-asymmetry-in-text-video-retrieval-a-data-centric-approach多模态的text-video-retrieval">Bridging
Information Asymmetry in Text-video Retrieval: A Data-centric
Approach（多模态的）（text-video retrieval）</h2>
<h2 id="benchmarking-multimodal-retrieval-augmented-generation-with-dynamic-vqa-dataset-and-self-adaptive-planning-agent算多模态的涉及vqa了但是是benchmark类的论文">Benchmarking
Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and
Self-adaptive Planning
Agent（算多模态的？涉及VQA了。但是是benchmark类的论文）</h2>
<h2 id="retrieval-augmented-diffusion-model-for-structure-informed-antibody-design-and-optimization算多模态的涉及diffusionwc是ai4sci的吧都有抗体了算了">Retrieval
Augmented Diffusion Model for Structure-informed Antibody Design and
Optimization（算多模态的？涉及diffusion）（wc，是ai4sci的吧，都有抗体了，算了）</h2>
<h2 id="mm-embed-universal-multimodal-retrieval-with-multimodal-llms多模态的">MM-EMBED:
Universal Multimodal Retrieval with Multimodal LLMs（多模态的）</h2>
<h2 id="tiger-unifying-text-to-image-generation-and-retrieval-with-large-multimodal-models多模态的好像是在做文生图和检索的统一工作">TIGeR:
Unifying Text-to-Image Generation and Retrieval with Large Multimodal
Models（多模态的）（好像是在做文生图和检索的统一工作）</h2>
<h2 id="mrag-bench-vision-centric-evaluation-for-retrieval-augmented-multimodal-models多模态的只不过是benchmark类的">MRAG-Bench:
Vision-Centric Evaluation for Retrieval-Augmented Multimodal
Models（多模态的，只不过是benchmark类的）</h2>
<h2 id="learning-fine-grained-representations-through-textual-token-disentanglement-in-composed-video-retrieval多模态的composed-video-retrieval">Learning
Fine-Grained Representations through Textual Token Disentanglement in
Composed Video Retrieval（多模态的）（composed video retrieval）</h2>
<h2 id="tempme-video-temporal-token-merging-for-efficient-text-video-retrieval多模态的text-video-retrieval">TempMe:
Video Temporal Token Merging for Efficient Text-Video
Retrieval（多模态的）（text-video retrieval）</h2>
<h2 id="mllm-as-retriever-interactively-learning-multimodal-retrieval-for-embodied-agents多模态的不过似乎是应用到具身方面">MLLM
as Retriever: Interactively Learning Multimodal Retrieval for Embodied
Agents（多模态的，不过似乎是应用到具身方面？）</h2>
<h2 id="generalized-video-moment-retrieval多模态的video相关的似乎具体是moment相关">Generalized
Video Moment
Retrieval（多模态的）（video相关的，似乎具体是moment相关？）</h2>
<h2 id="exploiting-distribution-constraints-for-scalable-and-efficient-image-retrieval多模态的image-retrieval">Exploiting
Distribution Constraints for Scalable and Efficient Image
Retrieval（多模态的）（Image Retrieval）</h2>
<h2 id="rapid-retrieval-augmented-training-of-differentially-private-diffusion-models算多模态的涉及diffusion-models">RAPID:
Retrieval Augmented Training of Differentially Private Diffusion
Models（算多模态的？涉及diffusion models）</h2>
<h2 id="g-retriever-retrieval-augmented-generation-for-textual-graph-understanding-and-question-answering"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95524">G-Retriever:
Retrieval-Augmented Generation for Textual Graph Understanding and
Question Answering</a></h2>
<h2 id="an-end-to-end-graph-attention-network-hashing-for-cross-modal-retrieval"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95267">An End-To-End Graph
Attention Network Hashing for Cross-Modal Retrieval</a></h2>
<h2 id="wheres-waldo-diffusion-features-for-personalized-segmentation-and-retrieval"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95609">Where's Waldo:
Diffusion Features For Personalized Segmentation and Retrieval</a></h2>
<h2 id="aligning-vision-models-with-human-aesthetics-in-retrieval-benchmarks-and-algorithmsbenchmark类啊"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/93151">Aligning Vision Models
with Human Aesthetics in Retrieval: Benchmarks and
Algorithms</a>（benchmark类啊）</h2>
<h2 id="diffusion-inspired-truncated-sampler-for-text-video-retrieval"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95072">Diffusion-Inspired
Truncated Sampler for Text-Video Retrieval</a></h2>
<h2 id="inquire-a-natural-world-text-to-image-retrieval-benchmark又是benchmark类"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97543">INQUIRE: A Natural
World Text-to-Image Retrieval Benchmark</a>（又是benchmark类）</h2>
<h2 id="bivlc-extending-vision-language-compositionality-evaluation-with-text-to-image-retrieval"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97657">BiVLC: Extending
Vision-Language Compositionality Evaluation with Text-to-Image
Retrieval</a></h2>
<h2 id="verified-a-video-corpus-moment-retrieval-benchmark-for-fine-grained-video-understanding依然benchmark"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97632">VERIFIED: A Video
Corpus Moment Retrieval Benchmark for Fine-Grained Video
Understanding</a>（依然benchmark）</h2>
<h2 id="wikido-a-new-benchmark-evaluating-cross-modal-retrieval-for-vision-language-modelsbenchmark-again"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97785">WikiDO: A New Benchmark
Evaluating Cross-Modal Retrieval for Vision-Language
Models</a>（benchmark again）</h2>
<h2 id="retrieval-fine-tuning-for-in-context-tabular-models似乎是研究表格型数据的算吗"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/96776">Retrieval &amp;
Fine-Tuning for In-Context Tabular
Models</a>（似乎是研究表格型数据的？算吗？）</h2>
<h2 id="uda-a-benchmark-suite-for-retrieval-augmented-generation-in-real-world-document-analysis研究document-analysis的"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/97735">UDA: A Benchmark Suite
for Retrieval Augmented Generation in Real-World Document
Analysis</a>（研究document analysis的）</h2>
<h2 id="semi-open-3d-object-retrieval-via-hierarchical-equilibrium-on-hypergraph有点杂啊检索3d物体"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/96281">Semi-Open 3D Object
Retrieval via Hierarchical Equilibrium on
Hypergraph</a>（有点杂啊，检索3D物体）</h2>
<h2 id="assembly-fuzzy-representation-on-hypergraph-for-open-set-3d-object-retrieval我去还有一篇吗研究3d-retrieval的"><a target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/93088">Assembly Fuzzy
Representation on Hypergraph for Open-Set 3D Object
Retrieval</a>（我去，还有一篇吗，研究3D retrieval的）</h2>
<h2 id="drvideo-document-retrieval-based-long-video-understandingvideo-understanding"><strong>DrVideo:
Document Retrieval Based Long Video Understanding</strong>（video
understanding）</h2>
<h2 id="salova-segment-augmented-long-video-assistant-for-targeted-retrieval-and-routing-in-long-form-video-analysisvideo相关应该是长视频理解"><strong><a target="_blank" rel="noopener" href="https://ivy-lvlm.github.io/SALOVA/">SALOVA: Segment-Augmented Long
Video Assistant for Targeted Retrieval and Routing in Long-Form Video
Analysis</a></strong>（video相关，应该是长视频理解）</h2>
<h2 id="vlog-video-language-models-by-generative-retrieval-of-narration-vocabularyvideo相关"><strong>VLog:
Video-Language Models by Generative Retrieval of Narration
Vocabulary</strong>（video相关）</h2>
<h2 id="collm-a-large-language-model-for-composed-image-retrievalcomposed-image-retrieval"><a target="_blank" rel="noopener" href="https://collm-cvpr25.github.io/">CoLLM: A Large Language Model for
Composed Image Retrieval</a>（Composed Image retrieval）</h2>
<h2 id="search-and-detect-training-free-long-tail-object-detection-via-web-image-retrieval有点杂物体检测用到了image-retrieval技术"><strong>Search
and Detect: Training-Free Long Tail Object Detection via Web-Image
Retrieval</strong>（有点杂，物体检测，用到了Image Retrieval技术）</h2>
<h2 id="learning-audio-guided-video-representation-with-gated-attention-for-video-text-retrievalaudio相关"><strong>Learning
Audio-guided Video Representation with Gated Attention for Video-Text
Retrieval</strong>（audio相关）</h2>
<h2 id="lamra-large-multimodal-model-as-your-advanced-retrieval-assistantvlm相关"><a target="_blank" rel="noopener" href="https://code-kunkun.github.io/LamRA/">LamRA: Large Multimodal
Model as Your Advanced Retrieval Assistant</a>（VLM相关）</h2>
<h2 id="recurrence-enhanced-vision-and-language-transformers-for-robust-multimodal-document-retrievalvlm相关以及文档相关"><a target="_blank" rel="noopener" href="https://github.com/aimagelab/ReT">Recurrence-Enhanced
Vision-and-Language Transformers for Robust Multimodal Document
Retrieval</a>（VLM相关，以及文档相关）</h2>
<h2 id="missing-target-relevant-information-prediction-with-world-model-for-accurate-zero-shot-composed-image-retrievalcomposed-image-retrieval"><strong>Missing
Target-Relevant Information Prediction with World Model for Accurate
Zero-Shot Composed Image Retrieval</strong>（composed Image
retrieval）</h2>
<h2 id="ilias-instance-level-image-retrieval-at-scaleimage-retrieval"><a target="_blank" rel="noopener" href="https://vrg.fel.cvut.cz/ilias/">ILIAS: Instance-Level Image
retrieval At Scale</a>（Image retrieval）</h2>
<h2 id="clip-is-almost-all-you-need-towards-parameter-efficient-scene-text-retrieval-without-ocrscene-text-retrieval"><strong>CLIP
is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval
without OCR</strong>（scene-text retrieval）</h2>
<h2 id="rethinking-noisy-video-text-retrieval-via-relation-aware-alignmentvideo相关video-text-retrieval只不过多了一个noisy设定这个感觉真好灌水吧"><strong>Rethinking
Noisy Video-Text Retrieval via Relation-aware
Alignment</strong>（video相关，video-text
retrieval，只不过多了一个noisy设定。这个感觉真好灌水吧……）</h2>
<h2 id="bridging-modalities-improving-universal-multimodal-retrieval-by-multimodal-large-language-modelsvlm相关"><a target="_blank" rel="noopener" href="https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct">Bridging
Modalities: Improving Universal Multimodal Retrieval by Multimodal Large
Language Models</a>（VLM相关）</h2>
<h2 id="reason-before-retrieve-one-stage-reflective-chain-of-thoughts-for-training-free-zero-shot-composed-image-retrievalcomposed-image-retrieval"><a target="_blank" rel="noopener" href="https://github.com/Pter61/osrcir">Reason-before-Retrieve:
One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot
Composed Image Retrieval</a>（Composed Image Retrieval）</h2>
<h2 id="vdocrag-retrieval-augmented-generation-over-visually-rich-documents这篇明确出现rag了啊document相关"><a target="_blank" rel="noopener" href="https://vdocrag.github.io/">VDocRAG: Retrieval-Augmented
Generation over Visually-Rich
Documents</a>（这篇明确出现RAG了啊）（document相关）</h2>
<h2 id="rap-retrieval-augmented-personalization-for-multimodal-large-language-models我去这么巧和icml-oral那篇感觉做的题材有点像啊vlm相关"><a target="_blank" rel="noopener" href="https://hoar012.github.io/RAP-Project/">RAP: Retrieval-Augmented
Personalization for Multimodal Large Language
Models</a>（我去，这么巧，和ICML
oral那篇感觉做的题材有点像啊）（VLM相关）</h2>
<h2 id="video-colbert-contextualized-late-interaction-for-text-to-video-retrievalvideo相关video-text-retrieval"><strong>Video-ColBERT:
Contextualized Late Interaction for Text-to-Video
Retrieval</strong>（video相关，video-text retrieval）</h2>
<h2 id="imagine-and-seek-improving-composed-image-retrieval-with-an-imagined-proxycomposed-image-retrieval"><strong>Imagine
and Seek: Improving Composed Image Retrieval with an Imagined
Proxy</strong>（Composed Image Retrieval）</h2>
<h2 id="the-devil-is-in-the-prompts-retrieval-augmented-prompt-optimization-for-text-to-video-generation不会是prompt-engineering吧video相关text-video-retrieval"><strong>The
Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for
Text-to-Video Generation</strong>（不会是prompt
engineering吧……）（video相关，text-video retrieval）</h2>
<h2 id="narrating-the-video-boosting-text-video-retrieval-via-comprehensive-utilization-of-frame-level-captionsvideo相关text-video-retrieval"><a target="_blank" rel="noopener" href="https://multimodal-understanding-group.github.io/NarVid/">Narrating
the Video: Boosting Text-Video Retrieval via Comprehensive Utilization
of Frame-Level Captions</a>（video相关，text-video retrieval）</h2>
<h2 id="learning-with-noisy-triplet-correspondence-for-composed-image-retrievalcomposed-image-retrieval"><a target="_blank" rel="noopener" href="https://github.com/CharlesNeilWilliams/TME">Learning with Noisy
Triplet Correspondence for Composed Image Retrieval</a>（Composed Image
Retrieval）</h2>
<h2 id="ccin-compositional-conflict-identification-and-neutralization-for-composed-image-retrievalcomposed-image-retrieval"><strong>CCIN:
Compositional Conflict Identification and Neutralization for Composed
Image Retrieval</strong>（Composed Image Retrieval）</h2>
<h2 id="generative-zero-shot-composed-image-retrievalcomposed-image-retrieval"><strong>Generative
Zero-Shot Composed Image Retrieval</strong>（Composed Image
Retrieval）</h2>
<h2 id="context-cir-learning-from-concepts-in-text-for-composed-image-retrieval好多这种composed-image-retrieval啊这个是啥啊"><strong>ConText-CIR:
Learning from Concepts in Text for Composed Image
Retrieval</strong>（好多这种composed image
retrieval啊，这个是啥啊）</h2>
<h2 id="discovla-discrepancy-reduction-in-vision-language-and-alignment-for-parameter-efficient-video-text-retrievaltext-video-retrieval"><a target="_blank" rel="noopener" href="https://github.com/LunarShen/DsicoVLA">DiscoVLA: Discrepancy
Reduction in Vision, Language, and Alignment for Parameter-Efficient
Video-Text Retrieval</a>（text-video retrieval）</h2>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/08/%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3/%E6%9C%AC%E5%9C%B0latex/%E5%85%B3%E4%BA%8E%E7%BC%96%E8%AF%91%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/08/%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3/%E6%9C%AC%E5%9C%B0latex/%E5%85%B3%E4%BA%8E%E7%BC%96%E8%AF%91%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">关于编译的一些常见问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-09-08 13:02:00 / Modified: 13:04:08" itemprop="dateCreated datePublished" datetime="2025-09-08T13:02:00+08:00">2025-09-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3/" itemprop="url" rel="index"><span itemprop="name">配置相关</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3/%E6%9C%AC%E5%9C%B0latex/" itemprop="url" rel="index"><span itemprop="name">本地latex</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="有时候编译完虽然没报错可是会发现结果和我们想象的不一样比如用beamer主题做ppt的时候模板一般会支持右下角带一个页码的显示当前页数总页数但是可能我们编译完发现总页数会出问题这时并不一定是代码的问题从操作上看我们多编译几次可能就显示正常了虽然不太清楚是为什么">有时候编译完，虽然没报错，可是会发现结果和我们想象的不一样。比如，用beamer主题做ppt的时候，模板一般会支持右下角带一个页码的显示：当前页数/总页数。但是，可能我们编译完发现总页数会出问题，这时并不一定是代码的问题。从操作上看，我们多编译几次，可能就显示正常了（虽然不太清楚是为什么）</h1>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/09/08/%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3/%E6%9C%AC%E5%9C%B0latex/%E5%85%B3%E4%BA%8E%E7%BC%96%E8%AF%91%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/RAG-transfer/RealRAG%EF%BC%9ARetrieval-augmented%20Realistic%20Image%20Generation%20via%20%20Self-reflective%20Contrastive%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/RAG-transfer/RealRAG%EF%BC%9ARetrieval-augmented%20Realistic%20Image%20Generation%20via%20%20Self-reflective%20Contrastive%20Learning/" class="post-title-link" itemprop="url">RealRAG：Retrieval-augmented Realistic Image Generation via  Self-reflective Contrastive Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-09-07 16:33:00" itemprop="dateCreated datePublished" datetime="2025-09-07T16:33:00+08:00">2025-09-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-09-20 11:29:08" itemprop="dateModified" datetime="2025-09-20T11:29:08+08:00">2025-09-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-RAG/" itemprop="url" rel="index"><span itemprop="name">multi-modal RAG</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="venueicml-25">Venue：ICML 25</h1>
<h1 id="date2025-05-12">date：2025-05-12</h1>
<h1 id="动机">动机：</h1>
<h2 id="扩散模型等文生图模型在生成一些训练时没见过的物体要求非常细的物体时可能会出现类似幻觉的现象生成出的图像非常失真因为参数知识里不包含这些内容我们也不能要求训练数据集里包含所有的知识因此论文引入rag希望检索相关图像来给模型提供知识从而让生成结果更准确">扩散模型等文生图模型，在生成一些训练时没见过的物体/要求非常细的物体时，可能会出现类似幻觉的现象/生成出的图像非常失真（因为参数知识里不包含这些内容。我们也不能要求训练数据集里包含所有的知识）。因此，论文引入RAG，希望检索相关图像来给模型提供知识，从而让生成结果更准确</h2>
<h1 id="简介">简介：</h1>
<h2 id="感觉是一篇好文啊需要说明的是尽管现有研究也有一些在做rag和diffusion结合的但是它们在检索方面依然是检索和query语义相关的内容却忽略了去检索真正需要的图像即模型参数里缺少相关知识的图像也就是检索和要求相关的图像有可能检索到一些不需要的图像比如我们想生成一个特定型号的车驾驶在草原的图像假定知识库里没有这个东西这个型号的车是模型没见过的但我们用这个prompt检索图像可能检索出一大堆其它车在草原上行驶的图像就是没检索出这个新型号的车的相关图像那这些结果对于模型而言都没有用因为它还是不知道新型号的车长什么样甚至说有可能被这些检索结果进一步带偏生成完全不是新型号的车的在草原行驶的结果">感觉是一篇好文啊……需要说明的是，尽管现有研究也有一些在做RAG和diffusion结合的，但是它们在检索方面依然是检索和query语义相关的内容，却忽略了去检索“真正需要”的图像，即模型参数里缺少相关知识的图像（也就是检索和要求相关的图像。有可能检索到一些不需要的图像。比如，我们想生成一个特定型号的车驾驶在草原的图像（假定知识库里没有这个东西），这个型号的车是模型没见过的。但我们用这个prompt检索图像，可能检索出一大堆其它车在草原上行驶的图像，就是没检索出这个新型号的车的相关图像。那这些结果对于模型而言都没有用，因为它还是不知道新型号的车长什么样。甚至说，有可能被这些检索结果进一步带偏，生成完全不是新型号的车的在草原行驶的结果）</h2>
<h2 id="因此论文提出了一个自反思对比学习专门用于训练一个类似于biased的retriever当然这个bias是我们主动造成的所以定义上和一般的bias不太一样简单来说训练后的效果就是这个检索器不会单单按照相似度的高低进行检索而是会检索那些模型缺失的也是模型真正需要的并且相关的图像仍以上面的例子来说明就是单看相似度的话应该是那些各种行驶在草原上的车的图像的相似度更高但训练后的检索器不会就这样结束而是会注意到新型号的车才是模型需要的并且也是相关的由此检索回新型号的车的图像给模型用以参考因此实现了知识的弥补具有针对性">因此论文提出了一个“自反思对比学习”，专门用于训练一个类似于biased的retriever（当然，这个bias是我们主动造成的，所以定义上和一般的bias不太一样）。简单来说，训练后的效果就是，这个检索器不会单单按照相似度的高低进行检索，而是会检索那些模型缺失的（也是模型真正需要的）、并且相关的图像（仍以上面的例子来说明，就是，单看相似度的话，应该是那些各种行驶在草原上的车的图像的相似度更高；但训练后的检索器不会就这样结束，而是会注意到新型号的车才是模型需要的，并且也是相关的，由此检索回新型号的车的图像，给模型用以参考）。因此实现了知识的弥补（具有针对性）</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/09/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/RAG-transfer/RealRAG%EF%BC%9ARetrieval-augmented%20Realistic%20Image%20Generation%20via%20%20Self-reflective%20Contrastive%20Learning/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/Video&Multimodal-Retrieval/SALOVA%EF%BC%9ASegment-Augmented%20Long%20Video%20Assistant%20%20for%20Targeted%20Retrieval%20and%20Routing%20in%20Long-Form%20Video%20Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/Video&Multimodal-Retrieval/SALOVA%EF%BC%9ASegment-Augmented%20Long%20Video%20Assistant%20%20for%20Targeted%20Retrieval%20and%20Routing%20in%20Long-Form%20Video%20Analysis/" class="post-title-link" itemprop="url">SALOVA：Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-09-07 11:46:00 / Modified: 12:32:57" itemprop="dateCreated datePublished" datetime="2025-09-07T11:46:00+08:00">2025-09-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-RAG/" itemprop="url" rel="index"><span itemprop="name">multi-modal RAG</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="venuecvpr-25">Venue：CVPR 25</h1>
<h1 id="date2025">date：2025</h1>
<h1 id="基础背景">基础背景：</h1>
<h2 id="fps是指frames-per-second也就是每秒多少帧">FPS，是指Frames Per
Second，也就是每秒多少帧</h2>
<h2 id="内存增强技术是一个与rag有点类似的技术其核心思路是在模型之外额外引入一个内存缓冲区memory-buffer用于存储长视频中超出模型固有上下文长度的帧特征语义信息它是按时间存储的且不支持语义检索也就是说现在用户给了一个query我们不能根据query的含义来检索要用buffer里的哪些特征而是只能根据定位的大概时间点把这个时间点附近的特征都拿来使用因此论文的方法的更加优秀的地方在于支持基于语义的检索">内存增强技术，是一个与RAG有点类似的技术。其核心思路是，在模型之外，额外引入一个”内存缓冲区“（Memory
Buffer），用于存储长视频中超出模型固有上下文长度的帧特征/语义信息。它是按时间存储的，且不支持语义检索（也就是说，现在用户给了一个query，我们不能根据query的含义来检索要用buffer里的哪些特征，而是只能根据定位的大概时间点，把这个时间点附近的特征都拿来使用（因此，论文的方法的更加优秀的地方在于支持基于语义的检索）</h2>
<h1 id="动机">动机：</h1>
<h2 id="论文研究的是长视频的理解这个问题重要无需多言但很难因为视频所需的上下文范围太大了现有的模型完全满足不了此外还有场景的一致性问题需要关注而现有方法在这两点上做得都不够好-基于稀疏帧采样和token压缩的显然这些方法会丢失很多信息因此对于细节的问题基本不太答得上来-基于内存增强和上下文扩展的它们不太能优化片段间的语义关系且上下文扩展带来的提升还是远远不够">论文研究的是长视频的理解。这个问题重要，无需多言；但很难，因为视频所需的上下文范围太大了，现有的模型完全满足不了；此外还有场景的一致性问题需要关注。而现有方法在这两点上做得都不够好：<br>基于稀疏帧采样和token压缩的，显然这些方法会丢失很多信息，因此对于细节的问题基本不太答得上来；<br>基于内存增强和上下文扩展的，它们不太能优化片段间的语义关系；且上下文扩展带来的提升还是远远不够</h2>
<h2 id="此外现有的视频数据集质量也不够高一个是描述粒度粗另一个是视频大多短无法支撑长视频模型的训练">此外，现有的视频数据集质量也不够高。一个是描述粒度粗，另一个是视频大多短，无法支撑长视频模型的训练</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/09/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/Video&Multimodal-Retrieval/SALOVA%EF%BC%9ASegment-Augmented%20Long%20Video%20Assistant%20%20for%20Targeted%20Retrieval%20and%20Routing%20in%20Long-Form%20Video%20Analysis/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/RAG-transfer/RAP%EF%BC%9ARetrieval-Augmented%20Personalization%20for%20Multimodal%20Large%20Language%20Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/RAG-transfer/RAP%EF%BC%9ARetrieval-Augmented%20Personalization%20for%20Multimodal%20Large%20Language%20Models/" class="post-title-link" itemprop="url">RAP：Retrieval-Augmented Personalization for Multimodal Large Language Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-09-07 10:23:00 / Modified: 17:25:05" itemprop="dateCreated datePublished" datetime="2025-09-07T10:23:00+08:00">2025-09-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-RAG/" itemprop="url" rel="index"><span itemprop="name">multi-modal RAG</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Venue：CVPR-25"><a href="#Venue：CVPR-25" class="headerlink" title="Venue：CVPR 25"></a>Venue：CVPR 25</h1><h1 id="date：2025"><a href="#date：2025" class="headerlink" title="date：2025"></a>date：2025</h1><h1 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h1><h2 id="论文讨论了两个问题：因为长期记忆做得不够好，导致无法记忆与理解用户专属的概念；进行用户个性化适配的难度很高（需要大量数据，难以收集，且微调也很麻烦，用户基本不太能在本地直接进行微调）"><a href="#论文讨论了两个问题：因为长期记忆做得不够好，导致无法记忆与理解用户专属的概念；进行用户个性化适配的难度很高（需要大量数据，难以收集，且微调也很麻烦，用户基本不太能在本地直接进行微调）" class="headerlink" title="论文讨论了两个问题：因为长期记忆做得不够好，导致无法记忆与理解用户专属的概念；进行用户个性化适配的难度很高（需要大量数据，难以收集，且微调也很麻烦，用户基本不太能在本地直接进行微调）"></a>论文讨论了两个问题：因为长期记忆做得不够好，导致无法记忆与理解用户专属的概念；<br>进行用户个性化适配的难度很高（需要大量数据，难以收集，且微调也很麻烦，用户基本不太能在本地直接进行微调）</h2><h1 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h1><h2 id="针对不方便进行个性化适配的问题，论文选择通过把用户相关个性知识存储到外部知识库，而不是让用户收集一批数据自行微调，来轻便地让模型学到用户相关的知识（这样，如果用户需要进行一些个性化操作，需要调整模型的知识，只需要对知识库进行编辑（例如增删改）即可）"><a href="#针对不方便进行个性化适配的问题，论文选择通过把用户相关个性知识存储到外部知识库，而不是让用户收集一批数据自行微调，来轻便地让模型学到用户相关的知识（这样，如果用户需要进行一些个性化操作，需要调整模型的知识，只需要对知识库进行编辑（例如增删改）即可）" class="headerlink" title="针对不方便进行个性化适配的问题，论文选择通过把用户相关个性知识存储到外部知识库，而不是让用户收集一批数据自行微调，来轻便地让模型学到用户相关的知识（这样，如果用户需要进行一些个性化操作，需要调整模型的知识，只需要对知识库进行编辑（例如增删改）即可）"></a>针对不方便进行个性化适配的问题，论文选择通过把用户相关个性知识存储到外部知识库，而不是让用户收集一批数据自行微调，来轻便地让模型学到用户相关的知识（这样，如果用户需要进行一些个性化操作，需要调整模型的知识，只需要对知识库进行编辑（例如增删改）即可）</h2><h2 id="话虽如此，即使把数据外挂到知识库，模型其实暂时还不会根据知识库检索到的知识，回答相应的内容。简单说，它还没有意识掉，从知识库里检索到的内容是用户相关的知识，所以此时它的回答可能还是一般化的回答。为此，论文收集了一批数据，并对模型进行lora微调，之后发现微调后的权重，泛化性比较好（因此微调这一步其实是不需要用户做的。事先由公司等对象提前做好了）"><a href="#话虽如此，即使把数据外挂到知识库，模型其实暂时还不会根据知识库检索到的知识，回答相应的内容。简单说，它还没有意识掉，从知识库里检索到的内容是用户相关的知识，所以此时它的回答可能还是一般化的回答。为此，论文收集了一批数据，并对模型进行lora微调，之后发现微调后的权重，泛化性比较好（因此微调这一步其实是不需要用户做的。事先由公司等对象提前做好了）" class="headerlink" title="话虽如此，即使把数据外挂到知识库，模型其实暂时还不会根据知识库检索到的知识，回答相应的内容。简单说，它还没有意识掉，从知识库里检索到的内容是用户相关的知识，所以此时它的回答可能还是一般化的回答。为此，论文收集了一批数据，并对模型进行lora微调，之后发现微调后的权重，泛化性比较好（因此微调这一步其实是不需要用户做的。事先由公司等对象提前做好了）"></a>话虽如此，即使把数据外挂到知识库，模型其实暂时还不会根据知识库检索到的知识，回答相应的内容。简单说，它还没有意识掉，从知识库里检索到的内容是用户相关的知识，所以此时它的回答可能还是一般化的回答。为此，论文收集了一批数据，并对模型进行lora微调，之后发现微调后的权重，泛化性比较好（因此微调这一步其实是不需要用户做的。事先由公司等对象提前做好了）</h2><h2 id="上面是大体的思路。说回到论文的RAP框架，大致是散步：记忆-检索-生成记忆阶段，支持对知识库的更改。其中知识库采用key-value的形式存储，key为图像的视觉特征，value为概念的文本属性；检索阶段，模型会根据query进行相应搜索：如果query里含有图像，则去检索key；如果query纯文字，则去检索value；（但这一步，把图像和文字分开处理了。如果用-VDocRAG：Retrieval-augmented-generation-over-visually-rich-documents-里面提到的方法，训练一个图像-文字双模态的embedding-model，那可能处理起来更方便）生成阶段，则是把检索到的内容，都投影到同一个空间（即语言token空间），然后拼到一起给模型让它生成（此处的模型用的是经过lora微调的。因此这里的模型它是学会了个性化回答的）"><a href="#上面是大体的思路。说回到论文的RAP框架，大致是散步：记忆-检索-生成记忆阶段，支持对知识库的更改。其中知识库采用key-value的形式存储，key为图像的视觉特征，value为概念的文本属性；检索阶段，模型会根据query进行相应搜索：如果query里含有图像，则去检索key；如果query纯文字，则去检索value；（但这一步，把图像和文字分开处理了。如果用-VDocRAG：Retrieval-augmented-generation-over-visually-rich-documents-里面提到的方法，训练一个图像-文字双模态的embedding-model，那可能处理起来更方便）生成阶段，则是把检索到的内容，都投影到同一个空间（即语言token空间），然后拼到一起给模型让它生成（此处的模型用的是经过lora微调的。因此这里的模型它是学会了个性化回答的）" class="headerlink" title="上面是大体的思路。说回到论文的RAP框架，大致是散步：记忆 - 检索 - 生成记忆阶段，支持对知识库的更改。其中知识库采用key-value的形式存储，key为图像的视觉特征，value为概念的文本属性；检索阶段，模型会根据query进行相应搜索：如果query里含有图像，则去检索key；如果query纯文字，则去检索value；（但这一步，把图像和文字分开处理了。如果用[[VDocRAG：Retrieval-augmented generation over visually-rich documents]]里面提到的方法，训练一个图像-文字双模态的embedding model，那可能处理起来更方便）生成阶段，则是把检索到的内容，都投影到同一个空间（即语言token空间），然后拼到一起给模型让它生成（此处的模型用的是经过lora微调的。因此这里的模型它是学会了个性化回答的）"></a>上面是大体的思路。说回到论文的RAP框架，大致是散步：记忆 - 检索 - 生成<br>记忆阶段，支持对知识库的更改。其中知识库采用key-value的形式存储，key为图像的视觉特征，value为概念的文本属性；<br>检索阶段，模型会根据query进行相应搜索：如果query里含有图像，则去检索key；如果query纯文字，则去检索value；（但这一步，把图像和文字分开处理了。如果用[[VDocRAG：Retrieval-augmented generation over visually-rich documents]]里面提到的方法，训练一个图像-文字双模态的embedding model，那可能处理起来更方便）<br>生成阶段，则是把检索到的内容，都投影到同一个空间（即语言token空间），然后拼到一起给模型让它生成（此处的模型用的是经过lora微调的。因此这里的模型它是学会了个性化回答的）</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/09/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/RAG-transfer/RAP%EF%BC%9ARetrieval-Augmented%20Personalization%20for%20Multimodal%20Large%20Language%20Models/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/RAG-transfer/VDocRAG%EF%BC%9ARetrieval-augmented%20generation%20over%20visually-rich%20documents/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/RAG-transfer/VDocRAG%EF%BC%9ARetrieval-augmented%20generation%20over%20visually-rich%20documents/" class="post-title-link" itemprop="url">VDocRAG：Retrieval-augmented generation over visually-rich documents</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-09-06 21:41:00 / Modified: 22:38:54" itemprop="dateCreated datePublished" datetime="2025-09-06T21:41:00+08:00">2025-09-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-RAG/" itemprop="url" rel="index"><span itemprop="name">multi-modal RAG</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="venuecvpr-25">Venue：CVPR 25</h1>
<h1 id="date2025">date：2025</h1>
<h1 id="基础背景">基础背景：</h1>
<h2 id="封闭域设定closed-setting是指模型仅需要基于单个给定的文档图像回答问题不需要从外部的文档库中检索相关信息对应的开放域设定open-domain-setting是指模型需要从外部文档库中检索相关文档">封闭域设定（closed
setting）是指，模型仅需要基于单个给定的文档图像回答问题，不需要从外部的文档库中检索相关信息。对应的，开放域设定（open
domain setting）是指模型需要从外部文档库中检索相关文档</h2>
<h1 id="动机">动机：</h1>
<h2 id="现有rag框架主要是针对纯文本不太能处理视觉丰富文档即含有很多图表表格之类的信息的文档这类视觉信息如果提取成文字会丢失信息且现有的rag方法不太能统一处理pdfpptx等格式不同的格式解析的逻辑差别很大容易解析错误">现有RAG框架主要是针对纯文本，不太能处理视觉丰富文档（即含有很多图表、表格之类的信息的文档）。这类视觉信息，如果提取成文字，会丢失信息；且现有的RAG方法不太能统一处理PDF、PPTX等格式（不同的格式解析的逻辑差别很大，容易解析错误）</h2>
<h1 id="简介">简介：</h1>
<h2 id="论文提出vdocrag框架实现了开放域内的qa且支持视觉丰富文档">论文提出VDocRAG框架，实现了开放域内的QA，且支持视觉丰富文档。</h2>
<h2 id="针对多种不同输入格式的视觉丰富文档论文采取的处理方法是全都处理成图像连文本带图像一起都切成固定尺寸的crops这样不管是哪个格式来了都可以直接处理成图像">针对多种不同输入格式的视觉丰富文档，论文采取的处理方法是，全都处理成图像（连文本带图像一起。都切成固定尺寸的crops）。这样不管是哪个格式来了，都可以直接处理成图像</h2>
<h2 id="之后的qa可能需要检索再生成而检索就需要把文档图像即上面切出来的crops的embedding以及query的embedding进行相似度计算那就需要把二者投影到同一个空间也就是说我们需要一个能把文本和图像投影到同一个embedding空间的模型现有的lvlms虽然确实可以同时处理图像和文本但它的向量是用于生成的和用于压缩表征信息的embedding是不太一样的所以论文提出了新的预训练的任务可以提高lvlms的embedding的表征能力最后就是用现有的lvlms的权重进行初始化然后用论文提出的任务进行训练得到一个文本-图像双模态的embedding-model它就被用来对文档图像和query进行编码实验也表明如果仅用原生的lvlms的权重得到的向量来检索效果是不如用它们继续训练后的模型提取得到的embedding向量检索的">之后的QA可能需要检索再生成，而检索就需要把文档图像（即上面切出来的crops）的embedding以及query的embedding进行相似度计算。那就需要把二者投影到同一个空间。也就是说，我们需要一个能把文本和图像投影到同一个embedding空间的模型。现有的LVLMs虽然确实可以同时处理图像和文本，但它的向量是用于生成的，和用于压缩表征信息的embedding是不太一样的。所以，论文提出了新的预训练的任务，可以提高LVLMs的embedding的表征能力（最后就是用现有的LVLMs的权重进行初始化，然后用论文提出的任务进行训练，得到一个文本-图像双模态的embedding
model。它就被用来对文档图像和query进行编码）（实验也表明，如果仅用原生的LVLMs的权重得到的向量来检索，效果是不如用它们继续训练后的模型提取得到的embedding向量检索的）</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/09/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/multi-modal-rag/RAG-transfer/VDocRAG%EF%BC%9ARetrieval-augmented%20generation%20over%20visually-rich%20documents/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/memory-management/Recursively%20Summarizing%20Enables%20Long-Term%20Dialogue%20%20Memory%20in%20Large%20Language%20Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/memory-management/Recursively%20Summarizing%20Enables%20Long-Term%20Dialogue%20%20Memory%20in%20Large%20Language%20Models/" class="post-title-link" itemprop="url">Recursively Summarizing Enables Long-Term Dialogue  Memory in Large Language Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-09-06 12:02:00 / Modified: 12:06:14" itemprop="dateCreated datePublished" datetime="2025-09-06T12:02:00+08:00">2025-09-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="venue">Venue：</h1>
<h1 id="date">date：</h1>
<h1 id="动机">动机：</h1>
<h1 id="insight">insight：</h1>
<h2 id="这篇论文对于记忆管理的核心创新是它是迭代地调用当前llm不断根据之前的记忆和当前的会话整理出新的记忆不需要别的专门的模型">这篇论文对于记忆管理的核心创新是，它是迭代地调用当前llm，不断根据之前的记忆和当前的会话，整理出新的记忆，不需要别的专门的模型</h2>
<h1 id="contribution">contribution：</h1>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/09/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/memory-management/Recursively%20Summarizing%20Enables%20Long-Term%20Dialogue%20%20Memory%20in%20Large%20Language%20Models/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blueeemouse.github.io/2025/09/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/memory-management/Crafting%20Personalized%20Agents%20through%20Retrieval-Augmented%20Generation%20on%20Editable%20Memory%20Graphs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="bluemouse">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bluemouse's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/memory-management/Crafting%20Personalized%20Agents%20through%20Retrieval-Augmented%20Generation%20on%20Editable%20Memory%20Graphs/" class="post-title-link" itemprop="url">Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-09-06 10:07:00" itemprop="dateCreated datePublished" datetime="2025-09-06T10:07:00+08:00">2025-09-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-09-07 17:25:42" itemprop="dateModified" datetime="2025-09-07T17:25:42+08:00">2025-09-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/" itemprop="url" rel="index"><span itemprop="name">llm</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/memory-management/" itemprop="url" rel="index"><span itemprop="name">memory management</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Venue：EMNLP-24"><a href="#Venue：EMNLP-24" class="headerlink" title="Venue：EMNLP 24"></a>Venue：EMNLP 24</h1><h1 id="date：2024-09-28"><a href="#date：2024-09-28" class="headerlink" title="date：2024-09-28"></a>date：2024-09-28</h1><h1 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h1><h2 id="论文主要研究，如何利用llm和用户手机上的记忆数据，打造个性化agent。主要的问题有三个：数据收集困难；没有一个特别适合编辑的记忆数据结构；检索记忆有困难（检索不够精准，而且有些场景需要组合多个记忆进行回答）"><a href="#论文主要研究，如何利用llm和用户手机上的记忆数据，打造个性化agent。主要的问题有三个：数据收集困难；没有一个特别适合编辑的记忆数据结构；检索记忆有困难（检索不够精准，而且有些场景需要组合多个记忆进行回答）" class="headerlink" title="论文主要研究，如何利用llm和用户手机上的记忆数据，打造个性化agent。主要的问题有三个：数据收集困难；没有一个特别适合编辑的记忆数据结构；检索记忆有困难（检索不够精准，而且有些场景需要组合多个记忆进行回答）"></a>论文主要研究，如何利用llm和用户手机上的记忆数据，打造个性化agent。主要的问题有三个：<br>数据收集困难；没有一个特别适合编辑的记忆数据结构；检索记忆有困难（检索不够精准，而且有些场景需要组合多个记忆进行回答）</h2><h1 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h1><h2 id="针对第一个问题，论文提出，从真实的ai助手产品上收集数据，并利用llm生成问答对，用于训练"><a href="#针对第一个问题，论文提出，从真实的ai助手产品上收集数据，并利用llm生成问答对，用于训练" class="headerlink" title="针对第一个问题，论文提出，从真实的ai助手产品上收集数据，并利用llm生成问答对，用于训练"></a>针对第一个问题，论文提出，从真实的ai助手产品上收集数据，并利用llm生成问答对，用于训练</h2><h2 id="针对第二个问题（其实也是同时考虑了第三个问题），论文设计了三层结构的可编辑记忆图（EMG）。一来，图结构可以相对方便地支持编辑（插入、删除和替换）；二来，用图构建记忆的话，检索记忆就可以变成在graph上遍历、游走。加上论文引入了RL，用RL来指导在graph上的游走。因此，相当于检索回来的片段不局限于top-k，而是更加灵活，按需检索（有点类似自适应的决定k）"><a href="#针对第二个问题（其实也是同时考虑了第三个问题），论文设计了三层结构的可编辑记忆图（EMG）。一来，图结构可以相对方便地支持编辑（插入、删除和替换）；二来，用图构建记忆的话，检索记忆就可以变成在graph上遍历、游走。加上论文引入了RL，用RL来指导在graph上的游走。因此，相当于检索回来的片段不局限于top-k，而是更加灵活，按需检索（有点类似自适应的决定k）" class="headerlink" title="针对第二个问题（其实也是同时考虑了第三个问题），论文设计了三层结构的可编辑记忆图（EMG）。一来，图结构可以相对方便地支持编辑（插入、删除和替换）；二来，用图构建记忆的话，检索记忆就可以变成在graph上遍历、游走。加上论文引入了RL，用RL来指导在graph上的游走。因此，相当于检索回来的片段不局限于top-k，而是更加灵活，按需检索（有点类似自适应的决定k）"></a>针对第二个问题（其实也是同时考虑了第三个问题），论文设计了三层结构的可编辑记忆图（EMG）。一来，图结构可以相对方便地支持编辑（插入、删除和替换）；二来，用图构建记忆的话，检索记忆就可以变成在graph上遍历、游走。加上论文引入了RL，用RL来指导在graph上的游走。因此，相当于检索回来的片段不局限于top-k，而是更加灵活，按需检索（有点类似自适应的决定k）</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/09/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/nlp/llm/RAG/memory-management/Crafting%20Personalized%20Agents%20through%20Retrieval-Augmented%20Generation%20on%20Editable%20Memory%20Graphs/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">bluemouse</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">217</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">67</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">103</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bluemouse</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
